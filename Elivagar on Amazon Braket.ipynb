{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46dca12a",
   "metadata": {},
   "source": [
    "# Élivágar: Efficient Quantum Circuit Search for Classification\n",
    "\n",
    "In this notebook, we will explore using Élivágar, an efficient method for performing Quantum Circuit Search (QCS) for classification tasks on quantum computers. We will apply Élivágar to find performant, noise-robust circuits that can be used to classify MNIST digits on the OQC Lucy quantum computer, which is accessible via Amazon Braket.\n",
    "\n",
    "## About Élivágar\n",
    "Élivágar is a QCS framework that differs from prior QCS works such as QuantumNAS by using training- and gradient-free strategies to select suitable circuits for classification tasks. Since gradient computation is expensive on quantum computers due to the high cost of methods such as the parameter shift rule, QuantumNAS and other methods which use SuperCircuit-inspired setups requiring training incur high runtime costs. In contrast, Élivágar is much faster due to using gradient-free performance predictors to evaluate candidate circuits. \n",
    "\n",
    "Furthermore, Élivágar also incorporates device-aware circuit generation, which eliminates the need to perform a costly circuit-mapping co-search to find good qubit mappings for candidate circuits onto the target device. Élivágar instead generates high-quality qubit mappings before generating candidate circuits, and chooses operations that obey the connectivity constraints of the select mappings. Finally, Élivágar also searches over data embeddings, which is not supported in other QCS frameworks. This leads to significant performance gains on various classification tasks, since, as shown by multiple works in the Quantum Machine Learning (QML) literature, the embedding used with a circuit plays an extremely important role in determining circuit performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319909d5",
   "metadata": {},
   "source": [
    "## Setting up Amazon Braket on your local development environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd044960",
   "metadata": {},
   "source": [
    "Before you can follow along with the steps performed in this notebook, you will need to have your local development environment set up in Amazon Braket. If you have not done so already, you can follow the guide at [https://aws.amazon.com/blogs/quantum-computing/setting-up-your-local-development-environment-in-amazon-braket/](https://aws.amazon.com/blogs/quantum-computing/setting-up-your-local-development-environment-in-amazon-braket/) to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a84f216",
   "metadata": {},
   "source": [
    "## Required imports\n",
    "\n",
    "First, we will import all of the packages and modules that we require for the demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c04bbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from braket.aws import AwsDevice\n",
    "\n",
    "from elivagar.circuits.device_aware import (\n",
    "    extract_properties_from_braket_device,\n",
    "    generate_device_aware_gate_circ\n",
    ")\n",
    "\n",
    "from elivagar.metric_computation.compute_clifford_nr import (\n",
    "    compute_clifford_nr_for_circuits\n",
    ")\n",
    "\n",
    "from elivagar.metric_computation.compute_rep_cap import (\n",
    "    compute_rep_cap_for_circuits\n",
    ")\n",
    "\n",
    "from elivagar.training.train_circuits_from_predictors import (\n",
    "    train_elivagar_circuits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "490e82a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'elivagar.circuits.device_aware' from 'D:\\\\Quantum_Computing\\\\Elivagar\\\\Elivagar\\\\elivagar\\\\circuits\\\\device_aware.py'>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import elivagar.circuits.create_circuit\n",
    "import elivagar.metric_computation.compute_clifford_nr\n",
    "import elivagar.metric_computation.compute_rep_cap\n",
    "import elivagar.circuits.device_aware\n",
    "\n",
    "reload(elivagar.circuits.create_circuit)\n",
    "reload(elivagar.metric_computation.compute_clifford_nr)\n",
    "reload(elivagar.metric_computation.compute_rep_cap)\n",
    "reload(elivagar.circuits.device_aware)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f82ee2",
   "metadata": {},
   "source": [
    "## Get target device information\n",
    "\n",
    "Before we can begin the process of generating candidate circuits for the MNIST task, we must first obtain the properties of the target device, in this case OQC Lucy. We require this because Elivagar generates candidate circuits in a device-aware way, using the differences in edge and readout fidelities as well as qubit coherence times to determine the best circuit structures to generate, and the best logical-to-physical qubit mappings for each generated candidate circuit. \n",
    "\n",
    "We will start by getting the device details from OQC Lucy using the Braket SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c699e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oqc_lucy_dev = AwsDevice(\"arn:aws:braket:eu-west-2::device/qpu/oqc/Lucy\")\n",
    "ionq_harmony_dev = AwsDevice(\"arn:aws:braket:us-east-1::device/qpu/ionq/Harmony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6f8f1c",
   "metadata": {},
   "source": [
    "## Generate candidate circuits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e642fa64",
   "metadata": {},
   "source": [
    "In order to generate candidate circuits for a dataset, we first need to obtain some basic information about the dataset. IN our case, the dataset is MNIST-2, a subset of the MNIST handwritten digit dataset consisting of 2 classes, 0 and 1. While we can apply Élivágar to larger datasets, such as the whole MNIST dataset, as well, for the purposes of this tutorial, we will only use a smaller subset to keep things short.\n",
    "\n",
    "We will search for a circuit with 4 qubits and 32 parameters. Since we will be using a preprocessed version of the MNIST dataset, the dimensionality of each sample is only 16, rather than the 784 in the original dataset. Additionally, since there are only two classes, we only need to measure 1 qubit - we can simply train the model to perform binary classification, i.e. output expectation values of either -1 or 1, depending on which class the input belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1bd655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_qubits = 4\n",
    "num_meas_qubits = 1\n",
    "num_params = 32\n",
    "num_embeds = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e46b4f",
   "metadata": {},
   "source": [
    "We also need to specify some options related to the circuit generation process:\n",
    "* num_circs: the number of candidate circuits to generate.\n",
    "* add_rotations: whether to add RX, RY, and RZ single qubit gates to the gateset of the circuits beign generated, or use only device-native gates. Adding rotation gates often improves performance due to greater circuit expressivity, and does not affect the qubit mappings since they are all single qubit gates.\n",
    "* param_focus: a number that specifies how important it is to generate gates that contain variational parameters versus non-paametrized gates. Increasing this will make the generated circuits contain fewer gates and result in reduced circuit depth, although it may reduce circuit expressivity.\n",
    "* num_trial_mappings: the number of mappings to generate before beginning the candidate generation process. Increasing this number will lead to the selection of more high-quality mappings, but will also increase circuit generation time.\n",
    "* temp: the temperate for various softmax distributions used by the circuit generation process. Increasing this results in higher circuit diversity, as different gates, gate placements, and qubit mappings are chosen with more uniform probabilities.\n",
    "\n",
    "For now, we will use sensible defaults for each of these values, although you can try out different values for each to see how they affect downstream performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03778d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_circs = 250\n",
    "add_rotations = True\n",
    "param_focus = 2.0\n",
    "num_trial_mappings = 200\n",
    "temp = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0feaf",
   "metadata": {},
   "source": [
    "Now, we can generate the candidate circuits. For each circuit, we randomly sample a ent_prob value, which determines what the proportion of entangling (2-qubit) gates in the circuit will be. Higher values can lead to better performance, but also lower circuit fidelity. We will discuss this effect of this in more detail in the next section, which focuses on estimating the noise robustness of generated candidate circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a81510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_save_folder = './candidate_circuits'\n",
    "\n",
    "if not os.path.exists(circuits_save_folder):\n",
    "    os.mkdir(circuits_save_folder)\n",
    "\n",
    "for i in range(num_circs):\n",
    "    curr_circ_folder = os.path.join(circuits_save_folder, f'circ_{i + 1}')\n",
    "        \n",
    "    if not os.path.exists(curr_circ_folder):\n",
    "        os.makedirs(curr_circ_folder)\n",
    "            \n",
    "    ent_prob = 0.3 + 0.5 * np.random.sample()\n",
    "        \n",
    "    (\n",
    "        circ_gates, gate_placements, inputs_bounds, weights_bounds,\n",
    "        selected_mapping, meas_qubits\n",
    "    ) = generate_device_aware_gate_circ(\n",
    "        None, num_qubits, num_embeds, num_params,\n",
    "        ent_prob, add_rotations, param_focus,\n",
    "        num_meas_qubits, num_trial_mappings,\n",
    "        temp, braket_device_properties=ionq_harmony_dev.properties,\n",
    "        braket_device_name='ionq_harmony'\n",
    "    )\n",
    "\n",
    "    np.savetxt(\n",
    "        os.path.join(curr_circ_folder, 'gate_params.txt'),\n",
    "        np.array(gate_placements, dtype=object), fmt=\"%s\"\n",
    "    )\n",
    "\n",
    "    np.savetxt(os.path.join(curr_circ_folder, 'gates.txt'), circ_gates, fmt=\"%s\")\n",
    "    np.savetxt(os.path.join(curr_circ_folder, 'inputs_bounds.txt'), inputs_bounds)\n",
    "    np.savetxt(os.path.join(curr_circ_folder, 'weights_bounds.txt'), weights_bounds)\n",
    "    np.savetxt(os.path.join(curr_circ_folder, 'qubit_mapping.txt'), selected_mapping)\n",
    "    np.savetxt(os.path.join(curr_circ_folder, 'meas_qubits.txt'), meas_qubits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42a166",
   "metadata": {},
   "source": [
    "Notice that when a circuit is generated, we not only receive information about the circuit, such as the gates and gate placements, but also the qubit mapping, and even which qubits we should measure at the end of the circuit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5438d1f4",
   "metadata": {},
   "source": [
    "## Compute Clifford noise resilience for candidate circuits\n",
    "After we finish generating candidate circuits, we want to rank them in some way that will enable us to choose the best circuit for the classification task we have in mind. The first step Élivágar uses to do this is to estimate how noise-robust each circuit is, which quantifies the degree to which circuit outputs remain unaffected by hardware errors occuring during circuit execution. This is a crucial component of predicting circuit performance on noisy hardware, since while a circuit might perform well on a noiseless somulator, it might be execssively deep, or contain many low-fidelity two-qubit gates, reducing performance when deployed on error-prone quantum hardware.\n",
    "\n",
    "Note: this step involves running circuits on the OQC Lucy device on the cloud, and the total time it takes will be subject to queueing delays, making it difficult to predict an expected runtime. Computing CNR with 32 Clifford replicas for 250 circuits involves running 8000 circuits on the device, which is likely to take at least a few hours. You can reduce this time by changing the number of candidate circuits to consider to a smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed905689",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_save_folder = './candidate_circuits'\n",
    "num_circs = 250\n",
    "device_name = 'ionq_harmony'\n",
    "num_qubits = 4\n",
    "num_clifford_replicas = 32\n",
    "num_shots = 2048\n",
    "compute_actual_fidelity = False\n",
    "num_trial_parameters = 128\n",
    "dataset = None\n",
    "encoding_type = 'angle'\n",
    "num_data_reps = 1\n",
    "use_qubit_mapping = True\n",
    "save_cnr_scores = True\n",
    "circ_index_offset = 0\n",
    "dataset_file_extension = 'txt'\n",
    "use_real_backend = False\n",
    "use_qiskit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ff12960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9726715087890625\n",
      "1 0.9785003662109375\n",
      "2 0.97552490234375\n",
      "3 0.97454833984375\n",
      "4 0.9777984619140625\n",
      "5 0.967498779296875\n",
      "6 0.977020263671875\n",
      "7 0.9721221923828125\n",
      "8 0.9700469970703125\n",
      "9 0.976531982421875\n",
      "10 0.9752655029296875\n",
      "11 0.9711456298828125\n",
      "12 0.9761199951171875\n",
      "13 0.9782867431640625\n",
      "14 0.98236083984375\n",
      "15 0.9711151123046875\n",
      "16 0.970458984375\n",
      "17 0.96502685546875\n",
      "18 0.9728851318359375\n",
      "19 0.9725189208984375\n",
      "20 0.9759368896484375\n",
      "21 0.9751739501953125\n",
      "22 0.9730377197265625\n",
      "23 0.9744415283203125\n",
      "24 0.9744873046875\n",
      "25 0.97259521484375\n",
      "26 0.97119140625\n",
      "27 0.9764404296875\n",
      "28 0.9725189208984375\n",
      "29 0.96954345703125\n",
      "30 0.9738311767578125\n",
      "31 0.9753265380859375\n",
      "32 0.9742431640625\n",
      "33 0.973419189453125\n",
      "34 0.9751129150390625\n",
      "35 0.9803314208984375\n",
      "36 0.9700164794921875\n",
      "37 0.968292236328125\n",
      "38 0.9719696044921875\n",
      "39 0.9793701171875\n",
      "40 0.97088623046875\n",
      "41 0.975738525390625\n",
      "42 0.9731903076171875\n",
      "43 0.9727020263671875\n",
      "44 0.973541259765625\n",
      "45 0.9696197509765625\n",
      "46 0.98016357421875\n",
      "47 0.9810943603515625\n",
      "48 0.970733642578125\n",
      "49 0.975067138671875\n",
      "50 0.969970703125\n",
      "51 0.96795654296875\n",
      "52 0.97064208984375\n",
      "53 0.977264404296875\n",
      "54 0.975250244140625\n",
      "55 0.9704132080078125\n",
      "56 0.97412109375\n",
      "57 0.9683990478515625\n",
      "58 0.980438232421875\n",
      "59 0.9736328125\n",
      "60 0.976318359375\n",
      "61 0.9708251953125\n",
      "62 0.969482421875\n",
      "63 0.9822235107421875\n",
      "64 0.9762115478515625\n",
      "65 0.9736785888671875\n",
      "66 0.973052978515625\n",
      "67 0.9725799560546875\n",
      "68 0.9727325439453125\n",
      "69 0.97412109375\n",
      "70 0.969512939453125\n",
      "71 0.970947265625\n",
      "72 0.9759063720703125\n",
      "73 0.9764862060546875\n",
      "74 0.97125244140625\n",
      "75 0.97393798828125\n",
      "76 0.9785308837890625\n",
      "77 0.971832275390625\n",
      "78 0.973388671875\n",
      "79 0.9754638671875\n",
      "80 0.9716644287109375\n",
      "81 0.9696197509765625\n",
      "82 0.9683685302734375\n",
      "83 0.978912353515625\n",
      "84 0.979705810546875\n",
      "85 0.9739227294921875\n",
      "86 0.9715423583984375\n",
      "87 0.970977783203125\n",
      "88 0.9705352783203125\n",
      "89 0.97515869140625\n",
      "90 0.9758453369140625\n",
      "91 0.9742889404296875\n",
      "92 0.9810791015625\n",
      "93 0.97479248046875\n",
      "94 0.975830078125\n",
      "95 0.973724365234375\n",
      "96 0.9755401611328125\n",
      "97 0.9682159423828125\n",
      "98 0.97564697265625\n",
      "99 0.9782867431640625\n",
      "100 0.9771881103515625\n",
      "101 0.9743804931640625\n",
      "102 0.9780120849609375\n",
      "103 0.9735565185546875\n",
      "104 0.975799560546875\n",
      "105 0.9769744873046875\n",
      "106 0.975341796875\n",
      "107 0.9722747802734375\n",
      "108 0.98492431640625\n",
      "109 0.970123291015625\n",
      "110 0.9772796630859375\n",
      "111 0.971038818359375\n",
      "112 0.973358154296875\n",
      "113 0.96783447265625\n",
      "114 0.973876953125\n",
      "115 0.978363037109375\n",
      "116 0.9727325439453125\n",
      "117 0.976318359375\n",
      "118 0.977752685546875\n",
      "119 0.970062255859375\n",
      "120 0.9744873046875\n",
      "121 0.979583740234375\n",
      "122 0.9718780517578125\n",
      "123 0.9722442626953125\n",
      "124 0.970001220703125\n",
      "125 0.9732818603515625\n",
      "126 0.9810333251953125\n",
      "127 0.9700469970703125\n",
      "128 0.973602294921875\n",
      "129 0.9721221923828125\n",
      "130 0.9782257080078125\n",
      "131 0.96966552734375\n",
      "132 0.9679107666015625\n",
      "133 0.9732208251953125\n",
      "134 0.9729156494140625\n",
      "135 0.975830078125\n",
      "136 0.9775390625\n",
      "137 0.9747467041015625\n",
      "138 0.9753265380859375\n",
      "139 0.9766845703125\n",
      "140 0.9796295166015625\n",
      "141 0.9708099365234375\n",
      "142 0.980682373046875\n",
      "143 0.96630859375\n",
      "144 0.9760894775390625\n",
      "145 0.973358154296875\n",
      "146 0.9766693115234375\n",
      "147 0.9678955078125\n",
      "148 0.9673919677734375\n",
      "149 0.97381591796875\n",
      "150 0.9737701416015625\n",
      "151 0.97552490234375\n",
      "152 0.979248046875\n",
      "153 0.9738922119140625\n",
      "154 0.973175048828125\n",
      "155 0.976470947265625\n",
      "156 0.9755401611328125\n",
      "157 0.9718475341796875\n",
      "158 0.968780517578125\n",
      "159 0.9749755859375\n",
      "160 0.983489990234375\n",
      "161 0.9811553955078125\n",
      "162 0.969635009765625\n",
      "163 0.977294921875\n",
      "164 0.9736480712890625\n",
      "165 0.97491455078125\n",
      "166 0.96453857421875\n",
      "167 0.9694061279296875\n",
      "168 0.9738922119140625\n",
      "169 0.976226806640625\n",
      "170 0.977813720703125\n",
      "171 0.97296142578125\n",
      "172 0.9730987548828125\n",
      "173 0.971954345703125\n",
      "174 0.9693145751953125\n",
      "175 0.9738616943359375\n",
      "176 0.9676513671875\n",
      "177 0.9754638671875\n",
      "178 0.971343994140625\n",
      "179 0.9696502685546875\n",
      "180 0.977447509765625\n",
      "181 0.9806671142578125\n",
      "182 0.9781036376953125\n",
      "183 0.9774932861328125\n",
      "184 0.975189208984375\n",
      "185 0.9768218994140625\n",
      "186 0.9669952392578125\n",
      "187 0.970123291015625\n",
      "188 0.9733428955078125\n",
      "189 0.97735595703125\n",
      "190 0.9740447998046875\n",
      "191 0.97271728515625\n",
      "192 0.9803619384765625\n",
      "193 0.9747467041015625\n",
      "194 0.97528076171875\n",
      "195 0.9754486083984375\n",
      "196 0.97723388671875\n",
      "197 0.9742889404296875\n",
      "198 0.9763641357421875\n",
      "199 0.9741058349609375\n",
      "200 0.97259521484375\n",
      "201 0.978057861328125\n",
      "202 0.973846435546875\n",
      "203 0.9781646728515625\n",
      "204 0.9766082763671875\n",
      "205 0.9705810546875\n",
      "206 0.9722137451171875\n",
      "207 0.9668426513671875\n",
      "208 0.9730987548828125\n",
      "209 0.9673614501953125\n",
      "210 0.974151611328125\n",
      "211 0.9761962890625\n",
      "212 0.979736328125\n",
      "213 0.973236083984375\n",
      "214 0.9716644287109375\n",
      "215 0.9780426025390625\n",
      "216 0.971527099609375\n",
      "217 0.9738006591796875\n",
      "218 0.9760589599609375\n",
      "219 0.972930908203125\n",
      "220 0.9713897705078125\n",
      "221 0.9756622314453125\n",
      "222 0.976898193359375\n",
      "223 0.9715576171875\n",
      "224 0.9716949462890625\n",
      "225 0.97869873046875\n",
      "226 0.976226806640625\n",
      "227 0.9691314697265625\n",
      "228 0.97064208984375\n",
      "229 0.974395751953125\n",
      "230 0.9734649658203125\n",
      "231 0.9703369140625\n",
      "232 0.974456787109375\n",
      "233 0.9792327880859375\n",
      "234 0.9772491455078125\n",
      "235 0.977874755859375\n",
      "236 0.9714508056640625\n",
      "237 0.97601318359375\n",
      "238 0.9750518798828125\n",
      "239 0.97430419921875\n",
      "240 0.9813690185546875\n",
      "241 0.9748382568359375\n",
      "242 0.9746551513671875\n",
      "243 0.9732818603515625\n",
      "244 0.9738922119140625\n",
      "245 0.9746551513671875\n",
      "246 0.9712677001953125\n",
      "247 0.9742584228515625\n",
      "248 0.9734039306640625\n",
      "249 0.97314453125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9726715087890625,\n",
       " 0.9785003662109375,\n",
       " 0.97552490234375,\n",
       " 0.97454833984375,\n",
       " 0.9777984619140625,\n",
       " 0.967498779296875,\n",
       " 0.977020263671875,\n",
       " 0.9721221923828125,\n",
       " 0.9700469970703125,\n",
       " 0.976531982421875,\n",
       " 0.9752655029296875,\n",
       " 0.9711456298828125,\n",
       " 0.9761199951171875,\n",
       " 0.9782867431640625,\n",
       " 0.98236083984375,\n",
       " 0.9711151123046875,\n",
       " 0.970458984375,\n",
       " 0.96502685546875,\n",
       " 0.9728851318359375,\n",
       " 0.9725189208984375,\n",
       " 0.9759368896484375,\n",
       " 0.9751739501953125,\n",
       " 0.9730377197265625,\n",
       " 0.9744415283203125,\n",
       " 0.9744873046875,\n",
       " 0.97259521484375,\n",
       " 0.97119140625,\n",
       " 0.9764404296875,\n",
       " 0.9725189208984375,\n",
       " 0.96954345703125,\n",
       " 0.9738311767578125,\n",
       " 0.9753265380859375,\n",
       " 0.9742431640625,\n",
       " 0.973419189453125,\n",
       " 0.9751129150390625,\n",
       " 0.9803314208984375,\n",
       " 0.9700164794921875,\n",
       " 0.968292236328125,\n",
       " 0.9719696044921875,\n",
       " 0.9793701171875,\n",
       " 0.97088623046875,\n",
       " 0.975738525390625,\n",
       " 0.9731903076171875,\n",
       " 0.9727020263671875,\n",
       " 0.973541259765625,\n",
       " 0.9696197509765625,\n",
       " 0.98016357421875,\n",
       " 0.9810943603515625,\n",
       " 0.970733642578125,\n",
       " 0.975067138671875,\n",
       " 0.969970703125,\n",
       " 0.96795654296875,\n",
       " 0.97064208984375,\n",
       " 0.977264404296875,\n",
       " 0.975250244140625,\n",
       " 0.9704132080078125,\n",
       " 0.97412109375,\n",
       " 0.9683990478515625,\n",
       " 0.980438232421875,\n",
       " 0.9736328125,\n",
       " 0.976318359375,\n",
       " 0.9708251953125,\n",
       " 0.969482421875,\n",
       " 0.9822235107421875,\n",
       " 0.9762115478515625,\n",
       " 0.9736785888671875,\n",
       " 0.973052978515625,\n",
       " 0.9725799560546875,\n",
       " 0.9727325439453125,\n",
       " 0.97412109375,\n",
       " 0.969512939453125,\n",
       " 0.970947265625,\n",
       " 0.9759063720703125,\n",
       " 0.9764862060546875,\n",
       " 0.97125244140625,\n",
       " 0.97393798828125,\n",
       " 0.9785308837890625,\n",
       " 0.971832275390625,\n",
       " 0.973388671875,\n",
       " 0.9754638671875,\n",
       " 0.9716644287109375,\n",
       " 0.9696197509765625,\n",
       " 0.9683685302734375,\n",
       " 0.978912353515625,\n",
       " 0.979705810546875,\n",
       " 0.9739227294921875,\n",
       " 0.9715423583984375,\n",
       " 0.970977783203125,\n",
       " 0.9705352783203125,\n",
       " 0.97515869140625,\n",
       " 0.9758453369140625,\n",
       " 0.9742889404296875,\n",
       " 0.9810791015625,\n",
       " 0.97479248046875,\n",
       " 0.975830078125,\n",
       " 0.973724365234375,\n",
       " 0.9755401611328125,\n",
       " 0.9682159423828125,\n",
       " 0.97564697265625,\n",
       " 0.9782867431640625,\n",
       " 0.9771881103515625,\n",
       " 0.9743804931640625,\n",
       " 0.9780120849609375,\n",
       " 0.9735565185546875,\n",
       " 0.975799560546875,\n",
       " 0.9769744873046875,\n",
       " 0.975341796875,\n",
       " 0.9722747802734375,\n",
       " 0.98492431640625,\n",
       " 0.970123291015625,\n",
       " 0.9772796630859375,\n",
       " 0.971038818359375,\n",
       " 0.973358154296875,\n",
       " 0.96783447265625,\n",
       " 0.973876953125,\n",
       " 0.978363037109375,\n",
       " 0.9727325439453125,\n",
       " 0.976318359375,\n",
       " 0.977752685546875,\n",
       " 0.970062255859375,\n",
       " 0.9744873046875,\n",
       " 0.979583740234375,\n",
       " 0.9718780517578125,\n",
       " 0.9722442626953125,\n",
       " 0.970001220703125,\n",
       " 0.9732818603515625,\n",
       " 0.9810333251953125,\n",
       " 0.9700469970703125,\n",
       " 0.973602294921875,\n",
       " 0.9721221923828125,\n",
       " 0.9782257080078125,\n",
       " 0.96966552734375,\n",
       " 0.9679107666015625,\n",
       " 0.9732208251953125,\n",
       " 0.9729156494140625,\n",
       " 0.975830078125,\n",
       " 0.9775390625,\n",
       " 0.9747467041015625,\n",
       " 0.9753265380859375,\n",
       " 0.9766845703125,\n",
       " 0.9796295166015625,\n",
       " 0.9708099365234375,\n",
       " 0.980682373046875,\n",
       " 0.96630859375,\n",
       " 0.9760894775390625,\n",
       " 0.973358154296875,\n",
       " 0.9766693115234375,\n",
       " 0.9678955078125,\n",
       " 0.9673919677734375,\n",
       " 0.97381591796875,\n",
       " 0.9737701416015625,\n",
       " 0.97552490234375,\n",
       " 0.979248046875,\n",
       " 0.9738922119140625,\n",
       " 0.973175048828125,\n",
       " 0.976470947265625,\n",
       " 0.9755401611328125,\n",
       " 0.9718475341796875,\n",
       " 0.968780517578125,\n",
       " 0.9749755859375,\n",
       " 0.983489990234375,\n",
       " 0.9811553955078125,\n",
       " 0.969635009765625,\n",
       " 0.977294921875,\n",
       " 0.9736480712890625,\n",
       " 0.97491455078125,\n",
       " 0.96453857421875,\n",
       " 0.9694061279296875,\n",
       " 0.9738922119140625,\n",
       " 0.976226806640625,\n",
       " 0.977813720703125,\n",
       " 0.97296142578125,\n",
       " 0.9730987548828125,\n",
       " 0.971954345703125,\n",
       " 0.9693145751953125,\n",
       " 0.9738616943359375,\n",
       " 0.9676513671875,\n",
       " 0.9754638671875,\n",
       " 0.971343994140625,\n",
       " 0.9696502685546875,\n",
       " 0.977447509765625,\n",
       " 0.9806671142578125,\n",
       " 0.9781036376953125,\n",
       " 0.9774932861328125,\n",
       " 0.975189208984375,\n",
       " 0.9768218994140625,\n",
       " 0.9669952392578125,\n",
       " 0.970123291015625,\n",
       " 0.9733428955078125,\n",
       " 0.97735595703125,\n",
       " 0.9740447998046875,\n",
       " 0.97271728515625,\n",
       " 0.9803619384765625,\n",
       " 0.9747467041015625,\n",
       " 0.97528076171875,\n",
       " 0.9754486083984375,\n",
       " 0.97723388671875,\n",
       " 0.9742889404296875,\n",
       " 0.9763641357421875,\n",
       " 0.9741058349609375,\n",
       " 0.97259521484375,\n",
       " 0.978057861328125,\n",
       " 0.973846435546875,\n",
       " 0.9781646728515625,\n",
       " 0.9766082763671875,\n",
       " 0.9705810546875,\n",
       " 0.9722137451171875,\n",
       " 0.9668426513671875,\n",
       " 0.9730987548828125,\n",
       " 0.9673614501953125,\n",
       " 0.974151611328125,\n",
       " 0.9761962890625,\n",
       " 0.979736328125,\n",
       " 0.973236083984375,\n",
       " 0.9716644287109375,\n",
       " 0.9780426025390625,\n",
       " 0.971527099609375,\n",
       " 0.9738006591796875,\n",
       " 0.9760589599609375,\n",
       " 0.972930908203125,\n",
       " 0.9713897705078125,\n",
       " 0.9756622314453125,\n",
       " 0.976898193359375,\n",
       " 0.9715576171875,\n",
       " 0.9716949462890625,\n",
       " 0.97869873046875,\n",
       " 0.976226806640625,\n",
       " 0.9691314697265625,\n",
       " 0.97064208984375,\n",
       " 0.974395751953125,\n",
       " 0.9734649658203125,\n",
       " 0.9703369140625,\n",
       " 0.974456787109375,\n",
       " 0.9792327880859375,\n",
       " 0.9772491455078125,\n",
       " 0.977874755859375,\n",
       " 0.9714508056640625,\n",
       " 0.97601318359375,\n",
       " 0.9750518798828125,\n",
       " 0.97430419921875,\n",
       " 0.9813690185546875,\n",
       " 0.9748382568359375,\n",
       " 0.9746551513671875,\n",
       " 0.9732818603515625,\n",
       " 0.9738922119140625,\n",
       " 0.9746551513671875,\n",
       " 0.9712677001953125,\n",
       " 0.9742584228515625,\n",
       " 0.9734039306640625,\n",
       " 0.97314453125]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_clifford_nr_for_circuits(\n",
    "    circuits_save_dire_folder, num_circs, device_name, num_qubits,\n",
    "    num_clifford_replicas, num_shots, compute_actual_fidelity, num_trial_parameters,\n",
    "    dataset, encoding_type, num_data_reps,\n",
    "    None, use_qiskit, None, None, use_qubit_mapping, save_cnr_scores,  \n",
    "    None, circ_index_offset, dataset_file_extension,\n",
    "    use_real_backend\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e3c94",
   "metadata": {},
   "source": [
    "## Compute representational capacity for candidate circuits\n",
    "Once we have computed CNR for each of the candidate circuits, we can move to predicting the performance of each circuit on the target dataset. Élivágar does this using Representational Capacity (RepCap), a gradient-free metric that enables accurate estimation of circuit performance. To compute RepCap for a circuit, we need to define a few hyperparameters, listed below:\n",
    "* num_param_samples: the number of randomly sampled parameter sets to use with the circuit while computing RepCap. More samples will lead to more accurate predictions, but experiments have shown that even as few as 32 parameter sets can be enough.\n",
    "* num_samples_per_class: the number of samples to choose from each class in the target dataset for using to compute RepCap. Choosing more samples leads to more accurate predictions, but more than 16 samples per class are not required for simple datasets such as MNIST-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16e40c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_save_dir = './candidate_circuits'\n",
    "num_circs = 250\n",
    "circ_prefix = 'circ'\n",
    "num_qubits = 4\n",
    "num_meas_qubits = 1\n",
    "dataset = 'mnist_2'\n",
    "num_classes = 2\n",
    "num_samples_per_class = 16\n",
    "num_param_samples = 32\n",
    "encoding_type = 'angle'\n",
    "num_data_reps = 1\n",
    "save_matrices = False\n",
    "dataset_file_extension = 'txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7219d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.]\n",
      " [ 1.]] (1600, 1)\n",
      "0.8209934234619141\n",
      "0.8502101898193359\n",
      "0.6621513366699219\n",
      "0.8642520904541016\n",
      "0.9496726989746094\n",
      "0.7711677551269531\n",
      "0.8501815795898438\n",
      "0.9283313751220703\n",
      "0.9201908111572266\n",
      "0.7827053070068359\n",
      "0.8208694458007812\n",
      "0.7746543884277344\n",
      "0.846588134765625\n",
      "0.8055324554443359\n",
      "0.8881587982177734\n",
      "0.8429241180419922\n",
      "0.9579677581787109\n",
      "0.8012771606445312\n",
      "0.8541412353515625\n",
      "0.7585334777832031\n",
      "0.8044300079345703\n",
      "0.8340282440185547\n",
      "0.8056850433349609\n",
      "0.8393459320068359\n",
      "0.7680873870849609\n",
      "0.8564529418945312\n",
      "0.8788700103759766\n",
      "0.7046527862548828\n",
      "0.7756462097167969\n",
      "0.6630420684814453\n",
      "0.8422775268554688\n",
      "0.7719173431396484\n",
      "0.6940860748291016\n",
      "0.9090404510498047\n",
      "0.7060508728027344\n",
      "0.757720947265625\n",
      "0.8512344360351562\n",
      "0.8611316680908203\n",
      "0.7894706726074219\n",
      "0.6631050109863281\n",
      "0.8568859100341797\n",
      "0.8999347686767578\n",
      "0.5892105102539062\n",
      "0.8155269622802734\n",
      "0.8405666351318359\n",
      "0.8009395599365234\n",
      "0.8895053863525391\n",
      "0.9194889068603516\n",
      "0.7470378875732422\n",
      "0.8480644226074219\n",
      "0.9103069305419922\n",
      "0.7726516723632812\n",
      "0.8912334442138672\n",
      "0.6809444427490234\n",
      "0.8285026550292969\n",
      "0.8814258575439453\n",
      "0.8031120300292969\n",
      "0.7990264892578125\n",
      "0.7996711730957031\n",
      "0.7597885131835938\n",
      "0.8731136322021484\n",
      "0.8248405456542969\n",
      "0.7512321472167969\n",
      "0.8175582885742188\n",
      "0.807952880859375\n",
      "0.6139583587646484\n",
      "0.7692413330078125\n",
      "0.8079929351806641\n",
      "0.8645992279052734\n",
      "0.7470588684082031\n",
      "0.81878662109375\n",
      "0.6858406066894531\n",
      "0.8407325744628906\n",
      "0.8267059326171875\n",
      "0.8701076507568359\n",
      "0.8936004638671875\n",
      "0.7495460510253906\n",
      "0.8545036315917969\n",
      "0.8840217590332031\n",
      "0.7270069122314453\n",
      "0.8305492401123047\n",
      "0.8677558898925781\n",
      "0.8003578186035156\n",
      "0.7931957244873047\n",
      "0.7392120361328125\n",
      "0.7949752807617188\n",
      "0.7981986999511719\n",
      "0.6883182525634766\n",
      "0.8690032958984375\n",
      "0.6386585235595703\n",
      "0.8375148773193359\n",
      "0.9038772583007812\n",
      "0.5837001800537109\n",
      "0.8668251037597656\n",
      "0.7667369842529297\n",
      "0.81915283203125\n",
      "0.86199951171875\n",
      "0.9263992309570312\n",
      "0.6801643371582031\n",
      "0.8088150024414062\n",
      "0.8750839233398438\n",
      "0.7743320465087891\n",
      "0.8369007110595703\n",
      "0.8885498046875\n",
      "0.8193893432617188\n",
      "0.898406982421875\n",
      "0.8832950592041016\n",
      "0.8259658813476562\n",
      "0.7538986206054688\n",
      "0.8215656280517578\n",
      "0.8094253540039062\n",
      "0.6704368591308594\n",
      "0.8189373016357422\n",
      "0.8265323638916016\n",
      "0.8327445983886719\n",
      "0.9265766143798828\n",
      "0.8250560760498047\n",
      "0.7644824981689453\n",
      "0.7678489685058594\n",
      "0.61297607421875\n",
      "0.6160907745361328\n",
      "0.8738975524902344\n",
      "0.8898887634277344\n",
      "0.7286205291748047\n",
      "0.86737060546875\n",
      "0.6803226470947266\n",
      "0.8999042510986328\n",
      "0.7931098937988281\n",
      "0.8820343017578125\n",
      "0.8608245849609375\n",
      "0.8465843200683594\n",
      "0.7993087768554688\n",
      "0.8326644897460938\n",
      "0.8019294738769531\n",
      "0.6213150024414062\n",
      "0.7475471496582031\n",
      "0.8873214721679688\n",
      "0.8562507629394531\n",
      "0.6895294189453125\n",
      "0.7777309417724609\n",
      "0.8580799102783203\n",
      "0.8541240692138672\n",
      "0.8267898559570312\n",
      "0.5441398620605469\n",
      "0.9397678375244141\n",
      "0.8661842346191406\n",
      "0.7600955963134766\n",
      "0.8159465789794922\n",
      "0.8541793823242188\n",
      "0.8305549621582031\n",
      "0.8066730499267578\n",
      "0.6721115112304688\n",
      "0.7490997314453125\n",
      "0.8537139892578125\n",
      "0.7815933227539062\n",
      "0.8100013732910156\n",
      "0.8962860107421875\n",
      "0.8961906433105469\n",
      "0.8411388397216797\n",
      "0.8540782928466797\n",
      "0.9723625183105469\n",
      "0.8782558441162109\n",
      "0.8962421417236328\n",
      "0.7889347076416016\n",
      "0.7683773040771484\n",
      "0.9019393920898438\n",
      "0.7731761932373047\n",
      "0.5948028564453125\n",
      "0.8348731994628906\n",
      "0.8107433319091797\n",
      "0.6739234924316406\n",
      "0.8142166137695312\n",
      "0.7798099517822266\n",
      "0.9401931762695312\n",
      "0.9047756195068359\n",
      "0.8191604614257812\n",
      "0.7748756408691406\n",
      "0.9107570648193359\n",
      "0.9205684661865234\n",
      "0.9226245880126953\n",
      "0.8263206481933594\n",
      "0.8293857574462891\n",
      "0.5497398376464844\n",
      "0.8466148376464844\n",
      "0.9905338287353516\n",
      "0.6224384307861328\n",
      "0.7416839599609375\n",
      "0.8091888427734375\n",
      "0.9236106872558594\n",
      "0.6551284790039062\n",
      "0.8553390502929688\n",
      "0.8346920013427734\n",
      "0.8224220275878906\n",
      "0.7716808319091797\n",
      "0.8845119476318359\n",
      "0.8379001617431641\n",
      "0.8284664154052734\n",
      "0.8140106201171875\n",
      "0.7899913787841797\n",
      "0.7772693634033203\n",
      "0.8278541564941406\n",
      "0.9147186279296875\n",
      "0.8657627105712891\n",
      "0.6642417907714844\n",
      "0.9285430908203125\n",
      "0.6811637878417969\n",
      "0.6224727630615234\n",
      "0.8180618286132812\n",
      "0.8002243041992188\n",
      "0.8400135040283203\n",
      "0.8413848876953125\n",
      "0.7360038757324219\n",
      "0.8186893463134766\n",
      "0.8362560272216797\n",
      "0.7104167938232422\n",
      "0.8981761932373047\n",
      "0.7563648223876953\n",
      "0.7752037048339844\n",
      "0.7628364562988281\n",
      "0.7973918914794922\n",
      "0.7990055084228516\n",
      "0.8285388946533203\n",
      "0.7598037719726562\n",
      "0.8725376129150391\n",
      "0.8515453338623047\n",
      "0.8562431335449219\n",
      "0.8622589111328125\n",
      "0.6018257141113281\n",
      "0.81378173828125\n",
      "0.92962646484375\n",
      "0.8591079711914062\n",
      "0.9385662078857422\n",
      "0.5541725158691406\n",
      "0.8698997497558594\n",
      "0.546875\n",
      "0.8269710540771484\n",
      "0.8473682403564453\n",
      "0.9021434783935547\n",
      "0.8677711486816406\n",
      "0.7922286987304688\n",
      "0.7730445861816406\n",
      "0.9044475555419922\n",
      "0.8299427032470703\n",
      "0.8296318054199219\n",
      "0.6382560729980469\n",
      "0.6801338195800781\n",
      "0.7901973724365234\n",
      "0.8753585815429688\n",
      "0.9126071929931641\n",
      "0.9162940979003906\n"
     ]
    }
   ],
   "source": [
    "compute_rep_cap_for_circuits(\n",
    "    circuits_save_dir, num_circs, circ_prefix, num_qubits, \n",
    "    num_meas_qubits, dataset, num_classes,\n",
    "    num_samples_per_class,\n",
    "    num_param_samples, encoding_type, \n",
    "    num_data_reps, save_matrices,\n",
    "    dataset_file_extension\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b515d6",
   "metadata": {},
   "source": [
    "## Compute composite scores for candidate circuits and train best circuit\n",
    "\n",
    "Now that we have computed both CNR and RepCap scores for the candidate circuits, we can combine the two to create composite scores for each circuit. To decide the relative importances of CNR and RepCap in the composite score, we set a hyperparameter - setting it to 1 results in equal importance being given to both CNR and RepCap. Here we will use a value of 0.25; the optimal value to use for other devices will likely be different. In general, the higher the noise level, the higher the optimal CNR importance value.\n",
    "\n",
    "Once we complete computing the composite scores, we can select the circuit with the highest composite score, and train it on the target dataset using a noiseless simulator. After training the circuit, we can evaluate it on the test portion of the target dataset before moving to running circuits on a real device. We do this to get a ballpark figure for the performance we can expect when we deploy the circuit on OQC Lucy, which is likely to be a little worse due to hardware noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "199c304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "circuits_save_dir = './candidate_circuits'\n",
    "dataset = 'mnist_2'\n",
    "encoding_type = 'angle'\n",
    "num_data_reps = 1\n",
    "device_name = 'ionq_harmony'\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "num_qubits = 4\n",
    "num_meas_qubits = 1\n",
    "num_data_for_rep_cap = 32\n",
    "num_params_for_rep_cap = 32\n",
    "num_cdcs = 32\n",
    "num_candidates_per_circ = 250\n",
    "num_circs = 250\n",
    "num_runs_per_circ = 5\n",
    "noise_importance = 0.25\n",
    "save_dir = './trained_circuit'\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b85d7258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[ 18 133  91 138 230 154 151 231  92  15 212 137  35  44  72 219 226   9\n",
      " 238 152 143 221  20  56  99 239  97  75  48 166 134 163  53 234 184  55\n",
      "  14 182 113  25   2 180  13  61 243 142 227 248 204 210  86 242 110 136\n",
      "  49  85 170 188  63 124 103 222 235  68 195 111 244 121 208  67 114 216\n",
      " 148  79 164  16 200  89  36  43  31 196   3  93 223 115  71  74 206 246\n",
      " 186   0   7 118  82  38 131  95 211  17 193  32  23 168 236 129 140  47\n",
      " 127 122 156  54 237 120 229  65  40 159  28 213 245  94  62  90 139 150\n",
      " 187  88  78  59 185   6 207 172 132 162 165 169  96  42  34 167 109 178\n",
      " 144 101  76 202 174   4 116 191 240  24   1 106 157 209 232 102  22 241\n",
      " 194  12  29 135 130  37 171 107 146 125 145 228 225  52 233 203  83 173\n",
      "  45 201 218 197 149  57 199  60  70 175 220  87 126 128  69 141  19 100\n",
      " 181  33 183  81 249  21   8 205 176 215 123  73 117  26  39 105  50 247\n",
      " 160  66   5 108 190 158 179  58 119  51  80 177  77  30  46 217  27 112\n",
      " 198 192  64  41 224  84 104 153  10 161  11 214 147 189  98 155] 184\n",
      "0.984331841175246\n",
      "Epoch 1 | Step 1 | Loss: 0.27951 | Acc: 0.96875\n",
      "Epoch 2 | Step 1 | Loss: 0.36806 | Acc: 0.97266\n",
      "Epoch 3 | Step 1 | Loss: 0.25174 | Acc: 0.94141\n",
      "Epoch 4 | Step 1 | Loss: 0.23785 | Acc: 0.96484\n",
      "Epoch 5 | Step 1 | Loss: 0.33507 | Acc: 0.98047\n",
      "Epoch 6 | Step 1 | Loss: 0.24826 | Acc: 0.96094\n",
      "Epoch 7 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 8 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 9 | Step 1 | Loss: 0.25868 | Acc: 0.97266\n",
      "Epoch 10 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 11 | Step 1 | Loss: 0.25347 | Acc: 0.96484\n",
      "Epoch 12 | Step 1 | Loss: 0.26042 | Acc: 0.97266\n",
      "Epoch 13 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 14 | Step 1 | Loss: 0.27083 | Acc: 0.95703\n",
      "Epoch 15 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 16 | Step 1 | Loss: 0.24653 | Acc: 0.96484\n",
      "Epoch 17 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 18 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 19 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 20 | Step 1 | Loss: 0.26215 | Acc: 0.97656\n",
      "Epoch 21 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 22 | Step 1 | Loss: 0.23611 | Acc: 0.98438\n",
      "Epoch 23 | Step 1 | Loss: 0.23785 | Acc: 0.96484\n",
      "Epoch 24 | Step 1 | Loss: 0.24479 | Acc: 0.98047\n",
      "Epoch 25 | Step 1 | Loss:    0.25 | Acc: 0.95703\n",
      "Epoch 26 | Step 1 | Loss: 0.22396 | Acc: 0.98047\n",
      "Epoch 27 | Step 1 | Loss: 0.29861 | Acc: 0.95312\n",
      "Epoch 28 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 29 | Step 1 | Loss: 0.26215 | Acc: 0.98438\n",
      "Epoch 30 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 31 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 32 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 33 | Step 1 | Loss: 0.24653 | Acc: 0.96484\n",
      "Epoch 34 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 35 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 36 | Step 1 | Loss: 0.23438 | Acc: 0.97656\n",
      "Epoch 37 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 38 | Step 1 | Loss: 0.20833 | Acc: 0.97656\n",
      "Epoch 39 | Step 1 | Loss: 0.25694 | Acc: 0.97656\n",
      "Epoch 40 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 41 | Step 1 | Loss: 0.23611 | Acc: 0.96875\n",
      "Epoch 42 | Step 1 | Loss: 0.25174 | Acc: 0.96094\n",
      "Epoch 43 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 44 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 45 | Step 1 | Loss: 0.21701 | Acc: 0.96875\n",
      "Epoch 46 | Step 1 | Loss: 0.23438 | Acc: 0.96484\n",
      "Epoch 47 | Step 1 | Loss:  0.2309 | Acc: 0.96875\n",
      "Epoch 48 | Step 1 | Loss: 0.22049 | Acc: 0.98438\n",
      "Epoch 49 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 50 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 51 | Step 1 | Loss: 0.25694 | Acc: 0.96094\n",
      "Epoch 52 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 53 | Step 1 | Loss: 0.35417 | Acc: 0.96875\n",
      "Epoch 54 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 55 | Step 1 | Loss: 0.25694 | Acc: 0.97266\n",
      "Epoch 56 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 57 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "Epoch 58 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 59 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 60 | Step 1 | Loss: 0.24826 | Acc: 0.98047\n",
      "Epoch 61 | Step 1 | Loss: 0.24653 | Acc: 0.98047\n",
      "Epoch 62 | Step 1 | Loss: 0.24653 | Acc: 0.96094\n",
      "Epoch 63 | Step 1 | Loss: 0.22222 | Acc: 0.97656\n",
      "Epoch 64 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 65 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 66 | Step 1 | Loss: 0.23438 | Acc: 0.96875\n",
      "Epoch 67 | Step 1 | Loss: 0.23611 | Acc: 0.96875\n",
      "Epoch 68 | Step 1 | Loss: 0.23438 | Acc: 0.98828\n",
      "Epoch 69 | Step 1 | Loss: 0.29688 | Acc: 0.95312\n",
      "Epoch 70 | Step 1 | Loss: 0.28125 | Acc: 0.95703\n",
      "Epoch 71 | Step 1 | Loss: 0.24653 | Acc: 0.96875\n",
      "Epoch 72 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 73 | Step 1 | Loss: 0.26215 | Acc: 0.97266\n",
      "Epoch 74 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 75 | Step 1 | Loss: 0.23958 | Acc: 0.96875\n",
      "Epoch 76 | Step 1 | Loss: 0.29514 | Acc: 0.94922\n",
      "Epoch 77 | Step 1 | Loss: 0.24826 | Acc: 0.96484\n",
      "Epoch 78 | Step 1 | Loss: 0.26736 | Acc: 0.97656\n",
      "Epoch 79 | Step 1 | Loss: 0.23438 | Acc: 0.96875\n",
      "Epoch 80 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 81 | Step 1 | Loss: 0.22743 | Acc: 0.98828\n",
      "Epoch 82 | Step 1 | Loss:    0.25 | Acc: 0.95703\n",
      "Epoch 83 | Step 1 | Loss: 0.35069 | Acc: 0.98047\n",
      "Epoch 84 | Step 1 | Loss: 0.25694 | Acc: 0.95703\n",
      "Epoch 85 | Step 1 | Loss: 0.23264 | Acc: 0.98438\n",
      "Epoch 86 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 87 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 88 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 89 | Step 1 | Loss: 0.21701 | Acc: 0.98438\n",
      "Epoch 90 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 91 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 92 | Step 1 | Loss: 0.24306 | Acc: 0.98438\n",
      "Epoch 93 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 94 | Step 1 | Loss: 0.21701 | Acc: 0.98438\n",
      "Epoch 95 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 96 | Step 1 | Loss: 0.21528 | Acc: 0.98438\n",
      "Epoch 97 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 98 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 99 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 100 | Step 1 | Loss: 0.27083 | Acc: 0.94922\n",
      "Epoch 101 | Step 1 | Loss: 0.24479 | Acc: 0.98047\n",
      "Epoch 102 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 103 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 104 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 105 | Step 1 | Loss: 0.23264 | Acc: 0.97656\n",
      "Epoch 106 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 107 | Step 1 | Loss:    0.25 | Acc: 0.97656\n",
      "Epoch 108 | Step 1 | Loss: 0.24826 | Acc: 0.98438\n",
      "Epoch 109 | Step 1 | Loss: 0.22743 | Acc: 0.98828\n",
      "Epoch 110 | Step 1 | Loss: 0.22743 | Acc: 0.98828\n",
      "Epoch 111 | Step 1 | Loss: 0.24479 | Acc: 0.96875\n",
      "Epoch 112 | Step 1 | Loss: 0.19792 | Acc: 0.98438\n",
      "Epoch 113 | Step 1 | Loss: 0.24653 | Acc: 0.95703\n",
      "Epoch 114 | Step 1 | Loss: 0.28299 | Acc: 0.95703\n",
      "Epoch 115 | Step 1 | Loss: 0.20139 | Acc: 0.98047\n",
      "Epoch 116 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 117 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 118 | Step 1 | Loss: 0.24306 | Acc: 0.98047\n",
      "Epoch 119 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 120 | Step 1 | Loss: 0.27083 | Acc: 0.96484\n",
      "Epoch 121 | Step 1 | Loss: 0.22222 | Acc: 0.97266\n",
      "Epoch 122 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 123 | Step 1 | Loss: 0.25174 | Acc: 0.96094\n",
      "Epoch 124 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 125 | Step 1 | Loss: 0.23611 | Acc: 0.97266\n",
      "Epoch 126 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 127 | Step 1 | Loss: 0.24653 | Acc: 0.96484\n",
      "Epoch 128 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 129 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 130 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 131 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 132 | Step 1 | Loss: 0.21181 | Acc: 0.97266\n",
      "Epoch 133 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 134 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 135 | Step 1 | Loss: 0.21181 | Acc: 0.98438\n",
      "Epoch 136 | Step 1 | Loss:  0.2309 | Acc: 0.98047\n",
      "Epoch 137 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 138 | Step 1 | Loss: 0.25868 | Acc: 0.96094\n",
      "Epoch 139 | Step 1 | Loss: 0.21181 | Acc: 0.98438\n",
      "Epoch 140 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 141 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 142 | Step 1 | Loss: 0.24132 | Acc: 0.97266\n",
      "Epoch 143 | Step 1 | Loss: 0.20486 | Acc: 0.98438\n",
      "Epoch 144 | Step 1 | Loss: 0.25868 | Acc: 0.96875\n",
      "Epoch 145 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 146 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 147 | Step 1 | Loss: 0.22569 | Acc: 0.98438\n",
      "Epoch 148 | Step 1 | Loss: 0.25521 | Acc: 0.97266\n",
      "Epoch 149 | Step 1 | Loss: 0.21875 | Acc: 0.97656\n",
      "Epoch 150 | Step 1 | Loss: 0.21007 | Acc: 0.97656\n",
      "Epoch 151 | Step 1 | Loss: 0.25347 | Acc: 0.98047\n",
      "Epoch 152 | Step 1 | Loss: 0.22049 | Acc: 0.98438\n",
      "Epoch 153 | Step 1 | Loss: 0.25174 | Acc: 0.97266\n",
      "Epoch 154 | Step 1 | Loss: 0.25521 | Acc: 0.97656\n",
      "Epoch 155 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 156 | Step 1 | Loss: 0.24306 | Acc: 0.97266\n",
      "Epoch 157 | Step 1 | Loss: 0.20833 | Acc: 0.98047\n",
      "Epoch 158 | Step 1 | Loss: 0.24306 | Acc: 0.97266\n",
      "Epoch 159 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 160 | Step 1 | Loss: 0.22049 | Acc: 0.97266\n",
      "Epoch 161 | Step 1 | Loss: 0.23958 | Acc: 0.96875\n",
      "Epoch 162 | Step 1 | Loss: 0.24132 | Acc: 0.97266\n",
      "Epoch 163 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 164 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 165 | Step 1 | Loss: 0.22049 | Acc: 0.97266\n",
      "Epoch 166 | Step 1 | Loss: 0.26389 | Acc: 0.96484\n",
      "Epoch 167 | Step 1 | Loss: 0.23611 | Acc: 0.98438\n",
      "Epoch 168 | Step 1 | Loss: 0.24653 | Acc: 0.96094\n",
      "Epoch 169 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 170 | Step 1 | Loss: 0.20486 | Acc: 0.98828\n",
      "Epoch 171 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 172 | Step 1 | Loss: 0.25174 | Acc: 0.98438\n",
      "Epoch 173 | Step 1 | Loss: 0.24826 | Acc: 0.98438\n",
      "Epoch 174 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "Epoch 175 | Step 1 | Loss: 0.25174 | Acc: 0.98047\n",
      "Epoch 176 | Step 1 | Loss: 0.21875 | Acc: 0.97266\n",
      "Epoch 177 | Step 1 | Loss: 0.34896 | Acc: 0.97266\n",
      "Epoch 178 | Step 1 | Loss: 0.24653 | Acc: 0.95312\n",
      "Epoch 179 | Step 1 | Loss: 0.23785 | Acc: 0.98828\n",
      "Epoch 180 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 181 | Step 1 | Loss: 0.27083 | Acc: 0.97266\n",
      "Epoch 182 | Step 1 | Loss: 0.21875 | Acc: 0.97656\n",
      "Epoch 183 | Step 1 | Loss: 0.27083 | Acc: 0.96094\n",
      "Epoch 184 | Step 1 | Loss:    0.25 | Acc: 0.96094\n",
      "Epoch 185 | Step 1 | Loss: 0.22049 | Acc: 0.97656\n",
      "Epoch 186 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 187 | Step 1 | Loss: 0.34201 | Acc: 0.97266\n",
      "Epoch 188 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 189 | Step 1 | Loss: 0.23438 | Acc: 0.98438\n",
      "Epoch 190 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 191 | Step 1 | Loss: 0.20833 | Acc: 0.99609\n",
      "Epoch 192 | Step 1 | Loss: 0.26389 | Acc: 0.97266\n",
      "Epoch 193 | Step 1 | Loss: 0.21528 | Acc: 0.96875\n",
      "Epoch 194 | Step 1 | Loss: 0.21528 | Acc: 0.98047\n",
      "Epoch 195 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 196 | Step 1 | Loss: 0.25694 | Acc: 0.97656\n",
      "Epoch 197 | Step 1 | Loss: 0.21528 | Acc: 0.99219\n",
      "Epoch 198 | Step 1 | Loss: 0.23785 | Acc: 0.96484\n",
      "Epoch 199 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 200 | Step 1 | Loss: 0.24826 | Acc: 0.98438\n",
      "\n",
      "Accuracy: 0.97 | MSE Loss: 0.274787870638201\n",
      "\n",
      "Epoch 1 | Step 1 | Loss: 0.98611 | Acc: 0.59375\n",
      "Epoch 2 | Step 1 | Loss: 0.55382 | Acc: 0.91406\n",
      "Epoch 3 | Step 1 | Loss: 0.48438 | Acc:  0.9375\n",
      "Epoch 4 | Step 1 | Loss:  0.4375 | Acc: 0.94531\n",
      "Epoch 5 | Step 1 | Loss: 0.29514 | Acc: 0.94141\n",
      "Epoch 6 | Step 1 | Loss: 0.35069 | Acc: 0.97656\n",
      "Epoch 7 | Step 1 | Loss: 0.27083 | Acc: 0.96875\n",
      "Epoch 8 | Step 1 | Loss: 0.26736 | Acc: 0.96094\n",
      "Epoch 9 | Step 1 | Loss: 0.26389 | Acc: 0.97656\n",
      "Epoch 10 | Step 1 | Loss: 0.25347 | Acc: 0.96875\n",
      "Epoch 11 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 12 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 13 | Step 1 | Loss: 0.21528 | Acc: 0.97266\n",
      "Epoch 14 | Step 1 | Loss: 0.27257 | Acc: 0.95312\n",
      "Epoch 15 | Step 1 | Loss: 0.24306 | Acc: 0.96484\n",
      "Epoch 16 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 17 | Step 1 | Loss: 0.27778 | Acc: 0.95703\n",
      "Epoch 18 | Step 1 | Loss: 0.24479 | Acc: 0.97656\n",
      "Epoch 19 | Step 1 | Loss:  0.2066 | Acc: 0.98047\n",
      "Epoch 20 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 21 | Step 1 | Loss: 0.27778 | Acc: 0.96484\n",
      "Epoch 22 | Step 1 | Loss: 0.24306 | Acc: 0.98047\n",
      "Epoch 23 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 24 | Step 1 | Loss: 0.25521 | Acc: 0.97266\n",
      "Epoch 25 | Step 1 | Loss: 0.26563 | Acc: 0.98047\n",
      "Epoch 26 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 27 | Step 1 | Loss: 0.22569 | Acc: 0.98438\n",
      "Epoch 28 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "Epoch 29 | Step 1 | Loss: 0.23611 | Acc: 0.96875\n",
      "Epoch 30 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 31 | Step 1 | Loss: 0.21181 | Acc: 0.98828\n",
      "Epoch 32 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 33 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 34 | Step 1 | Loss: 0.23611 | Acc: 0.98047\n",
      "Epoch 35 | Step 1 | Loss: 0.26042 | Acc: 0.96484\n",
      "Epoch 36 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 37 | Step 1 | Loss: 0.19792 | Acc: 0.98828\n",
      "Epoch 38 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 39 | Step 1 | Loss: 0.25694 | Acc: 0.96484\n",
      "Epoch 40 | Step 1 | Loss: 0.27083 | Acc: 0.95703\n",
      "Epoch 41 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 42 | Step 1 | Loss: 0.18924 | Acc: 0.99219\n",
      "Epoch 43 | Step 1 | Loss: 0.22569 | Acc: 0.99219\n",
      "Epoch 44 | Step 1 | Loss: 0.24306 | Acc: 0.98047\n",
      "Epoch 45 | Step 1 | Loss: 0.21701 | Acc: 0.98047\n",
      "Epoch 46 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 47 | Step 1 | Loss: 0.27951 | Acc: 0.95312\n",
      "Epoch 48 | Step 1 | Loss: 0.23438 | Acc: 0.98828\n",
      "Epoch 49 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 50 | Step 1 | Loss: 0.23785 | Acc: 0.96094\n",
      "Epoch 51 | Step 1 | Loss: 0.19965 | Acc: 0.98828\n",
      "Epoch 52 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 53 | Step 1 | Loss: 0.25347 | Acc: 0.96875\n",
      "Epoch 54 | Step 1 | Loss: 0.24479 | Acc: 0.98438\n",
      "Epoch 55 | Step 1 | Loss: 0.22743 | Acc: 0.97266\n",
      "Epoch 56 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 57 | Step 1 | Loss: 0.22049 | Acc: 0.97656\n",
      "Epoch 58 | Step 1 | Loss: 0.27778 | Acc: 0.96484\n",
      "Epoch 59 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 60 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 61 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 62 | Step 1 | Loss: 0.20313 | Acc: 0.98438\n",
      "Epoch 63 | Step 1 | Loss: 0.24653 | Acc: 0.95703\n",
      "Epoch 64 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 65 | Step 1 | Loss:  0.2691 | Acc: 0.96094\n",
      "Epoch 66 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 67 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 68 | Step 1 | Loss: 0.21701 | Acc: 0.97656\n",
      "Epoch 69 | Step 1 | Loss: 0.19965 | Acc: 0.98438\n",
      "Epoch 70 | Step 1 | Loss: 0.25868 | Acc: 0.96094\n",
      "Epoch 71 | Step 1 | Loss: 0.23958 | Acc: 0.96484\n",
      "Epoch 72 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 73 | Step 1 | Loss: 0.35764 | Acc: 0.97266\n",
      "Epoch 74 | Step 1 | Loss: 0.26042 | Acc: 0.96094\n",
      "Epoch 75 | Step 1 | Loss: 0.36458 | Acc: 0.96875\n",
      "Epoch 76 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 77 | Step 1 | Loss: 0.23958 | Acc: 0.98438\n",
      "Epoch 78 | Step 1 | Loss: 0.25347 | Acc: 0.97266\n",
      "Epoch 79 | Step 1 | Loss: 0.20139 | Acc: 0.98438\n",
      "Epoch 80 | Step 1 | Loss:  0.2691 | Acc: 0.96094\n",
      "Epoch 81 | Step 1 | Loss: 0.25521 | Acc: 0.96875\n",
      "Epoch 82 | Step 1 | Loss: 0.20486 | Acc: 0.98047\n",
      "Epoch 83 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 84 | Step 1 | Loss: 0.35938 | Acc: 0.97266\n",
      "Epoch 85 | Step 1 | Loss:  0.2066 | Acc: 0.97656\n",
      "Epoch 86 | Step 1 | Loss: 0.23611 | Acc: 0.97266\n",
      "Epoch 87 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 88 | Step 1 | Loss: 0.22049 | Acc: 0.98828\n",
      "Epoch 89 | Step 1 | Loss: 0.23438 | Acc: 0.98828\n",
      "Epoch 90 | Step 1 | Loss:  0.2691 | Acc: 0.96484\n",
      "Epoch 91 | Step 1 | Loss: 0.22917 | Acc: 0.96875\n",
      "Epoch 92 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 93 | Step 1 | Loss: 0.20139 | Acc: 0.97266\n",
      "Epoch 94 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 95 | Step 1 | Loss: 0.26215 | Acc: 0.96094\n",
      "Epoch 96 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 97 | Step 1 | Loss: 0.21007 | Acc: 0.98828\n",
      "Epoch 98 | Step 1 | Loss: 0.28646 | Acc: 0.95312\n",
      "Epoch 99 | Step 1 | Loss: 0.28125 | Acc: 0.96484\n",
      "Epoch 100 | Step 1 | Loss: 0.27257 | Acc: 0.97266\n",
      "Epoch 101 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 102 | Step 1 | Loss: 0.25521 | Acc: 0.96484\n",
      "Epoch 103 | Step 1 | Loss: 0.23438 | Acc: 0.96484\n",
      "Epoch 104 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 105 | Step 1 | Loss: 0.20313 | Acc: 0.98438\n",
      "Epoch 106 | Step 1 | Loss: 0.24479 | Acc: 0.98047\n",
      "Epoch 107 | Step 1 | Loss: 0.32986 | Acc: 0.98047\n",
      "Epoch 108 | Step 1 | Loss: 0.21181 | Acc: 0.97656\n",
      "Epoch 109 | Step 1 | Loss: 0.32813 | Acc: 0.98047\n",
      "Epoch 110 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 111 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 112 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 114 | Step 1 | Loss: 0.21181 | Acc: 0.98828\n",
      "Epoch 115 | Step 1 | Loss: 0.21007 | Acc: 0.97266\n",
      "Epoch 116 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 117 | Step 1 | Loss: 0.24479 | Acc: 0.97656\n",
      "Epoch 118 | Step 1 | Loss: 0.24653 | Acc: 0.96094\n",
      "Epoch 119 | Step 1 | Loss: 0.22049 | Acc: 0.98828\n",
      "Epoch 120 | Step 1 | Loss: 0.25174 | Acc: 0.97656\n",
      "Epoch 121 | Step 1 | Loss: 0.26042 | Acc: 0.95703\n",
      "Epoch 122 | Step 1 | Loss: 0.21875 | Acc: 0.99219\n",
      "Epoch 123 | Step 1 | Loss: 0.23437 | Acc: 0.96875\n",
      "Epoch 124 | Step 1 | Loss: 0.22222 | Acc: 0.98828\n",
      "Epoch 125 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 126 | Step 1 | Loss: 0.25174 | Acc: 0.96094\n",
      "Epoch 127 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 128 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 129 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 130 | Step 1 | Loss: 0.19097 | Acc: 0.98828\n",
      "Epoch 131 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 132 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 133 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 134 | Step 1 | Loss: 0.22743 | Acc: 0.97266\n",
      "Epoch 135 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 136 | Step 1 | Loss:  0.2309 | Acc: 0.98828\n",
      "Epoch 137 | Step 1 | Loss: 0.27083 | Acc: 0.95703\n",
      "Epoch 138 | Step 1 | Loss:    0.25 | Acc: 0.98438\n",
      "Epoch 139 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 140 | Step 1 | Loss:    0.25 | Acc: 0.97656\n",
      "Epoch 141 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n",
      "Epoch 142 | Step 1 | Loss:  0.3125 | Acc: 0.99219\n",
      "Epoch 143 | Step 1 | Loss: 0.22222 | Acc: 0.97656\n",
      "Epoch 144 | Step 1 | Loss: 0.27257 | Acc: 0.96484\n",
      "Epoch 145 | Step 1 | Loss: 0.20833 | Acc: 0.98828\n",
      "Epoch 146 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 147 | Step 1 | Loss: 0.25868 | Acc: 0.96875\n",
      "Epoch 148 | Step 1 | Loss: 0.21875 | Acc: 0.98828\n",
      "Epoch 149 | Step 1 | Loss: 0.24653 | Acc: 0.96875\n",
      "Epoch 150 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 151 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 152 | Step 1 | Loss:  0.2066 | Acc: 0.98438\n",
      "Epoch 153 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 154 | Step 1 | Loss: 0.23611 | Acc: 0.98047\n",
      "Epoch 155 | Step 1 | Loss: 0.26042 | Acc: 0.97656\n",
      "Epoch 156 | Step 1 | Loss: 0.21701 | Acc: 0.98047\n",
      "Epoch 157 | Step 1 | Loss: 0.24479 | Acc: 0.95703\n",
      "Epoch 158 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 159 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 160 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 161 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 162 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 163 | Step 1 | Loss: 0.23611 | Acc: 0.98047\n",
      "Epoch 164 | Step 1 | Loss: 0.25694 | Acc: 0.96484\n",
      "Epoch 165 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 166 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "Epoch 167 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 168 | Step 1 | Loss: 0.22396 | Acc: 0.99609\n",
      "Epoch 169 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 170 | Step 1 | Loss: 0.35764 | Acc: 0.96484\n",
      "Epoch 171 | Step 1 | Loss: 0.22743 | Acc: 0.97656\n",
      "Epoch 172 | Step 1 | Loss: 0.31771 | Acc: 0.99219\n",
      "Epoch 173 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 174 | Step 1 | Loss: 0.27431 | Acc: 0.97656\n",
      "Epoch 175 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 176 | Step 1 | Loss:    0.25 | Acc: 0.96094\n",
      "Epoch 177 | Step 1 | Loss:  0.2691 | Acc: 0.96484\n",
      "Epoch 178 | Step 1 | Loss: 0.23264 | Acc: 0.97656\n",
      "Epoch 179 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 180 | Step 1 | Loss:  0.2309 | Acc: 0.98828\n",
      "Epoch 181 | Step 1 | Loss: 0.30556 | Acc: 0.94141\n",
      "Epoch 182 | Step 1 | Loss: 0.22049 | Acc: 0.98438\n",
      "Epoch 183 | Step 1 | Loss: 0.21701 | Acc: 0.99219\n",
      "Epoch 184 | Step 1 | Loss: 0.25174 | Acc: 0.95703\n",
      "Epoch 185 | Step 1 | Loss: 0.22917 | Acc: 0.98828\n",
      "Epoch 186 | Step 1 | Loss: 0.27431 | Acc: 0.95703\n",
      "Epoch 187 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 188 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 189 | Step 1 | Loss: 0.20313 | Acc: 0.98047\n",
      "Epoch 190 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 191 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "Epoch 192 | Step 1 | Loss: 0.25347 | Acc: 0.96484\n",
      "Epoch 193 | Step 1 | Loss: 0.27604 | Acc: 0.95312\n",
      "Epoch 194 | Step 1 | Loss: 0.26736 | Acc: 0.96484\n",
      "Epoch 195 | Step 1 | Loss: 0.21354 | Acc:     1.0\n",
      "Epoch 196 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 197 | Step 1 | Loss: 0.32986 | Acc: 0.98828\n",
      "Epoch 198 | Step 1 | Loss: 0.25347 | Acc: 0.96875\n",
      "Epoch 199 | Step 1 | Loss: 0.24479 | Acc: 0.97656\n",
      "Epoch 200 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "\n",
      "Accuracy: 0.97 | MSE Loss: 0.26996533269135137\n",
      "\n",
      "Epoch 1 | Step 1 | Loss:  2.1319 | Acc: 0.50781\n",
      "Epoch 2 | Step 1 | Loss:  1.0642 | Acc: 0.53516\n",
      "Epoch 3 | Step 1 | Loss:  1.6684 | Acc: 0.082031\n",
      "Epoch 4 | Step 1 | Loss:  1.5226 | Acc: 0.19141\n",
      "Epoch 5 | Step 1 | Loss: 0.56424 | Acc: 0.91016\n",
      "Epoch 6 | Step 1 | Loss: 0.49653 | Acc: 0.96094\n",
      "Epoch 7 | Step 1 | Loss: 0.49132 | Acc: 0.96484\n",
      "Epoch 8 | Step 1 | Loss: 0.46007 | Acc: 0.98828\n",
      "Epoch 9 | Step 1 | Loss: 0.49653 | Acc: 0.96094\n",
      "Epoch 10 | Step 1 | Loss:  0.4809 | Acc: 0.97266\n",
      "Epoch 11 | Step 1 | Loss: 0.48611 | Acc: 0.96875\n",
      "Epoch 12 | Step 1 | Loss: 0.49132 | Acc: 0.96484\n",
      "Epoch 13 | Step 1 | Loss: 0.48264 | Acc: 0.96484\n",
      "Epoch 14 | Step 1 | Loss: 0.46007 | Acc: 0.96484\n",
      "Epoch 15 | Step 1 | Loss:  0.4809 | Acc: 0.94531\n",
      "Epoch 16 | Step 1 | Loss: 0.47396 | Acc: 0.96094\n",
      "Epoch 17 | Step 1 | Loss: 0.44965 | Acc: 0.96484\n",
      "Epoch 18 | Step 1 | Loss: 0.44271 | Acc: 0.96875\n",
      "Epoch 19 | Step 1 | Loss: 0.43924 | Acc: 0.96094\n",
      "Epoch 20 | Step 1 | Loss: 0.41146 | Acc: 0.97656\n",
      "Epoch 21 | Step 1 | Loss: 0.37847 | Acc: 0.98047\n",
      "Epoch 22 | Step 1 | Loss: 0.28646 | Acc: 0.97266\n",
      "Epoch 23 | Step 1 | Loss: 0.35937 | Acc: 0.92969\n",
      "Epoch 24 | Step 1 | Loss: 0.27257 | Acc: 0.96875\n",
      "Epoch 25 | Step 1 | Loss: 0.37847 | Acc: 0.97266\n",
      "Epoch 26 | Step 1 | Loss: 0.28125 | Acc: 0.97656\n",
      "Epoch 27 | Step 1 | Loss: 0.39236 | Acc: 0.96094\n",
      "Epoch 28 | Step 1 | Loss: 0.26042 | Acc: 0.96875\n",
      "Epoch 29 | Step 1 | Loss: 0.26736 | Acc: 0.97656\n",
      "Epoch 30 | Step 1 | Loss: 0.36979 | Acc: 0.96875\n",
      "Epoch 31 | Step 1 | Loss: 0.29514 | Acc: 0.97266\n",
      "Epoch 32 | Step 1 | Loss: 0.38542 | Acc: 0.96484\n",
      "Epoch 33 | Step 1 | Loss: 0.27257 | Acc: 0.97656\n",
      "Epoch 34 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 35 | Step 1 | Loss: 0.25694 | Acc: 0.98828\n",
      "Epoch 36 | Step 1 | Loss: 0.28993 | Acc: 0.96484\n",
      "Epoch 37 | Step 1 | Loss: 0.25521 | Acc: 0.97656\n",
      "Epoch 38 | Step 1 | Loss: 0.25174 | Acc: 0.98828\n",
      "Epoch 39 | Step 1 | Loss: 0.28125 | Acc: 0.96094\n",
      "Epoch 40 | Step 1 | Loss: 0.30903 | Acc: 0.96875\n",
      "Epoch 41 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 42 | Step 1 | Loss:  0.2691 | Acc: 0.96875\n",
      "Epoch 43 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 44 | Step 1 | Loss: 0.22743 | Acc: 0.96875\n",
      "Epoch 45 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 46 | Step 1 | Loss:  0.2309 | Acc: 0.98047\n",
      "Epoch 47 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 48 | Step 1 | Loss: 0.24306 | Acc: 0.97266\n",
      "Epoch 49 | Step 1 | Loss:  0.2309 | Acc: 0.98438\n",
      "Epoch 50 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 51 | Step 1 | Loss: 0.22396 | Acc: 0.98438\n",
      "Epoch 52 | Step 1 | Loss: 0.24479 | Acc: 0.98047\n",
      "Epoch 53 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 54 | Step 1 | Loss: 0.24826 | Acc: 0.98047\n",
      "Epoch 55 | Step 1 | Loss: 0.22917 | Acc: 0.96875\n",
      "Epoch 56 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 57 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 58 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "Epoch 59 | Step 1 | Loss: 0.36632 | Acc: 0.97656\n",
      "Epoch 60 | Step 1 | Loss: 0.25174 | Acc: 0.98047\n",
      "Epoch 61 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 62 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 63 | Step 1 | Loss: 0.24132 | Acc: 0.97266\n",
      "Epoch 64 | Step 1 | Loss: 0.35938 | Acc: 0.96484\n",
      "Epoch 65 | Step 1 | Loss: 0.28125 | Acc: 0.96875\n",
      "Epoch 66 | Step 1 | Loss: 0.27257 | Acc: 0.95312\n",
      "Epoch 67 | Step 1 | Loss:  0.2691 | Acc: 0.96875\n",
      "Epoch 68 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 69 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 70 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 71 | Step 1 | Loss: 0.24132 | Acc: 0.97266\n",
      "Epoch 72 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 73 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 74 | Step 1 | Loss:  0.2309 | Acc: 0.98828\n",
      "Epoch 75 | Step 1 | Loss: 0.22396 | Acc: 0.99219\n",
      "Epoch 76 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 77 | Step 1 | Loss: 0.21181 | Acc: 0.99609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78 | Step 1 | Loss: 0.25174 | Acc: 0.96484\n",
      "Epoch 79 | Step 1 | Loss: 0.21181 | Acc: 0.98047\n",
      "Epoch 80 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 81 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 82 | Step 1 | Loss: 0.28299 | Acc: 0.98438\n",
      "Epoch 83 | Step 1 | Loss: 0.22049 | Acc: 0.98438\n",
      "Epoch 84 | Step 1 | Loss: 0.22569 | Acc: 0.98438\n",
      "Epoch 85 | Step 1 | Loss: 0.26389 | Acc: 0.96484\n",
      "Epoch 86 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 87 | Step 1 | Loss: 0.25521 | Acc: 0.97656\n",
      "Epoch 88 | Step 1 | Loss: 0.19965 | Acc: 0.99609\n",
      "Epoch 89 | Step 1 | Loss:  0.2691 | Acc: 0.96484\n",
      "Epoch 90 | Step 1 | Loss: 0.25521 | Acc: 0.96094\n",
      "Epoch 91 | Step 1 | Loss: 0.21701 | Acc: 0.97266\n",
      "Epoch 92 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 93 | Step 1 | Loss: 0.26215 | Acc: 0.97266\n",
      "Epoch 94 | Step 1 | Loss: 0.23438 | Acc: 0.97656\n",
      "Epoch 95 | Step 1 | Loss: 0.35069 | Acc: 0.97656\n",
      "Epoch 96 | Step 1 | Loss: 0.19965 | Acc: 0.98438\n",
      "Epoch 97 | Step 1 | Loss: 0.24826 | Acc: 0.98438\n",
      "Epoch 98 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 99 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 100 | Step 1 | Loss: 0.23438 | Acc: 0.98438\n",
      "Epoch 101 | Step 1 | Loss: 0.25694 | Acc: 0.96875\n",
      "Epoch 102 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 103 | Step 1 | Loss: 0.26215 | Acc: 0.96875\n",
      "Epoch 104 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 105 | Step 1 | Loss: 0.25347 | Acc: 0.98047\n",
      "Epoch 106 | Step 1 | Loss: 0.22396 | Acc: 0.98438\n",
      "Epoch 107 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 108 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 109 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 110 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 111 | Step 1 | Loss: 0.26389 | Acc: 0.96875\n",
      "Epoch 112 | Step 1 | Loss: 0.25694 | Acc: 0.97266\n",
      "Epoch 113 | Step 1 | Loss: 0.25347 | Acc: 0.97656\n",
      "Epoch 114 | Step 1 | Loss: 0.24653 | Acc: 0.96875\n",
      "Epoch 115 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 116 | Step 1 | Loss:    0.25 | Acc: 0.97656\n",
      "Epoch 117 | Step 1 | Loss: 0.27431 | Acc: 0.96484\n",
      "Epoch 118 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 119 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 120 | Step 1 | Loss: 0.27431 | Acc: 0.97266\n",
      "Epoch 121 | Step 1 | Loss: 0.23438 | Acc: 0.99219\n",
      "Epoch 122 | Step 1 | Loss: 0.23264 | Acc: 0.97266\n",
      "Epoch 123 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 124 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 125 | Step 1 | Loss: 0.33854 | Acc: 0.97266\n",
      "Epoch 126 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 127 | Step 1 | Loss: 0.21181 | Acc: 0.99219\n",
      "Epoch 128 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 129 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 130 | Step 1 | Loss: 0.20833 | Acc: 0.99219\n",
      "Epoch 131 | Step 1 | Loss: 0.23438 | Acc: 0.97656\n",
      "Epoch 132 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n",
      "Epoch 133 | Step 1 | Loss: 0.22049 | Acc: 0.96875\n",
      "Epoch 134 | Step 1 | Loss: 0.25694 | Acc: 0.96875\n",
      "Epoch 135 | Step 1 | Loss: 0.32465 | Acc: 0.99219\n",
      "Epoch 136 | Step 1 | Loss: 0.26563 | Acc: 0.95703\n",
      "Epoch 137 | Step 1 | Loss:  0.2309 | Acc: 0.98047\n",
      "Epoch 138 | Step 1 | Loss: 0.26215 | Acc: 0.95312\n",
      "Epoch 139 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 140 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 141 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 142 | Step 1 | Loss: 0.22917 | Acc: 0.96484\n",
      "Epoch 143 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 144 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 145 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 146 | Step 1 | Loss: 0.23611 | Acc: 0.98438\n",
      "Epoch 147 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 148 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 149 | Step 1 | Loss: 0.24653 | Acc: 0.98047\n",
      "Epoch 150 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 151 | Step 1 | Loss: 0.25174 | Acc: 0.98047\n",
      "Epoch 152 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 153 | Step 1 | Loss: 0.24479 | Acc: 0.96875\n",
      "Epoch 154 | Step 1 | Loss: 0.26215 | Acc: 0.97266\n",
      "Epoch 155 | Step 1 | Loss: 0.26389 | Acc: 0.96875\n",
      "Epoch 156 | Step 1 | Loss: 0.23958 | Acc: 0.96875\n",
      "Epoch 157 | Step 1 | Loss: 0.22917 | Acc: 0.96875\n",
      "Epoch 158 | Step 1 | Loss: 0.23611 | Acc: 0.98438\n",
      "Epoch 159 | Step 1 | Loss: 0.23264 | Acc: 0.98438\n",
      "Epoch 160 | Step 1 | Loss: 0.26042 | Acc: 0.96875\n",
      "Epoch 161 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 162 | Step 1 | Loss: 0.22396 | Acc: 0.98438\n",
      "Epoch 163 | Step 1 | Loss: 0.25694 | Acc: 0.98438\n",
      "Epoch 164 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 165 | Step 1 | Loss: 0.21181 | Acc: 0.98047\n",
      "Epoch 166 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 167 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 168 | Step 1 | Loss: 0.25521 | Acc: 0.97656\n",
      "Epoch 169 | Step 1 | Loss: 0.25174 | Acc: 0.96484\n",
      "Epoch 170 | Step 1 | Loss: 0.22396 | Acc: 0.98047\n",
      "Epoch 171 | Step 1 | Loss: 0.23785 | Acc: 0.96484\n",
      "Epoch 172 | Step 1 | Loss: 0.25174 | Acc: 0.97266\n",
      "Epoch 173 | Step 1 | Loss: 0.22222 | Acc: 0.97656\n",
      "Epoch 174 | Step 1 | Loss: 0.26215 | Acc: 0.97266\n",
      "Epoch 175 | Step 1 | Loss: 0.25521 | Acc: 0.96094\n",
      "Epoch 176 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 177 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 178 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 179 | Step 1 | Loss: 0.22396 | Acc: 0.98047\n",
      "Epoch 180 | Step 1 | Loss: 0.22396 | Acc: 0.97266\n",
      "Epoch 181 | Step 1 | Loss: 0.25174 | Acc: 0.97656\n",
      "Epoch 182 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 183 | Step 1 | Loss: 0.25521 | Acc: 0.96875\n",
      "Epoch 184 | Step 1 | Loss: 0.25347 | Acc: 0.96875\n",
      "Epoch 185 | Step 1 | Loss: 0.20313 | Acc: 0.98438\n",
      "Epoch 186 | Step 1 | Loss: 0.20486 | Acc: 0.98438\n",
      "Epoch 187 | Step 1 | Loss: 0.26389 | Acc: 0.96875\n",
      "Epoch 188 | Step 1 | Loss: 0.25347 | Acc: 0.96484\n",
      "Epoch 189 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 190 | Step 1 | Loss: 0.21354 | Acc: 0.97656\n",
      "Epoch 191 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 192 | Step 1 | Loss: 0.25174 | Acc: 0.97656\n",
      "Epoch 193 | Step 1 | Loss: 0.18229 | Acc: 0.99219\n",
      "Epoch 194 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 195 | Step 1 | Loss: 0.25174 | Acc: 0.95312\n",
      "Epoch 196 | Step 1 | Loss: 0.24479 | Acc: 0.97656\n",
      "Epoch 197 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 198 | Step 1 | Loss: 0.28125 | Acc: 0.96094\n",
      "Epoch 199 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n",
      "Epoch 200 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "\n",
      "Accuracy: 0.97 | MSE Loss: 0.2642747373952876\n",
      "\n",
      "Epoch 1 | Step 1 | Loss:  1.7691 | Acc: 0.50781\n",
      "Epoch 2 | Step 1 | Loss:  1.1111 | Acc: 0.55078\n",
      "Epoch 3 | Step 1 | Loss: 0.81597 | Acc: 0.64062\n",
      "Epoch 4 | Step 1 | Loss: 0.47049 | Acc: 0.82812\n",
      "Epoch 5 | Step 1 | Loss: 0.25347 | Acc: 0.94531\n",
      "Epoch 6 | Step 1 | Loss: 0.19792 | Acc: 0.99219\n",
      "Epoch 7 | Step 1 | Loss: 0.32986 | Acc: 0.96875\n",
      "Epoch 8 | Step 1 | Loss: 0.32813 | Acc: 0.98828\n",
      "Epoch 9 | Step 1 | Loss: 0.26215 | Acc: 0.96875\n",
      "Epoch 10 | Step 1 | Loss: 0.24479 | Acc: 0.94531\n",
      "Epoch 11 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 12 | Step 1 | Loss:    0.25 | Acc: 0.96094\n",
      "Epoch 13 | Step 1 | Loss: 0.35764 | Acc: 0.97266\n",
      "Epoch 14 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 15 | Step 1 | Loss: 0.26042 | Acc: 0.97266\n",
      "Epoch 16 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 17 | Step 1 | Loss: 0.21528 | Acc: 0.98047\n",
      "Epoch 18 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 19 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 20 | Step 1 | Loss: 0.22396 | Acc: 0.98438\n",
      "Epoch 21 | Step 1 | Loss: 0.22049 | Acc: 0.98828\n",
      "Epoch 22 | Step 1 | Loss: 0.26389 | Acc: 0.97266\n",
      "Epoch 23 | Step 1 | Loss: 0.24132 | Acc: 0.98047\n",
      "Epoch 24 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 25 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "Epoch 26 | Step 1 | Loss: 0.21007 | Acc: 0.98438\n",
      "Epoch 27 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 28 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 29 | Step 1 | Loss: 0.20313 | Acc: 0.99219\n",
      "Epoch 30 | Step 1 | Loss: 0.22396 | Acc: 0.97266\n",
      "Epoch 31 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 32 | Step 1 | Loss: 0.23264 | Acc: 0.98438\n",
      "Epoch 33 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 34 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 35 | Step 1 | Loss: 0.21007 | Acc: 0.98828\n",
      "Epoch 36 | Step 1 | Loss: 0.21354 | Acc: 0.97656\n",
      "Epoch 37 | Step 1 | Loss: 0.24132 | Acc: 0.98438\n",
      "Epoch 38 | Step 1 | Loss: 0.22049 | Acc: 0.98438\n",
      "Epoch 39 | Step 1 | Loss: 0.28646 | Acc: 0.94922\n",
      "Epoch 40 | Step 1 | Loss: 0.22049 | Acc: 0.98828\n",
      "Epoch 41 | Step 1 | Loss:  0.2309 | Acc: 0.98828\n",
      "Epoch 42 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 | Step 1 | Loss: 0.24479 | Acc: 0.96484\n",
      "Epoch 44 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 45 | Step 1 | Loss: 0.22917 | Acc: 0.98828\n",
      "Epoch 46 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 47 | Step 1 | Loss: 0.20833 | Acc: 0.98828\n",
      "Epoch 48 | Step 1 | Loss: 0.21354 | Acc: 0.99219\n",
      "Epoch 49 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 50 | Step 1 | Loss: 0.21701 | Acc: 0.98438\n",
      "Epoch 51 | Step 1 | Loss: 0.25694 | Acc: 0.95312\n",
      "Epoch 52 | Step 1 | Loss: 0.24479 | Acc: 0.98828\n",
      "Epoch 53 | Step 1 | Loss: 0.24479 | Acc: 0.95703\n",
      "Epoch 54 | Step 1 | Loss: 0.22049 | Acc: 0.99219\n",
      "Epoch 55 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 56 | Step 1 | Loss: 0.26215 | Acc: 0.97266\n",
      "Epoch 57 | Step 1 | Loss: 0.28993 | Acc: 0.95312\n",
      "Epoch 58 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 59 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 60 | Step 1 | Loss: 0.20139 | Acc: 0.98438\n",
      "Epoch 61 | Step 1 | Loss: 0.24826 | Acc: 0.96484\n",
      "Epoch 62 | Step 1 | Loss: 0.34201 | Acc: 0.97656\n",
      "Epoch 63 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 64 | Step 1 | Loss: 0.19444 | Acc: 0.98828\n",
      "Epoch 65 | Step 1 | Loss: 0.24479 | Acc: 0.98438\n",
      "Epoch 66 | Step 1 | Loss: 0.27257 | Acc: 0.96875\n",
      "Epoch 67 | Step 1 | Loss: 0.25694 | Acc: 0.95703\n",
      "Epoch 68 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 69 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 70 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 71 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n",
      "Epoch 72 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 73 | Step 1 | Loss: 0.27257 | Acc: 0.96484\n",
      "Epoch 74 | Step 1 | Loss: 0.24132 | Acc: 0.98047\n",
      "Epoch 75 | Step 1 | Loss: 0.27431 | Acc: 0.96875\n",
      "Epoch 76 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 77 | Step 1 | Loss: 0.22049 | Acc: 0.97656\n",
      "Epoch 78 | Step 1 | Loss: 0.26563 | Acc: 0.97266\n",
      "Epoch 79 | Step 1 | Loss: 0.24479 | Acc: 0.98438\n",
      "Epoch 80 | Step 1 | Loss: 0.22396 | Acc: 0.97266\n",
      "Epoch 81 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 82 | Step 1 | Loss: 0.24653 | Acc: 0.96875\n",
      "Epoch 83 | Step 1 | Loss: 0.18403 | Acc: 0.98828\n",
      "Epoch 84 | Step 1 | Loss: 0.25347 | Acc: 0.98438\n",
      "Epoch 85 | Step 1 | Loss: 0.36979 | Acc: 0.96094\n",
      "Epoch 86 | Step 1 | Loss:  0.2934 | Acc: 0.95312\n",
      "Epoch 87 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 88 | Step 1 | Loss: 0.21181 | Acc: 0.97656\n",
      "Epoch 89 | Step 1 | Loss: 0.24132 | Acc: 0.97266\n",
      "Epoch 90 | Step 1 | Loss: 0.21528 | Acc: 0.97656\n",
      "Epoch 91 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 92 | Step 1 | Loss: 0.23438 | Acc: 0.96484\n",
      "Epoch 93 | Step 1 | Loss: 0.24826 | Acc: 0.96484\n",
      "Epoch 94 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 95 | Step 1 | Loss: 0.22569 | Acc: 0.98438\n",
      "Epoch 96 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 97 | Step 1 | Loss: 0.25868 | Acc: 0.97266\n",
      "Epoch 98 | Step 1 | Loss: 0.26215 | Acc: 0.98047\n",
      "Epoch 99 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 100 | Step 1 | Loss: 0.21875 | Acc: 0.97656\n",
      "Epoch 101 | Step 1 | Loss: 0.23958 | Acc: 0.96484\n",
      "Epoch 102 | Step 1 | Loss: 0.26736 | Acc: 0.97656\n",
      "Epoch 103 | Step 1 | Loss: 0.24826 | Acc: 0.98047\n",
      "Epoch 104 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 105 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 106 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 107 | Step 1 | Loss: 0.25174 | Acc: 0.96094\n",
      "Epoch 108 | Step 1 | Loss: 0.25694 | Acc: 0.96484\n",
      "Epoch 109 | Step 1 | Loss: 0.21181 | Acc: 0.97656\n",
      "Epoch 110 | Step 1 | Loss: 0.22743 | Acc: 0.98438\n",
      "Epoch 111 | Step 1 | Loss: 0.21875 | Acc: 0.97656\n",
      "Epoch 112 | Step 1 | Loss: 0.23958 | Acc: 0.96484\n",
      "Epoch 113 | Step 1 | Loss: 0.23264 | Acc: 0.97266\n",
      "Epoch 114 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 115 | Step 1 | Loss: 0.23785 | Acc: 0.96875\n",
      "Epoch 116 | Step 1 | Loss: 0.20313 | Acc: 0.98047\n",
      "Epoch 117 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 118 | Step 1 | Loss: 0.35243 | Acc: 0.98047\n",
      "Epoch 119 | Step 1 | Loss: 0.23611 | Acc: 0.96875\n",
      "Epoch 120 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 121 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 122 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 123 | Step 1 | Loss: 0.23438 | Acc: 0.98438\n",
      "Epoch 124 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n",
      "Epoch 125 | Step 1 | Loss: 0.23611 | Acc: 0.96484\n",
      "Epoch 126 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 127 | Step 1 | Loss:    0.25 | Acc: 0.97656\n",
      "Epoch 128 | Step 1 | Loss: 0.35764 | Acc: 0.98047\n",
      "Epoch 129 | Step 1 | Loss: 0.21354 | Acc: 0.99219\n",
      "Epoch 130 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 131 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 132 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 133 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 134 | Step 1 | Loss: 0.22396 | Acc: 0.98438\n",
      "Epoch 135 | Step 1 | Loss: 0.21701 | Acc: 0.98828\n",
      "Epoch 136 | Step 1 | Loss: 0.25694 | Acc: 0.96875\n",
      "Epoch 137 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 138 | Step 1 | Loss: 0.28993 | Acc: 0.95312\n",
      "Epoch 139 | Step 1 | Loss: 0.34028 | Acc: 0.98438\n",
      "Epoch 140 | Step 1 | Loss: 0.22917 | Acc: 0.96875\n",
      "Epoch 141 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 142 | Step 1 | Loss: 0.20486 | Acc: 0.98828\n",
      "Epoch 143 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 144 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 145 | Step 1 | Loss: 0.21875 | Acc: 0.98047\n",
      "Epoch 146 | Step 1 | Loss: 0.25174 | Acc: 0.97266\n",
      "Epoch 147 | Step 1 | Loss:  0.2309 | Acc: 0.96875\n",
      "Epoch 148 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 149 | Step 1 | Loss: 0.22049 | Acc: 0.97266\n",
      "Epoch 150 | Step 1 | Loss: 0.22743 | Acc: 0.97266\n",
      "Epoch 151 | Step 1 | Loss: 0.21875 | Acc: 0.98438\n",
      "Epoch 152 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 153 | Step 1 | Loss: 0.22917 | Acc: 0.97266\n",
      "Epoch 154 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 155 | Step 1 | Loss:  0.2309 | Acc: 0.98438\n",
      "Epoch 156 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 157 | Step 1 | Loss: 0.25174 | Acc: 0.96484\n",
      "Epoch 158 | Step 1 | Loss: 0.25868 | Acc: 0.96484\n",
      "Epoch 159 | Step 1 | Loss: 0.25347 | Acc: 0.96094\n",
      "Epoch 160 | Step 1 | Loss: 0.24479 | Acc: 0.96484\n",
      "Epoch 161 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 162 | Step 1 | Loss: 0.23264 | Acc: 0.97656\n",
      "Epoch 163 | Step 1 | Loss: 0.24479 | Acc: 0.96484\n",
      "Epoch 164 | Step 1 | Loss: 0.22396 | Acc: 0.96875\n",
      "Epoch 165 | Step 1 | Loss: 0.24479 | Acc: 0.96875\n",
      "Epoch 166 | Step 1 | Loss: 0.25347 | Acc: 0.97266\n",
      "Epoch 167 | Step 1 | Loss: 0.25347 | Acc: 0.96484\n",
      "Epoch 168 | Step 1 | Loss: 0.20313 | Acc: 0.98828\n",
      "Epoch 169 | Step 1 | Loss: 0.28472 | Acc: 0.96484\n",
      "Epoch 170 | Step 1 | Loss: 0.21875 | Acc: 0.97266\n",
      "Epoch 171 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 172 | Step 1 | Loss: 0.20833 | Acc: 0.98438\n",
      "Epoch 173 | Step 1 | Loss:  0.2309 | Acc: 0.96875\n",
      "Epoch 174 | Step 1 | Loss: 0.23438 | Acc: 0.98438\n",
      "Epoch 175 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 176 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 177 | Step 1 | Loss: 0.23958 | Acc: 0.96094\n",
      "Epoch 178 | Step 1 | Loss: 0.27431 | Acc: 0.96875\n",
      "Epoch 179 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 180 | Step 1 | Loss: 0.25694 | Acc: 0.96484\n",
      "Epoch 181 | Step 1 | Loss: 0.22569 | Acc: 0.99219\n",
      "Epoch 182 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 183 | Step 1 | Loss: 0.23264 | Acc: 0.98438\n",
      "Epoch 184 | Step 1 | Loss: 0.21875 | Acc: 0.98828\n",
      "Epoch 185 | Step 1 | Loss: 0.23264 | Acc: 0.97656\n",
      "Epoch 186 | Step 1 | Loss: 0.25521 | Acc: 0.97266\n",
      "Epoch 187 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 188 | Step 1 | Loss: 0.23785 | Acc: 0.98438\n",
      "Epoch 189 | Step 1 | Loss: 0.25868 | Acc: 0.96484\n",
      "Epoch 190 | Step 1 | Loss: 0.24826 | Acc: 0.98047\n",
      "Epoch 191 | Step 1 | Loss: 0.25694 | Acc: 0.95312\n",
      "Epoch 192 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 193 | Step 1 | Loss: 0.18924 | Acc: 0.99219\n",
      "Epoch 194 | Step 1 | Loss: 0.23611 | Acc: 0.96484\n",
      "Epoch 195 | Step 1 | Loss: 0.21701 | Acc: 0.98047\n",
      "Epoch 196 | Step 1 | Loss: 0.25868 | Acc: 0.97656\n",
      "Epoch 197 | Step 1 | Loss: 0.22222 | Acc: 0.97656\n",
      "Epoch 198 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 199 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 200 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "\n",
      "Accuracy: 0.9725 | MSE Loss: 0.26012735813859433\n",
      "\n",
      "Epoch 1 | Step 1 | Loss: 0.54167 | Acc: 0.78906\n",
      "Epoch 2 | Step 1 | Loss: 0.43056 | Acc: 0.98047\n",
      "Epoch 3 | Step 1 | Loss: 0.41146 | Acc: 0.98438\n",
      "Epoch 4 | Step 1 | Loss: 0.47743 | Acc: 0.94531\n",
      "Epoch 5 | Step 1 | Loss:  0.4184 | Acc: 0.96875\n",
      "Epoch 6 | Step 1 | Loss: 0.40278 | Acc: 0.96875\n",
      "Epoch 7 | Step 1 | Loss: 0.31944 | Acc: 0.96484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 | Step 1 | Loss: 0.29861 | Acc: 0.95703\n",
      "Epoch 9 | Step 1 | Loss:  0.2309 | Acc: 0.98438\n",
      "Epoch 10 | Step 1 | Loss:    0.25 | Acc: 0.98047\n",
      "Epoch 11 | Step 1 | Loss: 0.35417 | Acc: 0.97656\n",
      "Epoch 12 | Step 1 | Loss: 0.25174 | Acc: 0.98047\n",
      "Epoch 13 | Step 1 | Loss: 0.27083 | Acc: 0.96875\n",
      "Epoch 14 | Step 1 | Loss: 0.27083 | Acc: 0.98828\n",
      "Epoch 15 | Step 1 | Loss: 0.24826 | Acc: 0.98047\n",
      "Epoch 16 | Step 1 | Loss: 0.35243 | Acc: 0.97266\n",
      "Epoch 17 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 18 | Step 1 | Loss: 0.22917 | Acc: 0.99219\n",
      "Epoch 19 | Step 1 | Loss: 0.23958 | Acc: 0.96875\n",
      "Epoch 20 | Step 1 | Loss: 0.27604 | Acc: 0.97656\n",
      "Epoch 21 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 22 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 23 | Step 1 | Loss: 0.27083 | Acc: 0.96484\n",
      "Epoch 24 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 25 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 26 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 27 | Step 1 | Loss:  0.2309 | Acc: 0.98438\n",
      "Epoch 28 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 29 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 30 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 31 | Step 1 | Loss: 0.25521 | Acc: 0.98047\n",
      "Epoch 32 | Step 1 | Loss: 0.25521 | Acc: 0.97266\n",
      "Epoch 33 | Step 1 | Loss: 0.21528 | Acc: 0.98438\n",
      "Epoch 34 | Step 1 | Loss: 0.23611 | Acc: 0.98438\n",
      "Epoch 35 | Step 1 | Loss: 0.22396 | Acc: 0.98828\n",
      "Epoch 36 | Step 1 | Loss: 0.25174 | Acc: 0.96875\n",
      "Epoch 37 | Step 1 | Loss: 0.22396 | Acc: 0.96875\n",
      "Epoch 38 | Step 1 | Loss: 0.22917 | Acc: 0.98438\n",
      "Epoch 39 | Step 1 | Loss:  0.2066 | Acc: 0.97656\n",
      "Epoch 40 | Step 1 | Loss: 0.22396 | Acc: 0.98047\n",
      "Epoch 41 | Step 1 | Loss: 0.35243 | Acc: 0.97266\n",
      "Epoch 42 | Step 1 | Loss: 0.24306 | Acc: 0.98047\n",
      "Epoch 43 | Step 1 | Loss: 0.23611 | Acc: 0.96875\n",
      "Epoch 44 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 45 | Step 1 | Loss: 0.21007 | Acc: 0.98828\n",
      "Epoch 46 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 47 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 48 | Step 1 | Loss: 0.20139 | Acc: 0.98828\n",
      "Epoch 49 | Step 1 | Loss: 0.21875 | Acc: 0.97266\n",
      "Epoch 50 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "Epoch 51 | Step 1 | Loss: 0.28299 | Acc: 0.96094\n",
      "Epoch 52 | Step 1 | Loss: 0.21528 | Acc: 0.98047\n",
      "Epoch 53 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 54 | Step 1 | Loss: 0.20833 | Acc: 0.99219\n",
      "Epoch 55 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 56 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 57 | Step 1 | Loss: 0.25174 | Acc: 0.96094\n",
      "Epoch 58 | Step 1 | Loss: 0.24653 | Acc: 0.98047\n",
      "Epoch 59 | Step 1 | Loss: 0.20313 | Acc: 0.99219\n",
      "Epoch 60 | Step 1 | Loss: 0.33333 | Acc: 0.98828\n",
      "Epoch 61 | Step 1 | Loss: 0.22917 | Acc: 0.97656\n",
      "Epoch 62 | Step 1 | Loss:  0.2691 | Acc: 0.96094\n",
      "Epoch 63 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 64 | Step 1 | Loss: 0.22743 | Acc: 0.97656\n",
      "Epoch 65 | Step 1 | Loss:    0.25 | Acc: 0.97656\n",
      "Epoch 66 | Step 1 | Loss: 0.25868 | Acc: 0.96484\n",
      "Epoch 67 | Step 1 | Loss: 0.24479 | Acc: 0.95312\n",
      "Epoch 68 | Step 1 | Loss: 0.27604 | Acc: 0.97266\n",
      "Epoch 69 | Step 1 | Loss: 0.21528 | Acc: 0.96875\n",
      "Epoch 70 | Step 1 | Loss: 0.24826 | Acc: 0.96484\n",
      "Epoch 71 | Step 1 | Loss:  0.2309 | Acc: 0.96875\n",
      "Epoch 72 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 73 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 74 | Step 1 | Loss: 0.23264 | Acc: 0.97266\n",
      "Epoch 75 | Step 1 | Loss: 0.21528 | Acc: 0.97656\n",
      "Epoch 76 | Step 1 | Loss: 0.21701 | Acc: 0.98047\n",
      "Epoch 77 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 78 | Step 1 | Loss: 0.27604 | Acc: 0.95703\n",
      "Epoch 79 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 80 | Step 1 | Loss: 0.25694 | Acc: 0.96094\n",
      "Epoch 81 | Step 1 | Loss: 0.22743 | Acc: 0.97656\n",
      "Epoch 82 | Step 1 | Loss: 0.27083 | Acc: 0.96094\n",
      "Epoch 83 | Step 1 | Loss: 0.22222 | Acc: 0.98438\n",
      "Epoch 84 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 85 | Step 1 | Loss: 0.22049 | Acc: 0.98047\n",
      "Epoch 86 | Step 1 | Loss: 0.28819 | Acc: 0.96094\n",
      "Epoch 87 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 88 | Step 1 | Loss: 0.25521 | Acc: 0.96484\n",
      "Epoch 89 | Step 1 | Loss: 0.26563 | Acc: 0.98047\n",
      "Epoch 90 | Step 1 | Loss: 0.28125 | Acc: 0.96484\n",
      "Epoch 91 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 92 | Step 1 | Loss: 0.22396 | Acc: 0.97656\n",
      "Epoch 93 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 94 | Step 1 | Loss: 0.22222 | Acc: 0.97656\n",
      "Epoch 95 | Step 1 | Loss: 0.22569 | Acc: 0.97266\n",
      "Epoch 96 | Step 1 | Loss: 0.25521 | Acc: 0.96094\n",
      "Epoch 97 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 98 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 99 | Step 1 | Loss: 0.21528 | Acc: 0.98828\n",
      "Epoch 100 | Step 1 | Loss: 0.25347 | Acc: 0.97266\n",
      "Epoch 101 | Step 1 | Loss: 0.22569 | Acc: 0.98047\n",
      "Epoch 102 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 103 | Step 1 | Loss: 0.25868 | Acc: 0.96484\n",
      "Epoch 104 | Step 1 | Loss: 0.24306 | Acc: 0.97266\n",
      "Epoch 105 | Step 1 | Loss: 0.22396 | Acc: 0.97266\n",
      "Epoch 106 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 107 | Step 1 | Loss:  0.3125 | Acc: 0.99219\n",
      "Epoch 108 | Step 1 | Loss: 0.24479 | Acc: 0.97266\n",
      "Epoch 109 | Step 1 | Loss: 0.36806 | Acc: 0.97656\n",
      "Epoch 110 | Step 1 | Loss: 0.21701 | Acc: 0.98438\n",
      "Epoch 111 | Step 1 | Loss: 0.27083 | Acc: 0.96484\n",
      "Epoch 112 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 113 | Step 1 | Loss: 0.23438 | Acc: 0.98047\n",
      "Epoch 114 | Step 1 | Loss: 0.24653 | Acc: 0.98047\n",
      "Epoch 115 | Step 1 | Loss:    0.25 | Acc: 0.96484\n",
      "Epoch 116 | Step 1 | Loss: 0.26042 | Acc: 0.96875\n",
      "Epoch 117 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 118 | Step 1 | Loss:  0.2066 | Acc: 0.98047\n",
      "Epoch 119 | Step 1 | Loss: 0.24826 | Acc: 0.97266\n",
      "Epoch 120 | Step 1 | Loss: 0.23264 | Acc: 0.98047\n",
      "Epoch 121 | Step 1 | Loss: 0.26042 | Acc: 0.97266\n",
      "Epoch 122 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 123 | Step 1 | Loss: 0.23785 | Acc: 0.98047\n",
      "Epoch 124 | Step 1 | Loss: 0.21354 | Acc: 0.98047\n",
      "Epoch 125 | Step 1 | Loss:  0.2309 | Acc: 0.98438\n",
      "Epoch 126 | Step 1 | Loss: 0.22743 | Acc: 0.98047\n",
      "Epoch 127 | Step 1 | Loss: 0.26563 | Acc: 0.96875\n",
      "Epoch 128 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 129 | Step 1 | Loss: 0.26215 | Acc: 0.96484\n",
      "Epoch 130 | Step 1 | Loss: 0.23611 | Acc: 0.97656\n",
      "Epoch 131 | Step 1 | Loss: 0.24653 | Acc: 0.97656\n",
      "Epoch 132 | Step 1 | Loss: 0.25174 | Acc: 0.97266\n",
      "Epoch 133 | Step 1 | Loss: 0.26215 | Acc: 0.95703\n",
      "Epoch 134 | Step 1 | Loss: 0.25174 | Acc: 0.97266\n",
      "Epoch 135 | Step 1 | Loss: 0.24653 | Acc: 0.97266\n",
      "Epoch 136 | Step 1 | Loss:    0.25 | Acc: 0.96875\n",
      "Epoch 137 | Step 1 | Loss: 0.21875 | Acc: 0.99219\n",
      "Epoch 138 | Step 1 | Loss: 0.25174 | Acc: 0.97656\n",
      "Epoch 139 | Step 1 | Loss: 0.25868 | Acc: 0.96094\n",
      "Epoch 140 | Step 1 | Loss: 0.24306 | Acc: 0.97656\n",
      "Epoch 141 | Step 1 | Loss: 0.26736 | Acc: 0.96875\n",
      "Epoch 142 | Step 1 | Loss: 0.23958 | Acc: 0.98047\n",
      "Epoch 143 | Step 1 | Loss: 0.22569 | Acc: 0.97266\n",
      "Epoch 144 | Step 1 | Loss: 0.23785 | Acc: 0.97656\n",
      "Epoch 145 | Step 1 | Loss: 0.22917 | Acc: 0.98047\n",
      "Epoch 146 | Step 1 | Loss: 0.23958 | Acc: 0.97656\n",
      "Epoch 147 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "Epoch 148 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 149 | Step 1 | Loss: 0.21701 | Acc: 0.99219\n",
      "Epoch 150 | Step 1 | Loss: 0.26563 | Acc: 0.97266\n",
      "Epoch 151 | Step 1 | Loss:  0.2309 | Acc: 0.97656\n",
      "Epoch 152 | Step 1 | Loss: 0.19965 | Acc: 0.99219\n",
      "Epoch 153 | Step 1 | Loss: 0.24306 | Acc: 0.98438\n",
      "Epoch 154 | Step 1 | Loss: 0.21701 | Acc: 0.98828\n",
      "Epoch 155 | Step 1 | Loss: 0.27083 | Acc: 0.96875\n",
      "Epoch 156 | Step 1 | Loss: 0.21354 | Acc: 0.96875\n",
      "Epoch 157 | Step 1 | Loss:  0.2309 | Acc: 0.98047\n",
      "Epoch 158 | Step 1 | Loss: 0.20139 | Acc: 0.98438\n",
      "Epoch 159 | Step 1 | Loss: 0.23958 | Acc: 0.97266\n",
      "Epoch 160 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 161 | Step 1 | Loss: 0.20833 | Acc: 0.98828\n",
      "Epoch 162 | Step 1 | Loss:    0.25 | Acc: 0.97266\n",
      "Epoch 163 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 164 | Step 1 | Loss: 0.21701 | Acc: 0.97656\n",
      "Epoch 165 | Step 1 | Loss: 0.22743 | Acc: 0.98828\n",
      "Epoch 166 | Step 1 | Loss: 0.22569 | Acc: 0.97656\n",
      "Epoch 167 | Step 1 | Loss: 0.23264 | Acc: 0.97656\n",
      "Epoch 168 | Step 1 | Loss: 0.27083 | Acc: 0.96484\n",
      "Epoch 169 | Step 1 | Loss: 0.20833 | Acc: 0.98438\n",
      "Epoch 170 | Step 1 | Loss: 0.24132 | Acc: 0.96484\n",
      "Epoch 171 | Step 1 | Loss: 0.24306 | Acc: 0.96484\n",
      "Epoch 172 | Step 1 | Loss: 0.26215 | Acc: 0.96094\n",
      "Epoch 173 | Step 1 | Loss: 0.23785 | Acc: 0.97266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174 | Step 1 | Loss: 0.25174 | Acc: 0.96484\n",
      "Epoch 175 | Step 1 | Loss: 0.34201 | Acc: 0.98438\n",
      "Epoch 176 | Step 1 | Loss:    0.25 | Acc: 0.95703\n",
      "Epoch 177 | Step 1 | Loss: 0.21354 | Acc: 0.98828\n",
      "Epoch 178 | Step 1 | Loss: 0.25868 | Acc: 0.95703\n",
      "Epoch 179 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 180 | Step 1 | Loss: 0.19618 | Acc: 0.98047\n",
      "Epoch 181 | Step 1 | Loss: 0.27431 | Acc: 0.96875\n",
      "Epoch 182 | Step 1 | Loss:  0.1875 | Acc: 0.99219\n",
      "Epoch 183 | Step 1 | Loss: 0.32813 | Acc: 0.98047\n",
      "Epoch 184 | Step 1 | Loss: 0.22743 | Acc: 0.97656\n",
      "Epoch 185 | Step 1 | Loss: 0.24826 | Acc: 0.97656\n",
      "Epoch 186 | Step 1 | Loss: 0.22222 | Acc: 0.98047\n",
      "Epoch 187 | Step 1 | Loss: 0.21528 | Acc: 0.98438\n",
      "Epoch 188 | Step 1 | Loss: 0.37153 | Acc: 0.96875\n",
      "Epoch 189 | Step 1 | Loss: 0.19618 | Acc: 0.98438\n",
      "Epoch 190 | Step 1 | Loss: 0.26389 | Acc: 0.96484\n",
      "Epoch 191 | Step 1 | Loss:  0.2309 | Acc: 0.97266\n",
      "Epoch 192 | Step 1 | Loss: 0.26563 | Acc: 0.96484\n",
      "Epoch 193 | Step 1 | Loss: 0.23438 | Acc: 0.97266\n",
      "Epoch 194 | Step 1 | Loss: 0.24132 | Acc: 0.97656\n",
      "Epoch 195 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 196 | Step 1 | Loss: 0.24826 | Acc: 0.96875\n",
      "Epoch 197 | Step 1 | Loss: 0.23264 | Acc: 0.96484\n",
      "Epoch 198 | Step 1 | Loss: 0.25174 | Acc: 0.96484\n",
      "Epoch 199 | Step 1 | Loss: 0.24306 | Acc: 0.96875\n",
      "Epoch 200 | Step 1 | Loss: 0.24132 | Acc: 0.96875\n",
      "\n",
      "Accuracy: 0.9725 | MSE Loss: 0.25597997170726655\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2650270541141402, 0.9710000000000001)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_elivagar_circuits(\n",
    "    circuits_save_dir, dataset, encoding_type, \n",
    "    num_data_reps, device_name, num_epochs, batch_size,\n",
    "    num_qubits, num_meas_qubits,\n",
    "    num_data_for_rep_cap, num_params_for_rep_cap,\n",
    "    num_cdcs, num_candidates_per_circ,\n",
    "    num_circs, num_runs_per_circ, noise_importance,\n",
    "    save_dir, learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b8e38",
   "metadata": {},
   "source": [
    "## Evaluate circuit on target device\n",
    "\n",
    "Finally, we can evaluate the circuit on our target hardware, the OQC Lucy device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df687f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "elivagar_venv",
   "language": "python",
   "name": "elivagar_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
