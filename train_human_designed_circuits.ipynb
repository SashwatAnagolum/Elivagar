{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
      "Collecting torchquantum\n",
      "  Using cached torchquantum-0.1.4-py3-none-any.whl (119 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (3.10.0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.36.2)\n",
      "Collecting tqdm>=4.56.0\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: pathos>=0.2.7 in /usr/local/lib/python3.7/site-packages (from torchquantum) (0.2.7)\n",
      "Collecting numpy>=1.19.2\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting matplotlib>=3.3.2\n",
      "  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Collecting pylatexenc>=2.10\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Collecting qiskit==0.32.1\n",
      "  Using cached qiskit-0.32.1-py3-none-any.whl\n",
      "Collecting torchvision>=0.9.0.dev20210130\n",
      "  Using cached torchvision-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (24.3 MB)\n",
      "Collecting torchpack>=0.3.0\n",
      "  Using cached torchpack-0.3.1-py3-none-any.whl (34 kB)\n",
      "Collecting qiskit-terra==0.18.3\n",
      "  Using cached qiskit_terra-0.18.3-cp37-cp37m-manylinux2010_x86_64.whl (6.1 MB)\n",
      "Collecting qiskit-aqua==0.9.5\n",
      "  Using cached qiskit_aqua-0.9.5-py3-none-any.whl (2.1 MB)\n",
      "Collecting qiskit-aer==0.9.1\n",
      "  Using cached qiskit_aer-0.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.9 MB)\n",
      "Collecting qiskit-ignis==0.6.0\n",
      "  Using cached qiskit_ignis-0.6.0-py3-none-any.whl (207 kB)\n",
      "Collecting qiskit-ibmq-provider==0.18.1\n",
      "  Using cached qiskit_ibmq_provider-0.18.1-py3-none-any.whl (237 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aer==0.9.1->qiskit==0.32.1->torchquantum) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (0.23.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.1.0)\n",
      "Collecting quandl\n",
      "  Using cached Quandl-3.7.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting dlx<=1.0.4\n",
      "  Using cached dlx-1.0.4-py3-none-any.whl\n",
      "Collecting fastdtw<=0.3.4\n",
      "  Using cached fastdtw-0.3.4-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting yfinance>=0.1.62\n",
      "  Using cached yfinance-0.1.84-py2.py3-none-any.whl (29 kB)\n",
      "Collecting docplex>=2.21.207\n",
      "  Using cached docplex-2.23.222-py3-none-any.whl\n",
      "Requirement already satisfied: retworkx>=0.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (0.11.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (5.7.2)\n",
      "Requirement already satisfied: h5py<3.3.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2.10.0)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.10.1)\n",
      "Requirement already satisfied: websocket-client>=1.0.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.4.1)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.24.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.8.2)\n",
      "Collecting jsonschema>=2.6\n",
      "  Using cached jsonschema-4.17.0-py3-none-any.whl (83 kB)\n",
      "Collecting fastjsonschema>=2.10\n",
      "  Using cached fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Collecting python-constraint>=1.4\n",
      "  Using cached python_constraint-1.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: symengine>0.7 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (0.9.2)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (0.3.3)\n",
      "Requirement already satisfied: tweedledum<2.0,>=1.1 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (1.1.1)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (3.11)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from docplex>=2.21.207->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.16.0)\n",
      "Collecting pkgutil-resolve-name>=1.3.10\n",
      "  Using cached pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (4.5.0)\n",
      "Collecting importlib-resources>=1.4.0\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (21.2.0)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.19.2-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (3.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (20.9)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (2.4.7)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (0.2.9)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (0.70.11.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2021.5.30)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.20)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2.1.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/site-packages (from sympy>=1.3->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.2.1)\n",
      "Collecting loguru\n",
      "  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (0.10.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (5.4.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (2.5.0)\n",
      "Collecting tensorpack\n",
      "  Using cached tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
      "Collecting multimethod\n",
      "  Using cached multimethod-1.9-py3-none-any.whl (10 kB)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting requests>=2.19\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/site-packages (from yfinance>=0.1.62->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.4.4)\n",
      "Collecting lxml>=4.5.1\n",
      "  Using cached lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2021.1)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Collecting inflection>=0.3.1\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.30.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.10.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.4.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.38.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.0.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.17.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (3.1.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (0.8.9)\n",
      "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (23.2.1)\n",
      "Collecting msgpack>=0.5.2\n",
      "  Using cached msgpack-1.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (1.1.0)\n",
      "Collecting msgpack-numpy>=0.4.4.2\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pyrsistent, pkgutil-resolve-name, numpy, importlib-resources, charset-normalizer, requests, python-constraint, nvidia-cublas-cu11, jsonschema, fastjsonschema, qiskit-terra, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, multitasking, msgpack, more-itertools, lxml, inflection, yfinance, tqdm, torch, quandl, qiskit-ignis, msgpack-numpy, fastdtw, docplex, dlx, torchvision, tensorpack, qiskit-ibmq-provider, qiskit-aqua, qiskit-aer, multimethod, loguru, kiwisolver, fonttools, cycler, torchpack, qiskit, pylatexenc, matplotlib, torchquantum\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: qiskit-terra\n",
      "    Found existing installation: qiskit-terra 0.21.2\n",
      "    Uninstalling qiskit-terra-0.21.2:\n",
      "      Successfully uninstalled qiskit-terra-0.21.2\n",
      "  Attempting uninstall: qiskit-ibmq-provider\n",
      "    Found existing installation: qiskit-ibmq-provider 0.19.2\n",
      "    Uninstalling qiskit-ibmq-provider-0.19.2:\n",
      "      Successfully uninstalled qiskit-ibmq-provider-0.19.2\n",
      "  Attempting uninstall: qiskit-aer\n",
      "    Found existing installation: qiskit-aer 0.11.0\n",
      "    Uninstalling qiskit-aer-0.11.0:\n",
      "      Successfully uninstalled qiskit-aer-0.11.0\n",
      "  Attempting uninstall: qiskit\n",
      "    Found existing installation: qiskit 0.38.0\n",
      "    Uninstalling qiskit-0.38.0:\n",
      "      Successfully uninstalled qiskit-0.38.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu 2.3.2 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\n",
      "mthree 1.1.0 requires qiskit-ibmq-provider>=0.19.2, but you have qiskit-ibmq-provider 0.18.1 which is incompatible.\n",
      "mthree 1.1.0 requires qiskit-terra>=0.21, but you have qiskit-terra 0.18.3 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1 cycler-0.11.0 dlx-1.0.4 docplex-2.23.222 fastdtw-0.3.4 fastjsonschema-2.16.2 fonttools-4.38.0 importlib-resources-5.10.0 inflection-0.5.1 jsonschema-4.17.0 kiwisolver-1.4.4 loguru-0.6.0 lxml-4.9.1 matplotlib-3.5.3 more-itertools-9.0.0 msgpack-1.0.4 msgpack-numpy-0.4.8 multimethod-1.9 multitasking-0.0.11 numpy-1.21.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pkgutil-resolve-name-1.3.10 pylatexenc-2.10 pyrsistent-0.19.2 python-constraint-1.4.0 qiskit-0.32.1 qiskit-aer-0.9.1 qiskit-aqua-0.9.5 qiskit-ibmq-provider-0.18.1 qiskit-ignis-0.6.0 qiskit-terra-0.18.3 quandl-3.7.0 requests-2.28.1 tensorpack-0.11 torch-1.13.0 torchpack-0.3.1 torchquantum-0.1.4 torchvision-0.14.0 tqdm-4.64.1 yfinance-0.1.84\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchquantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.26.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 26.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pennylane-qiskit\n",
      "  Using cached PennyLane_qiskit-0.24.0-py3-none-any.whl (34 kB)\n",
      "Collecting qiskit\n",
      "  Using cached qiskit-0.38.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pennylane) (1.18.5)\n",
      "Collecting autograd\n",
      "  Using cached autograd-1.4-py3-none-any.whl (48 kB)\n",
      "Collecting retworkx\n",
      "  Using cached retworkx-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Collecting networkx\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting autoray>=0.3.1\n",
      "  Using cached autoray-0.3.2-py3-none-any.whl (36 kB)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting semantic-version>=2.7\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pennylane-lightning>=0.26\n",
      "  Downloading PennyLane_Lightning-0.26.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from pennylane) (1.5.2)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/site-packages (from pennylane) (4.2.2)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
      "Collecting mthree>=0.17\n",
      "  Downloading mthree-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 84.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting qiskit-aer==0.11.0\n",
      "  Using cached qiskit_aer-0.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "Collecting qiskit-ibmq-provider==0.19.2\n",
      "  Using cached qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
      "Collecting qiskit-terra==0.21.2\n",
      "  Using cached qiskit_terra-0.21.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.25.11)\n",
      "Collecting websocket-client>=1.0.1\n",
      "  Using cached websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
      "Collecting websockets>=10.0\n",
      "  Using cached websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
      "Collecting requests-ntlm>=1.1.0\n",
      "  Using cached requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (0.3.3)\n",
      "Collecting stevedore>=3.0.0\n",
      "  Using cached stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "Collecting tweedledum<2.0,>=1.1\n",
      "  Using cached tweedledum-1.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (943 kB)\n",
      "Collecting shared-memory38\n",
      "  Using cached shared_memory38-0.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (25 kB)\n",
      "Collecting sympy>=1.3\n",
      "  Using cached sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (3.10.0.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (5.7.2)\n",
      "Collecting ply>=3.10\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Collecting symengine>=0.9\n",
      "  Using cached symengine-0.9.2-cp37-cp37m-manylinux2010_x86_64.whl (37.5 MB)\n",
      "Collecting cython>=0.29\n",
      "  Using cached Cython-0.29.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Collecting orjson>=3.0.0\n",
      "  Using cached orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2.10)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (3.4.7)\n",
      "Collecting ntlm-auth>=1.0.2\n",
      "  Using cached ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (2.20)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Using cached pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/site-packages (from stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (3.4.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Collecting future>=0.15.2\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Installing collected packages: pbr, mpmath, tweedledum, sympy, symengine, stevedore, shared-memory38, retworkx, ply, ntlm-auth, websockets, websocket-client, requests-ntlm, qiskit-terra, ninja, future, toml, semantic-version, qiskit-ibmq-provider, qiskit-aer, pennylane-lightning, orjson, networkx, cython, autoray, autograd, appdirs, qiskit, pennylane, mthree, pennylane-qiskit\n",
      "Successfully installed appdirs-1.4.4 autograd-1.4 autoray-0.3.2 cython-0.29.32 future-0.18.2 mpmath-1.2.1 mthree-1.1.0 networkx-2.6.3 ninja-1.10.2.3 ntlm-auth-1.5.0 orjson-3.8.0 pbr-5.10.0 pennylane-0.26.0 pennylane-lightning-0.26.0 pennylane-qiskit-0.24.0 ply-3.11 qiskit-0.38.0 qiskit-aer-0.11.0 qiskit-ibmq-provider-0.19.2 qiskit-terra-0.21.2 requests-ntlm-1.1.0 retworkx-0.11.0 semantic-version-2.10.0 shared-memory38-0.1.2 stevedore-3.5.0 symengine-0.9.2 sympy-1.10.1 toml-0.10.2 tweedledum-1.1.1 websocket-client-1.4.1 websockets-10.3\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane pennylane-qiskit qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import create_gate_circs\n",
    "import train_circ\n",
    "\n",
    "reload(datasets)\n",
    "reload(create_gate_circs)\n",
    "reload(train_circ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_supernet_circ_into_gate_circ(subnet, num_embeds, layer_rots, layer_cnots, num_qubits, angle_embed=False):\n",
    "    circ_gates = []\n",
    "    gate_params = []\n",
    "    weights_bounds = [0]\n",
    "    inputs_bounds = [0]\n",
    "\n",
    "    curr_layers = subnet\n",
    "    num_cnots = len(layer_cnots)\n",
    "    \n",
    "    for i in range(1):\n",
    "        num_qubits = len(layer_rots[0])\n",
    "\n",
    "        if angle_embed:\n",
    "            rots = ['ry', 'rx', 'rz']\n",
    "            \n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += [rots[j % 3] for i in range(num_qubits)]\n",
    "                gate_params += [[i] for i in range(num_qubits)]\n",
    "                weights_bounds += [0 for i in range(num_qubits)]\n",
    "                inputs_bounds += [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]            \n",
    "        else:\n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += ['h' for i in range(num_qubits)] + ['ry' for i in range(num_qubits)] + ['cry' for i in range(num_qubits - 1)]\n",
    "                gate_params += [[i] for i in range(num_qubits)] * 2 + [[i, i + 1] for i in range(num_qubits - 1)]\n",
    "                weights_bounds += [0 for i in range(3 * num_qubits - 1)]\n",
    "                inputs_bounds += [inputs_bounds[-1] for i in range(num_qubits)] + [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]\n",
    "\n",
    "        for j in range(len(curr_layers)):\n",
    "            circ_gates += layer_rots[curr_layers[j] // num_cnots]\n",
    "            circ_gates += ['cx' for k in layer_cnots[curr_layers[j] % num_cnots]]\n",
    "\n",
    "            gate_params += [[k] for k in range(len(layer_rots[curr_layers[j] // num_cnots]))]\n",
    "            gate_params += layer_cnots[curr_layers[j] % num_cnots]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] + k + 1 for k in range(num_qubits)]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(num_qubits)]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] for k in range(len(layer_cnots[curr_layers[j] % num_cnots]))]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(len(layer_cnots[curr_layers[j] % num_cnots]))]\n",
    "            \n",
    "    return circ_gates, gate_params, inputs_bounds, weights_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-10-02 14:00:41,708: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101765  32307 129433] 0.2975 1.9897984790802001\n",
      "[290807 211609  61758] 0.25 2.105005216598511\n",
      "[284107 307257 260656] 0.2375 2.1334425449371337\n",
      "[ 28323  29580 131678] 0.2825 2.080800714492798\n",
      "[318774 311374   7658] 0.215 2.191361122131348\n",
      "[144752  17549  69071] 0.2825 1.990298776626587\n",
      "[147166  69454 126891] 0.2625 2.067657699584961\n",
      "[  5875 136782 275953] 0.215 2.174487619400024\n",
      "[149329 157548 331568] 0.2675 2.0413222217559817\n",
      "[ 25467 242967 147419] 0.23 2.086214056015015\n",
      "[212848  95199  97713] 0.22 2.0612947082519533\n",
      "[136552  68778 318672] 0.2225 2.1677363872528077\n",
      "[ 34573 295443  31216] 0.255 2.0048794460296633\n",
      "[286148  71361 189745] 0.2975 2.0056074237823487\n",
      "[270468 275639 312131] 0.27 2.0485662174224855\n",
      "[121336 317720 130520] 0.275 2.163728380203247\n",
      "[282998 285747  66468] 0.23 2.0393245124816897\n",
      "[245839 301068 294313] 0.2025 2.228119926452637\n",
      "[157512  83255 171482] 0.19 2.0555042266845702\n",
      "[215744  59488 125146] 0.2325 2.106196060180664\n",
      "[  3699 160246  49896] 0.23 2.1106607723236084\n",
      "[101690 212100 115658] 0.285 2.0500304889678955\n",
      "[ 89078 207948 219074] 0.255 2.0655835819244386\n",
      "[ 68587 188287  94808] 0.285 2.0301205921173096\n",
      "[206454 106520 129502] 0.27 2.115344467163086\n",
      "[244080 218828   9792] 0.2225 2.1732326412200926\n",
      "[ 86058 118647  15563] 0.25 2.0801628112792967\n",
      "[24702 46905 65480] 0.2375 2.0423597526550292\n",
      "[301167 201820 208692] 0.2475 2.127601957321167\n",
      "[325553  58441 217604] 0.2675 2.2117638969421387\n",
      "[176110 172081 144403] 0.2475 2.0783420753479005\n",
      "[317377 287034 317060] 0.225 2.093519582748413\n",
      "[100907  38469 290535] 0.2225 2.151331958770752\n",
      "[ 99222 324190  32956] 0.21 2.11372163772583\n",
      "[229057 117147 216266] 0.35 1.9747258949279785\n",
      "[172673 177942  35129] 0.2775 2.0289963245391847\n",
      "[232695 146196 249853] 0.2675 2.07059494972229\n",
      "[132622 116333 276228] 0.255 1.990548667907715\n",
      "[229136 309371 328070] 0.2375 2.1674084758758543\n",
      "[122513  83065 326088] 0.23 2.003282079696655\n",
      "[259233 225281  81993] 0.3075 2.0609090614318846\n",
      "[  4405 258380 248095] 0.2575 2.0642697048187255\n",
      "[208692 253864  82857] 0.2475 2.105523920059204\n",
      "[  7285 158686 248810] 0.175 2.3712709045410154\n",
      "[245087 252245 217911] 0.1875 2.2410715103149412\n",
      "[ 85461 217475  42233] 0.3075 1.982060775756836\n",
      "[ 52320  56164 297358] 0.2875 2.091214294433594\n",
      "[191793  11504 110170] 0.2525 2.0641166877746584\n",
      "[108298 119655 326083] 0.215 2.177182741165161\n",
      "[184773 294371 326258] 0.24 2.095335292816162\n",
      "[112696 225547  44115] 0.215 2.056638059616089\n",
      "[ 68753 219452 298155] 0.2475 2.0791483688354493\n",
      "[243194 331311 281686] 0.3425 1.9695103549957276\n",
      "[248215 154702  26163] 0.27 2.0559831142425535\n",
      "[248107  72992 107444] 0.285 2.011388072967529\n",
      "[297812  37753 187540] 0.285 2.127917251586914\n",
      "[ 98224 102696  10796] 0.27 2.0775726890563964\n",
      "[329844  94977 136447] 0.2625 2.1806453037261964\n",
      "[167784 247040  60531] 0.26 2.086672697067261\n",
      "[ 81406 223794  57889] 0.2475 2.118234004974365\n",
      "[ 11214 295691 227887] 0.23 2.107911615371704\n",
      "[  5923 112831 267821] 0.1825 2.156280908584595\n",
      "[113677   6699   8230] 0.2025 2.188867950439453\n",
      "[269487 284326 125899] 0.2875 2.037848472595215\n",
      "[114158  94414 233117] 0.24 2.4130760192871095\n",
      "[265400  36757 176652] 0.265 2.1151137161254883\n",
      "[324966  66987 306092] 0.2 2.1295133781433107\n",
      "[187689 312494 237777] 0.235 2.1434712505340574\n",
      "[317498 162756 293564] 0.245 2.0002216625213625\n",
      "[  1382 293270  44483] 0.205 2.086059446334839\n",
      "[ 89313 150893 288042] 0.2625 2.044004564285278\n",
      "[  5912 245652  47237] 0.3 2.083586645126343\n",
      "[270813  39402 310145] 0.2275 2.120466585159302\n",
      "[ 28043 252912  21648] 0.225 2.1591806983947754\n",
      "[ 87009 130693 167629] 0.2475 2.0659738636016844\n",
      "[158705 195534 291757] 0.2975 1.9985948848724364\n",
      "[249119 163929 241329] 0.2825 2.068026828765869\n",
      "[188230 242740 153802] 0.235 2.14009090423584\n",
      "[172307 142104 199972] 0.1675 2.181764631271362\n",
      "[231519 253309 318905] 0.23 2.230028581619263\n",
      "[220784  43594 236113] 0.235 2.0435609340667726\n",
      "[194688    792 224163] 0.2075 2.1111980724334716\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "\n",
    "dataset = 'fmnist_4'\n",
    "main_dir = './supernet/fmnist_4/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'supernet', 1)\n",
    "\n",
    "inds = np.random.choice(len(x_test), 400, False)\n",
    "x_test = x_test[inds]\n",
    "y_test = y_test[inds]\n",
    "\n",
    "num_qubits = 4\n",
    "num_cnot_configs = 4096\n",
    "num_circs = 2500\n",
    "num_embed_layers = 2\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "param_nums = [12, 16, 20, 24]\n",
    "\n",
    "for p in param_nums:\n",
    "    supernet_dir = main_dir + 'search-{}_params_mb'.format(p)\n",
    "\n",
    "    num_layers = p // num_qubits\n",
    "\n",
    "    layer_rots = open(supernet_dir + '/rotations.txt').read().split('\\n')[:-1]\n",
    "    layer_rots = [''.join([j for j in i if j.isupper()]) for i in layer_rots]\n",
    "    layer_rots = [[i[j * 2:j * 2 + 2].lower() for j in range(len(i) // 2)] for i in layer_rots]\n",
    "\n",
    "    layer_cnots = open(supernet_dir + '/cnots.txt').read().split('\\n')[:-1]\n",
    "    layer_cnots = [''.join([j for j in i[1:] if j not in ['[', ']', ',', ' ']]) for i in layer_cnots]\n",
    "    layer_cnots = [[[int(i[2 * j]), int(i[2 * j + 1])] for j in range(len(i) // 2)] for i in layer_cnots]\n",
    "        \n",
    "    search_space = len(layer_rots) * len(layer_cnots)\n",
    "        \n",
    "    accs = []\n",
    "    losses = []\n",
    "    circ_layers = []\n",
    "\n",
    "    params = np.genfromtxt(supernet_dir + '/training_params.txt')[-1].reshape((num_layers, search_space // num_cnot_configs, num_qubits))\n",
    "        \n",
    "    supernet_device_dir = supernet_dir + '/' + device_name\n",
    "    \n",
    "    if not os.path.exists(supernet_device_dir):\n",
    "        os.mkdir(supernet_device_dir)\n",
    "        \n",
    "    for i in range(num_circs):\n",
    "        curr_circ_desc = np.random.randint(0, search_space, num_layers)\n",
    "\n",
    "        curr_params = np.concatenate([params[k, curr_circ_desc[k] // num_cnot_configs] for k in range(num_layers)]).flatten()\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = convert_supernet_circ_into_gate_circ(curr_circ_desc, num_embed_layers, layer_rots, layer_cnots,\n",
    "                                                                                                          num_qubits, False) \n",
    "        \n",
    "        circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, [0, 1], 'exp')\n",
    "        \n",
    "        val_exp_list = []\n",
    "        \n",
    "        for j in range(len(x_test)):\n",
    "            val_exp_list.append(circ(x_test[j], curr_params))\n",
    "            \n",
    "        val_exps = np.array(val_exp_list)\n",
    "        val_losses = np.sum(np.power(val_exps - y_test, 2), 1)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "            \n",
    "        print(curr_circ_desc, acc, val_loss)\n",
    "\n",
    "        accs.append(acc)\n",
    "        losses.append(val_loss)\n",
    "        circ_layers.append(curr_circ_desc)\n",
    "        \n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_layers.txt', np.array(circ_layers))\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_accs.txt', accs)\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_losses.txt', losses)  \n",
    "#     np.savetxt(supernet_dir + '/searched_circ_accs.txt', ['{} {}'.format(circ_layers[i], accs[i]) for i in range(num_circs)], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import pennylane as qml\n",
    "\n",
    "reload(qml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-23 05:02:09.093894: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2022-09-23 05:02:09.094011: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2022-09-23 05:02:09.123053: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "(1600, 4, 4) (400, 4, 4)\n",
      "subnet: [11524, 82582, 153173], expert_idx: 0\n",
      "2022-09-23 05:02:21.814385: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 05:02:21.822319: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499995000 Hz\n",
      "2022-09-23 05:02:21.822786: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b8946b5d40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-09-23 05:02:21.822816: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-09-23 05:02:21.822950: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "[2022-09-23 05:02:21.948 tensorflow-2-3-cpu-p-ml-t3-2xlarge-3307c89dd45f659ec43fdf6423d3:26969 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-23 05:02:22.004 tensorflow-2-3-cpu-p-ml-t3-2xlarge-3307c89dd45f659ec43fdf6423d3:26969 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "subnet: [11524, 82582, 153173], loss: [1.06300064], acc: 0.53125, samples: 32\n",
      "subnet: [28402, 100634, 235164], expert_idx: 0\n",
      "subnet: [28402, 100634, 235164], loss: [1.01712463], acc: 0.59375, samples: 32\n",
      "subnet: [273614, 62424, 35156], expert_idx: 0\n",
      "subnet: [273614, 62424, 35156], loss: [1.04384316], acc: 0.5625, samples: 32\n",
      "subnet: [47141, 311830, 62882], expert_idx: 0\n",
      "subnet: [47141, 311830, 62882], loss: [0.88483833], acc: 0.625, samples: 32\n",
      "subnet: [174337, 194582, 287596], expert_idx: 0\n",
      "subnet: [174337, 194582, 287596], loss: [1.02092439], acc: 0.59375, samples: 32\n",
      "subnet: [57457, 21306, 260203], expert_idx: 0\n",
      "subnet: [57457, 21306, 260203], loss: [1.09811478], acc: 0.46875, samples: 32\n",
      "subnet: [114358, 204391, 258796], expert_idx: 0\n",
      "subnet: [114358, 204391, 258796], loss: [1.0930672], acc: 0.5625, samples: 32\n",
      "subnet: [8721, 327112, 217624], expert_idx: 0\n",
      "subnet: [8721, 327112, 217624], loss: [1.03014582], acc: 0.5625, samples: 32\n",
      "subnet: [209868, 204923, 54138], expert_idx: 0\n",
      "subnet: [209868, 204923, 54138], loss: [0.97027694], acc: 0.65625, samples: 32\n",
      "subnet: [61708, 310461, 9841], expert_idx: 0\n",
      "subnet: [61708, 310461, 9841], loss: [1.17679237], acc: 0.5, samples: 32\n",
      "subnet: [94772, 23360, 262950], expert_idx: 0\n",
      "subnet: [94772, 23360, 262950], loss: [1.09403707], acc: 0.5, samples: 32\n",
      "subnet: [152533, 77015, 113470], expert_idx: 0\n",
      "subnet: [152533, 77015, 113470], loss: [0.94749013], acc: 0.59375, samples: 32\n",
      "subnet: [279671, 98756, 97041], expert_idx: 0\n",
      "subnet: [279671, 98756, 97041], loss: [1.10200097], acc: 0.40625, samples: 32\n",
      "subnet: [204955, 236507, 310811], expert_idx: 0\n",
      "subnet: [204955, 236507, 310811], loss: [1.21804294], acc: 0.375, samples: 32\n",
      "subnet: [311160, 28684, 327511], expert_idx: 0\n",
      "subnet: [311160, 28684, 327511], loss: [1.07388153], acc: 0.46875, samples: 32\n",
      "subnet: [149232, 100827, 163036], expert_idx: 0\n",
      "subnet: [149232, 100827, 163036], loss: [1.28398645], acc: 0.375, samples: 32\n",
      "subnet: [61782, 307924, 169247], expert_idx: 0\n",
      "subnet: [61782, 307924, 169247], loss: [1.01891822], acc: 0.5625, samples: 32\n",
      "subnet: [278851, 170407, 77150], expert_idx: 0\n",
      "subnet: [278851, 170407, 77150], loss: [1.03590181], acc: 0.625, samples: 32\n",
      "subnet: [61875, 110585, 280988], expert_idx: 0\n",
      "subnet: [61875, 110585, 280988], loss: [1.07188491], acc: 0.4375, samples: 32\n",
      "subnet: [247757, 297695, 247090], expert_idx: 0\n",
      "subnet: [247757, 297695, 247090], loss: [0.99313435], acc: 0.625, samples: 32\n",
      "subnet: [45125, 245355, 63351], expert_idx: 0\n",
      "subnet: [45125, 245355, 63351], loss: [0.97696467], acc: 0.5625, samples: 32\n",
      "subnet: [278082, 81596, 114862], expert_idx: 0\n",
      "subnet: [278082, 81596, 114862], loss: [1.13965293], acc: 0.46875, samples: 32\n",
      "subnet: [260449, 72003, 148682], expert_idx: 0\n",
      "subnet: [260449, 72003, 148682], loss: [1.03743093], acc: 0.5, samples: 32\n",
      "subnet: [267260, 249125, 322754], expert_idx: 0\n",
      "subnet: [267260, 249125, 322754], loss: [0.99332914], acc: 0.53125, samples: 32\n",
      "subnet: [231506, 292418, 51526], expert_idx: 0\n",
      "subnet: [231506, 292418, 51526], loss: [1.06775502], acc: 0.46875, samples: 32\n",
      "subnet: [68893, 66372, 317890], expert_idx: 0\n",
      "subnet: [68893, 66372, 317890], loss: [0.87037621], acc: 0.625, samples: 32\n",
      "subnet: [172628, 249846, 15607], expert_idx: 0\n",
      "subnet: [172628, 249846, 15607], loss: [0.98039256], acc: 0.59375, samples: 32\n",
      "subnet: [147286, 297790, 304261], expert_idx: 0\n",
      "subnet: [147286, 297790, 304261], loss: [1.2016975], acc: 0.375, samples: 32\n",
      "subnet: [168113, 85895, 119282], expert_idx: 0\n",
      "subnet: [168113, 85895, 119282], loss: [0.95163121], acc: 0.59375, samples: 32\n",
      "subnet: [189934, 235909, 285148], expert_idx: 0\n",
      "subnet: [189934, 235909, 285148], loss: [0.96959841], acc: 0.6875, samples: 32\n",
      "subnet: [99915, 77504, 7222], expert_idx: 0\n",
      "subnet: [99915, 77504, 7222], loss: [1.22053291], acc: 0.40625, samples: 32\n",
      "subnet: [140684, 245757, 109296], expert_idx: 0\n",
      "subnet: [140684, 245757, 109296], loss: [1.11508148], acc: 0.40625, samples: 32\n",
      "subnet: [221947, 230732, 192680], expert_idx: 0\n",
      "subnet: [221947, 230732, 192680], loss: [0.96246354], acc: 0.59375, samples: 32\n",
      "subnet: [325514, 140659, 84408], expert_idx: 0\n",
      "subnet: [325514, 140659, 84408], loss: [1.13737365], acc: 0.3125, samples: 32\n",
      "subnet: [302469, 194907, 267860], expert_idx: 0\n",
      "subnet: [302469, 194907, 267860], loss: [1.15251784], acc: 0.46875, samples: 32\n",
      "subnet: [171339, 165998, 191273], expert_idx: 0\n",
      "subnet: [171339, 165998, 191273], loss: [0.84621404], acc: 0.625, samples: 32\n",
      "subnet: [189920, 1810, 166802], expert_idx: 0\n",
      "subnet: [189920, 1810, 166802], loss: [1.09208207], acc: 0.4375, samples: 32\n",
      "subnet: [295919, 215246, 167152], expert_idx: 0\n",
      "subnet: [295919, 215246, 167152], loss: [0.99255344], acc: 0.5625, samples: 32\n",
      "subnet: [44055, 201263, 297528], expert_idx: 0\n",
      "subnet: [44055, 201263, 297528], loss: [1.02716653], acc: 0.40625, samples: 32\n",
      "subnet: [160610, 237779, 135703], expert_idx: 0\n",
      "subnet: [160610, 237779, 135703], loss: [1.19046593], acc: 0.40625, samples: 32\n",
      "subnet: [331706, 255147, 241452], expert_idx: 0\n",
      "subnet: [331706, 255147, 241452], loss: [1.09115487], acc: 0.5625, samples: 32\n",
      "subnet: [74945, 4417, 153792], expert_idx: 0\n",
      "subnet: [74945, 4417, 153792], loss: [0.99335162], acc: 0.5, samples: 32\n",
      "subnet: [81444, 6138, 139019], expert_idx: 0\n",
      "subnet: [81444, 6138, 139019], loss: [1.0589893], acc: 0.5, samples: 32\n",
      "subnet: [195696, 325329, 191815], expert_idx: 0\n",
      "subnet: [195696, 325329, 191815], loss: [1.04826808], acc: 0.46875, samples: 32\n",
      "subnet: [24668, 290575, 55949], expert_idx: 0\n",
      "subnet: [24668, 290575, 55949], loss: [1.15069457], acc: 0.4375, samples: 32\n",
      "subnet: [116146, 268500, 133305], expert_idx: 0\n",
      "subnet: [116146, 268500, 133305], loss: [1.07124434], acc: 0.46875, samples: 32\n",
      "subnet: [83890, 59802, 60559], expert_idx: 0\n",
      "subnet: [83890, 59802, 60559], loss: [1.03803238], acc: 0.5625, samples: 32\n",
      "subnet: [219021, 262810, 317725], expert_idx: 0\n",
      "subnet: [219021, 262810, 317725], loss: [0.93126409], acc: 0.53125, samples: 32\n",
      "subnet: [170306, 61504, 104039], expert_idx: 0\n",
      "subnet: [170306, 61504, 104039], loss: [1.1270387], acc: 0.40625, samples: 32\n",
      "subnet: [63252, 63505, 249216], expert_idx: 0\n",
      "subnet: [63252, 63505, 249216], loss: [1.08243827], acc: 0.40625, samples: 32\n",
      "subnet: [26957, 34750, 294294], expert_idx: 0\n",
      "subnet: [26957, 34750, 294294], loss: [1.08744194], acc: 0.4375, samples: 32\n",
      "subnet: [67218, 147196, 308425], expert_idx: 0\n",
      "subnet: [67218, 147196, 308425], loss: [1.13484951], acc: 0.5, samples: 32\n",
      "subnet: [327878, 69943, 66727], expert_idx: 0\n",
      "subnet: [327878, 69943, 66727], loss: [1.03057598], acc: 0.5625, samples: 32\n",
      "subnet: [317154, 271920, 26344], expert_idx: 0\n",
      "subnet: [317154, 271920, 26344], loss: [1.11294854], acc: 0.40625, samples: 32\n",
      "subnet: [245780, 197218, 300513], expert_idx: 0\n",
      "subnet: [245780, 197218, 300513], loss: [1.02468355], acc: 0.53125, samples: 32\n",
      "subnet: [227317, 193594, 303970], expert_idx: 0\n",
      "subnet: [227317, 193594, 303970], loss: [1.11933815], acc: 0.5, samples: 32\n",
      "subnet: [205376, 178407, 94665], expert_idx: 0\n",
      "subnet: [205376, 178407, 94665], loss: [1.0114149], acc: 0.5625, samples: 32\n",
      "subnet: [139298, 62632, 14247], expert_idx: 0\n",
      "subnet: [139298, 62632, 14247], loss: [0.99001234], acc: 0.5625, samples: 32\n",
      "subnet: [156246, 239620, 165785], expert_idx: 0\n",
      "subnet: [156246, 239620, 165785], loss: [1.11590816], acc: 0.34375, samples: 32\n",
      "subnet: [159431, 98153, 81228], expert_idx: 0\n",
      "subnet: [159431, 98153, 81228], loss: [0.9055394], acc: 0.625, samples: 32\n",
      "subnet: [92529, 14301, 330851], expert_idx: 0\n",
      "subnet: [92529, 14301, 330851], loss: [1.1348744], acc: 0.40625, samples: 32\n",
      "subnet: [309812, 254161, 167571], expert_idx: 0\n",
      "subnet: [309812, 254161, 167571], loss: [1.11929066], acc: 0.4375, samples: 32\n",
      "subnet: [210092, 307530, 242886], expert_idx: 0\n",
      "subnet: [210092, 307530, 242886], loss: [0.93520226], acc: 0.71875, samples: 32\n",
      "subnet: [152235, 211439, 298934], expert_idx: 0\n",
      "subnet: [152235, 211439, 298934], loss: [1.29090425], acc: 0.4375, samples: 32\n",
      "subnet: [28441, 293507, 56195], expert_idx: 0\n",
      "subnet: [28441, 293507, 56195], loss: [1.0845405], acc: 0.53125, samples: 32\n",
      "subnet: [60196, 89226, 252230], expert_idx: 0\n",
      "subnet: [60196, 89226, 252230], loss: [1.12425862], acc: 0.5, samples: 32\n",
      "subnet: [90561, 140204, 263842], expert_idx: 0\n",
      "subnet: [90561, 140204, 263842], loss: [1.04395711], acc: 0.4375, samples: 32\n",
      "subnet: [172493, 193249, 68998], expert_idx: 0\n",
      "subnet: [172493, 193249, 68998], loss: [1.08243687], acc: 0.5, samples: 32\n",
      "subnet: [268808, 73732, 290610], expert_idx: 0\n",
      "subnet: [268808, 73732, 290610], loss: [1.12408153], acc: 0.46875, samples: 32\n",
      "subnet: [323639, 47399, 93992], expert_idx: 0\n",
      "subnet: [323639, 47399, 93992], loss: [1.18245274], acc: 0.375, samples: 32\n",
      "subnet: [203553, 92455, 234246], expert_idx: 0\n",
      "subnet: [203553, 92455, 234246], loss: [1.02039127], acc: 0.53125, samples: 32\n",
      "subnet: [105075, 310009, 74126], expert_idx: 0\n",
      "subnet: [105075, 310009, 74126], loss: [0.93844953], acc: 0.5, samples: 32\n",
      "subnet: [118850, 64544, 73229], expert_idx: 0\n",
      "subnet: [118850, 64544, 73229], loss: [0.97840421], acc: 0.53125, samples: 32\n",
      "subnet: [190369, 144866, 295824], expert_idx: 0\n",
      "subnet: [190369, 144866, 295824], loss: [1.19989339], acc: 0.34375, samples: 32\n",
      "subnet: [65758, 248996, 217712], expert_idx: 0\n",
      "subnet: [65758, 248996, 217712], loss: [1.02303787], acc: 0.5625, samples: 32\n",
      "subnet: [250967, 36925, 232504], expert_idx: 0\n",
      "subnet: [250967, 36925, 232504], loss: [1.06369446], acc: 0.5, samples: 32\n",
      "subnet: [52560, 49424, 33249], expert_idx: 0\n",
      "subnet: [52560, 49424, 33249], loss: [1.06626066], acc: 0.5625, samples: 32\n",
      "subnet: [114517, 149238, 120739], expert_idx: 0\n",
      "subnet: [114517, 149238, 120739], loss: [0.9765068], acc: 0.625, samples: 32\n",
      "subnet: [27380, 81243, 182467], expert_idx: 0\n",
      "subnet: [27380, 81243, 182467], loss: [0.89429041], acc: 0.625, samples: 32\n",
      "subnet: [320338, 24621, 292300], expert_idx: 0\n",
      "subnet: [320338, 24621, 292300], loss: [0.9958469], acc: 0.5625, samples: 32\n",
      "subnet: [309619, 178260, 137066], expert_idx: 0\n",
      "subnet: [309619, 178260, 137066], loss: [1.15002296], acc: 0.46875, samples: 32\n",
      "subnet: [11892, 259736, 84074], expert_idx: 0\n",
      "subnet: [11892, 259736, 84074], loss: [1.05337814], acc: 0.5, samples: 32\n",
      "subnet: [199965, 114279, 152358], expert_idx: 0\n",
      "subnet: [199965, 114279, 152358], loss: [1.16269656], acc: 0.40625, samples: 32\n",
      "subnet: [212807, 329106, 320836], expert_idx: 0\n",
      "subnet: [212807, 329106, 320836], loss: [1.11642656], acc: 0.4375, samples: 32\n",
      "subnet: [300548, 173976, 28629], expert_idx: 0\n",
      "subnet: [300548, 173976, 28629], loss: [1.02904337], acc: 0.53125, samples: 32\n",
      "subnet: [224220, 128508, 147934], expert_idx: 0\n",
      "subnet: [224220, 128508, 147934], loss: [0.90260007], acc: 0.71875, samples: 32\n",
      "subnet: [286132, 231076, 113563], expert_idx: 0\n",
      "subnet: [286132, 231076, 113563], loss: [0.92468961], acc: 0.53125, samples: 32\n",
      "subnet: [55763, 198449, 164954], expert_idx: 0\n",
      "subnet: [55763, 198449, 164954], loss: [1.18867951], acc: 0.34375, samples: 32\n",
      "subnet: [59155, 320845, 195632], expert_idx: 0\n",
      "subnet: [59155, 320845, 195632], loss: [1.1818851], acc: 0.375, samples: 32\n",
      "subnet: [29781, 95356, 280569], expert_idx: 0\n",
      "subnet: [29781, 95356, 280569], loss: [1.02904976], acc: 0.5, samples: 32\n",
      "subnet: [284755, 9844, 91416], expert_idx: 0\n",
      "subnet: [284755, 9844, 91416], loss: [0.95649152], acc: 0.65625, samples: 32\n",
      "subnet: [44364, 217971, 206297], expert_idx: 0\n",
      "subnet: [44364, 217971, 206297], loss: [1.10910024], acc: 0.34375, samples: 32\n",
      "subnet: [28922, 212263, 147520], expert_idx: 0\n",
      "subnet: [28922, 212263, 147520], loss: [1.11483955], acc: 0.46875, samples: 32\n",
      "subnet: [151939, 16482, 198101], expert_idx: 0\n",
      "subnet: [151939, 16482, 198101], loss: [1.19741005], acc: 0.40625, samples: 32\n",
      "subnet: [196880, 241970, 256401], expert_idx: 0\n",
      "subnet: [196880, 241970, 256401], loss: [1.07494975], acc: 0.5625, samples: 32\n",
      "subnet: [134653, 322780, 53798], expert_idx: 0\n",
      "subnet: [134653, 322780, 53798], loss: [1.06362413], acc: 0.5, samples: 32\n",
      "subnet: [138662, 60503, 172804], expert_idx: 0\n",
      "subnet: [138662, 60503, 172804], loss: [1.18862016], acc: 0.4375, samples: 32\n",
      "subnet: [195895, 6869, 2621], expert_idx: 0\n",
      "subnet: [195895, 6869, 2621], loss: [1.07555673], acc: 0.5625, samples: 32\n",
      "subnet: [245307, 78603, 248236], expert_idx: 0\n",
      "subnet: [245307, 78603, 248236], loss: [0.88974585], acc: 0.59375, samples: 32\n",
      "subnet: [52106, 178167, 52935], expert_idx: 0\n",
      "subnet: [52106, 178167, 52935], loss: [1.02032234], acc: 0.5, samples: 32\n",
      "subnet: [323140, 28906, 272525], expert_idx: 0\n",
      "subnet: [323140, 28906, 272525], loss: [1.06687815], acc: 0.46875, samples: 32\n",
      "subnet: [261440, 224049, 213725], expert_idx: 0\n",
      "subnet: [261440, 224049, 213725], loss: [1.10044706], acc: 0.4375, samples: 32\n",
      "subnet: [58880, 331295, 310245], expert_idx: 0\n",
      "subnet: [58880, 331295, 310245], loss: [1.3308503], acc: 0.3125, samples: 32\n",
      "subnet: [239160, 161540, 576], expert_idx: 0\n",
      "subnet: [239160, 161540, 576], loss: [1.14080626], acc: 0.4375, samples: 32\n",
      "subnet: [55469, 32694, 201374], expert_idx: 0\n",
      "subnet: [55469, 32694, 201374], loss: [0.98322455], acc: 0.59375, samples: 32\n",
      "subnet: [191152, 220710, 1593], expert_idx: 0\n",
      "subnet: [191152, 220710, 1593], loss: [1.0984726], acc: 0.46875, samples: 32\n",
      "subnet: [28721, 313990, 113680], expert_idx: 0\n",
      "subnet: [28721, 313990, 113680], loss: [1.24739369], acc: 0.34375, samples: 32\n",
      "subnet: [329851, 149394, 44086], expert_idx: 0\n",
      "subnet: [329851, 149394, 44086], loss: [0.97189479], acc: 0.65625, samples: 32\n",
      "subnet: [77438, 107018, 300553], expert_idx: 0\n",
      "subnet: [77438, 107018, 300553], loss: [0.98141738], acc: 0.46875, samples: 32\n",
      "subnet: [186948, 267838, 110], expert_idx: 0\n",
      "subnet: [186948, 267838, 110], loss: [1.13694115], acc: 0.46875, samples: 32\n",
      "subnet: [12038, 151817, 75678], expert_idx: 0\n",
      "subnet: [12038, 151817, 75678], loss: [1.02436787], acc: 0.53125, samples: 32\n",
      "subnet: [40347, 257416, 128745], expert_idx: 0\n",
      "subnet: [40347, 257416, 128745], loss: [1.22715331], acc: 0.4375, samples: 32\n",
      "subnet: [63117, 276995, 34363], expert_idx: 0\n",
      "subnet: [63117, 276995, 34363], loss: [1.04957838], acc: 0.4375, samples: 32\n",
      "subnet: [65274, 5000, 276450], expert_idx: 0\n",
      "subnet: [65274, 5000, 276450], loss: [0.87236665], acc: 0.78125, samples: 32\n",
      "subnet: [279273, 179927, 328153], expert_idx: 0\n",
      "subnet: [279273, 179927, 328153], loss: [1.17059358], acc: 0.375, samples: 32\n",
      "subnet: [201909, 160867, 298783], expert_idx: 0\n",
      "subnet: [201909, 160867, 298783], loss: [0.92867852], acc: 0.5625, samples: 32\n",
      "subnet: [55162, 63643, 330284], expert_idx: 0\n",
      "subnet: [55162, 63643, 330284], loss: [1.17904873], acc: 0.375, samples: 32\n",
      "subnet: [107801, 279715, 130334], expert_idx: 0\n",
      "subnet: [107801, 279715, 130334], loss: [1.12432151], acc: 0.53125, samples: 32\n",
      "subnet: [740, 83994, 257091], expert_idx: 0\n",
      "subnet: [740, 83994, 257091], loss: [1.10491435], acc: 0.40625, samples: 32\n",
      "subnet: [60335, 10492, 317367], expert_idx: 0\n",
      "subnet: [60335, 10492, 317367], loss: [1.05540106], acc: 0.46875, samples: 32\n",
      "subnet: [47051, 97751, 63473], expert_idx: 0\n",
      "subnet: [47051, 97751, 63473], loss: [1.0960155], acc: 0.4375, samples: 32\n",
      "subnet: [282418, 198868, 270187], expert_idx: 0\n",
      "subnet: [282418, 198868, 270187], loss: [1.22995982], acc: 0.4375, samples: 32\n",
      "subnet: [64354, 286096, 89822], expert_idx: 0\n",
      "subnet: [64354, 286096, 89822], loss: [1.11540442], acc: 0.46875, samples: 32\n",
      "subnet: [302462, 67579, 238252], expert_idx: 0\n",
      "subnet: [302462, 67579, 238252], loss: [1.05811427], acc: 0.4375, samples: 32\n",
      "subnet: [131890, 130857, 225014], expert_idx: 0\n",
      "subnet: [131890, 130857, 225014], loss: [1.235039], acc: 0.46875, samples: 32\n",
      "subnet: [222075, 292920, 215924], expert_idx: 0\n",
      "subnet: [222075, 292920, 215924], loss: [1.08888296], acc: 0.375, samples: 32\n",
      "subnet: [163691, 282348, 164276], expert_idx: 0\n",
      "subnet: [163691, 282348, 164276], loss: [1.09371998], acc: 0.4375, samples: 32\n",
      "subnet: [324384, 169802, 329061], expert_idx: 0\n",
      "subnet: [324384, 169802, 329061], loss: [1.0358062], acc: 0.5, samples: 32\n",
      "subnet: [219326, 61386, 22101], expert_idx: 0\n",
      "subnet: [219326, 61386, 22101], loss: [1.03351013], acc: 0.5625, samples: 32\n",
      "subnet: [157611, 258735, 10564], expert_idx: 0\n",
      "subnet: [157611, 258735, 10564], loss: [1.0482354], acc: 0.5, samples: 32\n",
      "subnet: [75866, 285953, 110961], expert_idx: 0\n",
      "subnet: [75866, 285953, 110961], loss: [1.09126828], acc: 0.5, samples: 32\n",
      "subnet: [68942, 176294, 64147], expert_idx: 0\n",
      "subnet: [68942, 176294, 64147], loss: [1.03047644], acc: 0.53125, samples: 32\n",
      "subnet: [117117, 214549, 261360], expert_idx: 0\n",
      "subnet: [117117, 214549, 261360], loss: [0.99925337], acc: 0.5625, samples: 32\n",
      "subnet: [106077, 226818, 248267], expert_idx: 0\n",
      "subnet: [106077, 226818, 248267], loss: [1.08956974], acc: 0.4375, samples: 32\n",
      "subnet: [110494, 323068, 278644], expert_idx: 0\n",
      "subnet: [110494, 323068, 278644], loss: [1.21403474], acc: 0.46875, samples: 32\n",
      "subnet: [81058, 118294, 275339], expert_idx: 0\n",
      "subnet: [81058, 118294, 275339], loss: [1.09262557], acc: 0.46875, samples: 32\n",
      "subnet: [271117, 110640, 313560], expert_idx: 0\n",
      "subnet: [271117, 110640, 313560], loss: [1.04671966], acc: 0.59375, samples: 32\n",
      "subnet: [76003, 12077, 58106], expert_idx: 0\n",
      "subnet: [76003, 12077, 58106], loss: [0.96024056], acc: 0.6875, samples: 32\n",
      "subnet: [79650, 94630, 319553], expert_idx: 0\n",
      "subnet: [79650, 94630, 319553], loss: [1.02664323], acc: 0.46875, samples: 32\n",
      "subnet: [127523, 296466, 31269], expert_idx: 0\n",
      "subnet: [127523, 296466, 31269], loss: [1.13031719], acc: 0.40625, samples: 32\n",
      "subnet: [245669, 259720, 193283], expert_idx: 0\n",
      "subnet: [245669, 259720, 193283], loss: [1.33244699], acc: 0.25, samples: 32\n",
      "subnet: [296378, 195395, 131283], expert_idx: 0\n",
      "subnet: [296378, 195395, 131283], loss: [0.95910793], acc: 0.625, samples: 32\n",
      "subnet: [189988, 149666, 331417], expert_idx: 0\n",
      "subnet: [189988, 149666, 331417], loss: [1.03739449], acc: 0.53125, samples: 32\n",
      "subnet: [230067, 96877, 313552], expert_idx: 0\n",
      "subnet: [230067, 96877, 313552], loss: [1.21386894], acc: 0.46875, samples: 32\n",
      "subnet: [136530, 271750, 116590], expert_idx: 0\n",
      "subnet: [136530, 271750, 116590], loss: [1.01541627], acc: 0.5, samples: 32\n",
      "subnet: [30118, 21874, 92849], expert_idx: 0\n",
      "subnet: [30118, 21874, 92849], loss: [1.14813778], acc: 0.5, samples: 32\n",
      "subnet: [271141, 47471, 132536], expert_idx: 0\n",
      "subnet: [271141, 47471, 132536], loss: [0.94514036], acc: 0.65625, samples: 32\n",
      "subnet: [109900, 329409, 291589], expert_idx: 0\n",
      "subnet: [109900, 329409, 291589], loss: [0.91901753], acc: 0.6875, samples: 32\n",
      "subnet: [163378, 11640, 135682], expert_idx: 0\n",
      "subnet: [163378, 11640, 135682], loss: [1.32942391], acc: 0.4375, samples: 32\n",
      "subnet: [174786, 274169, 83792], expert_idx: 0\n",
      "subnet: [174786, 274169, 83792], loss: [0.87388777], acc: 0.625, samples: 32\n",
      "subnet: [65839, 322773, 15611], expert_idx: 0\n",
      "subnet: [65839, 322773, 15611], loss: [1.14168534], acc: 0.5, samples: 32\n",
      "subnet: [276414, 34199, 275006], expert_idx: 0\n",
      "subnet: [276414, 34199, 275006], loss: [1.06849077], acc: 0.46875, samples: 32\n",
      "subnet: [116332, 294423, 52826], expert_idx: 0\n",
      "subnet: [116332, 294423, 52826], loss: [1.04540597], acc: 0.5, samples: 32\n",
      "subnet: [294738, 49815, 19296], expert_idx: 0\n",
      "subnet: [294738, 49815, 19296], loss: [0.86635652], acc: 0.65625, samples: 32\n",
      "subnet: [36218, 123594, 207161], expert_idx: 0\n",
      "subnet: [36218, 123594, 207161], loss: [0.90552331], acc: 0.75, samples: 32\n",
      "subnet: [139361, 303802, 28804], expert_idx: 0\n",
      "subnet: [139361, 303802, 28804], loss: [1.18075328], acc: 0.46875, samples: 32\n",
      "subnet: [22998, 311029, 143114], expert_idx: 0\n",
      "subnet: [22998, 311029, 143114], loss: [1.05107185], acc: 0.5, samples: 32\n",
      "subnet: [4682, 128385, 132997], expert_idx: 0\n",
      "subnet: [4682, 128385, 132997], loss: [1.05871266], acc: 0.5, samples: 32\n",
      "subnet: [71988, 215676, 94011], expert_idx: 0\n",
      "subnet: [71988, 215676, 94011], loss: [1.1039862], acc: 0.46875, samples: 32\n",
      "subnet: [95299, 30736, 53818], expert_idx: 0\n",
      "subnet: [95299, 30736, 53818], loss: [1.21022976], acc: 0.34375, samples: 32\n",
      "subnet: [31796, 153240, 150069], expert_idx: 0\n",
      "subnet: [31796, 153240, 150069], loss: [0.93803963], acc: 0.59375, samples: 32\n",
      "subnet: [63684, 209923, 297207], expert_idx: 0\n",
      "subnet: [63684, 209923, 297207], loss: [0.97244873], acc: 0.59375, samples: 32\n",
      "subnet: [191642, 109095, 330216], expert_idx: 0\n",
      "subnet: [191642, 109095, 330216], loss: [1.00447275], acc: 0.5625, samples: 32\n",
      "subnet: [83922, 53591, 1103], expert_idx: 0\n",
      "subnet: [83922, 53591, 1103], loss: [1.09389208], acc: 0.53125, samples: 32\n",
      "subnet: [247790, 251786, 316683], expert_idx: 0\n",
      "subnet: [247790, 251786, 316683], loss: [1.00032948], acc: 0.53125, samples: 32\n",
      "subnet: [22497, 188308, 312743], expert_idx: 0\n",
      "subnet: [22497, 188308, 312743], loss: [1.08479993], acc: 0.5, samples: 32\n",
      "subnet: [245103, 198878, 207153], expert_idx: 0\n",
      "subnet: [245103, 198878, 207153], loss: [1.02940626], acc: 0.53125, samples: 32\n",
      "subnet: [36242, 182312, 144848], expert_idx: 0\n",
      "subnet: [36242, 182312, 144848], loss: [1.09279346], acc: 0.5625, samples: 32\n",
      "subnet: [262899, 102702, 170316], expert_idx: 0\n",
      "subnet: [262899, 102702, 170316], loss: [1.30793087], acc: 0.40625, samples: 32\n",
      "subnet: [186180, 312911, 170095], expert_idx: 0\n",
      "subnet: [186180, 312911, 170095], loss: [1.03582233], acc: 0.53125, samples: 32\n",
      "subnet: [295363, 154696, 120855], expert_idx: 0\n",
      "subnet: [295363, 154696, 120855], loss: [0.96431792], acc: 0.5625, samples: 32\n",
      "subnet: [8139, 260687, 312027], expert_idx: 0\n",
      "subnet: [8139, 260687, 312027], loss: [0.9171188], acc: 0.59375, samples: 32\n",
      "subnet: [212727, 65099, 236672], expert_idx: 0\n",
      "subnet: [212727, 65099, 236672], loss: [0.97240789], acc: 0.53125, samples: 32\n",
      "subnet: [305488, 331186, 53252], expert_idx: 0\n",
      "subnet: [305488, 331186, 53252], loss: [1.06688832], acc: 0.5, samples: 32\n",
      "subnet: [177971, 4116, 196677], expert_idx: 0\n",
      "subnet: [177971, 4116, 196677], loss: [1.00575211], acc: 0.46875, samples: 32\n",
      "subnet: [75811, 77127, 45146], expert_idx: 0\n",
      "subnet: [75811, 77127, 45146], loss: [0.97987296], acc: 0.46875, samples: 32\n",
      "subnet: [210819, 207575, 6310], expert_idx: 0\n",
      "subnet: [210819, 207575, 6310], loss: [1.0332326], acc: 0.625, samples: 32\n",
      "subnet: [311513, 166233, 119154], expert_idx: 0\n",
      "subnet: [311513, 166233, 119154], loss: [0.9428679], acc: 0.625, samples: 32\n",
      "subnet: [20338, 101468, 250690], expert_idx: 0\n",
      "subnet: [20338, 101468, 250690], loss: [1.02131817], acc: 0.53125, samples: 32\n",
      "subnet: [215489, 317818, 3772], expert_idx: 0\n",
      "subnet: [215489, 317818, 3772], loss: [1.0825239], acc: 0.40625, samples: 32\n",
      "subnet: [319844, 170005, 226995], expert_idx: 0\n",
      "subnet: [319844, 170005, 226995], loss: [1.04098881], acc: 0.40625, samples: 32\n",
      "subnet: [40012, 31392, 315925], expert_idx: 0\n",
      "subnet: [40012, 31392, 315925], loss: [1.00765697], acc: 0.40625, samples: 32\n",
      "subnet: [234553, 52789, 305917], expert_idx: 0\n",
      "subnet: [234553, 52789, 305917], loss: [1.23169354], acc: 0.375, samples: 32\n",
      "subnet: [168388, 10099, 234769], expert_idx: 0\n",
      "subnet: [168388, 10099, 234769], loss: [1.10970583], acc: 0.40625, samples: 32\n",
      "subnet: [167061, 135405, 132329], expert_idx: 0\n",
      "subnet: [167061, 135405, 132329], loss: [1.19401664], acc: 0.46875, samples: 32\n",
      "subnet: [31784, 190573, 106240], expert_idx: 0\n",
      "subnet: [31784, 190573, 106240], loss: [1.03259647], acc: 0.53125, samples: 32\n",
      "subnet: [246268, 255002, 11854], expert_idx: 0\n",
      "subnet: [246268, 255002, 11854], loss: [1.00002333], acc: 0.5625, samples: 32\n",
      "subnet: [248373, 174293, 297413], expert_idx: 0\n",
      "subnet: [248373, 174293, 297413], loss: [1.11673897], acc: 0.53125, samples: 32\n",
      "subnet: [176561, 219908, 35678], expert_idx: 0\n",
      "subnet: [176561, 219908, 35678], loss: [0.99989604], acc: 0.5625, samples: 32\n",
      "subnet: [318426, 233240, 2263], expert_idx: 0\n",
      "subnet: [318426, 233240, 2263], loss: [0.81197473], acc: 0.75, samples: 32\n",
      "subnet: [177774, 90983, 246178], expert_idx: 0\n",
      "subnet: [177774, 90983, 246178], loss: [1.08914165], acc: 0.46875, samples: 32\n",
      "subnet: [288037, 197027, 33949], expert_idx: 0\n",
      "subnet: [288037, 197027, 33949], loss: [1.06998857], acc: 0.46875, samples: 32\n",
      "subnet: [137044, 245080, 161690], expert_idx: 0\n",
      "subnet: [137044, 245080, 161690], loss: [0.99673495], acc: 0.53125, samples: 32\n",
      "subnet: [4400, 156925, 220214], expert_idx: 0\n",
      "subnet: [4400, 156925, 220214], loss: [1.1181713], acc: 0.5, samples: 32\n",
      "subnet: [310113, 142537, 48663], expert_idx: 0\n",
      "subnet: [310113, 142537, 48663], loss: [1.09938211], acc: 0.46875, samples: 32\n",
      "subnet: [60949, 83020, 185295], expert_idx: 0\n",
      "subnet: [60949, 83020, 185295], loss: [1.09440255], acc: 0.3125, samples: 32\n",
      "subnet: [87714, 289368, 101528], expert_idx: 0\n",
      "subnet: [87714, 289368, 101528], loss: [0.96972247], acc: 0.59375, samples: 32\n",
      "subnet: [210016, 200869, 58795], expert_idx: 0\n",
      "subnet: [210016, 200869, 58795], loss: [1.03358048], acc: 0.5, samples: 32\n",
      "subnet: [230447, 11346, 4090], expert_idx: 0\n",
      "subnet: [230447, 11346, 4090], loss: [0.91706144], acc: 0.59375, samples: 32\n",
      "subnet: [37673, 18094, 167634], expert_idx: 0\n",
      "subnet: [37673, 18094, 167634], loss: [0.98938788], acc: 0.59375, samples: 32\n",
      "subnet: [23006, 246671, 97421], expert_idx: 0\n",
      "subnet: [23006, 246671, 97421], loss: [0.98134988], acc: 0.46875, samples: 32\n",
      "subnet: [145923, 92198, 130399], expert_idx: 0\n",
      "subnet: [145923, 92198, 130399], loss: [1.0027319], acc: 0.5, samples: 32\n",
      "subnet: [243760, 141680, 62728], expert_idx: 0\n",
      "subnet: [243760, 141680, 62728], loss: [1.17597269], acc: 0.375, samples: 32\n",
      "subnet: [1369, 177220, 31436], expert_idx: 0\n",
      "subnet: [1369, 177220, 31436], loss: [1.05507739], acc: 0.40625, samples: 32\n",
      "subnet: [146751, 278675, 203526], expert_idx: 0\n",
      "subnet: [146751, 278675, 203526], loss: [1.1171088], acc: 0.5, samples: 32\n",
      "subnet: [271793, 162450, 37509], expert_idx: 0\n",
      "subnet: [271793, 162450, 37509], loss: [1.13548532], acc: 0.46875, samples: 32\n",
      "subnet: [100159, 132157, 91092], expert_idx: 0\n",
      "subnet: [100159, 132157, 91092], loss: [1.06643124], acc: 0.5, samples: 32\n",
      "subnet: [173061, 145959, 128811], expert_idx: 0\n",
      "subnet: [173061, 145959, 128811], loss: [1.00246553], acc: 0.40625, samples: 32\n",
      "subnet: [262161, 148757, 6991], expert_idx: 0\n",
      "subnet: [262161, 148757, 6991], loss: [1.01988161], acc: 0.59375, samples: 32\n",
      "subnet: [260856, 129685, 262795], expert_idx: 0\n",
      "subnet: [260856, 129685, 262795], loss: [0.94253117], acc: 0.625, samples: 32\n",
      "subnet: [287015, 312452, 119324], expert_idx: 0\n",
      "subnet: [287015, 312452, 119324], loss: [0.96782912], acc: 0.53125, samples: 32\n",
      "subnet: [211599, 7681, 217667], expert_idx: 0\n",
      "subnet: [211599, 7681, 217667], loss: [0.94567811], acc: 0.5625, samples: 32\n",
      "subnet: [217818, 283447, 192476], expert_idx: 0\n",
      "subnet: [217818, 283447, 192476], loss: [1.09224923], acc: 0.46875, samples: 32\n",
      "subnet: [31228, 239448, 153480], expert_idx: 0\n",
      "subnet: [31228, 239448, 153480], loss: [1.09420822], acc: 0.34375, samples: 32\n",
      "subnet: [204326, 3486, 104650], expert_idx: 0\n",
      "subnet: [204326, 3486, 104650], loss: [1.11168305], acc: 0.5, samples: 32\n",
      "subnet: [41546, 150977, 295030], expert_idx: 0\n",
      "subnet: [41546, 150977, 295030], loss: [1.13513129], acc: 0.5, samples: 32\n",
      "subnet: [315183, 130714, 4826], expert_idx: 0\n",
      "subnet: [315183, 130714, 4826], loss: [1.1832746], acc: 0.4375, samples: 32\n",
      "subnet: [216684, 137125, 54010], expert_idx: 0\n",
      "subnet: [216684, 137125, 54010], loss: [0.95168568], acc: 0.5625, samples: 32\n",
      "subnet: [11875, 267718, 123820], expert_idx: 0\n",
      "subnet: [11875, 267718, 123820], loss: [1.16652017], acc: 0.375, samples: 32\n",
      "subnet: [61106, 257629, 162718], expert_idx: 0\n",
      "subnet: [61106, 257629, 162718], loss: [1.00193959], acc: 0.5625, samples: 32\n",
      "subnet: [22749, 138189, 169357], expert_idx: 0\n",
      "subnet: [22749, 138189, 169357], loss: [1.07306517], acc: 0.4375, samples: 32\n",
      "subnet: [29944, 97241, 169508], expert_idx: 0\n",
      "subnet: [29944, 97241, 169508], loss: [1.13506652], acc: 0.46875, samples: 32\n",
      "subnet: [145209, 74475, 49938], expert_idx: 0\n",
      "subnet: [145209, 74475, 49938], loss: [0.93349918], acc: 0.59375, samples: 32\n",
      "subnet: [315684, 184270, 49566], expert_idx: 0\n",
      "subnet: [315684, 184270, 49566], loss: [1.09162215], acc: 0.53125, samples: 32\n",
      "subnet: [295919, 154513, 169530], expert_idx: 0\n",
      "subnet: [295919, 154513, 169530], loss: [1.05659399], acc: 0.46875, samples: 32\n",
      "subnet: [130468, 1327, 282688], expert_idx: 0\n",
      "subnet: [130468, 1327, 282688], loss: [1.04163889], acc: 0.53125, samples: 32\n",
      "subnet: [19058, 161380, 170160], expert_idx: 0\n",
      "subnet: [19058, 161380, 170160], loss: [1.15045039], acc: 0.40625, samples: 32\n",
      "subnet: [134760, 12553, 157306], expert_idx: 0\n",
      "subnet: [134760, 12553, 157306], loss: [1.1220672], acc: 0.4375, samples: 32\n",
      "subnet: [287515, 262965, 104731], expert_idx: 0\n",
      "subnet: [287515, 262965, 104731], loss: [1.10604654], acc: 0.375, samples: 32\n",
      "subnet: [262165, 13883, 82643], expert_idx: 0\n",
      "subnet: [262165, 13883, 82643], loss: [1.19339137], acc: 0.34375, samples: 32\n",
      "subnet: [298605, 77522, 69365], expert_idx: 0\n",
      "subnet: [298605, 77522, 69365], loss: [1.02789252], acc: 0.5, samples: 32\n",
      "subnet: [118338, 65187, 258256], expert_idx: 0\n",
      "subnet: [118338, 65187, 258256], loss: [1.01057939], acc: 0.5, samples: 32\n",
      "subnet: [187885, 130154, 120211], expert_idx: 0\n",
      "subnet: [187885, 130154, 120211], loss: [0.86649542], acc: 0.6875, samples: 32\n",
      "subnet: [83980, 304002, 160785], expert_idx: 0\n",
      "subnet: [83980, 304002, 160785], loss: [1.08862671], acc: 0.5, samples: 32\n",
      "subnet: [188073, 159241, 192813], expert_idx: 0\n",
      "subnet: [188073, 159241, 192813], loss: [0.97834845], acc: 0.53125, samples: 32\n",
      "subnet: [24079, 187132, 275129], expert_idx: 0\n",
      "subnet: [24079, 187132, 275129], loss: [0.98168538], acc: 0.59375, samples: 32\n",
      "subnet: [309925, 228793, 296446], expert_idx: 0\n",
      "subnet: [309925, 228793, 296446], loss: [1.15041492], acc: 0.4375, samples: 32\n",
      "subnet: [7926, 136854, 189711], expert_idx: 0\n",
      "subnet: [7926, 136854, 189711], loss: [0.97464027], acc: 0.625, samples: 32\n",
      "subnet: [288102, 229875, 161201], expert_idx: 0\n",
      "subnet: [288102, 229875, 161201], loss: [1.04506125], acc: 0.4375, samples: 32\n",
      "subnet: [201985, 183109, 273971], expert_idx: 0\n",
      "subnet: [201985, 183109, 273971], loss: [1.1037659], acc: 0.40625, samples: 32\n",
      "subnet: [287732, 288519, 125164], expert_idx: 0\n",
      "subnet: [287732, 288519, 125164], loss: [1.23482913], acc: 0.5, samples: 32\n",
      "subnet: [98747, 213460, 160509], expert_idx: 0\n",
      "subnet: [98747, 213460, 160509], loss: [1.00398066], acc: 0.53125, samples: 32\n",
      "subnet: [303311, 155519, 203369], expert_idx: 0\n",
      "subnet: [303311, 155519, 203369], loss: [1.09364696], acc: 0.34375, samples: 32\n",
      "subnet: [220443, 323848, 91998], expert_idx: 0\n",
      "subnet: [220443, 323848, 91998], loss: [1.04204052], acc: 0.5625, samples: 32\n",
      "subnet: [200351, 143832, 17635], expert_idx: 0\n",
      "subnet: [200351, 143832, 17635], loss: [1.19173086], acc: 0.375, samples: 32\n",
      "subnet: [302829, 216117, 263730], expert_idx: 0\n",
      "subnet: [302829, 216117, 263730], loss: [0.99975523], acc: 0.59375, samples: 32\n",
      "subnet: [50871, 186110, 15032], expert_idx: 0\n",
      "subnet: [50871, 186110, 15032], loss: [0.99922161], acc: 0.65625, samples: 32\n",
      "subnet: [4352, 44856, 148217], expert_idx: 0\n",
      "subnet: [4352, 44856, 148217], loss: [1.07341318], acc: 0.46875, samples: 32\n",
      "subnet: [152194, 85983, 279871], expert_idx: 0\n",
      "subnet: [152194, 85983, 279871], loss: [1.00824504], acc: 0.40625, samples: 32\n",
      "subnet: [304441, 124104, 96153], expert_idx: 0\n",
      "subnet: [304441, 124104, 96153], loss: [1.03744204], acc: 0.5625, samples: 32\n",
      "subnet: [317135, 107351, 149789], expert_idx: 0\n",
      "subnet: [317135, 107351, 149789], loss: [1.01172438], acc: 0.40625, samples: 32\n",
      "subnet: [247952, 8421, 62946], expert_idx: 0\n",
      "subnet: [247952, 8421, 62946], loss: [0.96960535], acc: 0.59375, samples: 32\n",
      "subnet: [6541, 155408, 67168], expert_idx: 0\n",
      "subnet: [6541, 155408, 67168], loss: [1.03639216], acc: 0.5, samples: 32\n",
      "subnet: [239284, 251334, 21728], expert_idx: 0\n",
      "subnet: [239284, 251334, 21728], loss: [0.9961518], acc: 0.5625, samples: 32\n",
      "subnet: [213216, 134914, 165135], expert_idx: 0\n",
      "subnet: [213216, 134914, 165135], loss: [1.11607264], acc: 0.53125, samples: 32\n",
      "subnet: [176706, 122473, 270683], expert_idx: 0\n",
      "subnet: [176706, 122473, 270683], loss: [0.99552328], acc: 0.625, samples: 32\n",
      "subnet: [282100, 328500, 267940], expert_idx: 0\n",
      "subnet: [282100, 328500, 267940], loss: [1.03701332], acc: 0.5, samples: 32\n",
      "subnet: [117206, 85649, 327560], expert_idx: 0\n",
      "subnet: [117206, 85649, 327560], loss: [0.95096802], acc: 0.65625, samples: 32\n",
      "subnet: [44311, 265652, 134098], expert_idx: 0\n",
      "subnet: [44311, 265652, 134098], loss: [1.12040237], acc: 0.53125, samples: 32\n",
      "subnet: [300118, 115596, 234423], expert_idx: 0\n",
      "subnet: [300118, 115596, 234423], loss: [1.07628897], acc: 0.53125, samples: 32\n",
      "subnet: [273941, 140750, 179310], expert_idx: 0\n",
      "subnet: [273941, 140750, 179310], loss: [1.05740068], acc: 0.53125, samples: 32\n",
      "subnet: [286689, 130911, 234040], expert_idx: 0\n",
      "subnet: [286689, 130911, 234040], loss: [1.05927471], acc: 0.5, samples: 32\n",
      "subnet: [307079, 153023, 36457], expert_idx: 0\n",
      "subnet: [307079, 153023, 36457], loss: [0.88084888], acc: 0.6875, samples: 32\n",
      "subnet: [69830, 26417, 278995], expert_idx: 0\n",
      "subnet: [69830, 26417, 278995], loss: [1.01305227], acc: 0.53125, samples: 32\n",
      "subnet: [264235, 279718, 57451], expert_idx: 0\n",
      "subnet: [264235, 279718, 57451], loss: [0.95889555], acc: 0.59375, samples: 32\n",
      "subnet: [270456, 115702, 248433], expert_idx: 0\n",
      "subnet: [270456, 115702, 248433], loss: [1.07711117], acc: 0.34375, samples: 32\n",
      "subnet: [155048, 98491, 150586], expert_idx: 0\n",
      "subnet: [155048, 98491, 150586], loss: [1.06157367], acc: 0.375, samples: 32\n",
      "subnet: [252432, 312621, 180636], expert_idx: 0\n",
      "subnet: [252432, 312621, 180636], loss: [1.27431531], acc: 0.40625, samples: 32\n",
      "subnet: [56848, 116330, 5959], expert_idx: 0\n",
      "subnet: [56848, 116330, 5959], loss: [1.02701487], acc: 0.5625, samples: 32\n",
      "subnet: [182848, 212427, 202443], expert_idx: 0\n",
      "subnet: [182848, 212427, 202443], loss: [0.97555435], acc: 0.59375, samples: 32\n",
      "subnet: [128556, 237828, 84880], expert_idx: 0\n",
      "subnet: [128556, 237828, 84880], loss: [1.07770572], acc: 0.46875, samples: 32\n",
      "subnet: [30918, 279713, 254163], expert_idx: 0\n",
      "subnet: [30918, 279713, 254163], loss: [1.12967741], acc: 0.375, samples: 32\n",
      "subnet: [309883, 42685, 178343], expert_idx: 0\n",
      "subnet: [309883, 42685, 178343], loss: [1.03730945], acc: 0.5, samples: 32\n",
      "subnet: [117157, 255400, 187197], expert_idx: 0\n",
      "subnet: [117157, 255400, 187197], loss: [1.13353636], acc: 0.46875, samples: 32\n",
      "subnet: [290828, 4519, 214925], expert_idx: 0\n",
      "subnet: [290828, 4519, 214925], loss: [1.27919609], acc: 0.28125, samples: 32\n",
      "subnet: [183968, 291942, 299917], expert_idx: 0\n",
      "subnet: [183968, 291942, 299917], loss: [1.08508869], acc: 0.59375, samples: 32\n",
      "subnet: [158623, 240597, 317918], expert_idx: 0\n",
      "subnet: [158623, 240597, 317918], loss: [0.91570112], acc: 0.65625, samples: 32\n",
      "subnet: [303474, 134988, 5216], expert_idx: 0\n",
      "subnet: [303474, 134988, 5216], loss: [1.13034199], acc: 0.46875, samples: 32\n",
      "subnet: [155883, 238669, 194821], expert_idx: 0\n",
      "subnet: [155883, 238669, 194821], loss: [0.95241284], acc: 0.53125, samples: 32\n",
      "subnet: [330504, 285137, 8234], expert_idx: 0\n",
      "subnet: [330504, 285137, 8234], loss: [1.07733591], acc: 0.46875, samples: 32\n",
      "subnet: [19078, 239284, 153561], expert_idx: 0\n",
      "subnet: [19078, 239284, 153561], loss: [0.96311155], acc: 0.625, samples: 32\n",
      "subnet: [47738, 17956, 156649], expert_idx: 0\n",
      "subnet: [47738, 17956, 156649], loss: [1.06206295], acc: 0.375, samples: 32\n",
      "subnet: [8410, 259945, 83567], expert_idx: 0\n",
      "subnet: [8410, 259945, 83567], loss: [0.98206689], acc: 0.5625, samples: 32\n",
      "subnet: [142155, 56069, 126761], expert_idx: 0\n",
      "subnet: [142155, 56069, 126761], loss: [1.02815849], acc: 0.4375, samples: 32\n",
      "subnet: [229685, 227359, 321218], expert_idx: 0\n",
      "subnet: [229685, 227359, 321218], loss: [1.18964046], acc: 0.5, samples: 32\n",
      "subnet: [268022, 67213, 49612], expert_idx: 0\n",
      "subnet: [268022, 67213, 49612], loss: [1.05967641], acc: 0.46875, samples: 32\n",
      "subnet: [9970, 116414, 302268], expert_idx: 0\n",
      "subnet: [9970, 116414, 302268], loss: [1.09863185], acc: 0.46875, samples: 32\n",
      "subnet: [322297, 144167, 230460], expert_idx: 0\n",
      "subnet: [322297, 144167, 230460], loss: [0.99589219], acc: 0.5, samples: 32\n",
      "subnet: [98551, 98389, 315778], expert_idx: 0\n",
      "subnet: [98551, 98389, 315778], loss: [1.10878691], acc: 0.40625, samples: 32\n",
      "subnet: [268566, 79643, 34946], expert_idx: 0\n",
      "subnet: [268566, 79643, 34946], loss: [1.04132826], acc: 0.5625, samples: 32\n",
      "subnet: [291057, 328248, 170002], expert_idx: 0\n",
      "subnet: [291057, 328248, 170002], loss: [1.02093175], acc: 0.5625, samples: 32\n",
      "subnet: [244260, 49098, 62848], expert_idx: 0\n",
      "subnet: [244260, 49098, 62848], loss: [1.16749542], acc: 0.40625, samples: 32\n",
      "subnet: [44808, 103074, 15202], expert_idx: 0\n",
      "subnet: [44808, 103074, 15202], loss: [1.04117443], acc: 0.5, samples: 32\n",
      "subnet: [13681, 119208, 101544], expert_idx: 0\n",
      "subnet: [13681, 119208, 101544], loss: [1.0396164], acc: 0.46875, samples: 32\n",
      "subnet: [60849, 246022, 38802], expert_idx: 0\n",
      "subnet: [60849, 246022, 38802], loss: [1.20277555], acc: 0.4375, samples: 32\n",
      "subnet: [214881, 235909, 119569], expert_idx: 0\n",
      "subnet: [214881, 235909, 119569], loss: [1.02947523], acc: 0.40625, samples: 32\n",
      "subnet: [229054, 46733, 319420], expert_idx: 0\n",
      "subnet: [229054, 46733, 319420], loss: [1.02246613], acc: 0.5, samples: 32\n",
      "subnet: [270935, 141733, 156993], expert_idx: 0\n",
      "subnet: [270935, 141733, 156993], loss: [1.06217835], acc: 0.53125, samples: 32\n",
      "subnet: [207363, 193342, 89192], expert_idx: 0\n",
      "subnet: [207363, 193342, 89192], loss: [1.08286087], acc: 0.46875, samples: 32\n",
      "subnet: [305337, 172210, 181417], expert_idx: 0\n",
      "subnet: [305337, 172210, 181417], loss: [0.93074224], acc: 0.5, samples: 32\n",
      "subnet: [149248, 171106, 160679], expert_idx: 0\n",
      "subnet: [149248, 171106, 160679], loss: [1.1666731], acc: 0.375, samples: 32\n",
      "subnet: [188043, 36371, 129481], expert_idx: 0\n",
      "subnet: [188043, 36371, 129481], loss: [1.04517068], acc: 0.5625, samples: 32\n",
      "subnet: [102424, 270621, 126380], expert_idx: 0\n",
      "subnet: [102424, 270621, 126380], loss: [1.01270892], acc: 0.4375, samples: 32\n",
      "subnet: [122716, 197178, 51747], expert_idx: 0\n",
      "subnet: [122716, 197178, 51747], loss: [0.97020179], acc: 0.53125, samples: 32\n",
      "subnet: [191549, 48255, 17716], expert_idx: 0\n",
      "subnet: [191549, 48255, 17716], loss: [0.98168767], acc: 0.59375, samples: 32\n",
      "subnet: [117535, 66415, 134345], expert_idx: 0\n",
      "subnet: [117535, 66415, 134345], loss: [0.97924659], acc: 0.625, samples: 32\n",
      "subnet: [204801, 147613, 24134], expert_idx: 0\n",
      "subnet: [204801, 147613, 24134], loss: [1.05504766], acc: 0.59375, samples: 32\n",
      "subnet: [73436, 21799, 210790], expert_idx: 0\n",
      "subnet: [73436, 21799, 210790], loss: [1.11920046], acc: 0.4375, samples: 32\n",
      "subnet: [331600, 10877, 322825], expert_idx: 0\n",
      "subnet: [331600, 10877, 322825], loss: [0.98508572], acc: 0.46875, samples: 32\n",
      "subnet: [173268, 62124, 16391], expert_idx: 0\n",
      "subnet: [173268, 62124, 16391], loss: [1.18110433], acc: 0.4375, samples: 32\n",
      "subnet: [11902, 239590, 28403], expert_idx: 0\n",
      "subnet: [11902, 239590, 28403], loss: [1.11239162], acc: 0.46875, samples: 32\n",
      "subnet: [321358, 260604, 120552], expert_idx: 0\n",
      "subnet: [321358, 260604, 120552], loss: [1.07378887], acc: 0.46875, samples: 32\n",
      "subnet: [315198, 291278, 117927], expert_idx: 0\n",
      "subnet: [315198, 291278, 117927], loss: [1.1931051], acc: 0.34375, samples: 32\n",
      "subnet: [69534, 201904, 11346], expert_idx: 0\n",
      "subnet: [69534, 201904, 11346], loss: [1.00364392], acc: 0.59375, samples: 32\n",
      "subnet: [180955, 232836, 40519], expert_idx: 0\n",
      "subnet: [180955, 232836, 40519], loss: [0.97177408], acc: 0.53125, samples: 32\n",
      "subnet: [232691, 283698, 150567], expert_idx: 0\n",
      "subnet: [232691, 283698, 150567], loss: [1.11518437], acc: 0.53125, samples: 32\n",
      "subnet: [139213, 60594, 251665], expert_idx: 0\n",
      "subnet: [139213, 60594, 251665], loss: [1.06021694], acc: 0.5, samples: 32\n",
      "subnet: [278122, 41377, 218716], expert_idx: 0\n",
      "subnet: [278122, 41377, 218716], loss: [1.01519738], acc: 0.53125, samples: 32\n",
      "subnet: [174926, 26957, 153193], expert_idx: 0\n",
      "subnet: [174926, 26957, 153193], loss: [1.07158147], acc: 0.46875, samples: 32\n",
      "subnet: [278336, 128568, 40131], expert_idx: 0\n",
      "subnet: [278336, 128568, 40131], loss: [1.0038359], acc: 0.53125, samples: 32\n",
      "subnet: [196090, 275090, 136648], expert_idx: 0\n",
      "subnet: [196090, 275090, 136648], loss: [1.08859223], acc: 0.5, samples: 32\n",
      "subnet: [262720, 12562, 113886], expert_idx: 0\n",
      "subnet: [262720, 12562, 113886], loss: [1.09464148], acc: 0.53125, samples: 32\n",
      "subnet: [58177, 172590, 19900], expert_idx: 0\n",
      "subnet: [58177, 172590, 19900], loss: [1.08208972], acc: 0.5625, samples: 32\n",
      "subnet: [304106, 123650, 230976], expert_idx: 0\n",
      "subnet: [304106, 123650, 230976], loss: [1.00489925], acc: 0.5625, samples: 32\n",
      "subnet: [175293, 244597, 315027], expert_idx: 0\n",
      "subnet: [175293, 244597, 315027], loss: [1.01875095], acc: 0.53125, samples: 32\n",
      "subnet: [71040, 256910, 326843], expert_idx: 0\n",
      "subnet: [71040, 256910, 326843], loss: [1.26744694], acc: 0.375, samples: 32\n",
      "subnet: [189240, 122434, 144462], expert_idx: 0\n",
      "subnet: [189240, 122434, 144462], loss: [1.18556534], acc: 0.40625, samples: 32\n",
      "subnet: [81610, 205343, 77043], expert_idx: 0\n",
      "subnet: [81610, 205343, 77043], loss: [1.11561634], acc: 0.34375, samples: 32\n",
      "subnet: [253037, 77437, 3170], expert_idx: 0\n",
      "subnet: [253037, 77437, 3170], loss: [1.12155418], acc: 0.5, samples: 32\n",
      "subnet: [45372, 53970, 302460], expert_idx: 0\n",
      "subnet: [45372, 53970, 302460], loss: [0.99693563], acc: 0.53125, samples: 32\n",
      "subnet: [280379, 62582, 218647], expert_idx: 0\n",
      "subnet: [280379, 62582, 218647], loss: [1.07268199], acc: 0.4375, samples: 32\n",
      "subnet: [124264, 67838, 164401], expert_idx: 0\n",
      "subnet: [124264, 67838, 164401], loss: [1.15858845], acc: 0.46875, samples: 32\n",
      "subnet: [126097, 238402, 328558], expert_idx: 0\n",
      "subnet: [126097, 238402, 328558], loss: [0.9624691], acc: 0.59375, samples: 32\n",
      "subnet: [291828, 83448, 189968], expert_idx: 0\n",
      "subnet: [291828, 83448, 189968], loss: [1.01253387], acc: 0.59375, samples: 32\n",
      "subnet: [272033, 264361, 140464], expert_idx: 0\n",
      "subnet: [272033, 264361, 140464], loss: [0.99209417], acc: 0.59375, samples: 32\n",
      "subnet: [217496, 97476, 84513], expert_idx: 0\n",
      "subnet: [217496, 97476, 84513], loss: [1.03432757], acc: 0.625, samples: 32\n",
      "subnet: [209191, 80899, 62243], expert_idx: 0\n",
      "subnet: [209191, 80899, 62243], loss: [1.16068208], acc: 0.34375, samples: 32\n",
      "subnet: [142540, 234846, 194566], expert_idx: 0\n",
      "subnet: [142540, 234846, 194566], loss: [0.9974674], acc: 0.46875, samples: 32\n",
      "subnet: [113792, 115159, 285242], expert_idx: 0\n",
      "subnet: [113792, 115159, 285242], loss: [1.04321902], acc: 0.53125, samples: 32\n",
      "subnet: [306454, 173451, 77879], expert_idx: 0\n",
      "subnet: [306454, 173451, 77879], loss: [0.84788555], acc: 0.6875, samples: 32\n",
      "subnet: [319905, 215600, 155566], expert_idx: 0\n",
      "subnet: [319905, 215600, 155566], loss: [0.9298848], acc: 0.5625, samples: 32\n",
      "subnet: [103688, 84440, 317899], expert_idx: 0\n",
      "subnet: [103688, 84440, 317899], loss: [0.96581518], acc: 0.625, samples: 32\n",
      "subnet: [94116, 244737, 250348], expert_idx: 0\n",
      "subnet: [94116, 244737, 250348], loss: [1.03107756], acc: 0.40625, samples: 32\n",
      "subnet: [36404, 330579, 253458], expert_idx: 0\n",
      "subnet: [36404, 330579, 253458], loss: [0.99813621], acc: 0.4375, samples: 32\n",
      "subnet: [256182, 321907, 81443], expert_idx: 0\n",
      "subnet: [256182, 321907, 81443], loss: [1.15790791], acc: 0.40625, samples: 32\n",
      "subnet: [176013, 97671, 158950], expert_idx: 0\n",
      "subnet: [176013, 97671, 158950], loss: [1.00979267], acc: 0.53125, samples: 32\n",
      "subnet: [196275, 301010, 173712], expert_idx: 0\n",
      "subnet: [196275, 301010, 173712], loss: [1.31898189], acc: 0.28125, samples: 32\n",
      "subnet: [293371, 25802, 118937], expert_idx: 0\n",
      "subnet: [293371, 25802, 118937], loss: [1.13560445], acc: 0.53125, samples: 32\n",
      "subnet: [202550, 248671, 307456], expert_idx: 0\n",
      "subnet: [202550, 248671, 307456], loss: [0.9467081], acc: 0.59375, samples: 32\n",
      "subnet: [53705, 50395, 22885], expert_idx: 0\n",
      "subnet: [53705, 50395, 22885], loss: [1.26104339], acc: 0.3125, samples: 32\n",
      "subnet: [317910, 150240, 74876], expert_idx: 0\n",
      "subnet: [317910, 150240, 74876], loss: [0.99949723], acc: 0.59375, samples: 32\n",
      "subnet: [195940, 288456, 43421], expert_idx: 0\n",
      "subnet: [195940, 288456, 43421], loss: [0.98895109], acc: 0.4375, samples: 32\n",
      "subnet: [320184, 221007, 13090], expert_idx: 0\n",
      "subnet: [320184, 221007, 13090], loss: [1.04798061], acc: 0.53125, samples: 32\n",
      "subnet: [55614, 268342, 115811], expert_idx: 0\n",
      "subnet: [55614, 268342, 115811], loss: [0.99704719], acc: 0.4375, samples: 32\n",
      "subnet: [235184, 201835, 128353], expert_idx: 0\n",
      "subnet: [235184, 201835, 128353], loss: [1.12348078], acc: 0.5625, samples: 32\n",
      "subnet: [24658, 61912, 89540], expert_idx: 0\n",
      "subnet: [24658, 61912, 89540], loss: [1.22745317], acc: 0.40625, samples: 32\n",
      "subnet: [207057, 210013, 223719], expert_idx: 0\n",
      "subnet: [207057, 210013, 223719], loss: [0.96818028], acc: 0.5625, samples: 32\n",
      "subnet: [95274, 117921, 10416], expert_idx: 0\n",
      "subnet: [95274, 117921, 10416], loss: [1.14825697], acc: 0.5, samples: 32\n",
      "subnet: [68530, 200725, 191481], expert_idx: 0\n",
      "subnet: [68530, 200725, 191481], loss: [1.10226617], acc: 0.5, samples: 32\n",
      "subnet: [327815, 160129, 25833], expert_idx: 0\n",
      "subnet: [327815, 160129, 25833], loss: [1.1253621], acc: 0.5, samples: 32\n",
      "subnet: [265392, 161885, 286029], expert_idx: 0\n",
      "subnet: [265392, 161885, 286029], loss: [1.22285604], acc: 0.40625, samples: 32\n",
      "subnet: [80815, 118001, 11162], expert_idx: 0\n",
      "subnet: [80815, 118001, 11162], loss: [1.10524754], acc: 0.5, samples: 32\n",
      "subnet: [246842, 187341, 167241], expert_idx: 0\n",
      "subnet: [246842, 187341, 167241], loss: [1.00745459], acc: 0.53125, samples: 32\n",
      "subnet: [308082, 266359, 182676], expert_idx: 0\n",
      "subnet: [308082, 266359, 182676], loss: [1.09439798], acc: 0.375, samples: 32\n",
      "subnet: [328709, 211066, 226138], expert_idx: 0\n",
      "subnet: [328709, 211066, 226138], loss: [1.00748298], acc: 0.5625, samples: 32\n",
      "subnet: [76872, 74233, 1231], expert_idx: 0\n",
      "subnet: [76872, 74233, 1231], loss: [1.12155076], acc: 0.46875, samples: 32\n",
      "subnet: [244988, 249556, 251277], expert_idx: 0\n",
      "subnet: [244988, 249556, 251277], loss: [1.07588955], acc: 0.46875, samples: 32\n",
      "subnet: [27847, 49137, 157838], expert_idx: 0\n",
      "subnet: [27847, 49137, 157838], loss: [0.97112902], acc: 0.5625, samples: 32\n",
      "subnet: [46663, 100584, 214383], expert_idx: 0\n",
      "subnet: [46663, 100584, 214383], loss: [0.98755232], acc: 0.46875, samples: 32\n",
      "subnet: [175019, 117392, 291362], expert_idx: 0\n",
      "subnet: [175019, 117392, 291362], loss: [1.06020744], acc: 0.5625, samples: 32\n",
      "subnet: [144252, 29039, 64835], expert_idx: 0\n",
      "subnet: [144252, 29039, 64835], loss: [1.01856524], acc: 0.40625, samples: 32\n",
      "subnet: [177097, 323007, 98068], expert_idx: 0\n",
      "subnet: [177097, 323007, 98068], loss: [1.19057584], acc: 0.40625, samples: 32\n",
      "subnet: [22459, 310267, 268382], expert_idx: 0\n",
      "subnet: [22459, 310267, 268382], loss: [1.01224545], acc: 0.53125, samples: 32\n",
      "subnet: [274045, 100903, 27155], expert_idx: 0\n",
      "subnet: [274045, 100903, 27155], loss: [1.30193204], acc: 0.5, samples: 32\n",
      "subnet: [181758, 288067, 248012], expert_idx: 0\n",
      "subnet: [181758, 288067, 248012], loss: [1.19786175], acc: 0.4375, samples: 32\n",
      "subnet: [75842, 235424, 267883], expert_idx: 0\n",
      "subnet: [75842, 235424, 267883], loss: [0.9979464], acc: 0.5625, samples: 32\n",
      "subnet: [186345, 51498, 265429], expert_idx: 0\n",
      "subnet: [186345, 51498, 265429], loss: [1.18425764], acc: 0.375, samples: 32\n",
      "subnet: [76113, 319215, 239764], expert_idx: 0\n",
      "subnet: [76113, 319215, 239764], loss: [1.05944124], acc: 0.5, samples: 32\n",
      "subnet: [167024, 87205, 147083], expert_idx: 0\n",
      "subnet: [167024, 87205, 147083], loss: [1.23653953], acc: 0.5, samples: 32\n",
      "subnet: [57630, 263571, 326342], expert_idx: 0\n",
      "subnet: [57630, 263571, 326342], loss: [0.88182993], acc: 0.65625, samples: 32\n",
      "subnet: [220221, 166908, 226161], expert_idx: 0\n",
      "subnet: [220221, 166908, 226161], loss: [1.06562137], acc: 0.5625, samples: 32\n",
      "subnet: [123080, 62675, 84409], expert_idx: 0\n",
      "subnet: [123080, 62675, 84409], loss: [1.15443363], acc: 0.53125, samples: 32\n",
      "subnet: [45292, 215227, 243900], expert_idx: 0\n",
      "subnet: [45292, 215227, 243900], loss: [1.15461481], acc: 0.46875, samples: 32\n",
      "subnet: [152468, 69898, 296332], expert_idx: 0\n",
      "subnet: [152468, 69898, 296332], loss: [1.0186893], acc: 0.53125, samples: 32\n",
      "subnet: [82327, 213284, 197119], expert_idx: 0\n",
      "subnet: [82327, 213284, 197119], loss: [1.11810399], acc: 0.46875, samples: 32\n",
      "subnet: [196578, 129719, 307539], expert_idx: 0\n",
      "subnet: [196578, 129719, 307539], loss: [0.93488131], acc: 0.625, samples: 32\n",
      "subnet: [178669, 307882, 23933], expert_idx: 0\n",
      "subnet: [178669, 307882, 23933], loss: [1.10734732], acc: 0.46875, samples: 32\n",
      "subnet: [14731, 74578, 151223], expert_idx: 0\n",
      "subnet: [14731, 74578, 151223], loss: [0.96749763], acc: 0.5625, samples: 32\n",
      "subnet: [27316, 312422, 132266], expert_idx: 0\n",
      "subnet: [27316, 312422, 132266], loss: [1.10672138], acc: 0.40625, samples: 32\n",
      "subnet: [4409, 215536, 285268], expert_idx: 0\n",
      "subnet: [4409, 215536, 285268], loss: [1.23594239], acc: 0.53125, samples: 32\n",
      "subnet: [253883, 307877, 150416], expert_idx: 0\n",
      "subnet: [253883, 307877, 150416], loss: [1.15624934], acc: 0.53125, samples: 32\n",
      "subnet: [138342, 222183, 254222], expert_idx: 0\n",
      "subnet: [138342, 222183, 254222], loss: [1.17832737], acc: 0.4375, samples: 32\n",
      "subnet: [168614, 30526, 156960], expert_idx: 0\n",
      "subnet: [168614, 30526, 156960], loss: [1.10936439], acc: 0.53125, samples: 32\n",
      "subnet: [98853, 167992, 215232], expert_idx: 0\n",
      "subnet: [98853, 167992, 215232], loss: [1.15300774], acc: 0.4375, samples: 32\n",
      "subnet: [57805, 104426, 124747], expert_idx: 0\n",
      "subnet: [57805, 104426, 124747], loss: [1.11037777], acc: 0.53125, samples: 32\n",
      "subnet: [303661, 322167, 98868], expert_idx: 0\n",
      "subnet: [303661, 322167, 98868], loss: [1.26205778], acc: 0.375, samples: 32\n",
      "subnet: [266054, 40852, 254966], expert_idx: 0\n",
      "subnet: [266054, 40852, 254966], loss: [1.06765837], acc: 0.4375, samples: 32\n",
      "subnet: [48805, 74507, 6879], expert_idx: 0\n",
      "subnet: [48805, 74507, 6879], loss: [0.96528649], acc: 0.65625, samples: 32\n",
      "subnet: [236725, 126715, 328060], expert_idx: 0\n",
      "subnet: [236725, 126715, 328060], loss: [0.74205824], acc: 0.75, samples: 32\n",
      "subnet: [167964, 219305, 100654], expert_idx: 0\n",
      "subnet: [167964, 219305, 100654], loss: [1.40006581], acc: 0.34375, samples: 32\n",
      "subnet: [217240, 321620, 329388], expert_idx: 0\n",
      "subnet: [217240, 321620, 329388], loss: [1.25065558], acc: 0.40625, samples: 32\n",
      "subnet: [273775, 249246, 147344], expert_idx: 0\n",
      "subnet: [273775, 249246, 147344], loss: [1.20596165], acc: 0.5, samples: 32\n",
      "subnet: [60741, 300266, 206979], expert_idx: 0\n",
      "subnet: [60741, 300266, 206979], loss: [0.97505814], acc: 0.65625, samples: 32\n",
      "subnet: [267745, 221586, 235505], expert_idx: 0\n",
      "subnet: [267745, 221586, 235505], loss: [1.08465805], acc: 0.40625, samples: 32\n",
      "subnet: [167582, 41016, 116339], expert_idx: 0\n",
      "subnet: [167582, 41016, 116339], loss: [1.00054791], acc: 0.40625, samples: 32\n",
      "subnet: [92943, 307138, 118905], expert_idx: 0\n",
      "subnet: [92943, 307138, 118905], loss: [1.17227006], acc: 0.46875, samples: 32\n",
      "subnet: [56793, 292532, 308810], expert_idx: 0\n",
      "subnet: [56793, 292532, 308810], loss: [1.01366348], acc: 0.4375, samples: 32\n",
      "subnet: [95412, 284693, 115646], expert_idx: 0\n",
      "subnet: [95412, 284693, 115646], loss: [1.12499113], acc: 0.4375, samples: 32\n",
      "subnet: [277537, 36749, 52036], expert_idx: 0\n",
      "subnet: [277537, 36749, 52036], loss: [0.98697644], acc: 0.53125, samples: 32\n",
      "subnet: [266489, 299422, 107453], expert_idx: 0\n",
      "subnet: [266489, 299422, 107453], loss: [0.95911346], acc: 0.65625, samples: 32\n",
      "subnet: [35191, 221209, 61717], expert_idx: 0\n",
      "subnet: [35191, 221209, 61717], loss: [1.07409209], acc: 0.4375, samples: 32\n",
      "subnet: [318032, 136827, 150668], expert_idx: 0\n",
      "subnet: [318032, 136827, 150668], loss: [1.1911404], acc: 0.375, samples: 32\n",
      "subnet: [218502, 203880, 142773], expert_idx: 0\n",
      "subnet: [218502, 203880, 142773], loss: [1.02661605], acc: 0.5625, samples: 32\n",
      "subnet: [150739, 47892, 297197], expert_idx: 0\n",
      "subnet: [150739, 47892, 297197], loss: [0.94105425], acc: 0.625, samples: 32\n",
      "subnet: [278265, 261910, 41444], expert_idx: 0\n",
      "subnet: [278265, 261910, 41444], loss: [0.98393935], acc: 0.625, samples: 32\n",
      "subnet: [91315, 60326, 39949], expert_idx: 0\n",
      "subnet: [91315, 60326, 39949], loss: [1.00713712], acc: 0.46875, samples: 32\n",
      "subnet: [62239, 133562, 80674], expert_idx: 0\n",
      "subnet: [62239, 133562, 80674], loss: [1.02463393], acc: 0.5, samples: 32\n",
      "subnet: [226884, 118045, 127818], expert_idx: 0\n",
      "subnet: [226884, 118045, 127818], loss: [1.07126586], acc: 0.4375, samples: 32\n",
      "subnet: [18433, 48155, 187026], expert_idx: 0\n",
      "subnet: [18433, 48155, 187026], loss: [1.04750978], acc: 0.4375, samples: 32\n",
      "subnet: [256481, 69686, 152972], expert_idx: 0\n",
      "subnet: [256481, 69686, 152972], loss: [0.98614993], acc: 0.53125, samples: 32\n",
      "subnet: [120919, 39345, 323525], expert_idx: 0\n",
      "subnet: [120919, 39345, 323525], loss: [1.06530353], acc: 0.5, samples: 32\n",
      "subnet: [282135, 35152, 326969], expert_idx: 0\n",
      "subnet: [282135, 35152, 326969], loss: [0.90141058], acc: 0.625, samples: 32\n",
      "subnet: [65310, 205641, 55580], expert_idx: 0\n",
      "subnet: [65310, 205641, 55580], loss: [1.09285756], acc: 0.375, samples: 32\n",
      "subnet: [230543, 265275, 125253], expert_idx: 0\n",
      "subnet: [230543, 265275, 125253], loss: [0.98151757], acc: 0.53125, samples: 32\n",
      "subnet: [220322, 2481, 230456], expert_idx: 0\n",
      "subnet: [220322, 2481, 230456], loss: [1.41060678], acc: 0.34375, samples: 32\n",
      "subnet: [25678, 264879, 202055], expert_idx: 0\n",
      "subnet: [25678, 264879, 202055], loss: [1.00827675], acc: 0.625, samples: 32\n",
      "subnet: [286622, 85685, 65977], expert_idx: 0\n",
      "subnet: [286622, 85685, 65977], loss: [1.0158604], acc: 0.5625, samples: 32\n",
      "subnet: [197261, 284394, 197291], expert_idx: 0\n",
      "subnet: [197261, 284394, 197291], loss: [0.95789063], acc: 0.625, samples: 32\n",
      "subnet: [255665, 285211, 303829], expert_idx: 0\n",
      "subnet: [255665, 285211, 303829], loss: [1.08688244], acc: 0.4375, samples: 32\n",
      "subnet: [146218, 40621, 73520], expert_idx: 0\n",
      "subnet: [146218, 40621, 73520], loss: [1.03105853], acc: 0.5, samples: 32\n",
      "subnet: [259383, 2873, 19917], expert_idx: 0\n",
      "subnet: [259383, 2873, 19917], loss: [1.08826511], acc: 0.5, samples: 32\n",
      "subnet: [96490, 62261, 64890], expert_idx: 0\n",
      "subnet: [96490, 62261, 64890], loss: [1.21069404], acc: 0.375, samples: 32\n",
      "subnet: [200496, 226358, 75518], expert_idx: 0\n",
      "subnet: [200496, 226358, 75518], loss: [1.14402717], acc: 0.3125, samples: 32\n",
      "subnet: [306266, 220948, 44347], expert_idx: 0\n",
      "subnet: [306266, 220948, 44347], loss: [1.14878073], acc: 0.53125, samples: 32\n",
      "subnet: [26690, 262644, 71421], expert_idx: 0\n"
     ]
    }
   ],
   "source": [
    "!python supernet/train_search_mnist.py --warmup_epochs 0 --steps 25000 --n_search 1 --n_qubits 4 --n_experts 1 --n_layers 3 --n_encode_layers 4 --save \"12_params\" --data \"./experiment_data/mnist_2/\" --save_dir \"./supernet/mnist_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python supernet/train_search_mnist.py --warmup_epochs 0 --steps 25000 --n_search 1 --n_qubits 4 --n_experts 1 --n_layers 4 --n_encode_layers 4 --save \"16_params\" --data \"./experiment_data/mnist_2/\" --save_dir \"./supernet/mnist_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train correlation circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 1.8400399684906006\n",
      "Step 101 | Loss: 0.9824213981628418\n",
      "Step 201 | Loss: 0.9988893270492554\n",
      "Step 301 | Loss: 0.9983148574829102\n",
      "Step 401 | Loss: 1.000512719154358\n",
      "Step 501 | Loss: 1.0025995969772339\n",
      "Step 601 | Loss: 1.005479335784912\n",
      "Step 701 | Loss: 1.0027689933776855\n",
      "Step 801 | Loss: 0.9973701238632202\n",
      "Step 901 | Loss: 0.979907751083374\n",
      "Step 1001 | Loss: 1.0157033205032349\n",
      "Step 1101 | Loss: 1.0111091136932373\n",
      "Step 1201 | Loss: 1.0007538795471191\n",
      "Step 1301 | Loss: 0.9992707371711731\n",
      "Step 1401 | Loss: 0.996511697769165\n",
      "Step 1501 | Loss: 1.0070708990097046\n",
      "Step 1601 | Loss: 1.0002305507659912\n",
      "Step 1701 | Loss: 0.9988362789154053\n",
      "Step 1801 | Loss: 1.0056716203689575\n",
      "Step 1901 | Loss: 0.9809823036193848\n",
      "Step 2001 | Loss: 1.0019094944000244\n",
      "Step 2101 | Loss: 0.9847947359085083\n",
      "Step 2201 | Loss: 0.9974161386489868\n",
      "Step 2301 | Loss: 0.9973360896110535\n",
      "Step 2401 | Loss: 0.996767520904541\n",
      "1.0092288692669085 0.5\n",
      "Step 1 | Loss: 1.4163897037506104\n",
      "Step 101 | Loss: 1.0265467166900635\n",
      "Step 201 | Loss: 1.0038056373596191\n",
      "Step 301 | Loss: 0.9785585403442383\n",
      "Step 401 | Loss: 1.0098159313201904\n",
      "Step 501 | Loss: 1.007118582725525\n",
      "Step 601 | Loss: 0.9961860775947571\n",
      "Step 701 | Loss: 1.0141258239746094\n",
      "Step 801 | Loss: 0.9764003157615662\n",
      "Step 901 | Loss: 0.9963982701301575\n",
      "Step 1001 | Loss: 0.9866830706596375\n",
      "Step 1101 | Loss: 1.042718768119812\n",
      "Step 1201 | Loss: 1.0001598596572876\n",
      "Step 1301 | Loss: 1.000927448272705\n",
      "Step 1401 | Loss: 1.0000324249267578\n",
      "Step 1501 | Loss: 0.9920371174812317\n",
      "Step 1601 | Loss: 0.9839128851890564\n",
      "Step 1701 | Loss: 0.9965859055519104\n",
      "Step 1801 | Loss: 1.0870881080627441\n",
      "Step 1901 | Loss: 1.000519871711731\n",
      "Step 2001 | Loss: 1.0346332788467407\n",
      "Step 2101 | Loss: 0.9965413212776184\n",
      "Step 2201 | Loss: 0.9944560527801514\n",
      "Step 2301 | Loss: 1.0121830701828003\n",
      "Step 2401 | Loss: 0.9986203908920288\n",
      "0.999870022439975 0.5\n",
      "Step 1 | Loss: 1.9567581415176392\n",
      "Step 101 | Loss: 1.0008103847503662\n",
      "Step 201 | Loss: 1.0137121677398682\n",
      "Step 301 | Loss: 0.9961467981338501\n",
      "Step 401 | Loss: 0.996529221534729\n",
      "Step 501 | Loss: 1.0008819103240967\n",
      "Step 601 | Loss: 1.0085209608078003\n",
      "Step 701 | Loss: 0.9877909421920776\n",
      "Step 801 | Loss: 1.0000064373016357\n",
      "Step 901 | Loss: 0.9961585402488708\n",
      "Step 1001 | Loss: 1.0023030042648315\n",
      "Step 1101 | Loss: 1.005836009979248\n",
      "Step 1201 | Loss: 1.0001766681671143\n",
      "Step 1301 | Loss: 0.9892246723175049\n",
      "Step 1401 | Loss: 0.9980542063713074\n",
      "Step 1501 | Loss: 1.0565272569656372\n",
      "Step 1601 | Loss: 0.9867371320724487\n",
      "Step 1701 | Loss: 1.000585913658142\n",
      "Step 1801 | Loss: 1.0208466053009033\n",
      "Step 1901 | Loss: 1.011517882347107\n",
      "Step 2001 | Loss: 0.9991750717163086\n",
      "Step 2101 | Loss: 1.0146418809890747\n",
      "Step 2201 | Loss: 0.9994008541107178\n",
      "Step 2301 | Loss: 1.0134943723678589\n",
      "Step 2401 | Loss: 1.0019065141677856\n",
      "1.002503098737918 0.5\n",
      "Step 1 | Loss: 1.0604439973831177\n",
      "Step 101 | Loss: 1.0103436708450317\n",
      "Step 201 | Loss: 0.9839025139808655\n",
      "Step 301 | Loss: 1.0272003412246704\n",
      "Step 401 | Loss: 0.9961551427841187\n",
      "Step 501 | Loss: 0.9983134865760803\n",
      "Step 601 | Loss: 0.9969062209129333\n",
      "Step 701 | Loss: 1.0024011135101318\n",
      "Step 801 | Loss: 0.9680480360984802\n",
      "Step 901 | Loss: 1.0069036483764648\n",
      "Step 1001 | Loss: 1.0002115964889526\n",
      "Step 1101 | Loss: 0.9965707659721375\n",
      "Step 1201 | Loss: 1.0000956058502197\n",
      "Step 1301 | Loss: 1.0017900466918945\n",
      "Step 1401 | Loss: 1.0066864490509033\n",
      "Step 1501 | Loss: 0.9961994886398315\n",
      "Step 1601 | Loss: 0.9964550733566284\n",
      "Step 1701 | Loss: 0.9962501525878906\n",
      "Step 1801 | Loss: 1.0492671728134155\n",
      "Step 1901 | Loss: 0.9478775858879089\n",
      "Step 2001 | Loss: 0.977269172668457\n",
      "Step 2101 | Loss: 0.9878723621368408\n",
      "Step 2201 | Loss: 0.9988175630569458\n",
      "Step 2301 | Loss: 1.0262072086334229\n",
      "Step 2401 | Loss: 1.015979290008545\n",
      "1.0035628384243285 0.5\n",
      "Step 1 | Loss: 1.1581642627716064\n",
      "Step 101 | Loss: 1.000025987625122\n",
      "Step 201 | Loss: 1.002272605895996\n",
      "Step 301 | Loss: 0.9881032705307007\n",
      "Step 401 | Loss: 0.9969302415847778\n",
      "Step 501 | Loss: 0.99615478515625\n",
      "Step 601 | Loss: 1.0005344152450562\n",
      "Step 701 | Loss: 1.0046015977859497\n",
      "Step 801 | Loss: 1.0011965036392212\n",
      "Step 901 | Loss: 0.9859198331832886\n",
      "Step 1001 | Loss: 0.996924638748169\n",
      "Step 1101 | Loss: 0.9874175786972046\n",
      "Step 1201 | Loss: 0.9973081946372986\n",
      "Step 1301 | Loss: 0.9995946884155273\n",
      "Step 1401 | Loss: 0.984542727470398\n",
      "Step 1501 | Loss: 1.0065113306045532\n",
      "Step 1601 | Loss: 0.9973551034927368\n",
      "Step 1701 | Loss: 1.0053958892822266\n",
      "Step 1801 | Loss: 1.0000767707824707\n",
      "Step 1901 | Loss: 1.0000145435333252\n",
      "Step 2001 | Loss: 1.0006643533706665\n",
      "Step 2101 | Loss: 1.0132976770401\n",
      "Step 2201 | Loss: 0.9963314533233643\n",
      "Step 2301 | Loss: 1.0100218057632446\n",
      "Step 2401 | Loss: 1.004833698272705\n",
      "1.0010031009173312 0.5\n",
      "Step 1 | Loss: 1.1409293413162231\n",
      "Step 101 | Loss: 0.6189461946487427\n",
      "Step 201 | Loss: 0.5629816055297852\n",
      "Step 301 | Loss: 0.4817139804363251\n",
      "Step 401 | Loss: 0.48610755801200867\n",
      "Step 501 | Loss: 0.3855881094932556\n",
      "Step 601 | Loss: 0.3936672508716583\n",
      "Step 701 | Loss: 0.42658931016921997\n",
      "Step 801 | Loss: 0.42998483777046204\n",
      "Step 901 | Loss: 0.5488466620445251\n",
      "Step 1001 | Loss: 0.5043140053749084\n",
      "Step 1101 | Loss: 0.4216321110725403\n",
      "Step 1201 | Loss: 0.41819122433662415\n",
      "Step 1301 | Loss: 0.3816935420036316\n",
      "Step 1401 | Loss: 0.26122111082077026\n",
      "Step 1501 | Loss: 0.48837369680404663\n",
      "Step 1601 | Loss: 0.5914335250854492\n",
      "Step 1701 | Loss: 0.4193921685218811\n",
      "Step 1801 | Loss: 0.43577226996421814\n",
      "Step 1901 | Loss: 0.5132169723510742\n",
      "Step 2001 | Loss: 0.4002630114555359\n",
      "Step 2101 | Loss: 0.3243461847305298\n",
      "Step 2201 | Loss: 0.44167211651802063\n",
      "Step 2301 | Loss: 0.29469794034957886\n",
      "Step 2401 | Loss: 0.3541923463344574\n",
      "0.4215467325090245 0.9125\n",
      "Step 1 | Loss: 0.8904927968978882\n",
      "Step 101 | Loss: 0.5226016640663147\n",
      "Step 201 | Loss: 0.6470621228218079\n",
      "Step 301 | Loss: 0.5277008414268494\n",
      "Step 401 | Loss: 0.6591842770576477\n",
      "Step 501 | Loss: 0.6044528484344482\n",
      "Step 601 | Loss: 0.49335968494415283\n",
      "Step 701 | Loss: 0.5034877061843872\n",
      "Step 801 | Loss: 0.6451634168624878\n",
      "Step 901 | Loss: 0.49178943037986755\n",
      "Step 1001 | Loss: 0.5437328815460205\n",
      "Step 1101 | Loss: 0.5024621486663818\n",
      "Step 1201 | Loss: 0.5580785274505615\n",
      "Step 1301 | Loss: 0.5012794733047485\n",
      "Step 1401 | Loss: 0.6056939363479614\n",
      "Step 1501 | Loss: 0.5034036636352539\n",
      "Step 1601 | Loss: 0.5468119382858276\n",
      "Step 1701 | Loss: 0.480635404586792\n",
      "Step 1801 | Loss: 0.40037405490875244\n",
      "Step 1901 | Loss: 0.6080992817878723\n",
      "Step 2001 | Loss: 0.5338084697723389\n",
      "Step 2101 | Loss: 0.456895112991333\n",
      "Step 2201 | Loss: 0.5046639442443848\n",
      "Step 2301 | Loss: 0.5899762511253357\n",
      "Step 2401 | Loss: 0.5085963606834412\n",
      "0.48376181571603755 0.95\n",
      "Step 1 | Loss: 0.981086015701294\n",
      "Step 101 | Loss: 0.5748234391212463\n",
      "Step 201 | Loss: 0.5353118777275085\n",
      "Step 301 | Loss: 0.3978358805179596\n",
      "Step 401 | Loss: 0.4347524046897888\n",
      "Step 501 | Loss: 0.4716469347476959\n",
      "Step 601 | Loss: 0.3672153651714325\n",
      "Step 701 | Loss: 0.42933306097984314\n",
      "Step 801 | Loss: 0.5327954888343811\n",
      "Step 901 | Loss: 0.38544321060180664\n",
      "Step 1001 | Loss: 0.39212900400161743\n",
      "Step 1101 | Loss: 0.3450045883655548\n",
      "Step 1201 | Loss: 0.34399446845054626\n",
      "Step 1301 | Loss: 0.39886942505836487\n",
      "Step 1401 | Loss: 0.4647238254547119\n",
      "Step 1501 | Loss: 0.30345726013183594\n",
      "Step 1601 | Loss: 0.4129382371902466\n",
      "Step 1701 | Loss: 0.3128637969493866\n",
      "Step 1801 | Loss: 0.29308652877807617\n",
      "Step 1901 | Loss: 0.35162031650543213\n",
      "Step 2001 | Loss: 0.35760465264320374\n",
      "Step 2101 | Loss: 0.3655712902545929\n",
      "Step 2201 | Loss: 0.414394348859787\n",
      "Step 2301 | Loss: 0.30244770646095276\n",
      "Step 2401 | Loss: 0.32661959528923035\n",
      "0.354447380161897 0.97\n",
      "Step 1 | Loss: 1.3602573871612549\n",
      "Step 101 | Loss: 0.5657972693443298\n",
      "Step 201 | Loss: 0.5924890041351318\n",
      "Step 301 | Loss: 0.5342509746551514\n",
      "Step 401 | Loss: 0.5058940052986145\n",
      "Step 501 | Loss: 0.4115505814552307\n",
      "Step 601 | Loss: 0.4123743176460266\n",
      "Step 701 | Loss: 0.3836594223976135\n",
      "Step 801 | Loss: 0.33786866068840027\n",
      "Step 901 | Loss: 0.4295373260974884\n",
      "Step 1001 | Loss: 0.37145861983299255\n",
      "Step 1101 | Loss: 0.5104795694351196\n",
      "Step 1201 | Loss: 0.3399755358695984\n",
      "Step 1301 | Loss: 0.46231168508529663\n",
      "Step 1401 | Loss: 0.37636107206344604\n",
      "Step 1501 | Loss: 0.39150574803352356\n",
      "Step 1601 | Loss: 0.3997015655040741\n",
      "Step 1701 | Loss: 0.41543376445770264\n",
      "Step 1801 | Loss: 0.3829265832901001\n",
      "Step 1901 | Loss: 0.3919893801212311\n",
      "Step 2001 | Loss: 0.33759453892707825\n",
      "Step 2101 | Loss: 0.40538737177848816\n",
      "Step 2201 | Loss: 0.35403281450271606\n",
      "Step 2301 | Loss: 0.37335899472236633\n",
      "Step 2401 | Loss: 0.3871557116508484\n",
      "0.4068007591308831 0.9525\n",
      "Step 1 | Loss: 1.2131919860839844\n",
      "Step 101 | Loss: 0.6510273218154907\n",
      "Step 201 | Loss: 0.5813490748405457\n",
      "Step 301 | Loss: 0.36470678448677063\n",
      "Step 401 | Loss: 0.36130204796791077\n",
      "Step 501 | Loss: 0.35243502259254456\n",
      "Step 601 | Loss: 0.4195079803466797\n",
      "Step 701 | Loss: 0.33857262134552\n",
      "Step 801 | Loss: 0.47210195660591125\n",
      "Step 901 | Loss: 0.486662894487381\n",
      "Step 1001 | Loss: 0.4463249444961548\n",
      "Step 1101 | Loss: 0.38314884901046753\n",
      "Step 1201 | Loss: 0.384535551071167\n",
      "Step 1301 | Loss: 0.3972387909889221\n",
      "Step 1401 | Loss: 0.536442756652832\n",
      "Step 1501 | Loss: 0.3962274193763733\n",
      "Step 1601 | Loss: 0.42885422706604004\n",
      "Step 1701 | Loss: 0.33395615220069885\n",
      "Step 1801 | Loss: 0.44947507977485657\n",
      "Step 1901 | Loss: 0.4517926573753357\n",
      "Step 2001 | Loss: 0.28589704632759094\n",
      "Step 2101 | Loss: 0.38693076372146606\n",
      "Step 2201 | Loss: 0.4202486574649811\n",
      "Step 2301 | Loss: 0.42559659481048584\n",
      "Step 2401 | Loss: 0.47202587127685547\n",
      "0.4150949925508385 0.935\n",
      "Step 1 | Loss: 0.8233307003974915\n",
      "Step 101 | Loss: 0.6360039710998535\n",
      "Step 201 | Loss: 0.5868589282035828\n",
      "Step 301 | Loss: 0.5633379817008972\n",
      "Step 401 | Loss: 0.7910662293434143\n",
      "Step 501 | Loss: 0.5446250438690186\n",
      "Step 601 | Loss: 0.7212432622909546\n",
      "Step 701 | Loss: 0.5548988580703735\n",
      "Step 801 | Loss: 0.5645133852958679\n",
      "Step 901 | Loss: 0.5187857747077942\n",
      "Step 1001 | Loss: 0.6507706642150879\n",
      "Step 1101 | Loss: 0.5048670768737793\n",
      "Step 1201 | Loss: 0.5138694047927856\n",
      "Step 1301 | Loss: 0.5124290585517883\n",
      "Step 1401 | Loss: 0.6132757067680359\n",
      "Step 1501 | Loss: 0.8382787704467773\n",
      "Step 1601 | Loss: 0.5026434659957886\n",
      "Step 1701 | Loss: 0.5231322646141052\n",
      "Step 1801 | Loss: 0.6638364195823669\n",
      "Step 1901 | Loss: 0.7453075647354126\n",
      "Step 2001 | Loss: 0.5136234760284424\n",
      "Step 2101 | Loss: 0.5125977993011475\n",
      "Step 2201 | Loss: 0.5073531866073608\n",
      "Step 2301 | Loss: 0.6629016995429993\n",
      "Step 2401 | Loss: 0.6637660264968872\n",
      "0.6336741820507504 0.8675\n",
      "Step 1 | Loss: 0.7973267436027527\n",
      "Step 101 | Loss: 0.9904526472091675\n",
      "Step 201 | Loss: 0.7486106157302856\n",
      "Step 301 | Loss: 0.7207162380218506\n",
      "Step 401 | Loss: 0.6691857576370239\n",
      "Step 501 | Loss: 0.5521765947341919\n",
      "Step 601 | Loss: 0.8852077722549438\n",
      "Step 701 | Loss: 0.7105463743209839\n",
      "Step 801 | Loss: 0.5115399360656738\n",
      "Step 901 | Loss: 0.7195330262184143\n",
      "Step 1001 | Loss: 0.7544757723808289\n",
      "Step 1101 | Loss: 0.8666534423828125\n",
      "Step 1201 | Loss: 0.6148911118507385\n",
      "Step 1301 | Loss: 0.7725052237510681\n",
      "Step 1401 | Loss: 0.641701340675354\n",
      "Step 1501 | Loss: 0.5755075812339783\n",
      "Step 1601 | Loss: 0.721050500869751\n",
      "Step 1701 | Loss: 0.6121364235877991\n",
      "Step 1801 | Loss: 0.860684871673584\n",
      "Step 1901 | Loss: 0.6286404132843018\n",
      "Step 2001 | Loss: 0.5787733197212219\n",
      "Step 2101 | Loss: 0.6445659399032593\n",
      "Step 2201 | Loss: 0.7390096783638\n",
      "Step 2301 | Loss: 0.5177100896835327\n",
      "Step 2401 | Loss: 0.5915887951850891\n",
      "0.6641434631347773 0.87\n",
      "Step 1 | Loss: 0.7843427658081055\n",
      "Step 101 | Loss: 0.5590928792953491\n",
      "Step 201 | Loss: 0.7901303768157959\n",
      "Step 301 | Loss: 0.5797035098075867\n",
      "Step 401 | Loss: 0.7337464094161987\n",
      "Step 501 | Loss: 0.5162497162818909\n",
      "Step 601 | Loss: 0.6439967751502991\n",
      "Step 701 | Loss: 0.6644946932792664\n",
      "Step 801 | Loss: 0.483916699886322\n",
      "Step 901 | Loss: 0.7616065740585327\n",
      "Step 1001 | Loss: 0.6149858236312866\n",
      "Step 1101 | Loss: 0.6585304737091064\n",
      "Step 1201 | Loss: 0.6095998287200928\n",
      "Step 1301 | Loss: 0.6413185596466064\n",
      "Step 1401 | Loss: 0.6696646213531494\n",
      "Step 1501 | Loss: 0.6810672283172607\n",
      "Step 1601 | Loss: 0.6540340781211853\n",
      "Step 1701 | Loss: 0.5973559617996216\n",
      "Step 1801 | Loss: 0.6432621479034424\n",
      "Step 1901 | Loss: 0.730008602142334\n",
      "Step 2001 | Loss: 0.6021444797515869\n",
      "Step 2101 | Loss: 0.7345027327537537\n",
      "Step 2201 | Loss: 0.5734856128692627\n",
      "Step 2301 | Loss: 0.6299460530281067\n",
      "Step 2401 | Loss: 0.8077998161315918\n",
      "0.703520378020789 0.855\n",
      "Step 1 | Loss: 0.8152879476547241\n",
      "Step 101 | Loss: 0.7508280277252197\n",
      "Step 201 | Loss: 0.8308267593383789\n",
      "Step 301 | Loss: 0.8747880458831787\n",
      "Step 401 | Loss: 0.7046874165534973\n",
      "Step 501 | Loss: 0.7644863128662109\n",
      "Step 601 | Loss: 0.8014796376228333\n",
      "Step 701 | Loss: 0.8002910017967224\n",
      "Step 801 | Loss: 0.6298432350158691\n",
      "Step 901 | Loss: 0.6416410207748413\n",
      "Step 1001 | Loss: 0.7559154033660889\n",
      "Step 1101 | Loss: 0.9438319802284241\n",
      "Step 1201 | Loss: 0.7414032220840454\n",
      "Step 1301 | Loss: 0.6432477235794067\n",
      "Step 1401 | Loss: 0.6249179244041443\n",
      "Step 1501 | Loss: 0.5793120861053467\n",
      "Step 1601 | Loss: 0.6090537309646606\n",
      "Step 1701 | Loss: 0.7507022619247437\n",
      "Step 1801 | Loss: 0.7399159669876099\n",
      "Step 1901 | Loss: 0.7284953594207764\n",
      "Step 2001 | Loss: 0.6608566045761108\n",
      "Step 2101 | Loss: 0.6618578433990479\n",
      "Step 2201 | Loss: 0.59665846824646\n",
      "Step 2301 | Loss: 1.0402792692184448\n",
      "Step 2401 | Loss: 0.7479588389396667\n",
      "0.6989020830799647 0.8525\n",
      "Step 1 | Loss: 1.0755866765975952\n",
      "Step 101 | Loss: 0.8388880491256714\n",
      "Step 201 | Loss: 0.6561757326126099\n",
      "Step 301 | Loss: 0.9661217331886292\n",
      "Step 401 | Loss: 0.8249765038490295\n",
      "Step 501 | Loss: 0.7938334345817566\n",
      "Step 601 | Loss: 0.8701162338256836\n",
      "Step 701 | Loss: 0.6199448704719543\n",
      "Step 801 | Loss: 0.7011944651603699\n",
      "Step 901 | Loss: 0.9690850973129272\n",
      "Step 1001 | Loss: 0.7102831602096558\n",
      "Step 1101 | Loss: 0.6739729642868042\n",
      "Step 1201 | Loss: 0.7137073874473572\n",
      "Step 1301 | Loss: 0.7470551133155823\n",
      "Step 1401 | Loss: 0.7564073801040649\n",
      "Step 1501 | Loss: 0.6651567220687866\n",
      "Step 1601 | Loss: 0.6320164799690247\n",
      "Step 1701 | Loss: 0.7005959153175354\n",
      "Step 1801 | Loss: 0.9278815388679504\n",
      "Step 1901 | Loss: 0.6388940215110779\n",
      "Step 2001 | Loss: 0.5950499773025513\n",
      "Step 2101 | Loss: 0.6651769280433655\n",
      "Step 2201 | Loss: 0.75190669298172\n",
      "Step 2301 | Loss: 0.8234667778015137\n",
      "Step 2401 | Loss: 0.5677153468132019\n",
      "0.6652198233140278 0.8775\n",
      "Step 1 | Loss: 2.75\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.5\n",
      "Step 301 | Loss: 2.125\n",
      "Step 401 | Loss: 1.875\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 1.875\n",
      "Step 701 | Loss: 2.25\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 2.5\n",
      "Step 1001 | Loss: 1.875\n",
      "Step 1101 | Loss: 1.5\n",
      "Step 1201 | Loss: 2.5\n",
      "Step 1301 | Loss: 2.375\n",
      "Step 1401 | Loss: 1.875\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 2.375\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 1.875\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 2.125\n",
      "Step 2301 | Loss: 1.875\n",
      "Step 2401 | Loss: 2.25\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 2.25\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.125\n",
      "Step 501 | Loss: 2.125\n",
      "Step 601 | Loss: 1.875\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 1.75\n",
      "Step 1001 | Loss: 1.625\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 1.625\n",
      "Step 1301 | Loss: 2.375\n",
      "Step 1401 | Loss: 1.875\n",
      "Step 1501 | Loss: 2.0\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 1.625\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 1.875\n",
      "Step 2101 | Loss: 2.5\n",
      "Step 2201 | Loss: 2.125\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 2.625\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.5\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 2.5\n",
      "Step 401 | Loss: 2.625\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 2.0\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 2.125\n",
      "Step 901 | Loss: 1.75\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 1.75\n",
      "Step 1401 | Loss: 2.375\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 2.125\n",
      "Step 1701 | Loss: 1.625\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 2.25\n",
      "Step 2001 | Loss: 2.375\n",
      "Step 2101 | Loss: 1.75\n",
      "Step 2201 | Loss: 2.375\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 1.75\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.375\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 1.875\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 2.0\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 1.75\n",
      "Step 901 | Loss: 1.875\n",
      "Step 1001 | Loss: 2.0\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 2.25\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 1.875\n",
      "Step 1501 | Loss: 1.875\n",
      "Step 1601 | Loss: 1.75\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 1.5\n",
      "Step 1901 | Loss: 2.0\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 2.125\n",
      "Step 2201 | Loss: 2.5\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 1.875\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 1.625\n",
      "Step 101 | Loss: 1.5\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 1.625\n",
      "Step 401 | Loss: 1.875\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 2.25\n",
      "Step 701 | Loss: 2.125\n",
      "Step 801 | Loss: 2.375\n",
      "Step 901 | Loss: 2.25\n",
      "Step 1001 | Loss: 2.375\n",
      "Step 1101 | Loss: 1.375\n",
      "Step 1201 | Loss: 1.875\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 2.0\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 1.25\n",
      "Step 1701 | Loss: 1.75\n",
      "Step 1801 | Loss: 2.625\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 1.625\n",
      "Step 2201 | Loss: 2.375\n",
      "Step 2301 | Loss: 2.25\n",
      "Step 2401 | Loss: 2.125\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.6191763877868652\n",
      "Step 101 | Loss: 0.15051542222499847\n",
      "Step 201 | Loss: 0.07245486229658127\n",
      "Step 301 | Loss: 0.1282709538936615\n",
      "Step 401 | Loss: 0.17187242209911346\n",
      "Step 501 | Loss: 0.11366604268550873\n",
      "Step 601 | Loss: 0.1742401123046875\n",
      "Step 701 | Loss: 0.09178008139133453\n",
      "Step 801 | Loss: 0.15671610832214355\n",
      "Step 901 | Loss: 0.17741525173187256\n",
      "Step 1001 | Loss: 0.09196429699659348\n",
      "Step 1101 | Loss: 0.10792108625173569\n",
      "Step 1201 | Loss: 0.13266335427761078\n",
      "Step 1301 | Loss: 0.1400158852338791\n",
      "Step 1401 | Loss: 0.1074780747294426\n",
      "Step 1501 | Loss: 0.11604835093021393\n",
      "Step 1601 | Loss: 0.07573959231376648\n",
      "Step 1701 | Loss: 0.08468896895647049\n",
      "Step 1801 | Loss: 0.11802986264228821\n",
      "Step 1901 | Loss: 0.20243680477142334\n",
      "Step 2001 | Loss: 0.11243947595357895\n",
      "Step 2101 | Loss: 0.0680510550737381\n",
      "Step 2201 | Loss: 0.16786320507526398\n",
      "Step 2301 | Loss: 0.10577058047056198\n",
      "Step 2401 | Loss: 0.21066270768642426\n",
      "0.1146504282630444 0.9775\n",
      "Step 1 | Loss: 0.5425659418106079\n",
      "Step 101 | Loss: 0.17453710734844208\n",
      "Step 201 | Loss: 0.13972008228302002\n",
      "Step 301 | Loss: 0.07614575326442719\n",
      "Step 401 | Loss: 0.2102002650499344\n",
      "Step 501 | Loss: 0.13355652987957\n",
      "Step 601 | Loss: 0.12327264249324799\n",
      "Step 701 | Loss: 0.08837763220071793\n",
      "Step 801 | Loss: 0.11218604445457458\n",
      "Step 901 | Loss: 0.13816359639167786\n",
      "Step 1001 | Loss: 0.1127786934375763\n",
      "Step 1101 | Loss: 0.17574989795684814\n",
      "Step 1201 | Loss: 0.25488609075546265\n",
      "Step 1301 | Loss: 0.08702147752046585\n",
      "Step 1401 | Loss: 0.11151650547981262\n",
      "Step 1501 | Loss: 0.18897798657417297\n",
      "Step 1601 | Loss: 0.07390239089727402\n",
      "Step 1701 | Loss: 0.08637990802526474\n",
      "Step 1801 | Loss: 0.10464667528867722\n",
      "Step 1901 | Loss: 0.07265540957450867\n",
      "Step 2001 | Loss: 0.13818931579589844\n",
      "Step 2101 | Loss: 0.1368829756975174\n",
      "Step 2201 | Loss: 0.06190221384167671\n",
      "Step 2301 | Loss: 0.13159459829330444\n",
      "Step 2401 | Loss: 0.08794441819190979\n",
      "0.11578303298473441 0.98\n",
      "Step 1 | Loss: 0.787706196308136\n",
      "Step 101 | Loss: 0.14886151254177094\n",
      "Step 201 | Loss: 0.2211233228445053\n",
      "Step 301 | Loss: 0.22537855803966522\n",
      "Step 401 | Loss: 0.19230227172374725\n",
      "Step 501 | Loss: 0.1454085111618042\n",
      "Step 601 | Loss: 0.06992559880018234\n",
      "Step 701 | Loss: 0.13905927538871765\n",
      "Step 801 | Loss: 0.1306128054857254\n",
      "Step 901 | Loss: 0.13326776027679443\n",
      "Step 1001 | Loss: 0.10557478666305542\n",
      "Step 1101 | Loss: 0.08311240375041962\n",
      "Step 1201 | Loss: 0.1092405617237091\n",
      "Step 1301 | Loss: 0.07120642066001892\n",
      "Step 1401 | Loss: 0.1447136551141739\n",
      "Step 1501 | Loss: 0.17711639404296875\n",
      "Step 1601 | Loss: 0.13602270185947418\n",
      "Step 1701 | Loss: 0.1539352536201477\n",
      "Step 1801 | Loss: 0.19320011138916016\n",
      "Step 1901 | Loss: 0.053916968405246735\n",
      "Step 2001 | Loss: 0.09196728467941284\n",
      "Step 2101 | Loss: 0.1422617882490158\n",
      "Step 2201 | Loss: 0.05624150484800339\n",
      "Step 2301 | Loss: 0.09492640197277069\n",
      "Step 2401 | Loss: 0.12275025248527527\n",
      "0.11527941252090218 0.9775\n",
      "Step 1 | Loss: 1.2034741640090942\n",
      "Step 101 | Loss: 0.15924745798110962\n",
      "Step 201 | Loss: 0.05045858398079872\n",
      "Step 301 | Loss: 0.15985353291034698\n",
      "Step 401 | Loss: 0.1255408525466919\n",
      "Step 501 | Loss: 0.1467157006263733\n",
      "Step 601 | Loss: 0.13184474408626556\n",
      "Step 701 | Loss: 0.13057374954223633\n",
      "Step 801 | Loss: 0.11237069219350815\n",
      "Step 901 | Loss: 0.09380374848842621\n",
      "Step 1001 | Loss: 0.12291121482849121\n",
      "Step 1101 | Loss: 0.12289350479841232\n",
      "Step 1201 | Loss: 0.132070392370224\n",
      "Step 1301 | Loss: 0.07670928537845612\n",
      "Step 1401 | Loss: 0.12789380550384521\n",
      "Step 1501 | Loss: 0.049757398664951324\n",
      "Step 1601 | Loss: 0.1149115264415741\n",
      "Step 1701 | Loss: 0.16504564881324768\n",
      "Step 1801 | Loss: 0.217927485704422\n",
      "Step 1901 | Loss: 0.0987943708896637\n",
      "Step 2001 | Loss: 0.08727355301380157\n",
      "Step 2101 | Loss: 0.07658630609512329\n",
      "Step 2201 | Loss: 0.08980978280305862\n",
      "Step 2301 | Loss: 0.16620206832885742\n",
      "Step 2401 | Loss: 0.13258987665176392\n",
      "0.11520652975116694 0.9775\n",
      "Step 1 | Loss: 1.9238851070404053\n",
      "Step 101 | Loss: 0.1265706717967987\n",
      "Step 201 | Loss: 0.09794239699840546\n",
      "Step 301 | Loss: 0.0866549164056778\n",
      "Step 401 | Loss: 0.051162347197532654\n",
      "Step 501 | Loss: 0.08814513683319092\n",
      "Step 601 | Loss: 0.11710689961910248\n",
      "Step 701 | Loss: 0.1399950087070465\n",
      "Step 801 | Loss: 0.09079237282276154\n",
      "Step 901 | Loss: 0.09471301734447479\n",
      "Step 1001 | Loss: 0.20309384167194366\n",
      "Step 1101 | Loss: 0.11875738948583603\n",
      "Step 1201 | Loss: 0.08301880210638046\n",
      "Step 1301 | Loss: 0.12930220365524292\n",
      "Step 1401 | Loss: 0.09463242441415787\n",
      "Step 1501 | Loss: 0.07375635206699371\n",
      "Step 1601 | Loss: 0.17512430250644684\n",
      "Step 1701 | Loss: 0.10852831602096558\n",
      "Step 1801 | Loss: 0.10595378279685974\n",
      "Step 1901 | Loss: 0.10995331406593323\n",
      "Step 2001 | Loss: 0.09453333169221878\n",
      "Step 2101 | Loss: 0.12617579102516174\n",
      "Step 2201 | Loss: 0.08856409788131714\n",
      "Step 2301 | Loss: 0.09619363397359848\n",
      "Step 2401 | Loss: 0.10429928451776505\n",
      "0.11503134886038933 0.9775\n",
      "Step 1 | Loss: 1.3262341022491455\n",
      "Step 101 | Loss: 0.21496257185935974\n",
      "Step 201 | Loss: 0.3195384442806244\n",
      "Step 301 | Loss: 0.2749073803424835\n",
      "Step 401 | Loss: 0.21849468350410461\n",
      "Step 501 | Loss: 0.28901225328445435\n",
      "Step 601 | Loss: 0.22846995294094086\n",
      "Step 701 | Loss: 0.26987525820732117\n",
      "Step 801 | Loss: 0.24220916628837585\n",
      "Step 901 | Loss: 0.18885166943073273\n",
      "Step 1001 | Loss: 0.3420500159263611\n",
      "Step 1101 | Loss: 0.20949584245681763\n",
      "Step 1201 | Loss: 0.31825098395347595\n",
      "Step 1301 | Loss: 0.337845116853714\n",
      "Step 1401 | Loss: 0.21928717195987701\n",
      "Step 1501 | Loss: 0.17558293044567108\n",
      "Step 1601 | Loss: 0.3068368434906006\n",
      "Step 1701 | Loss: 0.22592946887016296\n",
      "Step 1801 | Loss: 0.34023863077163696\n",
      "Step 1901 | Loss: 0.2795924246311188\n",
      "Step 2001 | Loss: 0.3312006890773773\n",
      "Step 2101 | Loss: 0.34323355555534363\n",
      "Step 2201 | Loss: 0.19265492260456085\n",
      "Step 2301 | Loss: 0.3622472286224365\n",
      "Step 2401 | Loss: 0.20740878582000732\n",
      "0.2325089470539191 0.975\n",
      "Step 1 | Loss: 1.7470353841781616\n",
      "Step 101 | Loss: 0.38391509652137756\n",
      "Step 201 | Loss: 0.3588349223136902\n",
      "Step 301 | Loss: 0.22249332070350647\n",
      "Step 401 | Loss: 0.2513009309768677\n",
      "Step 501 | Loss: 0.2516319155693054\n",
      "Step 601 | Loss: 0.2061089426279068\n",
      "Step 701 | Loss: 0.2407011240720749\n",
      "Step 801 | Loss: 0.28137895464897156\n",
      "Step 901 | Loss: 0.30191531777381897\n",
      "Step 1001 | Loss: 0.21400156617164612\n",
      "Step 1101 | Loss: 0.38891011476516724\n",
      "Step 1201 | Loss: 0.24947544932365417\n",
      "Step 1301 | Loss: 0.2114982008934021\n",
      "Step 1401 | Loss: 0.28804510831832886\n",
      "Step 1501 | Loss: 0.2580961585044861\n",
      "Step 1601 | Loss: 0.31336402893066406\n",
      "Step 1701 | Loss: 0.3590620160102844\n",
      "Step 1801 | Loss: 0.16578887403011322\n",
      "Step 1901 | Loss: 0.3170071244239807\n",
      "Step 2001 | Loss: 0.23609739542007446\n",
      "Step 2101 | Loss: 0.23987971246242523\n",
      "Step 2201 | Loss: 0.21158596873283386\n",
      "Step 2301 | Loss: 0.2705065608024597\n",
      "Step 2401 | Loss: 0.19074954092502594\n",
      "0.2345050315181832 0.97\n",
      "Step 1 | Loss: 1.7571828365325928\n",
      "Step 101 | Loss: 0.5149499177932739\n",
      "Step 201 | Loss: 0.19103293120861053\n",
      "Step 301 | Loss: 0.3018500804901123\n",
      "Step 401 | Loss: 0.33158761262893677\n",
      "Step 501 | Loss: 0.29405248165130615\n",
      "Step 601 | Loss: 0.3682074546813965\n",
      "Step 701 | Loss: 0.28050416707992554\n",
      "Step 801 | Loss: 0.43830960988998413\n",
      "Step 901 | Loss: 0.38402047753334045\n",
      "Step 1001 | Loss: 0.255492240190506\n",
      "Step 1101 | Loss: 0.22682954370975494\n",
      "Step 1201 | Loss: 0.2509946823120117\n",
      "Step 1301 | Loss: 0.17837223410606384\n",
      "Step 1401 | Loss: 0.28282830119132996\n",
      "Step 1501 | Loss: 0.218689426779747\n",
      "Step 1601 | Loss: 0.19966086745262146\n",
      "Step 1701 | Loss: 0.1588037759065628\n",
      "Step 1801 | Loss: 0.22507193684577942\n",
      "Step 1901 | Loss: 0.27277594804763794\n",
      "Step 2001 | Loss: 0.18597732484340668\n",
      "Step 2101 | Loss: 0.37926244735717773\n",
      "Step 2201 | Loss: 0.2091297209262848\n",
      "Step 2301 | Loss: 0.2887185513973236\n",
      "Step 2401 | Loss: 0.2463068813085556\n",
      "0.2387870324097306 0.9725\n",
      "Step 1 | Loss: 1.372140645980835\n",
      "Step 101 | Loss: 0.46656838059425354\n",
      "Step 201 | Loss: 0.269896000623703\n",
      "Step 301 | Loss: 0.25171327590942383\n",
      "Step 401 | Loss: 0.17193210124969482\n",
      "Step 501 | Loss: 0.30379462242126465\n",
      "Step 601 | Loss: 0.23223397135734558\n",
      "Step 701 | Loss: 0.24634918570518494\n",
      "Step 801 | Loss: 0.2572409212589264\n",
      "Step 901 | Loss: 0.2823985517024994\n",
      "Step 1001 | Loss: 0.27184048295021057\n",
      "Step 1101 | Loss: 0.22715194523334503\n",
      "Step 1201 | Loss: 0.25565457344055176\n",
      "Step 1301 | Loss: 0.24637249112129211\n",
      "Step 1401 | Loss: 0.17337393760681152\n",
      "Step 1501 | Loss: 0.2615874707698822\n",
      "Step 1601 | Loss: 0.19060398638248444\n",
      "Step 1701 | Loss: 0.2441457360982895\n",
      "Step 1801 | Loss: 0.27304962277412415\n",
      "Step 1901 | Loss: 0.3102189302444458\n",
      "Step 2001 | Loss: 0.14643043279647827\n",
      "Step 2101 | Loss: 0.1443319469690323\n",
      "Step 2201 | Loss: 0.20654088258743286\n",
      "Step 2301 | Loss: 0.22123610973358154\n",
      "Step 2401 | Loss: 0.15732312202453613\n",
      "0.23116556188173623 0.975\n",
      "Step 1 | Loss: 1.1068711280822754\n",
      "Step 101 | Loss: 0.4738795757293701\n",
      "Step 201 | Loss: 0.2443651556968689\n",
      "Step 301 | Loss: 0.343806654214859\n",
      "Step 401 | Loss: 0.24215039610862732\n",
      "Step 501 | Loss: 0.23165056109428406\n",
      "Step 601 | Loss: 0.29651400446891785\n",
      "Step 701 | Loss: 0.47160249948501587\n",
      "Step 801 | Loss: 0.4879339933395386\n",
      "Step 901 | Loss: 0.20158657431602478\n",
      "Step 1001 | Loss: 0.40841835737228394\n",
      "Step 1101 | Loss: 0.2563392221927643\n",
      "Step 1201 | Loss: 0.22344762086868286\n",
      "Step 1301 | Loss: 0.324796199798584\n",
      "Step 1401 | Loss: 0.2998339533805847\n",
      "Step 1501 | Loss: 0.3065078854560852\n",
      "Step 1601 | Loss: 0.19106225669384003\n",
      "Step 1701 | Loss: 0.2601681351661682\n",
      "Step 1801 | Loss: 0.24405580759048462\n",
      "Step 1901 | Loss: 0.25889861583709717\n",
      "Step 2001 | Loss: 0.37039491534233093\n",
      "Step 2101 | Loss: 0.19487550854682922\n",
      "Step 2201 | Loss: 0.29299187660217285\n",
      "Step 2301 | Loss: 0.46297532320022583\n",
      "Step 2401 | Loss: 0.30109018087387085\n",
      "0.23622326347142059 0.97\n",
      "Step 1 | Loss: 1.1370240449905396\n",
      "Step 101 | Loss: 0.8332209587097168\n",
      "Step 201 | Loss: 0.546004593372345\n",
      "Step 301 | Loss: 0.477370947599411\n",
      "Step 401 | Loss: 0.7364468574523926\n",
      "Step 501 | Loss: 0.41485920548439026\n",
      "Step 601 | Loss: 0.5349863767623901\n",
      "Step 701 | Loss: 0.5210537910461426\n",
      "Step 801 | Loss: 0.4477020502090454\n",
      "Step 901 | Loss: 0.5558032393455505\n",
      "Step 1001 | Loss: 0.5981732606887817\n",
      "Step 1101 | Loss: 0.48382705450057983\n",
      "Step 1201 | Loss: 0.613163948059082\n",
      "Step 1301 | Loss: 0.5589765310287476\n",
      "Step 1401 | Loss: 0.5432805418968201\n",
      "Step 1501 | Loss: 0.6012991666793823\n",
      "Step 1601 | Loss: 0.34528300166130066\n",
      "Step 1701 | Loss: 0.5900548696517944\n",
      "Step 1801 | Loss: 0.5565729141235352\n",
      "Step 1901 | Loss: 0.4663504362106323\n",
      "Step 2001 | Loss: 0.5074052810668945\n",
      "Step 2101 | Loss: 0.46254000067710876\n",
      "Step 2201 | Loss: 0.6328349113464355\n",
      "Step 2301 | Loss: 0.4707702696323395\n",
      "Step 2401 | Loss: 0.4331742227077484\n",
      "0.49926280091457936 0.87\n",
      "Step 1 | Loss: 1.115956425666809\n",
      "Step 101 | Loss: 0.9509676098823547\n",
      "Step 201 | Loss: 1.0625017881393433\n",
      "Step 301 | Loss: 0.984871506690979\n",
      "Step 401 | Loss: 0.7583147287368774\n",
      "Step 501 | Loss: 0.7115415334701538\n",
      "Step 601 | Loss: 0.6204328536987305\n",
      "Step 701 | Loss: 0.5265771150588989\n",
      "Step 801 | Loss: 0.5506543517112732\n",
      "Step 901 | Loss: 0.5286096334457397\n",
      "Step 1001 | Loss: 0.5972456932067871\n",
      "Step 1101 | Loss: 0.5762735605239868\n",
      "Step 1201 | Loss: 0.5076814293861389\n",
      "Step 1301 | Loss: 0.4146948754787445\n",
      "Step 1401 | Loss: 0.5498285889625549\n",
      "Step 1501 | Loss: 0.4954998195171356\n",
      "Step 1601 | Loss: 0.4596687853336334\n",
      "Step 1701 | Loss: 0.5991894602775574\n",
      "Step 1801 | Loss: 0.6972450613975525\n",
      "Step 1901 | Loss: 0.542915403842926\n",
      "Step 2001 | Loss: 0.5777713656425476\n",
      "Step 2101 | Loss: 0.45335784554481506\n",
      "Step 2201 | Loss: 0.4362882971763611\n",
      "Step 2301 | Loss: 0.4976294934749603\n",
      "Step 2401 | Loss: 0.2849099636077881\n",
      "0.5434627619500698 0.8775\n",
      "Step 1 | Loss: 1.7706013917922974\n",
      "Step 101 | Loss: 1.246494174003601\n",
      "Step 201 | Loss: 0.7236776351928711\n",
      "Step 301 | Loss: 0.6876327991485596\n",
      "Step 401 | Loss: 0.4647420346736908\n",
      "Step 501 | Loss: 0.3618300259113312\n",
      "Step 601 | Loss: 0.5173395276069641\n",
      "Step 701 | Loss: 0.4681321382522583\n",
      "Step 801 | Loss: 0.5587106347084045\n",
      "Step 901 | Loss: 0.4719661474227905\n",
      "Step 1001 | Loss: 0.4969017207622528\n",
      "Step 1101 | Loss: 0.590461254119873\n",
      "Step 1201 | Loss: 0.45780426263809204\n",
      "Step 1301 | Loss: 0.5457972884178162\n",
      "Step 1401 | Loss: 0.4826778769493103\n",
      "Step 1501 | Loss: 0.4997299611568451\n",
      "Step 1601 | Loss: 0.49333104491233826\n",
      "Step 1701 | Loss: 0.4782087802886963\n",
      "Step 1801 | Loss: 0.6053743362426758\n",
      "Step 1901 | Loss: 0.5586665272712708\n",
      "Step 2001 | Loss: 0.5795130729675293\n",
      "Step 2101 | Loss: 0.5492174029350281\n",
      "Step 2201 | Loss: 0.5394835472106934\n",
      "Step 2301 | Loss: 0.5851742029190063\n",
      "Step 2401 | Loss: 0.48238539695739746\n",
      "0.5044144533667824 0.88\n",
      "Step 1 | Loss: 1.7668101787567139\n",
      "Step 101 | Loss: 0.5872766971588135\n",
      "Step 201 | Loss: 0.42667368054389954\n",
      "Step 301 | Loss: 0.6078823208808899\n",
      "Step 401 | Loss: 0.5806516408920288\n",
      "Step 501 | Loss: 0.5101198554039001\n",
      "Step 601 | Loss: 0.5703765153884888\n",
      "Step 701 | Loss: 0.44200682640075684\n",
      "Step 801 | Loss: 0.5213109850883484\n",
      "Step 901 | Loss: 0.5050499439239502\n",
      "Step 1001 | Loss: 0.4201791286468506\n",
      "Step 1101 | Loss: 0.4466569423675537\n",
      "Step 1201 | Loss: 0.7082408666610718\n",
      "Step 1301 | Loss: 0.48588070273399353\n",
      "Step 1401 | Loss: 0.5297289490699768\n",
      "Step 1501 | Loss: 0.5320026874542236\n",
      "Step 1601 | Loss: 0.5512315630912781\n",
      "Step 1701 | Loss: 0.6060877442359924\n",
      "Step 1801 | Loss: 0.4152336120605469\n",
      "Step 1901 | Loss: 0.4279019236564636\n",
      "Step 2001 | Loss: 0.6064195036888123\n",
      "Step 2101 | Loss: 0.46299296617507935\n",
      "Step 2201 | Loss: 0.5384185910224915\n",
      "Step 2301 | Loss: 0.5405798554420471\n",
      "Step 2401 | Loss: 0.6833384037017822\n",
      "0.545068292133053 0.88\n",
      "Step 1 | Loss: 1.330034852027893\n",
      "Step 101 | Loss: 0.8856256604194641\n",
      "Step 201 | Loss: 0.7638655304908752\n",
      "Step 301 | Loss: 0.94221031665802\n",
      "Step 401 | Loss: 0.7868106365203857\n",
      "Step 501 | Loss: 0.6971603631973267\n",
      "Step 601 | Loss: 0.5582455992698669\n",
      "Step 701 | Loss: 0.4693567156791687\n",
      "Step 801 | Loss: 0.5285147428512573\n",
      "Step 901 | Loss: 0.5793734192848206\n",
      "Step 1001 | Loss: 0.5457556843757629\n",
      "Step 1101 | Loss: 0.643406331539154\n",
      "Step 1201 | Loss: 0.5376008152961731\n",
      "Step 1301 | Loss: 0.4828130602836609\n",
      "Step 1401 | Loss: 0.50571608543396\n",
      "Step 1501 | Loss: 0.5151392221450806\n",
      "Step 1601 | Loss: 0.5607151985168457\n",
      "Step 1701 | Loss: 0.5394728183746338\n",
      "Step 1801 | Loss: 0.45250046253204346\n",
      "Step 1901 | Loss: 0.4289969503879547\n",
      "Step 2001 | Loss: 0.43071094155311584\n",
      "Step 2101 | Loss: 0.6462492346763611\n",
      "Step 2201 | Loss: 0.44882437586784363\n",
      "Step 2301 | Loss: 0.43144690990448\n",
      "Step 2401 | Loss: 0.5766696333885193\n",
      "0.5462070825481158 0.8625\n",
      "Step 1 | Loss: 1.070061206817627\n",
      "Step 101 | Loss: 0.4481273591518402\n",
      "Step 201 | Loss: 0.46199601888656616\n",
      "Step 301 | Loss: 0.6327856779098511\n",
      "Step 401 | Loss: 0.41332870721817017\n",
      "Step 501 | Loss: 0.1555141657590866\n",
      "Step 601 | Loss: 0.2509611248970032\n",
      "Step 701 | Loss: 0.23416756093502045\n",
      "Step 801 | Loss: 0.20920127630233765\n",
      "Step 901 | Loss: 0.45101261138916016\n",
      "Step 1001 | Loss: 0.3430045247077942\n",
      "Step 1101 | Loss: 0.3414350748062134\n",
      "Step 1201 | Loss: 0.44068723917007446\n",
      "Step 1301 | Loss: 0.3972839117050171\n",
      "Step 1401 | Loss: 0.3225407302379608\n",
      "Step 1501 | Loss: 0.17935603857040405\n",
      "Step 1601 | Loss: 0.47023871541023254\n",
      "Step 1701 | Loss: 0.15682832896709442\n",
      "Step 1801 | Loss: 0.46371781826019287\n",
      "Step 1901 | Loss: 0.24162015318870544\n",
      "Step 2001 | Loss: 0.30151402950286865\n",
      "Step 2101 | Loss: 0.3367531895637512\n",
      "Step 2201 | Loss: 0.25076574087142944\n",
      "Step 2301 | Loss: 0.2681881785392761\n",
      "Step 2401 | Loss: 0.24687492847442627\n",
      "0.3363013555456106 0.885\n",
      "Step 1 | Loss: 1.1580694913864136\n",
      "Step 101 | Loss: 0.48120126128196716\n",
      "Step 201 | Loss: 0.28204551339149475\n",
      "Step 301 | Loss: 0.403982937335968\n",
      "Step 401 | Loss: 0.48307251930236816\n",
      "Step 501 | Loss: 0.340027779340744\n",
      "Step 601 | Loss: 0.19178247451782227\n",
      "Step 701 | Loss: 0.26668500900268555\n",
      "Step 801 | Loss: 0.3187839388847351\n",
      "Step 901 | Loss: 0.29926031827926636\n",
      "Step 1001 | Loss: 0.40237370133399963\n",
      "Step 1101 | Loss: 0.4390222430229187\n",
      "Step 1201 | Loss: 0.11584244668483734\n",
      "Step 1301 | Loss: 0.32303670048713684\n",
      "Step 1401 | Loss: 0.3001212477684021\n",
      "Step 1501 | Loss: 0.3252229690551758\n",
      "Step 1601 | Loss: 0.2483714371919632\n",
      "Step 1701 | Loss: 0.23262225091457367\n",
      "Step 1801 | Loss: 0.4327874481678009\n",
      "Step 1901 | Loss: 0.22822023928165436\n",
      "Step 2001 | Loss: 0.26234111189842224\n",
      "Step 2101 | Loss: 0.4376630187034607\n",
      "Step 2201 | Loss: 0.5277212858200073\n",
      "Step 2301 | Loss: 0.42117398977279663\n",
      "Step 2401 | Loss: 0.40283438563346863\n",
      "0.33768184547347424 0.885\n",
      "Step 1 | Loss: 1.0265438556671143\n",
      "Step 101 | Loss: 0.49630802869796753\n",
      "Step 201 | Loss: 0.3492233157157898\n",
      "Step 301 | Loss: 0.3471823036670685\n",
      "Step 401 | Loss: 0.4537920355796814\n",
      "Step 501 | Loss: 0.3467089533805847\n",
      "Step 601 | Loss: 0.5002806782722473\n",
      "Step 701 | Loss: 0.4463537931442261\n",
      "Step 801 | Loss: 0.16990593075752258\n",
      "Step 901 | Loss: 0.43488121032714844\n",
      "Step 1001 | Loss: 0.3095190227031708\n",
      "Step 1101 | Loss: 0.5152347087860107\n",
      "Step 1201 | Loss: 0.34456443786621094\n",
      "Step 1301 | Loss: 0.3615555167198181\n",
      "Step 1401 | Loss: 0.5368574857711792\n",
      "Step 1501 | Loss: 0.507771372795105\n",
      "Step 1601 | Loss: 0.20957903563976288\n",
      "Step 1701 | Loss: 0.29296615719795227\n",
      "Step 1801 | Loss: 0.20608003437519073\n",
      "Step 1901 | Loss: 0.24894221127033234\n",
      "Step 2001 | Loss: 0.31329604983329773\n",
      "Step 2101 | Loss: 0.3289586305618286\n",
      "Step 2201 | Loss: 0.3251609802246094\n",
      "Step 2301 | Loss: 0.43840375542640686\n",
      "Step 2401 | Loss: 0.19957245886325836\n",
      "0.33653655860792414 0.8775\n",
      "Step 1 | Loss: 1.3097591400146484\n",
      "Step 101 | Loss: 0.5811331868171692\n",
      "Step 201 | Loss: 0.37534719705581665\n",
      "Step 301 | Loss: 0.30801984667778015\n",
      "Step 401 | Loss: 0.14848637580871582\n",
      "Step 501 | Loss: 0.39437994360923767\n",
      "Step 601 | Loss: 0.4608868658542633\n",
      "Step 701 | Loss: 0.2947015166282654\n",
      "Step 801 | Loss: 0.24328769743442535\n",
      "Step 901 | Loss: 0.36714380979537964\n",
      "Step 1001 | Loss: 0.3522799611091614\n",
      "Step 1101 | Loss: 0.28589844703674316\n",
      "Step 1201 | Loss: 0.29060399532318115\n",
      "Step 1301 | Loss: 0.19300229847431183\n",
      "Step 1401 | Loss: 0.342104434967041\n",
      "Step 1501 | Loss: 0.37816137075424194\n",
      "Step 1601 | Loss: 0.42644691467285156\n",
      "Step 1701 | Loss: 0.28898292779922485\n",
      "Step 1801 | Loss: 0.24577951431274414\n",
      "Step 1901 | Loss: 0.27743086218833923\n",
      "Step 2001 | Loss: 0.46846461296081543\n",
      "Step 2101 | Loss: 0.2332003265619278\n",
      "Step 2201 | Loss: 0.3997635841369629\n",
      "Step 2301 | Loss: 0.4030632972717285\n",
      "Step 2401 | Loss: 0.4232623875141144\n",
      "0.34304088138002986 0.8725\n",
      "Step 1 | Loss: 0.7946637868881226\n",
      "Step 101 | Loss: 0.5040313601493835\n",
      "Step 201 | Loss: 0.4779767692089081\n",
      "Step 301 | Loss: 0.3268454670906067\n",
      "Step 401 | Loss: 0.4008167088031769\n",
      "Step 501 | Loss: 0.4336051046848297\n",
      "Step 601 | Loss: 0.4524395763874054\n",
      "Step 701 | Loss: 0.23347045481204987\n",
      "Step 801 | Loss: 0.296764612197876\n",
      "Step 901 | Loss: 0.4347260892391205\n",
      "Step 1001 | Loss: 0.6161437630653381\n",
      "Step 1101 | Loss: 0.36421436071395874\n",
      "Step 1201 | Loss: 0.26768895983695984\n",
      "Step 1301 | Loss: 0.3427405059337616\n",
      "Step 1401 | Loss: 0.19253060221672058\n",
      "Step 1501 | Loss: 0.20531854033470154\n",
      "Step 1601 | Loss: 0.31856873631477356\n",
      "Step 1701 | Loss: 0.339505136013031\n",
      "Step 1801 | Loss: 0.4501359164714813\n",
      "Step 1901 | Loss: 0.3915325403213501\n",
      "Step 2001 | Loss: 0.5623769164085388\n",
      "Step 2101 | Loss: 0.31595727801322937\n",
      "Step 2201 | Loss: 0.19045549631118774\n",
      "Step 2301 | Loss: 0.17905384302139282\n",
      "Step 2401 | Loss: 0.300496906042099\n",
      "0.33625129163030637 0.8875\n",
      "Step 1 | Loss: 0.9406346678733826\n",
      "Step 101 | Loss: 0.47405412793159485\n",
      "Step 201 | Loss: 0.49461984634399414\n",
      "Step 301 | Loss: 0.46770310401916504\n",
      "Step 401 | Loss: 0.4302540123462677\n",
      "Step 501 | Loss: 0.6242251396179199\n",
      "Step 601 | Loss: 0.6249443292617798\n",
      "Step 701 | Loss: 0.6245753169059753\n",
      "Step 801 | Loss: 0.625838041305542\n",
      "Step 901 | Loss: 0.5397894382476807\n",
      "Step 1001 | Loss: 0.7193881273269653\n",
      "Step 1101 | Loss: 0.5094934105873108\n",
      "Step 1201 | Loss: 0.44865337014198303\n",
      "Step 1301 | Loss: 0.5368360877037048\n",
      "Step 1401 | Loss: 0.6210957169532776\n",
      "Step 1501 | Loss: 0.3826020658016205\n",
      "Step 1601 | Loss: 0.45391419529914856\n",
      "Step 1701 | Loss: 0.41607704758644104\n",
      "Step 1801 | Loss: 0.5737258195877075\n",
      "Step 1901 | Loss: 0.4629981815814972\n",
      "Step 2001 | Loss: 0.45367956161499023\n",
      "Step 2101 | Loss: 0.3884614109992981\n",
      "Step 2201 | Loss: 0.5087466835975647\n",
      "Step 2301 | Loss: 0.6078341603279114\n",
      "Step 2401 | Loss: 0.5869434475898743\n",
      "0.5134805588353222 0.87\n",
      "Step 1 | Loss: 0.9462840557098389\n",
      "Step 101 | Loss: 0.3816162943840027\n",
      "Step 201 | Loss: 0.37517470121383667\n",
      "Step 301 | Loss: 0.3084167540073395\n",
      "Step 401 | Loss: 0.2536168098449707\n",
      "Step 501 | Loss: 0.33243298530578613\n",
      "Step 601 | Loss: 0.3153572678565979\n",
      "Step 701 | Loss: 0.41818147897720337\n",
      "Step 801 | Loss: 0.3140256106853485\n",
      "Step 901 | Loss: 0.2304549217224121\n",
      "Step 1001 | Loss: 0.2146579623222351\n",
      "Step 1101 | Loss: 0.3823917806148529\n",
      "Step 1201 | Loss: 0.33030301332473755\n",
      "Step 1301 | Loss: 0.31251755356788635\n",
      "Step 1401 | Loss: 0.36854255199432373\n",
      "Step 1501 | Loss: 0.2537117004394531\n",
      "Step 1601 | Loss: 0.4764038920402527\n",
      "Step 1701 | Loss: 0.4797309339046478\n",
      "Step 1801 | Loss: 0.3896484673023224\n",
      "Step 1901 | Loss: 0.28817611932754517\n",
      "Step 2001 | Loss: 0.4515768885612488\n",
      "Step 2101 | Loss: 0.40182939171791077\n",
      "Step 2201 | Loss: 0.33419525623321533\n",
      "Step 2301 | Loss: 0.25680065155029297\n",
      "Step 2401 | Loss: 0.32042109966278076\n",
      "0.35743517882298437 0.945\n",
      "Step 1 | Loss: 1.2138546705245972\n",
      "Step 101 | Loss: 0.7244522571563721\n",
      "Step 201 | Loss: 0.5863338708877563\n",
      "Step 301 | Loss: 0.5543715357780457\n",
      "Step 401 | Loss: 0.7956185936927795\n",
      "Step 501 | Loss: 0.47719666361808777\n",
      "Step 601 | Loss: 0.6606853008270264\n",
      "Step 701 | Loss: 0.41003304719924927\n",
      "Step 801 | Loss: 0.39423924684524536\n",
      "Step 901 | Loss: 0.4747730493545532\n",
      "Step 1001 | Loss: 0.4904019832611084\n",
      "Step 1101 | Loss: 0.2515757381916046\n",
      "Step 1201 | Loss: 0.3962313234806061\n",
      "Step 1301 | Loss: 0.3677903115749359\n",
      "Step 1401 | Loss: 0.40189263224601746\n",
      "Step 1501 | Loss: 0.36326852440834045\n",
      "Step 1601 | Loss: 0.3435436189174652\n",
      "Step 1701 | Loss: 0.4184643626213074\n",
      "Step 1801 | Loss: 0.18592765927314758\n",
      "Step 1901 | Loss: 0.2873953878879547\n",
      "Step 2001 | Loss: 0.3828740417957306\n",
      "Step 2101 | Loss: 0.37324240803718567\n",
      "Step 2201 | Loss: 0.2501005232334137\n",
      "Step 2301 | Loss: 0.26427361369132996\n",
      "Step 2401 | Loss: 0.41897398233413696\n",
      "0.3560729500090608 0.9475\n",
      "Step 1 | Loss: 1.4473735094070435\n",
      "Step 101 | Loss: 1.2722395658493042\n",
      "Step 201 | Loss: 0.9995076656341553\n",
      "Step 301 | Loss: 0.8630080819129944\n",
      "Step 401 | Loss: 0.7196232080459595\n",
      "Step 501 | Loss: 0.6355828046798706\n",
      "Step 601 | Loss: 0.838356077671051\n",
      "Step 701 | Loss: 0.685141921043396\n",
      "Step 801 | Loss: 0.7359808087348938\n",
      "Step 901 | Loss: 0.6706361770629883\n",
      "Step 1001 | Loss: 0.7850470542907715\n",
      "Step 1101 | Loss: 0.7323732376098633\n",
      "Step 1201 | Loss: 0.8032599687576294\n",
      "Step 1301 | Loss: 0.8157630562782288\n",
      "Step 1401 | Loss: 0.8493319153785706\n",
      "Step 1501 | Loss: 0.8513491749763489\n",
      "Step 1601 | Loss: 0.7960067987442017\n",
      "Step 1701 | Loss: 1.0185010433197021\n",
      "Step 1801 | Loss: 0.5823924541473389\n",
      "Step 1901 | Loss: 0.7664055824279785\n",
      "Step 2001 | Loss: 0.8837610483169556\n",
      "Step 2101 | Loss: 0.8442855477333069\n",
      "Step 2201 | Loss: 0.690767765045166\n",
      "Step 2301 | Loss: 0.799764096736908\n",
      "Step 2401 | Loss: 0.5782282948493958\n",
      "0.8779870369940901 0.66\n",
      "Step 1 | Loss: 1.1608772277832031\n",
      "Step 101 | Loss: 0.5774843096733093\n",
      "Step 201 | Loss: 0.38854432106018066\n",
      "Step 301 | Loss: 0.3994288742542267\n",
      "Step 401 | Loss: 0.3380219638347626\n",
      "Step 501 | Loss: 0.37502574920654297\n",
      "Step 601 | Loss: 0.3865194320678711\n",
      "Step 701 | Loss: 0.3606443405151367\n",
      "Step 801 | Loss: 0.5187570452690125\n",
      "Step 901 | Loss: 0.40240687131881714\n",
      "Step 1001 | Loss: 0.28384533524513245\n",
      "Step 1101 | Loss: 0.4748035669326782\n",
      "Step 1201 | Loss: 0.3277088701725006\n",
      "Step 1301 | Loss: 0.2495485544204712\n",
      "Step 1401 | Loss: 0.3029370605945587\n",
      "Step 1501 | Loss: 0.2601320147514343\n",
      "Step 1601 | Loss: 0.3244139552116394\n",
      "Step 1701 | Loss: 0.239115372300148\n",
      "Step 1801 | Loss: 0.23183682560920715\n",
      "Step 1901 | Loss: 0.25592923164367676\n",
      "Step 2001 | Loss: 0.3118889033794403\n",
      "Step 2101 | Loss: 0.5156709551811218\n",
      "Step 2201 | Loss: 0.30597105622291565\n",
      "Step 2301 | Loss: 0.23319242894649506\n",
      "Step 2401 | Loss: 0.2417469322681427\n",
      "0.3579443580625614 0.945\n",
      "Step 1 | Loss: 1.3733623027801514\n",
      "Step 101 | Loss: 0.9955524206161499\n",
      "Step 201 | Loss: 0.9499296545982361\n",
      "Step 301 | Loss: 0.8310337066650391\n",
      "Step 401 | Loss: 0.8504546284675598\n",
      "Step 501 | Loss: 0.7243019342422485\n",
      "Step 601 | Loss: 0.6117763519287109\n",
      "Step 701 | Loss: 0.5539962649345398\n",
      "Step 801 | Loss: 0.3246345520019531\n",
      "Step 901 | Loss: 0.13967910408973694\n",
      "Step 1001 | Loss: 0.14997312426567078\n",
      "Step 1101 | Loss: 0.15577535331249237\n",
      "Step 1201 | Loss: 0.28257784247398376\n",
      "Step 1301 | Loss: 0.18971571326255798\n",
      "Step 1401 | Loss: 0.14703698456287384\n",
      "Step 1501 | Loss: 0.32079237699508667\n",
      "Step 1601 | Loss: 0.20622701942920685\n",
      "Step 1701 | Loss: 0.10716398060321808\n",
      "Step 1801 | Loss: 0.16979382932186127\n",
      "Step 1901 | Loss: 0.26117801666259766\n",
      "Step 2001 | Loss: 0.08808814734220505\n",
      "Step 2101 | Loss: 0.18199363350868225\n",
      "Step 2201 | Loss: 0.14683210849761963\n",
      "Step 2301 | Loss: 0.11150951683521271\n",
      "Step 2401 | Loss: 0.17868947982788086\n",
      "0.14720695964680705 0.97\n",
      "Step 1 | Loss: 1.4099400043487549\n",
      "Step 101 | Loss: 0.47175920009613037\n",
      "Step 201 | Loss: 0.547278106212616\n",
      "Step 301 | Loss: 0.30561304092407227\n",
      "Step 401 | Loss: 0.390159010887146\n",
      "Step 501 | Loss: 0.36341580748558044\n",
      "Step 601 | Loss: 0.35185152292251587\n",
      "Step 701 | Loss: 0.4120415449142456\n",
      "Step 801 | Loss: 0.39160168170928955\n",
      "Step 901 | Loss: 0.38921642303466797\n",
      "Step 1001 | Loss: 0.28680580854415894\n",
      "Step 1101 | Loss: 0.3122318387031555\n",
      "Step 1201 | Loss: 0.49850988388061523\n",
      "Step 1301 | Loss: 0.3513651490211487\n",
      "Step 1401 | Loss: 0.4524652361869812\n",
      "Step 1501 | Loss: 0.22935272753238678\n",
      "Step 1601 | Loss: 0.3170299530029297\n",
      "Step 1701 | Loss: 0.29882317781448364\n",
      "Step 1801 | Loss: 0.4717520475387573\n",
      "Step 1901 | Loss: 0.4420243203639984\n",
      "Step 2001 | Loss: 0.2402186244726181\n",
      "Step 2101 | Loss: 0.3493998050689697\n",
      "Step 2201 | Loss: 0.3164525628089905\n",
      "Step 2301 | Loss: 0.2766614556312561\n",
      "Step 2401 | Loss: 0.3290994465351105\n",
      "0.35581613283690855 0.9025\n",
      "Step 1 | Loss: 0.8165126442909241\n",
      "Step 101 | Loss: 0.5976355671882629\n",
      "Step 201 | Loss: 0.4688149392604828\n",
      "Step 301 | Loss: 0.4040738046169281\n",
      "Step 401 | Loss: 0.2625912129878998\n",
      "Step 501 | Loss: 0.4588831961154938\n",
      "Step 601 | Loss: 0.3371594548225403\n",
      "Step 701 | Loss: 0.27442803978919983\n",
      "Step 801 | Loss: 0.355203777551651\n",
      "Step 901 | Loss: 0.274503231048584\n",
      "Step 1001 | Loss: 0.34639430046081543\n",
      "Step 1101 | Loss: 0.39127153158187866\n",
      "Step 1201 | Loss: 0.3746662735939026\n",
      "Step 1301 | Loss: 0.31249767541885376\n",
      "Step 1401 | Loss: 0.2896898090839386\n",
      "Step 1501 | Loss: 0.25657060742378235\n",
      "Step 1601 | Loss: 0.2848592698574066\n",
      "Step 1701 | Loss: 0.21909235417842865\n",
      "Step 1801 | Loss: 0.32160812616348267\n",
      "Step 1901 | Loss: 0.2191818356513977\n",
      "Step 2001 | Loss: 0.3909936547279358\n",
      "Step 2101 | Loss: 0.24918651580810547\n",
      "Step 2201 | Loss: 0.29725244641304016\n",
      "Step 2301 | Loss: 0.24843358993530273\n",
      "Step 2401 | Loss: 0.3283271789550781\n",
      "0.2675709499823981 0.96\n",
      "Step 1 | Loss: 1.3466144800186157\n",
      "Step 101 | Loss: 0.645043671131134\n",
      "Step 201 | Loss: 0.3350437879562378\n",
      "Step 301 | Loss: 0.21022507548332214\n",
      "Step 401 | Loss: 0.2155197113752365\n",
      "Step 501 | Loss: 0.21937116980552673\n",
      "Step 601 | Loss: 0.3358116149902344\n",
      "Step 701 | Loss: 0.3775599002838135\n",
      "Step 801 | Loss: 0.2812366783618927\n",
      "Step 901 | Loss: 0.2600921392440796\n",
      "Step 1001 | Loss: 0.250918447971344\n",
      "Step 1101 | Loss: 0.25626224279403687\n",
      "Step 1201 | Loss: 0.2703399062156677\n",
      "Step 1301 | Loss: 0.18069890141487122\n",
      "Step 1401 | Loss: 0.31982454657554626\n",
      "Step 1501 | Loss: 0.32175853848457336\n",
      "Step 1601 | Loss: 0.2737385630607605\n",
      "Step 1701 | Loss: 0.30136966705322266\n",
      "Step 1801 | Loss: 0.35548049211502075\n",
      "Step 1901 | Loss: 0.3638947308063507\n",
      "Step 2001 | Loss: 0.28126904368400574\n",
      "Step 2101 | Loss: 0.2711943984031677\n",
      "Step 2201 | Loss: 0.3048923909664154\n",
      "Step 2301 | Loss: 0.16452789306640625\n",
      "Step 2401 | Loss: 0.34807029366493225\n",
      "0.2644674695469197 0.965\n",
      "Step 1 | Loss: 0.4186801612377167\n",
      "Step 101 | Loss: 0.3838428258895874\n",
      "Step 201 | Loss: 0.2362336963415146\n",
      "Step 301 | Loss: 0.3707614839076996\n",
      "Step 401 | Loss: 0.23458677530288696\n",
      "Step 501 | Loss: 0.117374949157238\n",
      "Step 601 | Loss: 0.19297412037849426\n",
      "Step 701 | Loss: 0.2728566527366638\n",
      "Step 801 | Loss: 0.1569213718175888\n",
      "Step 901 | Loss: 0.14636944234371185\n",
      "Step 1001 | Loss: 0.2711220979690552\n",
      "Step 1101 | Loss: 0.17330379784107208\n",
      "Step 1201 | Loss: 0.1613430380821228\n",
      "Step 1301 | Loss: 0.12015096843242645\n",
      "Step 1401 | Loss: 0.18282082676887512\n",
      "Step 1501 | Loss: 0.21081072092056274\n",
      "Step 1601 | Loss: 0.2917095124721527\n",
      "Step 1701 | Loss: 0.2685382068157196\n",
      "Step 1801 | Loss: 0.300729900598526\n",
      "Step 1901 | Loss: 0.188284233212471\n",
      "Step 2001 | Loss: 0.13664166629314423\n",
      "Step 2101 | Loss: 0.13007518649101257\n",
      "Step 2201 | Loss: 0.2771190106868744\n",
      "Step 2301 | Loss: 0.1849437952041626\n",
      "Step 2401 | Loss: 0.19479216635227203\n",
      "0.21968148471542978 0.945\n",
      "Step 1 | Loss: 0.7856807112693787\n",
      "Step 101 | Loss: 0.5176398754119873\n",
      "Step 201 | Loss: 0.6361202597618103\n",
      "Step 301 | Loss: 0.6066197752952576\n",
      "Step 401 | Loss: 0.44744712114334106\n",
      "Step 501 | Loss: 0.49059855937957764\n",
      "Step 601 | Loss: 0.5061770081520081\n",
      "Step 701 | Loss: 0.6370828747749329\n",
      "Step 801 | Loss: 0.5026864409446716\n",
      "Step 901 | Loss: 0.5319786071777344\n",
      "Step 1001 | Loss: 0.48750901222229004\n",
      "Step 1101 | Loss: 0.6234979033470154\n",
      "Step 1201 | Loss: 0.5707873702049255\n",
      "Step 1301 | Loss: 0.7162556648254395\n",
      "Step 1401 | Loss: 0.508557140827179\n",
      "Step 1501 | Loss: 0.5462408065795898\n",
      "Step 1601 | Loss: 0.38405120372772217\n",
      "Step 1701 | Loss: 0.5685744285583496\n",
      "Step 1801 | Loss: 0.49607959389686584\n",
      "Step 1901 | Loss: 0.584462583065033\n",
      "Step 2001 | Loss: 0.6615417003631592\n",
      "Step 2101 | Loss: 0.501023530960083\n",
      "Step 2201 | Loss: 0.6193406581878662\n",
      "Step 2301 | Loss: 0.45317524671554565\n",
      "Step 2401 | Loss: 0.7413495182991028\n",
      "0.5757027561255067 0.7925\n",
      "Step 1 | Loss: 1.5945780277252197\n",
      "Step 101 | Loss: 0.5738563537597656\n",
      "Step 201 | Loss: 0.5546477437019348\n",
      "Step 301 | Loss: 0.4513690769672394\n",
      "Step 401 | Loss: 0.7683397531509399\n",
      "Step 501 | Loss: 0.7270067930221558\n",
      "Step 601 | Loss: 0.5992628335952759\n",
      "Step 701 | Loss: 0.5076698660850525\n",
      "Step 801 | Loss: 0.6135926246643066\n",
      "Step 901 | Loss: 0.7400537729263306\n",
      "Step 1001 | Loss: 0.6524172425270081\n",
      "Step 1101 | Loss: 0.6399033069610596\n",
      "Step 1201 | Loss: 0.7544883489608765\n",
      "Step 1301 | Loss: 0.6322454214096069\n",
      "Step 1401 | Loss: 0.6078525185585022\n",
      "Step 1501 | Loss: 0.575556755065918\n",
      "Step 1601 | Loss: 0.7187445759773254\n",
      "Step 1701 | Loss: 0.5662319660186768\n",
      "Step 1801 | Loss: 0.4690013527870178\n",
      "Step 1901 | Loss: 0.500545084476471\n",
      "Step 2001 | Loss: 0.6746768951416016\n",
      "Step 2101 | Loss: 0.6091180443763733\n",
      "Step 2201 | Loss: 0.6212117075920105\n",
      "Step 2301 | Loss: 0.455296128988266\n",
      "Step 2401 | Loss: 0.6005533933639526\n",
      "0.5778796127095719 0.7825\n",
      "Step 1 | Loss: 2.0115413665771484\n",
      "Step 101 | Loss: 0.8040325045585632\n",
      "Step 201 | Loss: 0.4697984457015991\n",
      "Step 301 | Loss: 0.6342129707336426\n",
      "Step 401 | Loss: 0.6133564710617065\n",
      "Step 501 | Loss: 0.50235515832901\n",
      "Step 601 | Loss: 0.6883701682090759\n",
      "Step 701 | Loss: 0.6037552952766418\n",
      "Step 801 | Loss: 0.6320364475250244\n",
      "Step 901 | Loss: 0.518774688243866\n",
      "Step 1001 | Loss: 0.62552809715271\n",
      "Step 1101 | Loss: 0.3896864354610443\n",
      "Step 1201 | Loss: 0.754138708114624\n",
      "Step 1301 | Loss: 0.5722653865814209\n",
      "Step 1401 | Loss: 0.694636881351471\n",
      "Step 1501 | Loss: 0.5931451916694641\n",
      "Step 1601 | Loss: 0.5484564304351807\n",
      "Step 1701 | Loss: 0.43518903851509094\n",
      "Step 1801 | Loss: 0.3029941916465759\n",
      "Step 1901 | Loss: 0.6610929369926453\n",
      "Step 2001 | Loss: 0.6461737751960754\n",
      "Step 2101 | Loss: 0.3841499388217926\n",
      "Step 2201 | Loss: 0.7053909301757812\n",
      "Step 2301 | Loss: 0.8015178442001343\n",
      "Step 2401 | Loss: 0.4643867015838623\n",
      "0.5781729399129634 0.7825\n",
      "Step 1 | Loss: 0.9920737743377686\n",
      "Step 101 | Loss: 0.9028046131134033\n",
      "Step 201 | Loss: 0.5446686744689941\n",
      "Step 301 | Loss: 0.4396016299724579\n",
      "Step 401 | Loss: 0.5744938254356384\n",
      "Step 501 | Loss: 0.6430900692939758\n",
      "Step 601 | Loss: 0.5400115251541138\n",
      "Step 701 | Loss: 0.5873090624809265\n",
      "Step 801 | Loss: 0.5140733122825623\n",
      "Step 901 | Loss: 0.7038611173629761\n",
      "Step 1001 | Loss: 0.6020064949989319\n",
      "Step 1101 | Loss: 0.4294298589229584\n",
      "Step 1201 | Loss: 0.6435114145278931\n",
      "Step 1301 | Loss: 0.4455063045024872\n",
      "Step 1401 | Loss: 0.7420263290405273\n",
      "Step 1501 | Loss: 0.5899616479873657\n",
      "Step 1601 | Loss: 0.5623319745063782\n",
      "Step 1701 | Loss: 0.7153365612030029\n",
      "Step 1801 | Loss: 0.4762120246887207\n",
      "Step 1901 | Loss: 0.5027164220809937\n",
      "Step 2001 | Loss: 0.7007041573524475\n",
      "Step 2101 | Loss: 0.40267524123191833\n",
      "Step 2201 | Loss: 0.5338268280029297\n",
      "Step 2301 | Loss: 0.5079153776168823\n",
      "Step 2401 | Loss: 0.600213348865509\n",
      "0.5752784942669678 0.7975\n",
      "Step 1 | Loss: 1.612870693206787\n",
      "Step 101 | Loss: 0.9874371886253357\n",
      "Step 201 | Loss: 0.6625491380691528\n",
      "Step 301 | Loss: 0.5183097124099731\n",
      "Step 401 | Loss: 0.6403483748435974\n",
      "Step 501 | Loss: 0.6759271621704102\n",
      "Step 601 | Loss: 0.44471830129623413\n",
      "Step 701 | Loss: 0.6818839311599731\n",
      "Step 801 | Loss: 0.5505750179290771\n",
      "Step 901 | Loss: 0.4786972999572754\n",
      "Step 1001 | Loss: 0.46370813250541687\n",
      "Step 1101 | Loss: 0.4189293086528778\n",
      "Step 1201 | Loss: 0.5146915912628174\n",
      "Step 1301 | Loss: 0.5744704604148865\n",
      "Step 1401 | Loss: 0.6559046506881714\n",
      "Step 1501 | Loss: 0.46844637393951416\n",
      "Step 1601 | Loss: 0.5832539796829224\n",
      "Step 1701 | Loss: 0.5694330930709839\n",
      "Step 1801 | Loss: 0.5536103844642639\n",
      "Step 1901 | Loss: 0.5119470357894897\n",
      "Step 2001 | Loss: 0.47022709250450134\n",
      "Step 2101 | Loss: 0.4429558217525482\n",
      "Step 2201 | Loss: 0.6264345645904541\n",
      "Step 2301 | Loss: 0.3638450801372528\n",
      "Step 2401 | Loss: 0.4874505400657654\n",
      "0.5753884850351065 0.7975\n",
      "Step 1 | Loss: 1.0720146894454956\n",
      "Step 101 | Loss: 0.6094065308570862\n",
      "Step 201 | Loss: 0.6192882061004639\n",
      "Step 301 | Loss: 0.4682476818561554\n",
      "Step 401 | Loss: 0.4450554847717285\n",
      "Step 501 | Loss: 0.3748253881931305\n",
      "Step 601 | Loss: 0.6321470141410828\n",
      "Step 701 | Loss: 0.2661273777484894\n",
      "Step 801 | Loss: 0.5699947476387024\n",
      "Step 901 | Loss: 0.3689456582069397\n",
      "Step 1001 | Loss: 0.3343661427497864\n",
      "Step 1101 | Loss: 0.7012850642204285\n",
      "Step 1201 | Loss: 0.6267341375350952\n",
      "Step 1301 | Loss: 0.331150084733963\n",
      "Step 1401 | Loss: 0.3597041368484497\n",
      "Step 1501 | Loss: 0.3908768892288208\n",
      "Step 1601 | Loss: 0.3058297038078308\n",
      "Step 1701 | Loss: 0.4666377305984497\n",
      "Step 1801 | Loss: 0.1981501430273056\n",
      "Step 1901 | Loss: 0.7206008434295654\n",
      "Step 2001 | Loss: 0.42603981494903564\n",
      "Step 2101 | Loss: 0.496297150850296\n",
      "Step 2201 | Loss: 0.41068050265312195\n",
      "Step 2301 | Loss: 0.5646147131919861\n",
      "Step 2401 | Loss: 0.48203396797180176\n",
      "0.469953260062705 0.8475\n",
      "Step 1 | Loss: 0.960641622543335\n",
      "Step 101 | Loss: 0.7023919224739075\n",
      "Step 201 | Loss: 0.47211211919784546\n",
      "Step 301 | Loss: 0.3675827383995056\n",
      "Step 401 | Loss: 0.7160159945487976\n",
      "Step 501 | Loss: 0.46966326236724854\n",
      "Step 601 | Loss: 0.660912275314331\n",
      "Step 701 | Loss: 0.5788521766662598\n",
      "Step 801 | Loss: 0.487163782119751\n",
      "Step 901 | Loss: 0.4171825051307678\n",
      "Step 1001 | Loss: 0.35896605253219604\n",
      "Step 1101 | Loss: 0.5107800960540771\n",
      "Step 1201 | Loss: 0.8812704682350159\n",
      "Step 1301 | Loss: 0.36653798818588257\n",
      "Step 1401 | Loss: 0.6118289828300476\n",
      "Step 1501 | Loss: 0.25389960408210754\n",
      "Step 1601 | Loss: 0.6132721304893494\n",
      "Step 1701 | Loss: 0.7027587890625\n",
      "Step 1801 | Loss: 0.5226978659629822\n",
      "Step 1901 | Loss: 0.42079973220825195\n",
      "Step 2001 | Loss: 0.7389969825744629\n",
      "Step 2101 | Loss: 0.7680039405822754\n",
      "Step 2201 | Loss: 0.4365464448928833\n",
      "Step 2301 | Loss: 0.39185279607772827\n",
      "Step 2401 | Loss: 0.515835165977478\n",
      "0.46994117370368915 0.845\n",
      "Step 1 | Loss: 1.2911717891693115\n",
      "Step 101 | Loss: 0.8175479769706726\n",
      "Step 201 | Loss: 0.8868624567985535\n",
      "Step 301 | Loss: 0.8477537631988525\n",
      "Step 401 | Loss: 0.4419066607952118\n",
      "Step 501 | Loss: 0.19501078128814697\n",
      "Step 601 | Loss: 0.432376503944397\n",
      "Step 701 | Loss: 0.2970452308654785\n",
      "Step 801 | Loss: 0.3454682230949402\n",
      "Step 901 | Loss: 0.2600901126861572\n",
      "Step 1001 | Loss: 0.3690849542617798\n",
      "Step 1101 | Loss: 0.644264817237854\n",
      "Step 1201 | Loss: 0.38437458872795105\n",
      "Step 1301 | Loss: 0.5901559591293335\n",
      "Step 1401 | Loss: 0.6068065166473389\n",
      "Step 1501 | Loss: 0.5351370573043823\n",
      "Step 1601 | Loss: 0.2921772301197052\n",
      "Step 1701 | Loss: 0.35179710388183594\n",
      "Step 1801 | Loss: 0.5104860663414001\n",
      "Step 1901 | Loss: 0.31323927640914917\n",
      "Step 2001 | Loss: 0.48113757371902466\n",
      "Step 2101 | Loss: 0.6249271631240845\n",
      "Step 2201 | Loss: 0.34389418363571167\n",
      "Step 2301 | Loss: 0.3387932777404785\n",
      "Step 2401 | Loss: 0.525998055934906\n",
      "0.4693296500063029 0.8475\n",
      "Step 1 | Loss: 1.1084681749343872\n",
      "Step 101 | Loss: 0.5595554113388062\n",
      "Step 201 | Loss: 0.7045639157295227\n",
      "Step 301 | Loss: 0.5657402276992798\n",
      "Step 401 | Loss: 0.6389929056167603\n",
      "Step 501 | Loss: 0.35139143466949463\n",
      "Step 601 | Loss: 0.3882034718990326\n",
      "Step 701 | Loss: 0.6203571557998657\n",
      "Step 801 | Loss: 0.7213218808174133\n",
      "Step 901 | Loss: 0.4362976551055908\n",
      "Step 1001 | Loss: 0.45932966470718384\n",
      "Step 1101 | Loss: 0.5197147727012634\n",
      "Step 1201 | Loss: 0.34543997049331665\n",
      "Step 1301 | Loss: 0.5542778372764587\n",
      "Step 1401 | Loss: 0.8220300674438477\n",
      "Step 1501 | Loss: 0.4558873176574707\n",
      "Step 1601 | Loss: 0.5697289705276489\n",
      "Step 1701 | Loss: 0.5343681573867798\n",
      "Step 1801 | Loss: 0.6618108749389648\n",
      "Step 1901 | Loss: 0.23910996317863464\n",
      "Step 2001 | Loss: 0.7974579930305481\n",
      "Step 2101 | Loss: 0.4031327962875366\n",
      "Step 2201 | Loss: 0.6829193830490112\n",
      "Step 2301 | Loss: 0.8784673810005188\n",
      "Step 2401 | Loss: 0.6458987593650818\n",
      "0.47014150912553343 0.845\n",
      "Step 1 | Loss: 1.4750088453292847\n",
      "Step 101 | Loss: 0.6526420712471008\n",
      "Step 201 | Loss: 0.6047536730766296\n",
      "Step 301 | Loss: 0.3740362524986267\n",
      "Step 401 | Loss: 0.7220727205276489\n",
      "Step 501 | Loss: 0.3400164544582367\n",
      "Step 601 | Loss: 0.44760966300964355\n",
      "Step 701 | Loss: 0.35053208470344543\n",
      "Step 801 | Loss: 0.39618179202079773\n",
      "Step 901 | Loss: 0.41902685165405273\n",
      "Step 1001 | Loss: 0.37330707907676697\n",
      "Step 1101 | Loss: 0.4396369457244873\n",
      "Step 1201 | Loss: 0.48520979285240173\n",
      "Step 1301 | Loss: 0.30853569507598877\n",
      "Step 1401 | Loss: 0.7194324731826782\n",
      "Step 1501 | Loss: 0.3431270718574524\n",
      "Step 1601 | Loss: 0.21858884394168854\n",
      "Step 1701 | Loss: 0.5252209305763245\n",
      "Step 1801 | Loss: 0.23539382219314575\n",
      "Step 1901 | Loss: 0.6684508323669434\n",
      "Step 2001 | Loss: 0.5316907167434692\n",
      "Step 2101 | Loss: 0.6748218536376953\n",
      "Step 2201 | Loss: 0.3922615349292755\n",
      "Step 2301 | Loss: 0.5317766666412354\n",
      "Step 2401 | Loss: 0.2531473636627197\n",
      "0.46982750960096 0.8475\n",
      "Step 1 | Loss: 0.8239916563034058\n",
      "Step 101 | Loss: 0.6925959587097168\n",
      "Step 201 | Loss: 0.6533409953117371\n",
      "Step 301 | Loss: 0.49235546588897705\n",
      "Step 401 | Loss: 0.6099231243133545\n",
      "Step 501 | Loss: 0.43279731273651123\n",
      "Step 601 | Loss: 0.5238317251205444\n",
      "Step 701 | Loss: 0.578776478767395\n",
      "Step 801 | Loss: 0.595629870891571\n",
      "Step 901 | Loss: 0.6695187091827393\n",
      "Step 1001 | Loss: 0.45771366357803345\n",
      "Step 1101 | Loss: 0.6929471492767334\n",
      "Step 1201 | Loss: 0.7476979494094849\n",
      "Step 1301 | Loss: 0.6931314468383789\n",
      "Step 1401 | Loss: 0.5803896188735962\n",
      "Step 1501 | Loss: 0.6060424447059631\n",
      "Step 1601 | Loss: 0.50920569896698\n",
      "Step 1701 | Loss: 0.5695725679397583\n",
      "Step 1801 | Loss: 0.5521208643913269\n",
      "Step 1901 | Loss: 0.6179317831993103\n",
      "Step 2001 | Loss: 0.5771809816360474\n",
      "Step 2101 | Loss: 0.5149325132369995\n",
      "Step 2201 | Loss: 0.5776501893997192\n",
      "Step 2301 | Loss: 0.5428594946861267\n",
      "Step 2401 | Loss: 0.6829069256782532\n",
      "0.6294466773709522 0.745\n",
      "Step 1 | Loss: 1.6665594577789307\n",
      "Step 101 | Loss: 0.8511152267456055\n",
      "Step 201 | Loss: 0.8996058106422424\n",
      "Step 301 | Loss: 0.7187976837158203\n",
      "Step 401 | Loss: 0.7423966526985168\n",
      "Step 501 | Loss: 0.754213809967041\n",
      "Step 601 | Loss: 0.6495984792709351\n",
      "Step 701 | Loss: 0.6780644059181213\n",
      "Step 801 | Loss: 0.508290708065033\n",
      "Step 901 | Loss: 0.6405806541442871\n",
      "Step 1001 | Loss: 0.6256267428398132\n",
      "Step 1101 | Loss: 0.6211144924163818\n",
      "Step 1201 | Loss: 0.573842465877533\n",
      "Step 1301 | Loss: 0.5452553033828735\n",
      "Step 1401 | Loss: 0.4305134117603302\n",
      "Step 1501 | Loss: 0.5093860626220703\n",
      "Step 1601 | Loss: 0.6385984420776367\n",
      "Step 1701 | Loss: 0.7776272892951965\n",
      "Step 1801 | Loss: 0.5301038026809692\n",
      "Step 1901 | Loss: 0.561602771282196\n",
      "Step 2001 | Loss: 0.37642043828964233\n",
      "Step 2101 | Loss: 0.41284841299057007\n",
      "Step 2201 | Loss: 0.5284360647201538\n",
      "Step 2301 | Loss: 0.4476008713245392\n",
      "Step 2401 | Loss: 0.5043774247169495\n",
      "0.5265414652036049 0.8675\n",
      "Step 1 | Loss: 0.8042544722557068\n",
      "Step 101 | Loss: 0.6562933325767517\n",
      "Step 201 | Loss: 0.6950217485427856\n",
      "Step 301 | Loss: 0.484241247177124\n",
      "Step 401 | Loss: 0.513221025466919\n",
      "Step 501 | Loss: 0.60161954164505\n",
      "Step 601 | Loss: 0.6182386875152588\n",
      "Step 701 | Loss: 0.5518136620521545\n",
      "Step 801 | Loss: 0.5676126480102539\n",
      "Step 901 | Loss: 0.47800227999687195\n",
      "Step 1001 | Loss: 0.598260223865509\n",
      "Step 1101 | Loss: 0.673270583152771\n",
      "Step 1201 | Loss: 0.43710291385650635\n",
      "Step 1301 | Loss: 0.5311683416366577\n",
      "Step 1401 | Loss: 0.4577923119068146\n",
      "Step 1501 | Loss: 0.63241046667099\n",
      "Step 1601 | Loss: 0.4704672694206238\n",
      "Step 1701 | Loss: 0.44870471954345703\n",
      "Step 1801 | Loss: 0.5508639812469482\n",
      "Step 1901 | Loss: 0.6323922872543335\n",
      "Step 2001 | Loss: 0.49450311064720154\n",
      "Step 2101 | Loss: 0.5489264726638794\n",
      "Step 2201 | Loss: 0.563332736492157\n",
      "Step 2301 | Loss: 0.7645597457885742\n",
      "Step 2401 | Loss: 0.3876492381095886\n",
      "0.5257278277262946 0.8725\n",
      "Step 1 | Loss: 0.6620844006538391\n",
      "Step 101 | Loss: 0.5730581879615784\n",
      "Step 201 | Loss: 0.6057302951812744\n",
      "Step 301 | Loss: 0.5881118178367615\n",
      "Step 401 | Loss: 0.6126046180725098\n",
      "Step 501 | Loss: 0.5905339121818542\n",
      "Step 601 | Loss: 0.5610243678092957\n",
      "Step 701 | Loss: 0.6082327365875244\n",
      "Step 801 | Loss: 0.527575671672821\n",
      "Step 901 | Loss: 0.6329783201217651\n",
      "Step 1001 | Loss: 0.4653306007385254\n",
      "Step 1101 | Loss: 0.5816092491149902\n",
      "Step 1201 | Loss: 0.6696219444274902\n",
      "Step 1301 | Loss: 0.6824566125869751\n",
      "Step 1401 | Loss: 0.6735532879829407\n",
      "Step 1501 | Loss: 0.8927594423294067\n",
      "Step 1601 | Loss: 0.6415021419525146\n",
      "Step 1701 | Loss: 0.6978377103805542\n",
      "Step 1801 | Loss: 0.783042848110199\n",
      "Step 1901 | Loss: 0.5945404767990112\n",
      "Step 2001 | Loss: 0.515748918056488\n",
      "Step 2101 | Loss: 0.604526162147522\n",
      "Step 2201 | Loss: 0.6292641758918762\n",
      "Step 2301 | Loss: 0.5406411290168762\n",
      "Step 2401 | Loss: 0.5838958024978638\n",
      "0.6303505027141297 0.7625\n",
      "Step 1 | Loss: 1.7054181098937988\n",
      "Step 101 | Loss: 0.6239103078842163\n",
      "Step 201 | Loss: 0.5416640043258667\n",
      "Step 301 | Loss: 0.42763465642929077\n",
      "Step 401 | Loss: 0.5749359726905823\n",
      "Step 501 | Loss: 0.5824544429779053\n",
      "Step 601 | Loss: 0.5149728655815125\n",
      "Step 701 | Loss: 0.5488308668136597\n",
      "Step 801 | Loss: 0.4458938241004944\n",
      "Step 901 | Loss: 0.7278695106506348\n",
      "Step 1001 | Loss: 0.5957696437835693\n",
      "Step 1101 | Loss: 0.4224846065044403\n",
      "Step 1201 | Loss: 0.5526235103607178\n",
      "Step 1301 | Loss: 0.6442716121673584\n",
      "Step 1401 | Loss: 0.613721489906311\n",
      "Step 1501 | Loss: 0.38161903619766235\n",
      "Step 1601 | Loss: 0.43706369400024414\n",
      "Step 1701 | Loss: 0.5293246507644653\n",
      "Step 1801 | Loss: 0.5298901796340942\n",
      "Step 1901 | Loss: 0.7002529501914978\n",
      "Step 2001 | Loss: 0.3707246780395508\n",
      "Step 2101 | Loss: 0.5285874605178833\n",
      "Step 2201 | Loss: 0.4347171485424042\n",
      "Step 2301 | Loss: 0.4546489417552948\n",
      "Step 2401 | Loss: 0.5648646950721741\n",
      "0.5257660638416672 0.8725\n",
      "Step 1 | Loss: 1.469118595123291\n",
      "Step 101 | Loss: 0.3599397540092468\n",
      "Step 201 | Loss: 0.16572096943855286\n",
      "Step 301 | Loss: 0.23540596663951874\n",
      "Step 401 | Loss: 0.3254799246788025\n",
      "Step 501 | Loss: 0.351957768201828\n",
      "Step 601 | Loss: 0.33203643560409546\n",
      "Step 701 | Loss: 0.2969779372215271\n",
      "Step 801 | Loss: 0.19052301347255707\n",
      "Step 901 | Loss: 0.148936927318573\n",
      "Step 1001 | Loss: 0.33817732334136963\n",
      "Step 1101 | Loss: 0.2577633559703827\n",
      "Step 1201 | Loss: 0.2204645723104477\n",
      "Step 1301 | Loss: 0.19758154451847076\n",
      "Step 1401 | Loss: 0.23865991830825806\n",
      "Step 1501 | Loss: 0.208442822098732\n",
      "Step 1601 | Loss: 0.3334028124809265\n",
      "Step 1701 | Loss: 0.1378626972436905\n",
      "Step 1801 | Loss: 0.2214931696653366\n",
      "Step 1901 | Loss: 0.24340477585792542\n",
      "Step 2001 | Loss: 0.17551922798156738\n",
      "Step 2101 | Loss: 0.20715253055095673\n",
      "Step 2201 | Loss: 0.2801004648208618\n",
      "Step 2301 | Loss: 0.2932533323764801\n",
      "Step 2401 | Loss: 0.26727133989334106\n",
      "0.2273253697442524 0.95\n",
      "Step 1 | Loss: 1.6972486972808838\n",
      "Step 101 | Loss: 0.6825143098831177\n",
      "Step 201 | Loss: 0.3547620475292206\n",
      "Step 301 | Loss: 0.2636612355709076\n",
      "Step 401 | Loss: 0.34028592705726624\n",
      "Step 501 | Loss: 0.2889977991580963\n",
      "Step 601 | Loss: 0.23871278762817383\n",
      "Step 701 | Loss: 0.15938511490821838\n",
      "Step 801 | Loss: 0.22031167149543762\n",
      "Step 901 | Loss: 0.3043864071369171\n",
      "Step 1001 | Loss: 0.2244967371225357\n",
      "Step 1101 | Loss: 0.27085423469543457\n",
      "Step 1201 | Loss: 0.29182541370391846\n",
      "Step 1301 | Loss: 0.2699190080165863\n",
      "Step 1401 | Loss: 0.22011615335941315\n",
      "Step 1501 | Loss: 0.23134064674377441\n",
      "Step 1601 | Loss: 0.23766830563545227\n",
      "Step 1701 | Loss: 0.2217172086238861\n",
      "Step 1801 | Loss: 0.24347415566444397\n",
      "Step 1901 | Loss: 0.366720974445343\n",
      "Step 2001 | Loss: 0.283230185508728\n",
      "Step 2101 | Loss: 0.21949121356010437\n",
      "Step 2201 | Loss: 0.1517014056444168\n",
      "Step 2301 | Loss: 0.23832064867019653\n",
      "Step 2401 | Loss: 0.2828409671783447\n",
      "0.20924108821495585 0.9575\n",
      "Step 1 | Loss: 1.225253939628601\n",
      "Step 101 | Loss: 0.5687831044197083\n",
      "Step 201 | Loss: 0.40082406997680664\n",
      "Step 301 | Loss: 0.3294895589351654\n",
      "Step 401 | Loss: 0.25854986906051636\n",
      "Step 501 | Loss: 0.31384342908859253\n",
      "Step 601 | Loss: 0.336897075176239\n",
      "Step 701 | Loss: 0.15809126198291779\n",
      "Step 801 | Loss: 0.2616221308708191\n",
      "Step 901 | Loss: 0.18275779485702515\n",
      "Step 1001 | Loss: 0.20776179432868958\n",
      "Step 1101 | Loss: 0.23611237108707428\n",
      "Step 1201 | Loss: 0.311963826417923\n",
      "Step 1301 | Loss: 0.30721017718315125\n",
      "Step 1401 | Loss: 0.3646584451198578\n",
      "Step 1501 | Loss: 0.3959575593471527\n",
      "Step 1601 | Loss: 0.26287388801574707\n",
      "Step 1701 | Loss: 0.43602415919303894\n",
      "Step 1801 | Loss: 0.2509540915489197\n",
      "Step 1901 | Loss: 0.23794355988502502\n",
      "Step 2001 | Loss: 0.3296022117137909\n",
      "Step 2101 | Loss: 0.12519045174121857\n",
      "Step 2201 | Loss: 0.1798611432313919\n",
      "Step 2301 | Loss: 0.30193567276000977\n",
      "Step 2401 | Loss: 0.27124956250190735\n",
      "0.22737393484445897 0.9475\n",
      "Step 1 | Loss: 0.38292497396469116\n",
      "Step 101 | Loss: 0.1256527453660965\n",
      "Step 201 | Loss: 0.21131357550621033\n",
      "Step 301 | Loss: 0.3880750238895416\n",
      "Step 401 | Loss: 0.16199344396591187\n",
      "Step 501 | Loss: 0.1577863246202469\n",
      "Step 601 | Loss: 0.2729817032814026\n",
      "Step 701 | Loss: 0.24624082446098328\n",
      "Step 801 | Loss: 0.2658482491970062\n",
      "Step 901 | Loss: 0.26598694920539856\n",
      "Step 1001 | Loss: 0.29524874687194824\n",
      "Step 1101 | Loss: 0.14787918329238892\n",
      "Step 1201 | Loss: 0.17313380539417267\n",
      "Step 1301 | Loss: 0.2301824539899826\n",
      "Step 1401 | Loss: 0.38388633728027344\n",
      "Step 1501 | Loss: 0.2769823372364044\n",
      "Step 1601 | Loss: 0.21355904638767242\n",
      "Step 1701 | Loss: 0.45014891028404236\n",
      "Step 1801 | Loss: 0.1914193034172058\n",
      "Step 1901 | Loss: 0.24340014159679413\n",
      "Step 2001 | Loss: 0.1402532309293747\n",
      "Step 2101 | Loss: 0.18218746781349182\n",
      "Step 2201 | Loss: 0.389332115650177\n",
      "Step 2301 | Loss: 0.15763428807258606\n",
      "Step 2401 | Loss: 0.23184801638126373\n",
      "0.21441551094105815 0.9525\n",
      "Step 1 | Loss: 1.6699796915054321\n",
      "Step 101 | Loss: 0.21003177762031555\n",
      "Step 201 | Loss: 0.4168878495693207\n",
      "Step 301 | Loss: 0.30806291103363037\n",
      "Step 401 | Loss: 0.24979473650455475\n",
      "Step 501 | Loss: 0.24709217250347137\n",
      "Step 601 | Loss: 0.38486266136169434\n",
      "Step 701 | Loss: 0.39107030630111694\n",
      "Step 801 | Loss: 0.2656269669532776\n",
      "Step 901 | Loss: 0.3943351209163666\n",
      "Step 1001 | Loss: 0.13256265223026276\n",
      "Step 1101 | Loss: 0.39799898862838745\n",
      "Step 1201 | Loss: 0.09936459362506866\n",
      "Step 1301 | Loss: 0.2809924781322479\n",
      "Step 1401 | Loss: 0.30784985423088074\n",
      "Step 1501 | Loss: 0.23038463294506073\n",
      "Step 1601 | Loss: 0.17450489103794098\n",
      "Step 1701 | Loss: 0.3421611785888672\n",
      "Step 1801 | Loss: 0.28345826268196106\n",
      "Step 1901 | Loss: 0.3301618993282318\n",
      "Step 2001 | Loss: 0.20153142511844635\n",
      "Step 2101 | Loss: 0.29437756538391113\n",
      "Step 2201 | Loss: 0.1659933626651764\n",
      "Step 2301 | Loss: 0.2721349596977234\n",
      "Step 2401 | Loss: 0.17625822126865387\n",
      "0.23662026420440135 0.9475\n",
      "Step 1 | Loss: 1.0259613990783691\n",
      "Step 101 | Loss: 0.9910511374473572\n",
      "Step 201 | Loss: 0.8306021690368652\n",
      "Step 301 | Loss: 0.9043611884117126\n",
      "Step 401 | Loss: 1.0156538486480713\n",
      "Step 501 | Loss: 0.9146744012832642\n",
      "Step 601 | Loss: 0.7499821186065674\n",
      "Step 701 | Loss: 0.7725785374641418\n",
      "Step 801 | Loss: 0.7483550906181335\n",
      "Step 901 | Loss: 0.6793988943099976\n",
      "Step 1001 | Loss: 0.7626170516014099\n",
      "Step 1101 | Loss: 0.778333306312561\n",
      "Step 1201 | Loss: 0.8395081758499146\n",
      "Step 1301 | Loss: 0.8270425796508789\n",
      "Step 1401 | Loss: 0.6480020880699158\n",
      "Step 1501 | Loss: 0.7724927067756653\n",
      "Step 1601 | Loss: 0.7179513573646545\n",
      "Step 1701 | Loss: 0.5798195600509644\n",
      "Step 1801 | Loss: 0.8159839510917664\n",
      "Step 1901 | Loss: 0.7869795560836792\n",
      "Step 2001 | Loss: 0.577700138092041\n",
      "Step 2101 | Loss: 0.6361215114593506\n",
      "Step 2201 | Loss: 0.7485413551330566\n",
      "Step 2301 | Loss: 0.6137641072273254\n",
      "Step 2401 | Loss: 0.6237163543701172\n",
      "0.7190493414516008 0.78\n",
      "Step 1 | Loss: 0.9410582780838013\n",
      "Step 101 | Loss: 0.7003933191299438\n",
      "Step 201 | Loss: 0.6449227333068848\n",
      "Step 301 | Loss: 0.47571536898612976\n",
      "Step 401 | Loss: 0.5050700902938843\n",
      "Step 501 | Loss: 0.4504634737968445\n",
      "Step 601 | Loss: 0.3773895502090454\n",
      "Step 701 | Loss: 0.4747411012649536\n",
      "Step 801 | Loss: 0.46412795782089233\n",
      "Step 901 | Loss: 0.5178706049919128\n",
      "Step 1001 | Loss: 0.5380626320838928\n",
      "Step 1101 | Loss: 0.558783233165741\n",
      "Step 1201 | Loss: 0.45644551515579224\n",
      "Step 1301 | Loss: 0.4888308644294739\n",
      "Step 1401 | Loss: 0.4289192259311676\n",
      "Step 1501 | Loss: 0.5096400380134583\n",
      "Step 1601 | Loss: 0.511046826839447\n",
      "Step 1701 | Loss: 0.49222564697265625\n",
      "Step 1801 | Loss: 0.5328235030174255\n",
      "Step 1901 | Loss: 0.42180201411247253\n",
      "Step 2001 | Loss: 0.5405626893043518\n",
      "Step 2101 | Loss: 0.4676123857498169\n",
      "Step 2201 | Loss: 0.4715845286846161\n",
      "Step 2301 | Loss: 0.4555720090866089\n",
      "Step 2401 | Loss: 0.5720670223236084\n",
      "0.5523494608647157 0.855\n",
      "Step 1 | Loss: 1.0223044157028198\n",
      "Step 101 | Loss: 1.1374090909957886\n",
      "Step 201 | Loss: 0.9751296639442444\n",
      "Step 301 | Loss: 1.0883532762527466\n",
      "Step 401 | Loss: 0.8390102386474609\n",
      "Step 501 | Loss: 0.7450995445251465\n",
      "Step 601 | Loss: 0.6458257436752319\n",
      "Step 701 | Loss: 0.45763593912124634\n",
      "Step 801 | Loss: 0.5427509546279907\n",
      "Step 901 | Loss: 0.49857574701309204\n",
      "Step 1001 | Loss: 0.5389222502708435\n",
      "Step 1101 | Loss: 0.4058015048503876\n",
      "Step 1201 | Loss: 0.572713315486908\n",
      "Step 1301 | Loss: 0.5920575261116028\n",
      "Step 1401 | Loss: 0.5440055727958679\n",
      "Step 1501 | Loss: 0.5394140481948853\n",
      "Step 1601 | Loss: 0.6057876348495483\n",
      "Step 1701 | Loss: 0.6201847791671753\n",
      "Step 1801 | Loss: 0.6066837906837463\n",
      "Step 1901 | Loss: 0.5389647483825684\n",
      "Step 2001 | Loss: 0.48479580879211426\n",
      "Step 2101 | Loss: 0.48023056983947754\n",
      "Step 2201 | Loss: 0.5112116932868958\n",
      "Step 2301 | Loss: 0.56640625\n",
      "Step 2401 | Loss: 0.44372954964637756\n",
      "0.5536323167600846 0.8625\n",
      "Step 1 | Loss: 0.6202863454818726\n",
      "Step 101 | Loss: 0.5740300416946411\n",
      "Step 201 | Loss: 0.4594402313232422\n",
      "Step 301 | Loss: 0.4632827639579773\n",
      "Step 401 | Loss: 0.43434566259384155\n",
      "Step 501 | Loss: 0.5208774209022522\n",
      "Step 601 | Loss: 0.5654640197753906\n",
      "Step 701 | Loss: 0.5297894477844238\n",
      "Step 801 | Loss: 0.5787491202354431\n",
      "Step 901 | Loss: 0.6030435562133789\n",
      "Step 1001 | Loss: 0.3812439739704132\n",
      "Step 1101 | Loss: 0.5589137673377991\n",
      "Step 1201 | Loss: 0.5973682999610901\n",
      "Step 1301 | Loss: 0.5293829441070557\n",
      "Step 1401 | Loss: 0.5104328989982605\n",
      "Step 1501 | Loss: 0.5085474252700806\n",
      "Step 1601 | Loss: 0.5421890020370483\n",
      "Step 1701 | Loss: 0.490078330039978\n",
      "Step 1801 | Loss: 0.43124479055404663\n",
      "Step 1901 | Loss: 0.4673651456832886\n",
      "Step 2001 | Loss: 0.5554904937744141\n",
      "Step 2101 | Loss: 0.49654969573020935\n",
      "Step 2201 | Loss: 0.3800274729728699\n",
      "Step 2301 | Loss: 0.5000758171081543\n",
      "Step 2401 | Loss: 0.36902767419815063\n",
      "0.5063362987424745 0.8875\n",
      "Step 1 | Loss: 0.9419559240341187\n",
      "Step 101 | Loss: 0.5500694513320923\n",
      "Step 201 | Loss: 0.8147066235542297\n",
      "Step 301 | Loss: 0.6803286075592041\n",
      "Step 401 | Loss: 0.7992843389511108\n",
      "Step 501 | Loss: 0.6764487028121948\n",
      "Step 601 | Loss: 0.6048310399055481\n",
      "Step 701 | Loss: 0.7254757285118103\n",
      "Step 801 | Loss: 0.7105036973953247\n",
      "Step 901 | Loss: 0.7617125511169434\n",
      "Step 1001 | Loss: 0.7000732421875\n",
      "Step 1101 | Loss: 0.5776641964912415\n",
      "Step 1201 | Loss: 0.6871480941772461\n",
      "Step 1301 | Loss: 0.5824668407440186\n",
      "Step 1401 | Loss: 0.5597050189971924\n",
      "Step 1501 | Loss: 0.7511948347091675\n",
      "Step 1601 | Loss: 0.6485111713409424\n",
      "Step 1701 | Loss: 0.5481760501861572\n",
      "Step 1801 | Loss: 0.8880256414413452\n",
      "Step 1901 | Loss: 0.6401230692863464\n",
      "Step 2001 | Loss: 0.624276876449585\n",
      "Step 2101 | Loss: 0.5502787828445435\n",
      "Step 2201 | Loss: 0.6561557054519653\n",
      "Step 2301 | Loss: 0.590750515460968\n",
      "Step 2401 | Loss: 0.752032458782196\n",
      "0.715415624077379 0.7925\n",
      "Step 1 | Loss: 0.7802750468254089\n",
      "Step 101 | Loss: 0.6967051029205322\n",
      "Step 201 | Loss: 0.6871351599693298\n",
      "Step 301 | Loss: 0.58644700050354\n",
      "Step 401 | Loss: 0.45987698435783386\n",
      "Step 501 | Loss: 0.5802740454673767\n",
      "Step 601 | Loss: 0.5124885439872742\n",
      "Step 701 | Loss: 0.4560282528400421\n",
      "Step 801 | Loss: 0.5235334634780884\n",
      "Step 901 | Loss: 0.40657639503479004\n",
      "Step 1001 | Loss: 0.3870939612388611\n",
      "Step 1101 | Loss: 0.4124520421028137\n",
      "Step 1201 | Loss: 0.6207562685012817\n",
      "Step 1301 | Loss: 0.4603806138038635\n",
      "Step 1401 | Loss: 0.5189424157142639\n",
      "Step 1501 | Loss: 0.41176941990852356\n",
      "Step 1601 | Loss: 0.393101304769516\n",
      "Step 1701 | Loss: 0.3239886462688446\n",
      "Step 1801 | Loss: 0.32753655314445496\n",
      "Step 1901 | Loss: 0.40178900957107544\n",
      "Step 2001 | Loss: 0.4460606276988983\n",
      "Step 2101 | Loss: 0.4427958130836487\n",
      "Step 2201 | Loss: 0.4005104899406433\n",
      "Step 2301 | Loss: 0.5204862356185913\n",
      "Step 2401 | Loss: 0.37122300267219543\n",
      "0.40735208652410493 0.955\n",
      "Step 1 | Loss: 0.7862602472305298\n",
      "Step 101 | Loss: 0.7052403688430786\n",
      "Step 201 | Loss: 0.507247269153595\n",
      "Step 301 | Loss: 0.5620859861373901\n",
      "Step 401 | Loss: 0.7543758153915405\n",
      "Step 501 | Loss: 0.6524018049240112\n",
      "Step 601 | Loss: 0.7020004391670227\n",
      "Step 701 | Loss: 0.5497967004776001\n",
      "Step 801 | Loss: 0.540777862071991\n",
      "Step 901 | Loss: 0.5564621686935425\n",
      "Step 1001 | Loss: 0.5142057538032532\n",
      "Step 1101 | Loss: 0.5855343341827393\n",
      "Step 1201 | Loss: 0.5796074271202087\n",
      "Step 1301 | Loss: 0.5666031837463379\n",
      "Step 1401 | Loss: 0.5258200168609619\n",
      "Step 1501 | Loss: 0.5279648900032043\n",
      "Step 1601 | Loss: 0.640876829624176\n",
      "Step 1701 | Loss: 0.45062655210494995\n",
      "Step 1801 | Loss: 0.6154570579528809\n",
      "Step 1901 | Loss: 0.43181824684143066\n",
      "Step 2001 | Loss: 0.45964741706848145\n",
      "Step 2101 | Loss: 0.39599573612213135\n",
      "Step 2201 | Loss: 0.49764299392700195\n",
      "Step 2301 | Loss: 0.4775449335575104\n",
      "Step 2401 | Loss: 0.4743039906024933\n",
      "0.4937296174671037 0.935\n",
      "Step 1 | Loss: 1.1366729736328125\n",
      "Step 101 | Loss: 0.7429765462875366\n",
      "Step 201 | Loss: 0.5660457015037537\n",
      "Step 301 | Loss: 0.5111710429191589\n",
      "Step 401 | Loss: 0.514886736869812\n",
      "Step 501 | Loss: 0.5213161110877991\n",
      "Step 601 | Loss: 0.35313770174980164\n",
      "Step 701 | Loss: 0.4573812484741211\n",
      "Step 801 | Loss: 0.5313189029693604\n",
      "Step 901 | Loss: 0.3259611129760742\n",
      "Step 1001 | Loss: 0.39327797293663025\n",
      "Step 1101 | Loss: 0.4362022578716278\n",
      "Step 1201 | Loss: 0.4130131006240845\n",
      "Step 1301 | Loss: 0.35749518871307373\n",
      "Step 1401 | Loss: 0.5545138716697693\n",
      "Step 1501 | Loss: 0.34764227271080017\n",
      "Step 1601 | Loss: 0.35051706433296204\n",
      "Step 1701 | Loss: 0.31523507833480835\n",
      "Step 1801 | Loss: 0.4267033338546753\n",
      "Step 1901 | Loss: 0.49821627140045166\n",
      "Step 2001 | Loss: 0.4629952311515808\n",
      "Step 2101 | Loss: 0.5041925311088562\n",
      "Step 2201 | Loss: 0.3536544442176819\n",
      "Step 2301 | Loss: 0.30492860078811646\n",
      "Step 2401 | Loss: 0.3327632248401642\n",
      "0.35223812896579404 0.9725\n",
      "Step 1 | Loss: 1.753225564956665\n",
      "Step 101 | Loss: 0.2871145009994507\n",
      "Step 201 | Loss: 0.10851380974054337\n",
      "Step 301 | Loss: 0.15045087039470673\n",
      "Step 401 | Loss: 0.24438412487506866\n",
      "Step 501 | Loss: 0.21359139680862427\n",
      "Step 601 | Loss: 0.23101717233657837\n",
      "Step 701 | Loss: 0.2112329602241516\n",
      "Step 801 | Loss: 0.14890864491462708\n",
      "Step 901 | Loss: 0.2721010148525238\n",
      "Step 1001 | Loss: 0.2674064636230469\n",
      "Step 1101 | Loss: 0.15462616086006165\n",
      "Step 1201 | Loss: 0.2797747552394867\n",
      "Step 1301 | Loss: 0.1518765091896057\n",
      "Step 1401 | Loss: 0.19021086394786835\n",
      "Step 1501 | Loss: 0.22024869918823242\n",
      "Step 1601 | Loss: 0.1100257933139801\n",
      "Step 1701 | Loss: 0.15282663702964783\n",
      "Step 1801 | Loss: 0.21345031261444092\n",
      "Step 1901 | Loss: 0.16852185130119324\n",
      "Step 2001 | Loss: 0.06192915886640549\n",
      "Step 2101 | Loss: 0.17647938430309296\n",
      "Step 2201 | Loss: 0.19483554363250732\n",
      "Step 2301 | Loss: 0.21372674405574799\n",
      "Step 2401 | Loss: 0.14290769398212433\n",
      "0.16241173111330565 0.97\n",
      "Step 1 | Loss: 1.2182471752166748\n",
      "Step 101 | Loss: 0.7719283103942871\n",
      "Step 201 | Loss: 0.4938003718852997\n",
      "Step 301 | Loss: 0.5171477198600769\n",
      "Step 401 | Loss: 0.3685394823551178\n",
      "Step 501 | Loss: 0.3271210789680481\n",
      "Step 601 | Loss: 0.24323680996894836\n",
      "Step 701 | Loss: 0.3642942011356354\n",
      "Step 801 | Loss: 0.37654420733451843\n",
      "Step 901 | Loss: 0.24534299969673157\n",
      "Step 1001 | Loss: 0.339802622795105\n",
      "Step 1101 | Loss: 0.29907098412513733\n",
      "Step 1201 | Loss: 0.3025882840156555\n",
      "Step 1301 | Loss: 0.2700917422771454\n",
      "Step 1401 | Loss: 0.2570360600948334\n",
      "Step 1501 | Loss: 0.30191099643707275\n",
      "Step 1601 | Loss: 0.2970694899559021\n",
      "Step 1701 | Loss: 0.19211599230766296\n",
      "Step 1801 | Loss: 0.28321370482444763\n",
      "Step 1901 | Loss: 0.2588314414024353\n",
      "Step 2001 | Loss: 0.22550129890441895\n",
      "Step 2101 | Loss: 0.28671854734420776\n",
      "Step 2201 | Loss: 0.20387278497219086\n",
      "Step 2301 | Loss: 0.25409743189811707\n",
      "Step 2401 | Loss: 0.3312220275402069\n",
      "0.2408612870971589 0.9775\n",
      "Step 1 | Loss: 1.1260902881622314\n",
      "Step 101 | Loss: 0.8437966108322144\n",
      "Step 201 | Loss: 0.9231791496276855\n",
      "Step 301 | Loss: 0.8092837333679199\n",
      "Step 401 | Loss: 0.8366561532020569\n",
      "Step 501 | Loss: 0.8731547594070435\n",
      "Step 601 | Loss: 0.8781619071960449\n",
      "Step 701 | Loss: 0.861146092414856\n",
      "Step 801 | Loss: 0.8399437665939331\n",
      "Step 901 | Loss: 0.8459535837173462\n",
      "Step 1001 | Loss: 0.9008914828300476\n",
      "Step 1101 | Loss: 0.8544789552688599\n",
      "Step 1201 | Loss: 0.8805519342422485\n",
      "Step 1301 | Loss: 0.8980824947357178\n",
      "Step 1401 | Loss: 0.9400511980056763\n",
      "Step 1501 | Loss: 0.8696531057357788\n",
      "Step 1601 | Loss: 0.8975510597229004\n",
      "Step 1701 | Loss: 0.9018692970275879\n",
      "Step 1801 | Loss: 0.8980919718742371\n",
      "Step 1901 | Loss: 0.860063910484314\n",
      "Step 2001 | Loss: 0.7996442317962646\n",
      "Step 2101 | Loss: 0.9118162393569946\n",
      "Step 2201 | Loss: 0.8992297649383545\n",
      "Step 2301 | Loss: 0.9082513451576233\n",
      "Step 2401 | Loss: 0.8730044364929199\n",
      "0.8671406478767553 0.78\n",
      "Step 1 | Loss: 1.1679713726043701\n",
      "Step 101 | Loss: 0.8456527590751648\n",
      "Step 201 | Loss: 0.9389433264732361\n",
      "Step 301 | Loss: 0.8994860649108887\n",
      "Step 401 | Loss: 0.8770047426223755\n",
      "Step 501 | Loss: 0.851862907409668\n",
      "Step 601 | Loss: 0.8409382700920105\n",
      "Step 701 | Loss: 0.8543645739555359\n",
      "Step 801 | Loss: 0.8804000020027161\n",
      "Step 901 | Loss: 0.8883943557739258\n",
      "Step 1001 | Loss: 0.9326826333999634\n",
      "Step 1101 | Loss: 0.890721321105957\n",
      "Step 1201 | Loss: 0.8986966609954834\n",
      "Step 1301 | Loss: 0.9104222655296326\n",
      "Step 1401 | Loss: 0.9173228740692139\n",
      "Step 1501 | Loss: 0.8426581025123596\n",
      "Step 1601 | Loss: 0.9422420263290405\n",
      "Step 1701 | Loss: 0.8645957708358765\n",
      "Step 1801 | Loss: 0.8419241309165955\n",
      "Step 1901 | Loss: 0.8397204875946045\n",
      "Step 2001 | Loss: 0.8459490537643433\n",
      "Step 2101 | Loss: 0.916263222694397\n",
      "Step 2201 | Loss: 0.8920801877975464\n",
      "Step 2301 | Loss: 0.8316372632980347\n",
      "Step 2401 | Loss: 0.823158323764801\n",
      "0.8666842912386403 0.7725\n",
      "Step 1 | Loss: 1.0173317193984985\n",
      "Step 101 | Loss: 0.9144448041915894\n",
      "Step 201 | Loss: 0.8430907726287842\n",
      "Step 301 | Loss: 0.8836842775344849\n",
      "Step 401 | Loss: 0.9044952988624573\n",
      "Step 501 | Loss: 0.8631066083908081\n",
      "Step 601 | Loss: 0.8700295686721802\n",
      "Step 701 | Loss: 0.856920599937439\n",
      "Step 801 | Loss: 0.8606711030006409\n",
      "Step 901 | Loss: 0.8964033126831055\n",
      "Step 1001 | Loss: 0.8907832503318787\n",
      "Step 1101 | Loss: 0.8714043498039246\n",
      "Step 1201 | Loss: 0.9007104635238647\n",
      "Step 1301 | Loss: 0.8412099480628967\n",
      "Step 1401 | Loss: 0.9626995921134949\n",
      "Step 1501 | Loss: 0.8683666586875916\n",
      "Step 1601 | Loss: 0.8799241781234741\n",
      "Step 1701 | Loss: 0.872424304485321\n",
      "Step 1801 | Loss: 0.8008160591125488\n",
      "Step 1901 | Loss: 0.840681791305542\n",
      "Step 2001 | Loss: 0.825509786605835\n",
      "Step 2101 | Loss: 0.8796883821487427\n",
      "Step 2201 | Loss: 0.9071159362792969\n",
      "Step 2301 | Loss: 0.8413894772529602\n",
      "Step 2401 | Loss: 0.789695143699646\n",
      "0.8669711702607076 0.78\n",
      "Step 1 | Loss: 1.0583959817886353\n",
      "Step 101 | Loss: 0.9630374908447266\n",
      "Step 201 | Loss: 0.8626165986061096\n",
      "Step 301 | Loss: 0.9221205115318298\n",
      "Step 401 | Loss: 0.9361200332641602\n",
      "Step 501 | Loss: 0.8789861798286438\n",
      "Step 601 | Loss: 0.867739737033844\n",
      "Step 701 | Loss: 0.9758299589157104\n",
      "Step 801 | Loss: 0.8724363446235657\n",
      "Step 901 | Loss: 0.8818073868751526\n",
      "Step 1001 | Loss: 0.8500946760177612\n",
      "Step 1101 | Loss: 0.8169320821762085\n",
      "Step 1201 | Loss: 0.8841687440872192\n",
      "Step 1301 | Loss: 0.8848140239715576\n",
      "Step 1401 | Loss: 0.8900073766708374\n",
      "Step 1501 | Loss: 0.8726204633712769\n",
      "Step 1601 | Loss: 0.8515459895133972\n",
      "Step 1701 | Loss: 0.8836530447006226\n",
      "Step 1801 | Loss: 0.8501878380775452\n",
      "Step 1901 | Loss: 0.8752302527427673\n",
      "Step 2001 | Loss: 0.8944882750511169\n",
      "Step 2101 | Loss: 0.8189312815666199\n",
      "Step 2201 | Loss: 0.8454343676567078\n",
      "Step 2301 | Loss: 0.8422980308532715\n",
      "Step 2401 | Loss: 0.9132217168807983\n",
      "0.8672612395106831 0.775\n",
      "Step 1 | Loss: 0.9722388386726379\n",
      "Step 101 | Loss: 0.8820800185203552\n",
      "Step 201 | Loss: 0.961789071559906\n",
      "Step 301 | Loss: 0.8803558349609375\n",
      "Step 401 | Loss: 0.8558026552200317\n",
      "Step 501 | Loss: 0.8441731929779053\n",
      "Step 601 | Loss: 0.8716564178466797\n",
      "Step 701 | Loss: 0.8371343612670898\n",
      "Step 801 | Loss: 0.8742997646331787\n",
      "Step 901 | Loss: 0.923231303691864\n",
      "Step 1001 | Loss: 0.8800342679023743\n",
      "Step 1101 | Loss: 0.8930795192718506\n",
      "Step 1201 | Loss: 0.8907997608184814\n",
      "Step 1301 | Loss: 0.9220777153968811\n",
      "Step 1401 | Loss: 0.9006770849227905\n",
      "Step 1501 | Loss: 0.8382365703582764\n",
      "Step 1601 | Loss: 0.8753703832626343\n",
      "Step 1701 | Loss: 0.8535540699958801\n",
      "Step 1801 | Loss: 0.8753356337547302\n",
      "Step 1901 | Loss: 0.9037066698074341\n",
      "Step 2001 | Loss: 0.873166561126709\n",
      "Step 2101 | Loss: 0.8115578293800354\n",
      "Step 2201 | Loss: 0.8541052937507629\n",
      "Step 2301 | Loss: 0.8612284064292908\n",
      "Step 2401 | Loss: 0.8009233474731445\n",
      "0.8663274474258891 0.7675\n",
      "Step 1 | Loss: 0.3631223142147064\n",
      "Step 101 | Loss: 0.44076603651046753\n",
      "Step 201 | Loss: 0.7642546892166138\n",
      "Step 301 | Loss: 0.570220947265625\n",
      "Step 401 | Loss: 0.34909939765930176\n",
      "Step 501 | Loss: 0.5063906311988831\n",
      "Step 601 | Loss: 0.4990496337413788\n",
      "Step 701 | Loss: 0.4417467713356018\n",
      "Step 801 | Loss: 0.6584691405296326\n",
      "Step 901 | Loss: 0.367021381855011\n",
      "Step 1001 | Loss: 0.5228145122528076\n",
      "Step 1101 | Loss: 0.6960182189941406\n",
      "Step 1201 | Loss: 0.5031881332397461\n",
      "Step 1301 | Loss: 0.6000347137451172\n",
      "Step 1401 | Loss: 0.5412114262580872\n",
      "Step 1501 | Loss: 0.5023657083511353\n",
      "Step 1601 | Loss: 0.5951662063598633\n",
      "Step 1701 | Loss: 0.3781083822250366\n",
      "Step 1801 | Loss: 0.7806236147880554\n",
      "Step 1901 | Loss: 0.5783206820487976\n",
      "Step 2001 | Loss: 0.8401904702186584\n",
      "Step 2101 | Loss: 0.6453368663787842\n",
      "Step 2201 | Loss: 0.6760737895965576\n",
      "Step 2301 | Loss: 0.6258560419082642\n",
      "Step 2401 | Loss: 0.6988698840141296\n",
      "0.5641309130032903 0.815\n",
      "Step 1 | Loss: 1.4367543458938599\n",
      "Step 101 | Loss: 0.5888546705245972\n",
      "Step 201 | Loss: 0.3262660801410675\n",
      "Step 301 | Loss: 0.6014045476913452\n",
      "Step 401 | Loss: 0.4211721122264862\n",
      "Step 501 | Loss: 0.6611500978469849\n",
      "Step 601 | Loss: 0.5633630752563477\n",
      "Step 701 | Loss: 0.4746953248977661\n",
      "Step 801 | Loss: 0.43265634775161743\n",
      "Step 901 | Loss: 0.5331602692604065\n",
      "Step 1001 | Loss: 0.4998328685760498\n",
      "Step 1101 | Loss: 0.4620620310306549\n",
      "Step 1201 | Loss: 0.5682677030563354\n",
      "Step 1301 | Loss: 0.4729745388031006\n",
      "Step 1401 | Loss: 0.6705682277679443\n",
      "Step 1501 | Loss: 0.5707922577857971\n",
      "Step 1601 | Loss: 0.4754699170589447\n",
      "Step 1701 | Loss: 0.8641898036003113\n",
      "Step 1801 | Loss: 0.6537309885025024\n",
      "Step 1901 | Loss: 0.4782867133617401\n",
      "Step 2001 | Loss: 0.29079994559288025\n",
      "Step 2101 | Loss: 0.6908208131790161\n",
      "Step 2201 | Loss: 0.5289686322212219\n",
      "Step 2301 | Loss: 0.6281818151473999\n",
      "Step 2401 | Loss: 0.3926467299461365\n",
      "0.5599008182589483 0.805\n",
      "Step 1 | Loss: 1.7729105949401855\n",
      "Step 101 | Loss: 0.5840883255004883\n",
      "Step 201 | Loss: 0.7038540840148926\n",
      "Step 301 | Loss: 0.5693265795707703\n",
      "Step 401 | Loss: 0.5153259038925171\n",
      "Step 501 | Loss: 0.27964216470718384\n",
      "Step 601 | Loss: 0.6011223793029785\n",
      "Step 701 | Loss: 0.5099875926971436\n",
      "Step 801 | Loss: 0.5425999760627747\n",
      "Step 901 | Loss: 0.5871199369430542\n",
      "Step 1001 | Loss: 0.4899614453315735\n",
      "Step 1101 | Loss: 0.4226391315460205\n",
      "Step 1201 | Loss: 0.5680949091911316\n",
      "Step 1301 | Loss: 0.6335256099700928\n",
      "Step 1401 | Loss: 0.5572022795677185\n",
      "Step 1501 | Loss: 0.6094422340393066\n",
      "Step 1601 | Loss: 0.5254271030426025\n",
      "Step 1701 | Loss: 0.5119878053665161\n",
      "Step 1801 | Loss: 0.6683642864227295\n",
      "Step 1901 | Loss: 0.41294875741004944\n",
      "Step 2001 | Loss: 0.631046712398529\n",
      "Step 2101 | Loss: 0.8751499652862549\n",
      "Step 2201 | Loss: 0.6408581137657166\n",
      "Step 2301 | Loss: 0.6830601096153259\n",
      "Step 2401 | Loss: 0.6271853446960449\n",
      "0.5608017454147869 0.8025\n",
      "Step 1 | Loss: 0.985352635383606\n",
      "Step 101 | Loss: 0.33700037002563477\n",
      "Step 201 | Loss: 0.6141685247421265\n",
      "Step 301 | Loss: 0.7197118997573853\n",
      "Step 401 | Loss: 0.553226113319397\n",
      "Step 501 | Loss: 0.5869010090827942\n",
      "Step 601 | Loss: 0.4552183747291565\n",
      "Step 701 | Loss: 0.6077380776405334\n",
      "Step 801 | Loss: 0.6833317279815674\n",
      "Step 901 | Loss: 0.6194028854370117\n",
      "Step 1001 | Loss: 0.7408218383789062\n",
      "Step 1101 | Loss: 0.6266354322433472\n",
      "Step 1201 | Loss: 0.6962530612945557\n",
      "Step 1301 | Loss: 0.5540695786476135\n",
      "Step 1401 | Loss: 0.619073748588562\n",
      "Step 1501 | Loss: 0.5326364636421204\n",
      "Step 1601 | Loss: 0.6178877949714661\n",
      "Step 1701 | Loss: 0.6051113605499268\n",
      "Step 1801 | Loss: 0.5265936851501465\n",
      "Step 1901 | Loss: 0.6015229821205139\n",
      "Step 2001 | Loss: 0.4363921880722046\n",
      "Step 2101 | Loss: 0.4664488732814789\n",
      "Step 2201 | Loss: 0.5049595832824707\n",
      "Step 2301 | Loss: 0.6961174011230469\n",
      "Step 2401 | Loss: 0.5430387258529663\n",
      "0.5601226999410747 0.8175\n",
      "Step 1 | Loss: 1.2081016302108765\n",
      "Step 101 | Loss: 0.4434710144996643\n",
      "Step 201 | Loss: 0.791342556476593\n",
      "Step 301 | Loss: 0.5268821716308594\n",
      "Step 401 | Loss: 0.5740487575531006\n",
      "Step 501 | Loss: 0.51682448387146\n",
      "Step 601 | Loss: 0.48714911937713623\n",
      "Step 701 | Loss: 0.5647019743919373\n",
      "Step 801 | Loss: 0.675442099571228\n",
      "Step 901 | Loss: 0.8807228803634644\n",
      "Step 1001 | Loss: 0.4725368320941925\n",
      "Step 1101 | Loss: 0.45062774419784546\n",
      "Step 1201 | Loss: 0.550652265548706\n",
      "Step 1301 | Loss: 0.621068000793457\n",
      "Step 1401 | Loss: 0.46476951241493225\n",
      "Step 1501 | Loss: 0.6110523343086243\n",
      "Step 1601 | Loss: 0.5736719369888306\n",
      "Step 1701 | Loss: 0.8290267586708069\n",
      "Step 1801 | Loss: 0.6222761273384094\n",
      "Step 1901 | Loss: 0.5594586730003357\n",
      "Step 2001 | Loss: 0.5950368642807007\n",
      "Step 2101 | Loss: 0.5715060234069824\n",
      "Step 2201 | Loss: 0.6197392344474792\n",
      "Step 2301 | Loss: 0.6316420435905457\n",
      "Step 2401 | Loss: 0.470798522233963\n",
      "0.5603653985356615 0.8175\n",
      "Step 1 | Loss: 2.8200438022613525\n",
      "Step 101 | Loss: 2.820648431777954\n",
      "Step 201 | Loss: 2.5413434505462646\n",
      "Step 301 | Loss: 2.6227939128875732\n",
      "Step 401 | Loss: 3.1048998832702637\n",
      "Step 501 | Loss: 2.531907558441162\n",
      "Step 601 | Loss: 2.8176538944244385\n",
      "Step 701 | Loss: 2.7549941539764404\n",
      "Step 801 | Loss: 2.313835620880127\n",
      "Step 901 | Loss: 2.539505958557129\n",
      "Step 1001 | Loss: 2.6130170822143555\n",
      "Step 1101 | Loss: 3.1050596237182617\n",
      "Step 1201 | Loss: 3.125309467315674\n",
      "Step 1301 | Loss: 2.7930049896240234\n",
      "Step 1401 | Loss: 2.3842101097106934\n",
      "Step 1501 | Loss: 2.7323851585388184\n",
      "Step 1601 | Loss: 2.963012218475342\n",
      "Step 1701 | Loss: 2.7555463314056396\n",
      "Step 1801 | Loss: 2.5658771991729736\n",
      "Step 1901 | Loss: 2.7480640411376953\n",
      "Step 2001 | Loss: 2.7449591159820557\n",
      "Step 2101 | Loss: 2.5359368324279785\n",
      "Step 2201 | Loss: 2.8514373302459717\n",
      "Step 2301 | Loss: 2.797309398651123\n",
      "Step 2401 | Loss: 2.6985244750976562\n",
      "2.803445520048357 0.155\n",
      "Step 1 | Loss: 2.8446807861328125\n",
      "Step 101 | Loss: 2.0910587310791016\n",
      "Step 201 | Loss: 2.3944058418273926\n",
      "Step 301 | Loss: 2.417515754699707\n",
      "Step 401 | Loss: 2.567627429962158\n",
      "Step 501 | Loss: 2.9058332443237305\n",
      "Step 601 | Loss: 3.014721155166626\n",
      "Step 701 | Loss: 2.228883743286133\n",
      "Step 801 | Loss: 3.1068944931030273\n",
      "Step 901 | Loss: 2.6313984394073486\n",
      "Step 1001 | Loss: 2.975146770477295\n",
      "Step 1101 | Loss: 2.7696633338928223\n",
      "Step 1201 | Loss: 2.4527053833007812\n",
      "Step 1301 | Loss: 3.1853373050689697\n",
      "Step 1401 | Loss: 2.6755337715148926\n",
      "Step 1501 | Loss: 2.3124966621398926\n",
      "Step 1601 | Loss: 2.7645084857940674\n",
      "Step 1701 | Loss: 2.9545094966888428\n",
      "Step 1801 | Loss: 2.764528274536133\n",
      "Step 1901 | Loss: 2.935434103012085\n",
      "Step 2001 | Loss: 2.6553127765655518\n",
      "Step 2101 | Loss: 2.9845142364501953\n",
      "Step 2201 | Loss: 2.78538179397583\n",
      "Step 2301 | Loss: 2.5217223167419434\n",
      "Step 2401 | Loss: 3.1897292137145996\n",
      "2.803445858381562 0.155\n",
      "Step 1 | Loss: 2.3705291748046875\n",
      "Step 101 | Loss: 2.691283702850342\n",
      "Step 201 | Loss: 2.743023157119751\n",
      "Step 301 | Loss: 2.8369383811950684\n",
      "Step 401 | Loss: 2.749983787536621\n",
      "Step 501 | Loss: 2.7781693935394287\n",
      "Step 601 | Loss: 2.8276407718658447\n",
      "Step 701 | Loss: 2.867445468902588\n",
      "Step 801 | Loss: 2.8213987350463867\n",
      "Step 901 | Loss: 2.6754188537597656\n",
      "Step 1001 | Loss: 3.0807080268859863\n",
      "Step 1101 | Loss: 2.752039909362793\n",
      "Step 1201 | Loss: 3.0144946575164795\n",
      "Step 1301 | Loss: 2.937375783920288\n",
      "Step 1401 | Loss: 3.0086617469787598\n",
      "Step 1501 | Loss: 2.5087506771087646\n",
      "Step 1601 | Loss: 2.5975146293640137\n",
      "Step 1701 | Loss: 2.9349210262298584\n",
      "Step 1801 | Loss: 2.9457130432128906\n",
      "Step 1901 | Loss: 2.65305495262146\n",
      "Step 2001 | Loss: 2.66701602935791\n",
      "Step 2101 | Loss: 2.6413698196411133\n",
      "Step 2201 | Loss: 2.6539909839630127\n",
      "Step 2301 | Loss: 2.7622501850128174\n",
      "Step 2401 | Loss: 2.9522438049316406\n",
      "2.8034458023078432 0.155\n",
      "Step 1 | Loss: 2.9572372436523438\n",
      "Step 101 | Loss: 3.2485625743865967\n",
      "Step 201 | Loss: 2.451237916946411\n",
      "Step 301 | Loss: 2.4605226516723633\n",
      "Step 401 | Loss: 2.5955233573913574\n",
      "Step 501 | Loss: 2.7083284854888916\n",
      "Step 601 | Loss: 2.713836431503296\n",
      "Step 701 | Loss: 2.4991087913513184\n",
      "Step 801 | Loss: 2.3796820640563965\n",
      "Step 901 | Loss: 2.7713735103607178\n",
      "Step 1001 | Loss: 2.812340021133423\n",
      "Step 1101 | Loss: 3.1403660774230957\n",
      "Step 1201 | Loss: 2.8282928466796875\n",
      "Step 1301 | Loss: 3.082503318786621\n",
      "Step 1401 | Loss: 2.496159791946411\n",
      "Step 1501 | Loss: 2.9137346744537354\n",
      "Step 1601 | Loss: 2.718341827392578\n",
      "Step 1701 | Loss: 2.5986168384552\n",
      "Step 1801 | Loss: 2.6651856899261475\n",
      "Step 1901 | Loss: 3.06659197807312\n",
      "Step 2001 | Loss: 2.861889123916626\n",
      "Step 2101 | Loss: 2.9698307514190674\n",
      "Step 2201 | Loss: 2.47566556930542\n",
      "Step 2301 | Loss: 2.8290395736694336\n",
      "Step 2401 | Loss: 2.7482001781463623\n",
      "2.803445510625898 0.155\n",
      "Step 1 | Loss: 2.5509495735168457\n",
      "Step 101 | Loss: 2.8533847332000732\n",
      "Step 201 | Loss: 2.3305797576904297\n",
      "Step 301 | Loss: 2.475928783416748\n",
      "Step 401 | Loss: 2.4054341316223145\n",
      "Step 501 | Loss: 2.912501096725464\n",
      "Step 601 | Loss: 2.6302430629730225\n",
      "Step 701 | Loss: 3.055830240249634\n",
      "Step 801 | Loss: 2.667645215988159\n",
      "Step 901 | Loss: 2.7093865871429443\n",
      "Step 1001 | Loss: 3.022691249847412\n",
      "Step 1101 | Loss: 2.8569560050964355\n",
      "Step 1201 | Loss: 2.7859854698181152\n",
      "Step 1301 | Loss: 2.446098566055298\n",
      "Step 1401 | Loss: 2.1716361045837402\n",
      "Step 1501 | Loss: 3.040093421936035\n",
      "Step 1601 | Loss: 2.727003574371338\n",
      "Step 1701 | Loss: 2.725851058959961\n",
      "Step 1801 | Loss: 3.185500144958496\n",
      "Step 1901 | Loss: 2.730271100997925\n",
      "Step 2001 | Loss: 3.0287115573883057\n",
      "Step 2101 | Loss: 2.5156919956207275\n",
      "Step 2201 | Loss: 2.988603353500366\n",
      "Step 2301 | Loss: 2.7513506412506104\n",
      "Step 2401 | Loss: 2.704636335372925\n",
      "2.803445912416602 0.155\n",
      "Step 1 | Loss: 1.0721535682678223\n",
      "Step 101 | Loss: 0.9906448721885681\n",
      "Step 201 | Loss: 0.9903976917266846\n",
      "Step 301 | Loss: 0.904836893081665\n",
      "Step 401 | Loss: 1.032381296157837\n",
      "Step 501 | Loss: 0.9571180939674377\n",
      "Step 601 | Loss: 1.0068309307098389\n",
      "Step 701 | Loss: 0.992325484752655\n",
      "Step 801 | Loss: 0.9956211447715759\n",
      "Step 901 | Loss: 0.9427821040153503\n",
      "Step 1001 | Loss: 0.9970238208770752\n",
      "Step 1101 | Loss: 1.0179064273834229\n",
      "Step 1201 | Loss: 1.0277862548828125\n",
      "Step 1301 | Loss: 0.9787843823432922\n",
      "Step 1401 | Loss: 1.009764313697815\n",
      "Step 1501 | Loss: 1.0212949514389038\n",
      "Step 1601 | Loss: 1.007246971130371\n",
      "Step 1701 | Loss: 0.9384351968765259\n",
      "Step 1801 | Loss: 0.9808993339538574\n",
      "Step 1901 | Loss: 0.9820603132247925\n",
      "Step 2001 | Loss: 0.9907841086387634\n",
      "Step 2101 | Loss: 0.9608497619628906\n",
      "Step 2201 | Loss: 0.9731576442718506\n",
      "Step 2301 | Loss: 0.9680755138397217\n",
      "Step 2401 | Loss: 1.0078641176223755\n",
      "0.9907477746741006 0.5725\n",
      "Step 1 | Loss: 1.2755553722381592\n",
      "Step 101 | Loss: 1.432982087135315\n",
      "Step 201 | Loss: 1.09931480884552\n",
      "Step 301 | Loss: 1.0388439893722534\n",
      "Step 401 | Loss: 0.9686594605445862\n",
      "Step 501 | Loss: 1.035483479499817\n",
      "Step 601 | Loss: 1.0014082193374634\n",
      "Step 701 | Loss: 0.9772031903266907\n",
      "Step 801 | Loss: 1.0064910650253296\n",
      "Step 901 | Loss: 1.0107280015945435\n",
      "Step 1001 | Loss: 1.0312674045562744\n",
      "Step 1101 | Loss: 0.9591118693351746\n",
      "Step 1201 | Loss: 1.0160884857177734\n",
      "Step 1301 | Loss: 0.9880650043487549\n",
      "Step 1401 | Loss: 1.0350375175476074\n",
      "Step 1501 | Loss: 1.0585768222808838\n",
      "Step 1601 | Loss: 0.9515770077705383\n",
      "Step 1701 | Loss: 0.9866474866867065\n",
      "Step 1801 | Loss: 0.9819042086601257\n",
      "Step 1901 | Loss: 0.9365061521530151\n",
      "Step 2001 | Loss: 0.9745761752128601\n",
      "Step 2101 | Loss: 0.954227089881897\n",
      "Step 2201 | Loss: 0.9552699327468872\n",
      "Step 2301 | Loss: 0.9644854068756104\n",
      "Step 2401 | Loss: 1.0048428773880005\n",
      "0.9871640746879489 0.5275\n",
      "Step 1 | Loss: 1.1598905324935913\n",
      "Step 101 | Loss: 0.993362545967102\n",
      "Step 201 | Loss: 1.032725214958191\n",
      "Step 301 | Loss: 1.0332310199737549\n",
      "Step 401 | Loss: 0.9523206353187561\n",
      "Step 501 | Loss: 1.026135802268982\n",
      "Step 601 | Loss: 0.950286328792572\n",
      "Step 701 | Loss: 0.9744205474853516\n",
      "Step 801 | Loss: 0.9567086696624756\n",
      "Step 901 | Loss: 0.9609907865524292\n",
      "Step 1001 | Loss: 0.9890285134315491\n",
      "Step 1101 | Loss: 1.141086459159851\n",
      "Step 1201 | Loss: 1.0244204998016357\n",
      "Step 1301 | Loss: 1.0171949863433838\n",
      "Step 1401 | Loss: 0.9778923392295837\n",
      "Step 1501 | Loss: 1.038759708404541\n",
      "Step 1601 | Loss: 0.9070220589637756\n",
      "Step 1701 | Loss: 0.9612388014793396\n",
      "Step 1801 | Loss: 1.0171881914138794\n",
      "Step 1901 | Loss: 0.9955497980117798\n",
      "Step 2001 | Loss: 1.0031670331954956\n",
      "Step 2101 | Loss: 0.976115882396698\n",
      "Step 2201 | Loss: 1.0204806327819824\n",
      "Step 2301 | Loss: 0.9795092344284058\n",
      "Step 2401 | Loss: 1.0198463201522827\n",
      "0.991665457523189 0.5625\n",
      "Step 1 | Loss: 1.3895634412765503\n",
      "Step 101 | Loss: 1.0067660808563232\n",
      "Step 201 | Loss: 0.9451490044593811\n",
      "Step 301 | Loss: 0.9118202924728394\n",
      "Step 401 | Loss: 0.9205125570297241\n",
      "Step 501 | Loss: 0.9780541062355042\n",
      "Step 601 | Loss: 1.0057207345962524\n",
      "Step 701 | Loss: 0.9964560270309448\n",
      "Step 801 | Loss: 1.0176595449447632\n",
      "Step 901 | Loss: 0.9714482426643372\n",
      "Step 1001 | Loss: 0.9613387584686279\n",
      "Step 1101 | Loss: 1.021535873413086\n",
      "Step 1201 | Loss: 1.0443487167358398\n",
      "Step 1301 | Loss: 0.9152989983558655\n",
      "Step 1401 | Loss: 0.9971405267715454\n",
      "Step 1501 | Loss: 0.9292038679122925\n",
      "Step 1601 | Loss: 1.0012404918670654\n",
      "Step 1701 | Loss: 1.0590050220489502\n",
      "Step 1801 | Loss: 1.0034898519515991\n",
      "Step 1901 | Loss: 0.9485555291175842\n",
      "Step 2001 | Loss: 1.0597853660583496\n",
      "Step 2101 | Loss: 0.9462254047393799\n",
      "Step 2201 | Loss: 0.969916045665741\n",
      "Step 2301 | Loss: 0.9886162877082825\n",
      "Step 2401 | Loss: 0.9874963760375977\n",
      "0.9938921974575652 0.555\n",
      "Step 1 | Loss: 1.5838967561721802\n",
      "Step 101 | Loss: 1.3726837635040283\n",
      "Step 201 | Loss: 0.9976176023483276\n",
      "Step 301 | Loss: 0.9712789058685303\n",
      "Step 401 | Loss: 0.9868891835212708\n",
      "Step 501 | Loss: 1.0322452783584595\n",
      "Step 601 | Loss: 1.018667459487915\n",
      "Step 701 | Loss: 0.9916359186172485\n",
      "Step 801 | Loss: 1.0136377811431885\n",
      "Step 901 | Loss: 1.002363920211792\n",
      "Step 1001 | Loss: 0.8952619433403015\n",
      "Step 1101 | Loss: 1.000131607055664\n",
      "Step 1201 | Loss: 1.0391889810562134\n",
      "Step 1301 | Loss: 1.0479403734207153\n",
      "Step 1401 | Loss: 1.0099914073944092\n",
      "Step 1501 | Loss: 1.011950969696045\n",
      "Step 1601 | Loss: 0.995958149433136\n",
      "Step 1701 | Loss: 0.9574986696243286\n",
      "Step 1801 | Loss: 1.018681526184082\n",
      "Step 1901 | Loss: 0.9753641486167908\n",
      "Step 2001 | Loss: 0.9460209012031555\n",
      "Step 2101 | Loss: 0.9981492757797241\n",
      "Step 2201 | Loss: 0.9180005192756653\n",
      "Step 2301 | Loss: 0.9813990592956543\n",
      "Step 2401 | Loss: 1.0111066102981567\n",
      "0.9982647948465752 0.5275\n",
      "Step 1 | Loss: 0.3532579243183136\n",
      "Step 101 | Loss: 0.3836371898651123\n",
      "Step 201 | Loss: 0.2630862891674042\n",
      "Step 301 | Loss: 0.22914522886276245\n",
      "Step 401 | Loss: 0.2843489646911621\n",
      "Step 501 | Loss: 0.22543752193450928\n",
      "Step 601 | Loss: 0.3064116835594177\n",
      "Step 701 | Loss: 0.34059542417526245\n",
      "Step 801 | Loss: 0.33812785148620605\n",
      "Step 901 | Loss: 0.2115195244550705\n",
      "Step 1001 | Loss: 0.41134244203567505\n",
      "Step 1101 | Loss: 0.22196854650974274\n",
      "Step 1201 | Loss: 0.22899344563484192\n",
      "Step 1301 | Loss: 0.2667241096496582\n",
      "Step 1401 | Loss: 0.4519397020339966\n",
      "Step 1501 | Loss: 0.30262061953544617\n",
      "Step 1601 | Loss: 0.17672449350357056\n",
      "Step 1701 | Loss: 0.18415877223014832\n",
      "Step 1801 | Loss: 0.3785612881183624\n",
      "Step 1901 | Loss: 0.2703612148761749\n",
      "Step 2001 | Loss: 0.18771910667419434\n",
      "Step 2101 | Loss: 0.18051405251026154\n",
      "Step 2201 | Loss: 0.22958002984523773\n",
      "Step 2301 | Loss: 0.3119570314884186\n",
      "Step 2401 | Loss: 0.16539514064788818\n",
      "0.2610160722035795 0.955\n",
      "Step 1 | Loss: 0.49318957328796387\n",
      "Step 101 | Loss: 0.23065945506095886\n",
      "Step 201 | Loss: 0.28798022866249084\n",
      "Step 301 | Loss: 0.32741600275039673\n",
      "Step 401 | Loss: 0.22301912307739258\n",
      "Step 501 | Loss: 0.29723426699638367\n",
      "Step 601 | Loss: 0.33875927329063416\n",
      "Step 701 | Loss: 0.33357274532318115\n",
      "Step 801 | Loss: 0.32023143768310547\n",
      "Step 901 | Loss: 0.2960493564605713\n",
      "Step 1001 | Loss: 0.17694145441055298\n",
      "Step 1101 | Loss: 0.2943456768989563\n",
      "Step 1201 | Loss: 0.2753535211086273\n",
      "Step 1301 | Loss: 0.3419856131076813\n",
      "Step 1401 | Loss: 0.2695782482624054\n",
      "Step 1501 | Loss: 0.2205565869808197\n",
      "Step 1601 | Loss: 0.3253406286239624\n",
      "Step 1701 | Loss: 0.31360766291618347\n",
      "Step 1801 | Loss: 0.45645272731781006\n",
      "Step 1901 | Loss: 0.18776361644268036\n",
      "Step 2001 | Loss: 0.24938040971755981\n",
      "Step 2101 | Loss: 0.23167112469673157\n",
      "Step 2201 | Loss: 0.31784313917160034\n",
      "Step 2301 | Loss: 0.2846148610115051\n",
      "Step 2401 | Loss: 0.19929808378219604\n",
      "0.2605883569038052 0.955\n",
      "Step 1 | Loss: 1.3226677179336548\n",
      "Step 101 | Loss: 0.32565000653266907\n",
      "Step 201 | Loss: 0.36227697134017944\n",
      "Step 301 | Loss: 0.32662975788116455\n",
      "Step 401 | Loss: 0.32315388321876526\n",
      "Step 501 | Loss: 0.23751665651798248\n",
      "Step 601 | Loss: 0.3369373083114624\n",
      "Step 701 | Loss: 0.24080999195575714\n",
      "Step 801 | Loss: 0.2577440142631531\n",
      "Step 901 | Loss: 0.35692283511161804\n",
      "Step 1001 | Loss: 0.3173181414604187\n",
      "Step 1101 | Loss: 0.21918365359306335\n",
      "Step 1201 | Loss: 0.23367664217948914\n",
      "Step 1301 | Loss: 0.2542388141155243\n",
      "Step 1401 | Loss: 0.23276850581169128\n",
      "Step 1501 | Loss: 0.2933579385280609\n",
      "Step 1601 | Loss: 0.2258482575416565\n",
      "Step 1701 | Loss: 0.22842812538146973\n",
      "Step 1801 | Loss: 0.308006227016449\n",
      "Step 1901 | Loss: 0.26870107650756836\n",
      "Step 2001 | Loss: 0.3877731263637543\n",
      "Step 2101 | Loss: 0.36208564043045044\n",
      "Step 2201 | Loss: 0.2520620822906494\n",
      "Step 2301 | Loss: 0.328687846660614\n",
      "Step 2401 | Loss: 0.331695020198822\n",
      "0.2609809828468583 0.955\n",
      "Step 1 | Loss: 1.6385998725891113\n",
      "Step 101 | Loss: 0.1813974380493164\n",
      "Step 201 | Loss: 0.19442729651927948\n",
      "Step 301 | Loss: 0.3133425712585449\n",
      "Step 401 | Loss: 0.3814113736152649\n",
      "Step 501 | Loss: 0.23796221613883972\n",
      "Step 601 | Loss: 0.24872973561286926\n",
      "Step 701 | Loss: 0.14711147546768188\n",
      "Step 801 | Loss: 0.31601327657699585\n",
      "Step 901 | Loss: 0.2789096534252167\n",
      "Step 1001 | Loss: 0.2431737780570984\n",
      "Step 1101 | Loss: 0.31601035594940186\n",
      "Step 1201 | Loss: 0.2892635464668274\n",
      "Step 1301 | Loss: 0.36722812056541443\n",
      "Step 1401 | Loss: 0.21944111585617065\n",
      "Step 1501 | Loss: 0.2983974814414978\n",
      "Step 1601 | Loss: 0.3017007112503052\n",
      "Step 1701 | Loss: 0.35819223523139954\n",
      "Step 1801 | Loss: 0.3047507405281067\n",
      "Step 1901 | Loss: 0.18321046233177185\n",
      "Step 2001 | Loss: 0.2937866449356079\n",
      "Step 2101 | Loss: 0.15031030774116516\n",
      "Step 2201 | Loss: 0.3254222273826599\n",
      "Step 2301 | Loss: 0.24015265703201294\n",
      "Step 2401 | Loss: 0.2324627786874771\n",
      "0.2604655881874599 0.955\n",
      "Step 1 | Loss: 2.451648712158203\n",
      "Step 101 | Loss: 0.16518042981624603\n",
      "Step 201 | Loss: 0.22692075371742249\n",
      "Step 301 | Loss: 0.19134870171546936\n",
      "Step 401 | Loss: 0.3244161009788513\n",
      "Step 501 | Loss: 0.31399106979370117\n",
      "Step 601 | Loss: 0.22341342270374298\n",
      "Step 701 | Loss: 0.2103237807750702\n",
      "Step 801 | Loss: 0.17396627366542816\n",
      "Step 901 | Loss: 0.24396248161792755\n",
      "Step 1001 | Loss: 0.385937362909317\n",
      "Step 1101 | Loss: 0.306460976600647\n",
      "Step 1201 | Loss: 0.24121639132499695\n",
      "Step 1301 | Loss: 0.26639047265052795\n",
      "Step 1401 | Loss: 0.296077698469162\n",
      "Step 1501 | Loss: 0.20394036173820496\n",
      "Step 1601 | Loss: 0.3234805166721344\n",
      "Step 1701 | Loss: 0.26468485593795776\n",
      "Step 1801 | Loss: 0.21443942189216614\n",
      "Step 1901 | Loss: 0.1714772880077362\n",
      "Step 2001 | Loss: 0.19617509841918945\n",
      "Step 2101 | Loss: 0.14862056076526642\n",
      "Step 2201 | Loss: 0.15326926112174988\n",
      "Step 2301 | Loss: 0.199950709939003\n",
      "Step 2401 | Loss: 0.37230420112609863\n",
      "0.26073497386655453 0.955\n",
      "Step 1 | Loss: 1.3863648176193237\n",
      "Step 101 | Loss: 1.9596185684204102\n",
      "Step 201 | Loss: 1.5063388347625732\n",
      "Step 301 | Loss: 1.4574264287948608\n",
      "Step 401 | Loss: 1.5183464288711548\n",
      "Step 501 | Loss: 1.8622050285339355\n",
      "Step 601 | Loss: 1.6718580722808838\n",
      "Step 701 | Loss: 1.6742150783538818\n",
      "Step 801 | Loss: 1.5715975761413574\n",
      "Step 901 | Loss: 0.7721468210220337\n",
      "Step 1001 | Loss: 1.6521923542022705\n",
      "Step 1101 | Loss: 1.8589385747909546\n",
      "Step 1201 | Loss: 1.4717044830322266\n",
      "Step 1301 | Loss: 1.7035725116729736\n",
      "Step 1401 | Loss: 2.0737032890319824\n",
      "Step 1501 | Loss: 1.3301904201507568\n",
      "Step 1601 | Loss: 1.4332283735275269\n",
      "Step 1701 | Loss: 1.9476886987686157\n",
      "Step 1801 | Loss: 1.774817943572998\n",
      "Step 1901 | Loss: 1.4059016704559326\n",
      "Step 2001 | Loss: 2.0056729316711426\n",
      "Step 2101 | Loss: 1.5196759700775146\n",
      "Step 2201 | Loss: 1.2913527488708496\n",
      "Step 2301 | Loss: 1.3272947072982788\n",
      "Step 2401 | Loss: 1.4175790548324585\n",
      "1.4866857816255303 0.5325\n",
      "Step 1 | Loss: 1.7478231191635132\n",
      "Step 101 | Loss: 1.4727684259414673\n",
      "Step 201 | Loss: 2.3735692501068115\n",
      "Step 301 | Loss: 1.664734959602356\n",
      "Step 401 | Loss: 1.7880195379257202\n",
      "Step 501 | Loss: 2.070779800415039\n",
      "Step 601 | Loss: 1.8180222511291504\n",
      "Step 701 | Loss: 1.8713417053222656\n",
      "Step 801 | Loss: 1.595402479171753\n",
      "Step 901 | Loss: 0.9551926255226135\n",
      "Step 1001 | Loss: 1.3194012641906738\n",
      "Step 1101 | Loss: 1.256847620010376\n",
      "Step 1201 | Loss: 1.7824633121490479\n",
      "Step 1301 | Loss: 1.499671459197998\n",
      "Step 1401 | Loss: 1.4161968231201172\n",
      "Step 1501 | Loss: 1.6002042293548584\n",
      "Step 1601 | Loss: 1.82078218460083\n",
      "Step 1701 | Loss: 1.3485183715820312\n",
      "Step 1801 | Loss: 1.7521106004714966\n",
      "Step 1901 | Loss: 1.683242917060852\n",
      "Step 2001 | Loss: 1.5442200899124146\n",
      "Step 2101 | Loss: 1.5048961639404297\n",
      "Step 2201 | Loss: 1.5039795637130737\n",
      "Step 2301 | Loss: 1.4298291206359863\n",
      "Step 2401 | Loss: 1.8181514739990234\n",
      "1.4866857216366731 0.5325\n",
      "Step 1 | Loss: 1.616355061531067\n",
      "Step 101 | Loss: 1.4224770069122314\n",
      "Step 201 | Loss: 1.759966492652893\n",
      "Step 301 | Loss: 1.4845645427703857\n",
      "Step 401 | Loss: 1.5942007303237915\n",
      "Step 501 | Loss: 1.4799566268920898\n",
      "Step 601 | Loss: 1.104356050491333\n",
      "Step 701 | Loss: 1.4971905946731567\n",
      "Step 801 | Loss: 1.1189377307891846\n",
      "Step 901 | Loss: 1.1077919006347656\n",
      "Step 1001 | Loss: 0.9721812605857849\n",
      "Step 1101 | Loss: 1.8921241760253906\n",
      "Step 1201 | Loss: 1.5871686935424805\n",
      "Step 1301 | Loss: 1.0838826894760132\n",
      "Step 1401 | Loss: 1.5615594387054443\n",
      "Step 1501 | Loss: 1.6822443008422852\n",
      "Step 1601 | Loss: 1.1997047662734985\n",
      "Step 1701 | Loss: 1.6600658893585205\n",
      "Step 1801 | Loss: 1.8175334930419922\n",
      "Step 1901 | Loss: 1.2789922952651978\n",
      "Step 2001 | Loss: 1.447283387184143\n",
      "Step 2101 | Loss: 1.254906177520752\n",
      "Step 2201 | Loss: 0.9750170707702637\n",
      "Step 2301 | Loss: 1.4170739650726318\n",
      "Step 2401 | Loss: 1.8675578832626343\n",
      "1.491785938420128 0.5275\n",
      "Step 1 | Loss: 2.06718373298645\n",
      "Step 101 | Loss: 2.245232105255127\n",
      "Step 201 | Loss: 1.706548810005188\n",
      "Step 301 | Loss: 1.4931117296218872\n",
      "Step 401 | Loss: 1.4359959363937378\n",
      "Step 501 | Loss: 1.322718858718872\n",
      "Step 601 | Loss: 2.4521656036376953\n",
      "Step 701 | Loss: 1.4335050582885742\n",
      "Step 801 | Loss: 1.5514464378356934\n",
      "Step 901 | Loss: 1.8686575889587402\n",
      "Step 1001 | Loss: 1.5230790376663208\n",
      "Step 1101 | Loss: 1.508626937866211\n",
      "Step 1201 | Loss: 2.0233988761901855\n",
      "Step 1301 | Loss: 1.5373280048370361\n",
      "Step 1401 | Loss: 1.2881989479064941\n",
      "Step 1501 | Loss: 0.8699455857276917\n",
      "Step 1601 | Loss: 1.0241060256958008\n",
      "Step 1701 | Loss: 0.9904175996780396\n",
      "Step 1801 | Loss: 1.3750615119934082\n",
      "Step 1901 | Loss: 1.5982692241668701\n",
      "Step 2001 | Loss: 1.0160242319107056\n",
      "Step 2101 | Loss: 1.57917320728302\n",
      "Step 2201 | Loss: 1.4563692808151245\n",
      "Step 2301 | Loss: 1.351393461227417\n",
      "Step 2401 | Loss: 1.959895372390747\n",
      "1.4868813865388797 0.5325\n",
      "Step 1 | Loss: 1.5766115188598633\n",
      "Step 101 | Loss: 1.3638864755630493\n",
      "Step 201 | Loss: 1.4114255905151367\n",
      "Step 301 | Loss: 1.9934035539627075\n",
      "Step 401 | Loss: 1.2567371129989624\n",
      "Step 501 | Loss: 1.3440731763839722\n",
      "Step 601 | Loss: 1.5039455890655518\n",
      "Step 701 | Loss: 1.7786448001861572\n",
      "Step 801 | Loss: 1.298987865447998\n",
      "Step 901 | Loss: 1.3225312232971191\n",
      "Step 1001 | Loss: 1.4257221221923828\n",
      "Step 1101 | Loss: 1.660740613937378\n",
      "Step 1201 | Loss: 1.6432301998138428\n",
      "Step 1301 | Loss: 1.3053895235061646\n",
      "Step 1401 | Loss: 1.9958679676055908\n",
      "Step 1501 | Loss: 1.733373999595642\n",
      "Step 1601 | Loss: 1.4276295900344849\n",
      "Step 1701 | Loss: 1.2960503101348877\n",
      "Step 1801 | Loss: 1.157364010810852\n",
      "Step 1901 | Loss: 1.540467381477356\n",
      "Step 2001 | Loss: 1.554099440574646\n",
      "Step 2101 | Loss: 1.6897273063659668\n",
      "Step 2201 | Loss: 1.6171655654907227\n",
      "Step 2301 | Loss: 1.470735788345337\n",
      "Step 2401 | Loss: 2.227407455444336\n",
      "1.4866858417506816 0.5325\n",
      "Step 1 | Loss: 0.9888388514518738\n",
      "Step 101 | Loss: 0.9975521564483643\n",
      "Step 201 | Loss: 1.0015177726745605\n",
      "Step 301 | Loss: 1.0015889406204224\n",
      "Step 401 | Loss: 0.9875243902206421\n",
      "Step 501 | Loss: 1.0199341773986816\n",
      "Step 601 | Loss: 1.036584496498108\n",
      "Step 701 | Loss: 0.9738270044326782\n",
      "Step 801 | Loss: 0.9965350031852722\n",
      "Step 901 | Loss: 1.0082310438156128\n",
      "Step 1001 | Loss: 0.9931837320327759\n",
      "Step 1101 | Loss: 0.9924527406692505\n",
      "Step 1201 | Loss: 0.9941630959510803\n",
      "Step 1301 | Loss: 1.0032479763031006\n",
      "Step 1401 | Loss: 0.993687629699707\n",
      "Step 1501 | Loss: 1.0007045269012451\n",
      "Step 1601 | Loss: 0.9922745823860168\n",
      "Step 1701 | Loss: 1.000016212463379\n",
      "Step 1801 | Loss: 0.9907007217407227\n",
      "Step 1901 | Loss: 0.9973928332328796\n",
      "Step 2001 | Loss: 0.9948731660842896\n",
      "Step 2101 | Loss: 0.990504801273346\n",
      "Step 2201 | Loss: 0.9898589849472046\n",
      "Step 2301 | Loss: 1.0140557289123535\n",
      "Step 2401 | Loss: 0.9758245944976807\n",
      "1.000027682939117 0.5\n",
      "Step 1 | Loss: 1.3804216384887695\n",
      "Step 101 | Loss: 1.0000003576278687\n",
      "Step 201 | Loss: 0.9962201118469238\n",
      "Step 301 | Loss: 1.02886164188385\n",
      "Step 401 | Loss: 0.9895519018173218\n",
      "Step 501 | Loss: 0.9943821430206299\n",
      "Step 601 | Loss: 1.0010251998901367\n",
      "Step 701 | Loss: 1.0011882781982422\n",
      "Step 801 | Loss: 0.9832625389099121\n",
      "Step 901 | Loss: 1.0215049982070923\n",
      "Step 1001 | Loss: 0.9954342842102051\n",
      "Step 1101 | Loss: 0.9867333173751831\n",
      "Step 1201 | Loss: 0.9958791136741638\n",
      "Step 1301 | Loss: 0.997413158416748\n",
      "Step 1401 | Loss: 0.9966515302658081\n",
      "Step 1501 | Loss: 1.029227614402771\n",
      "Step 1601 | Loss: 0.9989969730377197\n",
      "Step 1701 | Loss: 1.0151869058609009\n",
      "Step 1801 | Loss: 0.997585654258728\n",
      "Step 1901 | Loss: 0.9903340339660645\n",
      "Step 2001 | Loss: 1.029442310333252\n",
      "Step 2101 | Loss: 0.9624144434928894\n",
      "Step 2201 | Loss: 0.9984791874885559\n",
      "Step 2301 | Loss: 1.0013519525527954\n",
      "Step 2401 | Loss: 0.9704220294952393\n",
      "1.0003966025430395 0.5\n",
      "Step 1 | Loss: 1.8445619344711304\n",
      "Step 101 | Loss: 1.0032711029052734\n",
      "Step 201 | Loss: 0.9993135929107666\n",
      "Step 301 | Loss: 1.015396237373352\n",
      "Step 401 | Loss: 0.9796377420425415\n",
      "Step 501 | Loss: 1.0102030038833618\n",
      "Step 601 | Loss: 0.9963394403457642\n",
      "Step 701 | Loss: 1.0040079355239868\n",
      "Step 801 | Loss: 0.9969261884689331\n",
      "Step 901 | Loss: 1.0164659023284912\n",
      "Step 1001 | Loss: 1.0329186916351318\n",
      "Step 1101 | Loss: 1.00997793674469\n",
      "Step 1201 | Loss: 1.0004992485046387\n",
      "Step 1301 | Loss: 1.0203496217727661\n",
      "Step 1401 | Loss: 1.0062246322631836\n",
      "Step 1501 | Loss: 1.0155154466629028\n",
      "Step 1601 | Loss: 1.015973448753357\n",
      "Step 1701 | Loss: 1.0004104375839233\n",
      "Step 1801 | Loss: 1.0586589574813843\n",
      "Step 1901 | Loss: 0.9977710247039795\n",
      "Step 2001 | Loss: 0.9822157621383667\n",
      "Step 2101 | Loss: 1.0040639638900757\n",
      "Step 2201 | Loss: 0.9909696578979492\n",
      "Step 2301 | Loss: 0.9891954064369202\n",
      "Step 2401 | Loss: 1.0132930278778076\n",
      "1.0002547426022848 0.5\n",
      "Step 1 | Loss: 1.968328833580017\n",
      "Step 101 | Loss: 0.9910873770713806\n",
      "Step 201 | Loss: 1.0000325441360474\n",
      "Step 301 | Loss: 0.9976772665977478\n",
      "Step 401 | Loss: 1.0015733242034912\n",
      "Step 501 | Loss: 0.9978200793266296\n",
      "Step 601 | Loss: 1.0000039339065552\n",
      "Step 701 | Loss: 0.9877944588661194\n",
      "Step 801 | Loss: 1.007001280784607\n",
      "Step 901 | Loss: 1.0012648105621338\n",
      "Step 1001 | Loss: 1.0164878368377686\n",
      "Step 1101 | Loss: 1.0079420804977417\n",
      "Step 1201 | Loss: 1.0002483129501343\n",
      "Step 1301 | Loss: 0.9876483678817749\n",
      "Step 1401 | Loss: 0.9983569979667664\n",
      "Step 1501 | Loss: 1.0670753717422485\n",
      "Step 1601 | Loss: 0.996745228767395\n",
      "Step 1701 | Loss: 0.9815459251403809\n",
      "Step 1801 | Loss: 0.9951764345169067\n",
      "Step 1901 | Loss: 1.0036170482635498\n",
      "Step 2001 | Loss: 1.0020395517349243\n",
      "Step 2101 | Loss: 0.999878466129303\n",
      "Step 2201 | Loss: 1.0007014274597168\n",
      "Step 2301 | Loss: 0.9922153353691101\n",
      "Step 2401 | Loss: 1.013033390045166\n",
      "1.0029829693719012 0.5\n",
      "Step 1 | Loss: 2.033564329147339\n",
      "Step 101 | Loss: 1.0000066757202148\n",
      "Step 201 | Loss: 0.9867694973945618\n",
      "Step 301 | Loss: 1.0069305896759033\n",
      "Step 401 | Loss: 0.9865714311599731\n",
      "Step 501 | Loss: 1.0098576545715332\n",
      "Step 601 | Loss: 1.0064951181411743\n",
      "Step 701 | Loss: 1.0361484289169312\n",
      "Step 801 | Loss: 1.0049560070037842\n",
      "Step 901 | Loss: 0.9946081638336182\n",
      "Step 1001 | Loss: 1.0096204280853271\n",
      "Step 1101 | Loss: 1.0010582208633423\n",
      "Step 1201 | Loss: 1.0113877058029175\n",
      "Step 1301 | Loss: 1.0089210271835327\n",
      "Step 1401 | Loss: 0.9997274875640869\n",
      "Step 1501 | Loss: 1.0030276775360107\n",
      "Step 1601 | Loss: 1.029801845550537\n",
      "Step 1701 | Loss: 0.9995349049568176\n",
      "Step 1801 | Loss: 1.0001106262207031\n",
      "Step 1901 | Loss: 0.9797242879867554\n",
      "Step 2001 | Loss: 0.9760457277297974\n",
      "Step 2101 | Loss: 1.01627779006958\n",
      "Step 2201 | Loss: 0.9983502626419067\n",
      "Step 2301 | Loss: 0.9961292147636414\n",
      "Step 2401 | Loss: 0.9863595366477966\n",
      "1.003037725130883 0.5\n",
      "Step 1 | Loss: 0.6934602856636047\n",
      "Step 101 | Loss: 0.3260270059108734\n",
      "Step 201 | Loss: 0.25389382243156433\n",
      "Step 301 | Loss: 0.2566182315349579\n",
      "Step 401 | Loss: 0.2932761311531067\n",
      "Step 501 | Loss: 0.22766819596290588\n",
      "Step 601 | Loss: 0.3075023889541626\n",
      "Step 701 | Loss: 0.3005082607269287\n",
      "Step 801 | Loss: 0.27925732731819153\n",
      "Step 901 | Loss: 0.35188937187194824\n",
      "Step 1001 | Loss: 0.34326207637786865\n",
      "Step 1101 | Loss: 0.3268577456474304\n",
      "Step 1201 | Loss: 0.3109574615955353\n",
      "Step 1301 | Loss: 0.1517837792634964\n",
      "Step 1401 | Loss: 0.3021838963031769\n",
      "Step 1501 | Loss: 0.27547305822372437\n",
      "Step 1601 | Loss: 0.22614090144634247\n",
      "Step 1701 | Loss: 0.1987757533788681\n",
      "Step 1801 | Loss: 0.29004842042922974\n",
      "Step 1901 | Loss: 0.4359682500362396\n",
      "Step 2001 | Loss: 0.30005234479904175\n",
      "Step 2101 | Loss: 0.27323105931282043\n",
      "Step 2201 | Loss: 0.32658958435058594\n",
      "Step 2301 | Loss: 0.38923776149749756\n",
      "Step 2401 | Loss: 0.297297865152359\n",
      "0.31702546940282367 0.9475\n",
      "Step 1 | Loss: 0.5474678874015808\n",
      "Step 101 | Loss: 0.3694400489330292\n",
      "Step 201 | Loss: 0.3012438118457794\n",
      "Step 301 | Loss: 0.29023221135139465\n",
      "Step 401 | Loss: 0.2870040535926819\n",
      "Step 501 | Loss: 0.3958318829536438\n",
      "Step 601 | Loss: 0.2605658173561096\n",
      "Step 701 | Loss: 0.42250922322273254\n",
      "Step 801 | Loss: 0.2703428864479065\n",
      "Step 901 | Loss: 0.22132477164268494\n",
      "Step 1001 | Loss: 0.2923382818698883\n",
      "Step 1101 | Loss: 0.22660911083221436\n",
      "Step 1201 | Loss: 0.3737517297267914\n",
      "Step 1301 | Loss: 0.49690577387809753\n",
      "Step 1401 | Loss: 0.37404555082321167\n",
      "Step 1501 | Loss: 0.21604964137077332\n",
      "Step 1601 | Loss: 0.29301053285598755\n",
      "Step 1701 | Loss: 0.32118532061576843\n",
      "Step 1801 | Loss: 0.25898489356040955\n",
      "Step 1901 | Loss: 0.2363872528076172\n",
      "Step 2001 | Loss: 0.18837139010429382\n",
      "Step 2101 | Loss: 0.407668799161911\n",
      "Step 2201 | Loss: 0.29517021775245667\n",
      "Step 2301 | Loss: 0.19954079389572144\n",
      "Step 2401 | Loss: 0.27594780921936035\n",
      "0.28584604990909307 0.9675\n",
      "Step 1 | Loss: 1.366123914718628\n",
      "Step 101 | Loss: 0.5369546413421631\n",
      "Step 201 | Loss: 0.4108244776725769\n",
      "Step 301 | Loss: 0.29424241185188293\n",
      "Step 401 | Loss: 0.2415873110294342\n",
      "Step 501 | Loss: 0.3613284230232239\n",
      "Step 601 | Loss: 0.3713470697402954\n",
      "Step 701 | Loss: 0.32879093289375305\n",
      "Step 801 | Loss: 0.42470765113830566\n",
      "Step 901 | Loss: 0.28713855147361755\n",
      "Step 1001 | Loss: 0.35339006781578064\n",
      "Step 1101 | Loss: 0.36754655838012695\n",
      "Step 1201 | Loss: 0.4490152597427368\n",
      "Step 1301 | Loss: 0.4117196798324585\n",
      "Step 1401 | Loss: 0.28822311758995056\n",
      "Step 1501 | Loss: 0.3703286945819855\n",
      "Step 1601 | Loss: 0.3281751871109009\n",
      "Step 1701 | Loss: 0.24606651067733765\n",
      "Step 1801 | Loss: 0.29162436723709106\n",
      "Step 1901 | Loss: 0.33517757058143616\n",
      "Step 2001 | Loss: 0.23548708856105804\n",
      "Step 2101 | Loss: 0.3028489053249359\n",
      "Step 2201 | Loss: 0.2620331943035126\n",
      "Step 2301 | Loss: 0.26033124327659607\n",
      "Step 2401 | Loss: 0.3660520911216736\n",
      "0.323475548672923 0.9525\n",
      "Step 1 | Loss: 0.3601981997489929\n",
      "Step 101 | Loss: 0.2886294424533844\n",
      "Step 201 | Loss: 0.2097640037536621\n",
      "Step 301 | Loss: 0.3637116551399231\n",
      "Step 401 | Loss: 0.28868377208709717\n",
      "Step 501 | Loss: 0.3714689016342163\n",
      "Step 601 | Loss: 0.27070486545562744\n",
      "Step 701 | Loss: 0.2282445728778839\n",
      "Step 801 | Loss: 0.29090455174446106\n",
      "Step 901 | Loss: 0.2676258087158203\n",
      "Step 1001 | Loss: 0.3100116550922394\n",
      "Step 1101 | Loss: 0.2346646934747696\n",
      "Step 1201 | Loss: 0.26107412576675415\n",
      "Step 1301 | Loss: 0.25699830055236816\n",
      "Step 1401 | Loss: 0.3180861473083496\n",
      "Step 1501 | Loss: 0.2686592638492584\n",
      "Step 1601 | Loss: 0.24737849831581116\n",
      "Step 1701 | Loss: 0.23950764536857605\n",
      "Step 1801 | Loss: 0.29947033524513245\n",
      "Step 1901 | Loss: 0.2587318420410156\n",
      "Step 2001 | Loss: 0.1920815408229828\n",
      "Step 2101 | Loss: 0.29195350408554077\n",
      "Step 2201 | Loss: 0.15874391794204712\n",
      "Step 2301 | Loss: 0.17713114619255066\n",
      "Step 2401 | Loss: 0.30593106150627136\n",
      "0.26470093079725765 0.965\n",
      "Step 1 | Loss: 1.4122098684310913\n",
      "Step 101 | Loss: 0.5120805501937866\n",
      "Step 201 | Loss: 0.27696508169174194\n",
      "Step 301 | Loss: 0.27197080850601196\n",
      "Step 401 | Loss: 0.2674418091773987\n",
      "Step 501 | Loss: 0.3886173665523529\n",
      "Step 601 | Loss: 0.2221698760986328\n",
      "Step 701 | Loss: 0.3896842300891876\n",
      "Step 801 | Loss: 0.389656662940979\n",
      "Step 901 | Loss: 0.21730761229991913\n",
      "Step 1001 | Loss: 0.27752870321273804\n",
      "Step 1101 | Loss: 0.2552753686904907\n",
      "Step 1201 | Loss: 0.43388858437538147\n",
      "Step 1301 | Loss: 0.38996660709381104\n",
      "Step 1401 | Loss: 0.2424324005842209\n",
      "Step 1501 | Loss: 0.34260767698287964\n",
      "Step 1601 | Loss: 0.4077114462852478\n",
      "Step 1701 | Loss: 0.36892029643058777\n",
      "Step 1801 | Loss: 0.3668074309825897\n",
      "Step 1901 | Loss: 0.17347946763038635\n",
      "Step 2001 | Loss: 0.3055298924446106\n",
      "Step 2101 | Loss: 0.3360806703567505\n",
      "Step 2201 | Loss: 0.4060635566711426\n",
      "Step 2301 | Loss: 0.4118388891220093\n",
      "Step 2401 | Loss: 0.21096760034561157\n",
      "0.3175163912668574 0.95\n",
      "Step 1 | Loss: 0.34764400124549866\n",
      "Step 101 | Loss: 0.4602615535259247\n",
      "Step 201 | Loss: 0.5456622242927551\n",
      "Step 301 | Loss: 0.35444799065589905\n",
      "Step 401 | Loss: 0.3097892999649048\n",
      "Step 501 | Loss: 0.3208141326904297\n",
      "Step 601 | Loss: 0.27735087275505066\n",
      "Step 701 | Loss: 0.2502744793891907\n",
      "Step 801 | Loss: 0.5925623774528503\n",
      "Step 901 | Loss: 0.4752234220504761\n",
      "Step 1001 | Loss: 0.3330227732658386\n",
      "Step 1101 | Loss: 0.3680703639984131\n",
      "Step 1201 | Loss: 0.30926430225372314\n",
      "Step 1301 | Loss: 0.38688498735427856\n",
      "Step 1401 | Loss: 0.16250410676002502\n",
      "Step 1501 | Loss: 0.41526108980178833\n",
      "Step 1601 | Loss: 0.3136000633239746\n",
      "Step 1701 | Loss: 0.3656810522079468\n",
      "Step 1801 | Loss: 0.24726296961307526\n",
      "Step 1901 | Loss: 0.304112046957016\n",
      "Step 2001 | Loss: 0.21483124792575836\n",
      "Step 2101 | Loss: 0.3340184688568115\n",
      "Step 2201 | Loss: 0.36452776193618774\n",
      "Step 2301 | Loss: 0.3737555146217346\n",
      "Step 2401 | Loss: 0.375493586063385\n",
      "0.2972055076826837 0.9025\n",
      "Step 1 | Loss: 0.6997239589691162\n",
      "Step 101 | Loss: 0.32719680666923523\n",
      "Step 201 | Loss: 0.23505911231040955\n",
      "Step 301 | Loss: 0.38118842244148254\n",
      "Step 401 | Loss: 0.3153780400753021\n",
      "Step 501 | Loss: 0.13847936689853668\n",
      "Step 601 | Loss: 0.2238129824399948\n",
      "Step 701 | Loss: 0.388760507106781\n",
      "Step 801 | Loss: 0.427592933177948\n",
      "Step 901 | Loss: 0.28395065665245056\n",
      "Step 1001 | Loss: 0.3122307062149048\n",
      "Step 1101 | Loss: 0.23729869723320007\n",
      "Step 1201 | Loss: 0.37587451934814453\n",
      "Step 1301 | Loss: 0.2469506412744522\n",
      "Step 1401 | Loss: 0.4287249743938446\n",
      "Step 1501 | Loss: 0.3145570755004883\n",
      "Step 1601 | Loss: 0.200674369931221\n",
      "Step 1701 | Loss: 0.26997110247612\n",
      "Step 1801 | Loss: 0.15252013504505157\n",
      "Step 1901 | Loss: 0.3950371742248535\n",
      "Step 2001 | Loss: 0.13882777094841003\n",
      "Step 2101 | Loss: 0.3467467427253723\n",
      "Step 2201 | Loss: 0.2450275719165802\n",
      "Step 2301 | Loss: 0.3311704099178314\n",
      "Step 2401 | Loss: 0.15010304749011993\n",
      "0.28966268070132534 0.9075\n",
      "Step 1 | Loss: 1.299094557762146\n",
      "Step 101 | Loss: 0.37470412254333496\n",
      "Step 201 | Loss: 0.28221023082733154\n",
      "Step 301 | Loss: 0.25799787044525146\n",
      "Step 401 | Loss: 0.25598853826522827\n",
      "Step 501 | Loss: 0.5040360689163208\n",
      "Step 601 | Loss: 0.33944636583328247\n",
      "Step 701 | Loss: 0.33451253175735474\n",
      "Step 801 | Loss: 0.30163848400115967\n",
      "Step 901 | Loss: 0.3980685770511627\n",
      "Step 1001 | Loss: 0.24067960679531097\n",
      "Step 1101 | Loss: 0.33782124519348145\n",
      "Step 1201 | Loss: 0.31443366408348083\n",
      "Step 1301 | Loss: 0.36310404539108276\n",
      "Step 1401 | Loss: 0.36786961555480957\n",
      "Step 1501 | Loss: 0.3713831603527069\n",
      "Step 1601 | Loss: 0.3109953999519348\n",
      "Step 1701 | Loss: 0.3356785178184509\n",
      "Step 1801 | Loss: 0.26421719789505005\n",
      "Step 1901 | Loss: 0.0880868211388588\n",
      "Step 2001 | Loss: 0.2771857678890228\n",
      "Step 2101 | Loss: 0.2862039804458618\n",
      "Step 2201 | Loss: 0.22180835902690887\n",
      "Step 2301 | Loss: 0.4351588785648346\n",
      "Step 2401 | Loss: 0.23354610800743103\n",
      "0.29369567049166306 0.905\n",
      "Step 1 | Loss: 1.7000921964645386\n",
      "Step 101 | Loss: 0.3738730251789093\n",
      "Step 201 | Loss: 0.18408924341201782\n",
      "Step 301 | Loss: 0.28159648180007935\n",
      "Step 401 | Loss: 0.3975736200809479\n",
      "Step 501 | Loss: 0.2647949159145355\n",
      "Step 601 | Loss: 0.22860771417617798\n",
      "Step 701 | Loss: 0.29573801159858704\n",
      "Step 801 | Loss: 0.3245447874069214\n",
      "Step 901 | Loss: 0.26782476902008057\n",
      "Step 1001 | Loss: 0.276451051235199\n",
      "Step 1101 | Loss: 0.26185545325279236\n",
      "Step 1201 | Loss: 0.2295038402080536\n",
      "Step 1301 | Loss: 0.1441066563129425\n",
      "Step 1401 | Loss: 0.35416746139526367\n",
      "Step 1501 | Loss: 0.3283746838569641\n",
      "Step 1601 | Loss: 0.49412623047828674\n",
      "Step 1701 | Loss: 0.24876581132411957\n",
      "Step 1801 | Loss: 0.39063793420791626\n",
      "Step 1901 | Loss: 0.2389591783285141\n",
      "Step 2001 | Loss: 0.32833513617515564\n",
      "Step 2101 | Loss: 0.576657235622406\n",
      "Step 2201 | Loss: 0.2506502568721771\n",
      "Step 2301 | Loss: 0.3979800343513489\n",
      "Step 2401 | Loss: 0.40390294790267944\n",
      "0.2924550150610321 0.905\n",
      "Step 1 | Loss: 2.025573968887329\n",
      "Step 101 | Loss: 0.28347399830818176\n",
      "Step 201 | Loss: 0.3093828856945038\n",
      "Step 301 | Loss: 0.22953036427497864\n",
      "Step 401 | Loss: 0.40406593680381775\n",
      "Step 501 | Loss: 0.34920862317085266\n",
      "Step 601 | Loss: 0.2635778784751892\n",
      "Step 701 | Loss: 0.16490186750888824\n",
      "Step 801 | Loss: 0.35048767924308777\n",
      "Step 901 | Loss: 0.29669827222824097\n",
      "Step 1001 | Loss: 0.2954343259334564\n",
      "Step 1101 | Loss: 0.350258469581604\n",
      "Step 1201 | Loss: 0.43845629692077637\n",
      "Step 1301 | Loss: 0.2668055593967438\n",
      "Step 1401 | Loss: 0.38838255405426025\n",
      "Step 1501 | Loss: 0.3777974545955658\n",
      "Step 1601 | Loss: 0.28796663880348206\n",
      "Step 1701 | Loss: 0.2998882234096527\n",
      "Step 1801 | Loss: 0.402008980512619\n",
      "Step 1901 | Loss: 0.26368412375450134\n",
      "Step 2001 | Loss: 0.380315363407135\n",
      "Step 2101 | Loss: 0.33565711975097656\n",
      "Step 2201 | Loss: 0.45571109652519226\n",
      "Step 2301 | Loss: 0.33531278371810913\n",
      "Step 2401 | Loss: 0.216337189078331\n",
      "0.2898713716248115 0.9\n",
      "Step 1 | Loss: 1.2364616394042969\n",
      "Step 101 | Loss: 1.026208519935608\n",
      "Step 201 | Loss: 1.0384324789047241\n",
      "Step 301 | Loss: 0.9900088906288147\n",
      "Step 401 | Loss: 1.0436766147613525\n",
      "Step 501 | Loss: 1.0491918325424194\n",
      "Step 601 | Loss: 1.0818532705307007\n",
      "Step 701 | Loss: 0.9567988514900208\n",
      "Step 801 | Loss: 1.0843071937561035\n",
      "Step 901 | Loss: 0.960888147354126\n",
      "Step 1001 | Loss: 0.9225082993507385\n",
      "Step 1101 | Loss: 1.061272382736206\n",
      "Step 1201 | Loss: 0.9859919548034668\n",
      "Step 1301 | Loss: 0.9920599460601807\n",
      "Step 1401 | Loss: 0.9727860689163208\n",
      "Step 1501 | Loss: 0.9633257985115051\n",
      "Step 1601 | Loss: 0.9191105365753174\n",
      "Step 1701 | Loss: 0.9143021106719971\n",
      "Step 1801 | Loss: 0.9945646524429321\n",
      "Step 1901 | Loss: 0.8953619599342346\n",
      "Step 2001 | Loss: 0.9965633153915405\n",
      "Step 2101 | Loss: 0.8668175339698792\n",
      "Step 2201 | Loss: 1.001217246055603\n",
      "Step 2301 | Loss: 0.9838245511054993\n",
      "Step 2401 | Loss: 1.0489565134048462\n",
      "0.957470231891568 0.58\n",
      "Step 1 | Loss: 1.0229692459106445\n",
      "Step 101 | Loss: 0.9316742420196533\n",
      "Step 201 | Loss: 0.9434704184532166\n",
      "Step 301 | Loss: 0.9372620582580566\n",
      "Step 401 | Loss: 0.9658374190330505\n",
      "Step 501 | Loss: 0.9049859642982483\n",
      "Step 601 | Loss: 0.9945955276489258\n",
      "Step 701 | Loss: 1.036351203918457\n",
      "Step 801 | Loss: 1.0223814249038696\n",
      "Step 901 | Loss: 0.9527060985565186\n",
      "Step 1001 | Loss: 1.0093657970428467\n",
      "Step 1101 | Loss: 1.0204218626022339\n",
      "Step 1201 | Loss: 1.0415669679641724\n",
      "Step 1301 | Loss: 0.9596412181854248\n",
      "Step 1401 | Loss: 1.0015935897827148\n",
      "Step 1501 | Loss: 1.1175271272659302\n",
      "Step 1601 | Loss: 0.9208990931510925\n",
      "Step 1701 | Loss: 0.9268138408660889\n",
      "Step 1801 | Loss: 1.0147168636322021\n",
      "Step 1901 | Loss: 1.0875297784805298\n",
      "Step 2001 | Loss: 0.9099428653717041\n",
      "Step 2101 | Loss: 0.9627602100372314\n",
      "Step 2201 | Loss: 1.0245885848999023\n",
      "Step 2301 | Loss: 0.9788978099822998\n",
      "Step 2401 | Loss: 0.9879252910614014\n",
      "0.9602357607560479 0.58\n",
      "Step 1 | Loss: 0.7205354571342468\n",
      "Step 101 | Loss: 1.0510560274124146\n",
      "Step 201 | Loss: 1.0189658403396606\n",
      "Step 301 | Loss: 0.8929280638694763\n",
      "Step 401 | Loss: 1.0151376724243164\n",
      "Step 501 | Loss: 0.9329370260238647\n",
      "Step 601 | Loss: 0.9714112877845764\n",
      "Step 701 | Loss: 1.0037719011306763\n",
      "Step 801 | Loss: 1.0252230167388916\n",
      "Step 901 | Loss: 0.9596919417381287\n",
      "Step 1001 | Loss: 0.9282806515693665\n",
      "Step 1101 | Loss: 1.0331560373306274\n",
      "Step 1201 | Loss: 0.8905028700828552\n",
      "Step 1301 | Loss: 0.8748994469642639\n",
      "Step 1401 | Loss: 1.0248291492462158\n",
      "Step 1501 | Loss: 0.9967901706695557\n",
      "Step 1601 | Loss: 0.9877223968505859\n",
      "Step 1701 | Loss: 0.9891524314880371\n",
      "Step 1801 | Loss: 0.9858925938606262\n",
      "Step 1901 | Loss: 0.8198449611663818\n",
      "Step 2001 | Loss: 0.8928918242454529\n",
      "Step 2101 | Loss: 1.0460087060928345\n",
      "Step 2201 | Loss: 1.068139672279358\n",
      "Step 2301 | Loss: 0.9905737042427063\n",
      "Step 2401 | Loss: 0.9960888624191284\n",
      "0.9575993992830305 0.58\n",
      "Step 1 | Loss: 1.105357050895691\n",
      "Step 101 | Loss: 0.9566853046417236\n",
      "Step 201 | Loss: 0.9742749333381653\n",
      "Step 301 | Loss: 0.9462810158729553\n",
      "Step 401 | Loss: 1.085132360458374\n",
      "Step 501 | Loss: 0.8733171224594116\n",
      "Step 601 | Loss: 0.9786224365234375\n",
      "Step 701 | Loss: 0.9523701667785645\n",
      "Step 801 | Loss: 1.028228521347046\n",
      "Step 901 | Loss: 0.9794029593467712\n",
      "Step 1001 | Loss: 1.074069619178772\n",
      "Step 1101 | Loss: 1.0139598846435547\n",
      "Step 1201 | Loss: 1.1037596464157104\n",
      "Step 1301 | Loss: 1.0135955810546875\n",
      "Step 1401 | Loss: 1.052258849143982\n",
      "Step 1501 | Loss: 1.047315001487732\n",
      "Step 1601 | Loss: 1.195081114768982\n",
      "Step 1701 | Loss: 0.9929288029670715\n",
      "Step 1801 | Loss: 0.936132550239563\n",
      "Step 1901 | Loss: 1.024815559387207\n",
      "Step 2001 | Loss: 0.9351858496665955\n",
      "Step 2101 | Loss: 0.9466809630393982\n",
      "Step 2201 | Loss: 0.9493389129638672\n",
      "Step 2301 | Loss: 0.9965128898620605\n",
      "Step 2401 | Loss: 0.9686099290847778\n",
      "0.963695574151114 0.58\n",
      "Step 1 | Loss: 1.2569198608398438\n",
      "Step 101 | Loss: 0.9878882169723511\n",
      "Step 201 | Loss: 0.9951586723327637\n",
      "Step 301 | Loss: 0.9867029190063477\n",
      "Step 401 | Loss: 0.8993568420410156\n",
      "Step 501 | Loss: 0.9909695982933044\n",
      "Step 601 | Loss: 1.0483744144439697\n",
      "Step 701 | Loss: 1.0401631593704224\n",
      "Step 801 | Loss: 1.0100458860397339\n",
      "Step 901 | Loss: 1.0075371265411377\n",
      "Step 1001 | Loss: 1.1045615673065186\n",
      "Step 1101 | Loss: 1.053916096687317\n",
      "Step 1201 | Loss: 1.058255672454834\n",
      "Step 1301 | Loss: 0.8945449590682983\n",
      "Step 1401 | Loss: 1.0187256336212158\n",
      "Step 1501 | Loss: 1.0297253131866455\n",
      "Step 1601 | Loss: 0.9789484143257141\n",
      "Step 1701 | Loss: 0.9862565994262695\n",
      "Step 1801 | Loss: 1.0214617252349854\n",
      "Step 1901 | Loss: 1.0377253293991089\n",
      "Step 2001 | Loss: 0.9956121444702148\n",
      "Step 2101 | Loss: 0.9233488440513611\n",
      "Step 2201 | Loss: 0.9429086446762085\n",
      "Step 2301 | Loss: 0.9422261714935303\n",
      "Step 2401 | Loss: 0.9977525472640991\n",
      "0.9566034889431747 0.58\n",
      "Step 1 | Loss: 1.3912990093231201\n",
      "Step 101 | Loss: 0.5835990905761719\n",
      "Step 201 | Loss: 0.5054841041564941\n",
      "Step 301 | Loss: 0.47253844141960144\n",
      "Step 401 | Loss: 0.5366967916488647\n",
      "Step 501 | Loss: 0.40796688199043274\n",
      "Step 601 | Loss: 0.39439359307289124\n",
      "Step 701 | Loss: 0.5085864067077637\n",
      "Step 801 | Loss: 0.37089335918426514\n",
      "Step 901 | Loss: 0.2502894103527069\n",
      "Step 1001 | Loss: 0.47581085562705994\n",
      "Step 1101 | Loss: 0.3372332453727722\n",
      "Step 1201 | Loss: 0.33375436067581177\n",
      "Step 1301 | Loss: 0.388787180185318\n",
      "Step 1401 | Loss: 0.44392600655555725\n",
      "Step 1501 | Loss: 0.4127209186553955\n",
      "Step 1601 | Loss: 0.406571626663208\n",
      "Step 1701 | Loss: 0.37194526195526123\n",
      "Step 1801 | Loss: 0.4006499946117401\n",
      "Step 1901 | Loss: 0.510427713394165\n",
      "Step 2001 | Loss: 0.25036466121673584\n",
      "Step 2101 | Loss: 0.3044782876968384\n",
      "Step 2201 | Loss: 0.17753204703330994\n",
      "Step 2301 | Loss: 0.3271705210208893\n",
      "Step 2401 | Loss: 0.2839777171611786\n",
      "0.34420686516612736 0.9325\n",
      "Step 1 | Loss: 1.079900860786438\n",
      "Step 101 | Loss: 0.6963958144187927\n",
      "Step 201 | Loss: 0.5895257592201233\n",
      "Step 301 | Loss: 0.4459189474582672\n",
      "Step 401 | Loss: 0.507350742816925\n",
      "Step 501 | Loss: 0.5002608299255371\n",
      "Step 601 | Loss: 0.36592763662338257\n",
      "Step 701 | Loss: 0.4000326693058014\n",
      "Step 801 | Loss: 0.37956342101097107\n",
      "Step 901 | Loss: 0.2730286419391632\n",
      "Step 1001 | Loss: 0.3196590542793274\n",
      "Step 1101 | Loss: 0.4225373864173889\n",
      "Step 1201 | Loss: 0.3700961172580719\n",
      "Step 1301 | Loss: 0.3669689893722534\n",
      "Step 1401 | Loss: 0.32777974009513855\n",
      "Step 1501 | Loss: 0.34542182087898254\n",
      "Step 1601 | Loss: 0.23943334817886353\n",
      "Step 1701 | Loss: 0.3366822302341461\n",
      "Step 1801 | Loss: 0.31293800473213196\n",
      "Step 1901 | Loss: 0.30013641715049744\n",
      "Step 2001 | Loss: 0.22654816508293152\n",
      "Step 2101 | Loss: 0.23838412761688232\n",
      "Step 2201 | Loss: 0.2554911971092224\n",
      "Step 2301 | Loss: 0.3441961407661438\n",
      "Step 2401 | Loss: 0.37954145669937134\n",
      "0.30709483223896095 0.9575\n",
      "Step 1 | Loss: 0.9185062646865845\n",
      "Step 101 | Loss: 0.5058003664016724\n",
      "Step 201 | Loss: 0.3929571509361267\n",
      "Step 301 | Loss: 0.3880428075790405\n",
      "Step 401 | Loss: 0.44438058137893677\n",
      "Step 501 | Loss: 0.34306055307388306\n",
      "Step 601 | Loss: 0.30106931924819946\n",
      "Step 701 | Loss: 0.3505495488643646\n",
      "Step 801 | Loss: 0.24947835505008698\n",
      "Step 901 | Loss: 0.22792917490005493\n",
      "Step 1001 | Loss: 0.2947002053260803\n",
      "Step 1101 | Loss: 0.27989643812179565\n",
      "Step 1201 | Loss: 0.23248068988323212\n",
      "Step 1301 | Loss: 0.3128655254840851\n",
      "Step 1401 | Loss: 0.2973214089870453\n",
      "Step 1501 | Loss: 0.2556014358997345\n",
      "Step 1601 | Loss: 0.20854035019874573\n",
      "Step 1701 | Loss: 0.2634502351284027\n",
      "Step 1801 | Loss: 0.257141649723053\n",
      "Step 1901 | Loss: 0.29137423634529114\n",
      "Step 2001 | Loss: 0.4466053545475006\n",
      "Step 2101 | Loss: 0.2855515480041504\n",
      "Step 2201 | Loss: 0.2668747007846832\n",
      "Step 2301 | Loss: 0.3519814908504486\n",
      "Step 2401 | Loss: 0.3127982020378113\n",
      "0.29140981374638264 0.9625\n",
      "Step 1 | Loss: 0.8201006650924683\n",
      "Step 101 | Loss: 0.4599480926990509\n",
      "Step 201 | Loss: 0.4208813011646271\n",
      "Step 301 | Loss: 0.4828668534755707\n",
      "Step 401 | Loss: 0.37005484104156494\n",
      "Step 501 | Loss: 0.46362608671188354\n",
      "Step 601 | Loss: 0.4181080460548401\n",
      "Step 701 | Loss: 0.47453510761260986\n",
      "Step 801 | Loss: 0.3345829248428345\n",
      "Step 901 | Loss: 0.3523268699645996\n",
      "Step 1001 | Loss: 0.376762330532074\n",
      "Step 1101 | Loss: 0.3997795879840851\n",
      "Step 1201 | Loss: 0.33675822615623474\n",
      "Step 1301 | Loss: 0.2675650119781494\n",
      "Step 1401 | Loss: 0.5779814124107361\n",
      "Step 1501 | Loss: 0.41051918268203735\n",
      "Step 1601 | Loss: 0.34629443287849426\n",
      "Step 1701 | Loss: 0.3574599623680115\n",
      "Step 1801 | Loss: 0.2638913691043854\n",
      "Step 1901 | Loss: 0.3550683259963989\n",
      "Step 2001 | Loss: 0.3164626657962799\n",
      "Step 2101 | Loss: 0.32974663376808167\n",
      "Step 2201 | Loss: 0.3775692582130432\n",
      "Step 2301 | Loss: 0.4019577205181122\n",
      "Step 2401 | Loss: 0.3241254389286041\n",
      "0.34349029113859647 0.9325\n",
      "Step 1 | Loss: 1.4690263271331787\n",
      "Step 101 | Loss: 0.7984735369682312\n",
      "Step 201 | Loss: 0.5815027952194214\n",
      "Step 301 | Loss: 0.6067969799041748\n",
      "Step 401 | Loss: 0.7316622138023376\n",
      "Step 501 | Loss: 0.6490129232406616\n",
      "Step 601 | Loss: 0.690131664276123\n",
      "Step 701 | Loss: 0.6568503975868225\n",
      "Step 801 | Loss: 0.5405839681625366\n",
      "Step 901 | Loss: 0.40033286809921265\n",
      "Step 1001 | Loss: 0.5325660705566406\n",
      "Step 1101 | Loss: 0.3833281695842743\n",
      "Step 1201 | Loss: 0.3577374219894409\n",
      "Step 1301 | Loss: 0.3849605321884155\n",
      "Step 1401 | Loss: 0.27622315287590027\n",
      "Step 1501 | Loss: 0.4248427152633667\n",
      "Step 1601 | Loss: 0.35172775387763977\n",
      "Step 1701 | Loss: 0.35810500383377075\n",
      "Step 1801 | Loss: 0.5831425786018372\n",
      "Step 1901 | Loss: 0.3250828683376312\n",
      "Step 2001 | Loss: 0.3759138882160187\n",
      "Step 2101 | Loss: 0.45910775661468506\n",
      "Step 2201 | Loss: 0.32421764731407166\n",
      "Step 2301 | Loss: 0.49616268277168274\n",
      "Step 2401 | Loss: 0.29138216376304626\n",
      "0.35568778836753495 0.9125\n",
      "Step 1 | Loss: 0.881929337978363\n",
      "Step 101 | Loss: 0.736860990524292\n",
      "Step 201 | Loss: 0.7130061388015747\n",
      "Step 301 | Loss: 0.60770583152771\n",
      "Step 401 | Loss: 0.5997456908226013\n",
      "Step 501 | Loss: 0.5809679627418518\n",
      "Step 601 | Loss: 0.4450784921646118\n",
      "Step 701 | Loss: 0.5143966674804688\n",
      "Step 801 | Loss: 0.4642900824546814\n",
      "Step 901 | Loss: 0.450110524892807\n",
      "Step 1001 | Loss: 0.5226961970329285\n",
      "Step 1101 | Loss: 0.5038455724716187\n",
      "Step 1201 | Loss: 0.4542335867881775\n",
      "Step 1301 | Loss: 0.5692397356033325\n",
      "Step 1401 | Loss: 0.4624456465244293\n",
      "Step 1501 | Loss: 0.4156849682331085\n",
      "Step 1601 | Loss: 0.4963710904121399\n",
      "Step 1701 | Loss: 0.466475248336792\n",
      "Step 1801 | Loss: 0.5660024881362915\n",
      "Step 1901 | Loss: 0.5030840039253235\n",
      "Step 2001 | Loss: 0.602724552154541\n",
      "Step 2101 | Loss: 0.5661752820014954\n",
      "Step 2201 | Loss: 0.4169284403324127\n",
      "Step 2301 | Loss: 0.4330955147743225\n",
      "Step 2401 | Loss: 0.49389079213142395\n",
      "0.5185214783033227 0.92\n",
      "Step 1 | Loss: 0.9735700488090515\n",
      "Step 101 | Loss: 0.8348833322525024\n",
      "Step 201 | Loss: 0.8074818849563599\n",
      "Step 301 | Loss: 0.671113908290863\n",
      "Step 401 | Loss: 0.6745606660842896\n",
      "Step 501 | Loss: 0.6134383678436279\n",
      "Step 601 | Loss: 0.48537230491638184\n",
      "Step 701 | Loss: 0.5009407997131348\n",
      "Step 801 | Loss: 0.37180209159851074\n",
      "Step 901 | Loss: 0.4105362892150879\n",
      "Step 1001 | Loss: 0.5830985903739929\n",
      "Step 1101 | Loss: 0.4175436496734619\n",
      "Step 1201 | Loss: 0.43670454621315\n",
      "Step 1301 | Loss: 0.4142500162124634\n",
      "Step 1401 | Loss: 0.41261065006256104\n",
      "Step 1501 | Loss: 0.36974474787712097\n",
      "Step 1601 | Loss: 0.3740577697753906\n",
      "Step 1701 | Loss: 0.3668856918811798\n",
      "Step 1801 | Loss: 0.3815910220146179\n",
      "Step 1901 | Loss: 0.5033886432647705\n",
      "Step 2001 | Loss: 0.37334656715393066\n",
      "Step 2101 | Loss: 0.4117146134376526\n",
      "Step 2201 | Loss: 0.39716556668281555\n",
      "Step 2301 | Loss: 0.37988120317459106\n",
      "Step 2401 | Loss: 0.41240522265434265\n",
      "0.4147568478293452 0.9525\n",
      "Step 1 | Loss: 0.8911591172218323\n",
      "Step 101 | Loss: 0.5524039268493652\n",
      "Step 201 | Loss: 0.5567877292633057\n",
      "Step 301 | Loss: 0.7154381275177002\n",
      "Step 401 | Loss: 0.599175214767456\n",
      "Step 501 | Loss: 0.48270201683044434\n",
      "Step 601 | Loss: 0.4323963224887848\n",
      "Step 701 | Loss: 0.3422153890132904\n",
      "Step 801 | Loss: 0.44149813055992126\n",
      "Step 901 | Loss: 0.43405961990356445\n",
      "Step 1001 | Loss: 0.42287302017211914\n",
      "Step 1101 | Loss: 0.3104146122932434\n",
      "Step 1201 | Loss: 0.45899397134780884\n",
      "Step 1301 | Loss: 0.4184200167655945\n",
      "Step 1401 | Loss: 0.33750152587890625\n",
      "Step 1501 | Loss: 0.48918548226356506\n",
      "Step 1601 | Loss: 0.42630958557128906\n",
      "Step 1701 | Loss: 0.32400789856910706\n",
      "Step 1801 | Loss: 0.4427849352359772\n",
      "Step 1901 | Loss: 0.3712999224662781\n",
      "Step 2001 | Loss: 0.46593254804611206\n",
      "Step 2101 | Loss: 0.4059959053993225\n",
      "Step 2201 | Loss: 0.36124593019485474\n",
      "Step 2301 | Loss: 0.3317248821258545\n",
      "Step 2401 | Loss: 0.27284562587738037\n",
      "0.3968236449877271 0.9575\n",
      "Step 1 | Loss: 1.2503933906555176\n",
      "Step 101 | Loss: 0.7873564958572388\n",
      "Step 201 | Loss: 0.5572531819343567\n",
      "Step 301 | Loss: 0.6705011129379272\n",
      "Step 401 | Loss: 0.6616910099983215\n",
      "Step 501 | Loss: 0.6202332377433777\n",
      "Step 601 | Loss: 0.5746058225631714\n",
      "Step 701 | Loss: 0.6040907502174377\n",
      "Step 801 | Loss: 0.5155351161956787\n",
      "Step 901 | Loss: 0.4937208890914917\n",
      "Step 1001 | Loss: 0.434616357088089\n",
      "Step 1101 | Loss: 0.42241477966308594\n",
      "Step 1201 | Loss: 0.46961891651153564\n",
      "Step 1301 | Loss: 0.5361241102218628\n",
      "Step 1401 | Loss: 0.46145540475845337\n",
      "Step 1501 | Loss: 0.5214541554450989\n",
      "Step 1601 | Loss: 0.4780072569847107\n",
      "Step 1701 | Loss: 0.3764490783214569\n",
      "Step 1801 | Loss: 0.5921562910079956\n",
      "Step 1901 | Loss: 0.5773768424987793\n",
      "Step 2001 | Loss: 0.43598994612693787\n",
      "Step 2101 | Loss: 0.5487172603607178\n",
      "Step 2201 | Loss: 0.47426265478134155\n",
      "Step 2301 | Loss: 0.5648200511932373\n",
      "Step 2401 | Loss: 0.4562128782272339\n",
      "0.4371642363402252 0.9575\n",
      "Step 1 | Loss: 1.2357419729232788\n",
      "Step 101 | Loss: 0.7554717063903809\n",
      "Step 201 | Loss: 0.4642687439918518\n",
      "Step 301 | Loss: 0.5345171093940735\n",
      "Step 401 | Loss: 0.5123435854911804\n",
      "Step 501 | Loss: 0.5781158804893494\n",
      "Step 601 | Loss: 0.4009050130844116\n",
      "Step 701 | Loss: 0.48811909556388855\n",
      "Step 801 | Loss: 0.5781347751617432\n",
      "Step 901 | Loss: 0.5804098844528198\n",
      "Step 1001 | Loss: 0.5514633059501648\n",
      "Step 1101 | Loss: 0.42984920740127563\n",
      "Step 1201 | Loss: 0.48653170466423035\n",
      "Step 1301 | Loss: 0.5584883093833923\n",
      "Step 1401 | Loss: 0.39267539978027344\n",
      "Step 1501 | Loss: 0.5730763673782349\n",
      "Step 1601 | Loss: 0.5266812443733215\n",
      "Step 1701 | Loss: 0.4575689435005188\n",
      "Step 1801 | Loss: 0.5753524899482727\n",
      "Step 1901 | Loss: 0.5352749228477478\n",
      "Step 2001 | Loss: 0.5126297473907471\n",
      "Step 2101 | Loss: 0.38341671228408813\n",
      "Step 2201 | Loss: 0.5197633504867554\n",
      "Step 2301 | Loss: 0.482807457447052\n",
      "Step 2401 | Loss: 0.4051441550254822\n",
      "0.40711510741747614 0.965\n",
      "Step 1 | Loss: 0.9965878129005432\n",
      "Step 101 | Loss: 0.6221617460250854\n",
      "Step 201 | Loss: 0.7379438877105713\n",
      "Step 301 | Loss: 0.6351388096809387\n",
      "Step 401 | Loss: 0.665202796459198\n",
      "Step 501 | Loss: 0.7309590578079224\n",
      "Step 601 | Loss: 0.5804615616798401\n",
      "Step 701 | Loss: 0.6003492474555969\n",
      "Step 801 | Loss: 0.646888792514801\n",
      "Step 901 | Loss: 0.5001852512359619\n",
      "Step 1001 | Loss: 0.7151104211807251\n",
      "Step 1101 | Loss: 0.6208393573760986\n",
      "Step 1201 | Loss: 0.8104956746101379\n",
      "Step 1301 | Loss: 0.571820855140686\n",
      "Step 1401 | Loss: 0.7220764756202698\n",
      "Step 1501 | Loss: 0.7042664289474487\n",
      "Step 1601 | Loss: 0.694373607635498\n",
      "Step 1701 | Loss: 0.5415000915527344\n",
      "Step 1801 | Loss: 0.6517578959465027\n",
      "Step 1901 | Loss: 0.789010763168335\n",
      "Step 2001 | Loss: 0.6181530952453613\n",
      "Step 2101 | Loss: 0.5746886730194092\n",
      "Step 2201 | Loss: 0.6291619539260864\n",
      "Step 2301 | Loss: 0.5918174982070923\n",
      "Step 2401 | Loss: 0.6628619432449341\n",
      "0.6650623629298033 0.795\n",
      "Step 1 | Loss: 1.2174553871154785\n",
      "Step 101 | Loss: 0.940699577331543\n",
      "Step 201 | Loss: 0.9650051593780518\n",
      "Step 301 | Loss: 0.5327554941177368\n",
      "Step 401 | Loss: 0.6415674686431885\n",
      "Step 501 | Loss: 0.6430610418319702\n",
      "Step 601 | Loss: 0.6568474769592285\n",
      "Step 701 | Loss: 0.5988532304763794\n",
      "Step 801 | Loss: 0.7132550477981567\n",
      "Step 901 | Loss: 0.6960955858230591\n",
      "Step 1001 | Loss: 0.6720075011253357\n",
      "Step 1101 | Loss: 0.6839784979820251\n",
      "Step 1201 | Loss: 0.6587268710136414\n",
      "Step 1301 | Loss: 0.8329795002937317\n",
      "Step 1401 | Loss: 0.5627846717834473\n",
      "Step 1501 | Loss: 0.5696319937705994\n",
      "Step 1601 | Loss: 0.6446828842163086\n",
      "Step 1701 | Loss: 0.5986103415489197\n",
      "Step 1801 | Loss: 0.6151612401008606\n",
      "Step 1901 | Loss: 0.5574088096618652\n",
      "Step 2001 | Loss: 0.6601826548576355\n",
      "Step 2101 | Loss: 0.6603259444236755\n",
      "Step 2201 | Loss: 0.6740565299987793\n",
      "Step 2301 | Loss: 0.6046018004417419\n",
      "Step 2401 | Loss: 0.6185206174850464\n",
      "0.6733969095097863 0.7725\n",
      "Step 1 | Loss: 1.3593195676803589\n",
      "Step 101 | Loss: 0.7915453314781189\n",
      "Step 201 | Loss: 0.747755229473114\n",
      "Step 301 | Loss: 0.7397357821464539\n",
      "Step 401 | Loss: 0.8149561882019043\n",
      "Step 501 | Loss: 0.7147808074951172\n",
      "Step 601 | Loss: 0.585170328617096\n",
      "Step 701 | Loss: 0.7531076669692993\n",
      "Step 801 | Loss: 0.8425740003585815\n",
      "Step 901 | Loss: 0.700876772403717\n",
      "Step 1001 | Loss: 0.9370685815811157\n",
      "Step 1101 | Loss: 0.7310593128204346\n",
      "Step 1201 | Loss: 0.6302062273025513\n",
      "Step 1301 | Loss: 0.5688704252243042\n",
      "Step 1401 | Loss: 0.5655319690704346\n",
      "Step 1501 | Loss: 0.8437566757202148\n",
      "Step 1601 | Loss: 0.6283119916915894\n",
      "Step 1701 | Loss: 0.6117395162582397\n",
      "Step 1801 | Loss: 0.6801617741584778\n",
      "Step 1901 | Loss: 0.6216769218444824\n",
      "Step 2001 | Loss: 0.5551621317863464\n",
      "Step 2101 | Loss: 0.7024097442626953\n",
      "Step 2201 | Loss: 0.8240197896957397\n",
      "Step 2301 | Loss: 0.7461215853691101\n",
      "Step 2401 | Loss: 0.8049795627593994\n",
      "0.6866544148341371 0.78\n",
      "Step 1 | Loss: 1.0521583557128906\n",
      "Step 101 | Loss: 0.722928524017334\n",
      "Step 201 | Loss: 0.6631239652633667\n",
      "Step 301 | Loss: 0.6878402233123779\n",
      "Step 401 | Loss: 0.7400471568107605\n",
      "Step 501 | Loss: 0.7402288913726807\n",
      "Step 601 | Loss: 0.6722480058670044\n",
      "Step 701 | Loss: 0.681419849395752\n",
      "Step 801 | Loss: 0.7860399484634399\n",
      "Step 901 | Loss: 0.7384332418441772\n",
      "Step 1001 | Loss: 0.6709359288215637\n",
      "Step 1101 | Loss: 0.6490802764892578\n",
      "Step 1201 | Loss: 0.6411323547363281\n",
      "Step 1301 | Loss: 0.7238649725914001\n",
      "Step 1401 | Loss: 0.6658239960670471\n",
      "Step 1501 | Loss: 0.5791366696357727\n",
      "Step 1601 | Loss: 0.7140631079673767\n",
      "Step 1701 | Loss: 0.6655662655830383\n",
      "Step 1801 | Loss: 0.6875787973403931\n",
      "Step 1901 | Loss: 0.8059178590774536\n",
      "Step 2001 | Loss: 0.5931850671768188\n",
      "Step 2101 | Loss: 0.6156970858573914\n",
      "Step 2201 | Loss: 0.5959176421165466\n",
      "Step 2301 | Loss: 0.8046562075614929\n",
      "Step 2401 | Loss: 0.7327942848205566\n",
      "0.6704404404667972 0.785\n",
      "Step 1 | Loss: 1.1730873584747314\n",
      "Step 101 | Loss: 1.0080721378326416\n",
      "Step 201 | Loss: 0.6648581027984619\n",
      "Step 301 | Loss: 0.7309977412223816\n",
      "Step 401 | Loss: 0.5398138761520386\n",
      "Step 501 | Loss: 0.5398998260498047\n",
      "Step 601 | Loss: 0.6334789991378784\n",
      "Step 701 | Loss: 0.7094480991363525\n",
      "Step 801 | Loss: 0.7397279143333435\n",
      "Step 901 | Loss: 0.6490383148193359\n",
      "Step 1001 | Loss: 0.5947220921516418\n",
      "Step 1101 | Loss: 0.7266579866409302\n",
      "Step 1201 | Loss: 0.6814664006233215\n",
      "Step 1301 | Loss: 0.5396466255187988\n",
      "Step 1401 | Loss: 0.5050222873687744\n",
      "Step 1501 | Loss: 0.5770925879478455\n",
      "Step 1601 | Loss: 0.680919885635376\n",
      "Step 1701 | Loss: 0.6939290165901184\n",
      "Step 1801 | Loss: 0.6482782959938049\n",
      "Step 1901 | Loss: 0.6285360455513\n",
      "Step 2001 | Loss: 0.6600009799003601\n",
      "Step 2101 | Loss: 0.6468430757522583\n",
      "Step 2201 | Loss: 0.7114602327346802\n",
      "Step 2301 | Loss: 0.8266924619674683\n",
      "Step 2401 | Loss: 0.673494815826416\n",
      "0.6666047388992754 0.8\n",
      "Step 1 | Loss: 2.0014312267303467\n",
      "Step 101 | Loss: 0.471561074256897\n",
      "Step 201 | Loss: 0.4053492248058319\n",
      "Step 301 | Loss: 0.367472380399704\n",
      "Step 401 | Loss: 0.4185182452201843\n",
      "Step 501 | Loss: 0.40454724431037903\n",
      "Step 601 | Loss: 0.456343412399292\n",
      "Step 701 | Loss: 0.41048306226730347\n",
      "Step 801 | Loss: 0.2612449824810028\n",
      "Step 901 | Loss: 0.38555169105529785\n",
      "Step 1001 | Loss: 0.4112887680530548\n",
      "Step 1101 | Loss: 0.3774171471595764\n",
      "Step 1201 | Loss: 0.3633229732513428\n",
      "Step 1301 | Loss: 0.39223650097846985\n",
      "Step 1401 | Loss: 0.5133864283561707\n",
      "Step 1501 | Loss: 0.2705434560775757\n",
      "Step 1601 | Loss: 0.3690551817417145\n",
      "Step 1701 | Loss: 0.2576735317707062\n",
      "Step 1801 | Loss: 0.48605257272720337\n",
      "Step 1901 | Loss: 0.32965371012687683\n",
      "Step 2001 | Loss: 0.4381728172302246\n",
      "Step 2101 | Loss: 0.34325098991394043\n",
      "Step 2201 | Loss: 0.4940713346004486\n",
      "Step 2301 | Loss: 0.3399318754673004\n",
      "Step 2401 | Loss: 0.3972039222717285\n",
      "0.34753135625735365 0.9275\n",
      "Step 1 | Loss: 2.1951777935028076\n",
      "Step 101 | Loss: 0.4690479338169098\n",
      "Step 201 | Loss: 0.3284395933151245\n",
      "Step 301 | Loss: 0.3929796814918518\n",
      "Step 401 | Loss: 0.2707839608192444\n",
      "Step 501 | Loss: 0.2232162207365036\n",
      "Step 601 | Loss: 0.25908324122428894\n",
      "Step 701 | Loss: 0.3506997525691986\n",
      "Step 801 | Loss: 0.13213245570659637\n",
      "Step 901 | Loss: 0.27000191807746887\n",
      "Step 1001 | Loss: 0.2522270679473877\n",
      "Step 1101 | Loss: 0.27773162722587585\n",
      "Step 1201 | Loss: 0.19151952862739563\n",
      "Step 1301 | Loss: 0.16177701950073242\n",
      "Step 1401 | Loss: 0.2714376747608185\n",
      "Step 1501 | Loss: 0.2565927803516388\n",
      "Step 1601 | Loss: 0.24752163887023926\n",
      "Step 1701 | Loss: 0.23453420400619507\n",
      "Step 1801 | Loss: 0.2280300259590149\n",
      "Step 1901 | Loss: 0.2726868689060211\n",
      "Step 2001 | Loss: 0.20154131948947906\n",
      "Step 2101 | Loss: 0.22229455411434174\n",
      "Step 2201 | Loss: 0.27175986766815186\n",
      "Step 2301 | Loss: 0.23373012244701385\n",
      "Step 2401 | Loss: 0.2187865972518921\n",
      "0.2390679483148605 0.98\n",
      "Step 1 | Loss: 1.4933300018310547\n",
      "Step 101 | Loss: 0.8863717913627625\n",
      "Step 201 | Loss: 0.8353159427642822\n",
      "Step 301 | Loss: 0.6362744569778442\n",
      "Step 401 | Loss: 0.33541950583457947\n",
      "Step 501 | Loss: 0.38078439235687256\n",
      "Step 601 | Loss: 0.28384318947792053\n",
      "Step 701 | Loss: 0.17686672508716583\n",
      "Step 801 | Loss: 0.27917686104774475\n",
      "Step 901 | Loss: 0.3202640414237976\n",
      "Step 1001 | Loss: 0.28403565287590027\n",
      "Step 1101 | Loss: 0.37911033630371094\n",
      "Step 1201 | Loss: 0.29331856966018677\n",
      "Step 1301 | Loss: 0.40277770161628723\n",
      "Step 1401 | Loss: 0.2708824872970581\n",
      "Step 1501 | Loss: 0.22938810288906097\n",
      "Step 1601 | Loss: 0.31400811672210693\n",
      "Step 1701 | Loss: 0.20443131029605865\n",
      "Step 1801 | Loss: 0.32777565717697144\n",
      "Step 1901 | Loss: 0.2435968816280365\n",
      "Step 2001 | Loss: 0.22259530425071716\n",
      "Step 2101 | Loss: 0.28631114959716797\n",
      "Step 2201 | Loss: 0.23621949553489685\n",
      "Step 2301 | Loss: 0.2228771150112152\n",
      "Step 2401 | Loss: 0.38447630405426025\n",
      "0.27663840226315617 0.9425\n",
      "Step 1 | Loss: 1.6092883348464966\n",
      "Step 101 | Loss: 0.8617275953292847\n",
      "Step 201 | Loss: 0.8781553506851196\n",
      "Step 301 | Loss: 0.7899503707885742\n",
      "Step 401 | Loss: 0.7371571063995361\n",
      "Step 501 | Loss: 0.48915761709213257\n",
      "Step 601 | Loss: 0.510350227355957\n",
      "Step 701 | Loss: 0.4379482567310333\n",
      "Step 801 | Loss: 0.3041791617870331\n",
      "Step 901 | Loss: 0.3773115873336792\n",
      "Step 1001 | Loss: 0.3084006607532501\n",
      "Step 1101 | Loss: 0.2867056727409363\n",
      "Step 1201 | Loss: 0.33281439542770386\n",
      "Step 1301 | Loss: 0.41353389620780945\n",
      "Step 1401 | Loss: 0.254265695810318\n",
      "Step 1501 | Loss: 0.2956058084964752\n",
      "Step 1601 | Loss: 0.40674924850463867\n",
      "Step 1701 | Loss: 0.16667629778385162\n",
      "Step 1801 | Loss: 0.26576629281044006\n",
      "Step 1901 | Loss: 0.46598634123802185\n",
      "Step 2001 | Loss: 0.29131144285202026\n",
      "Step 2101 | Loss: 0.30285823345184326\n",
      "Step 2201 | Loss: 0.3677857518196106\n",
      "Step 2301 | Loss: 0.3264998495578766\n",
      "Step 2401 | Loss: 0.3765656352043152\n",
      "0.3469839914906726 0.925\n",
      "Step 1 | Loss: 1.2883868217468262\n",
      "Step 101 | Loss: 0.21905842423439026\n",
      "Step 201 | Loss: 0.32800281047821045\n",
      "Step 301 | Loss: 0.24284125864505768\n",
      "Step 401 | Loss: 0.40942904353141785\n",
      "Step 501 | Loss: 0.3385920524597168\n",
      "Step 601 | Loss: 0.33296313881874084\n",
      "Step 701 | Loss: 0.21640519797801971\n",
      "Step 801 | Loss: 0.28774622082710266\n",
      "Step 901 | Loss: 0.2322814017534256\n",
      "Step 1001 | Loss: 0.24449557065963745\n",
      "Step 1101 | Loss: 0.20144793391227722\n",
      "Step 1201 | Loss: 0.2564970850944519\n",
      "Step 1301 | Loss: 0.21198412775993347\n",
      "Step 1401 | Loss: 0.1539907604455948\n",
      "Step 1501 | Loss: 0.3022329807281494\n",
      "Step 1601 | Loss: 0.166514053940773\n",
      "Step 1701 | Loss: 0.2499617040157318\n",
      "Step 1801 | Loss: 0.31540849804878235\n",
      "Step 1901 | Loss: 0.24233342707157135\n",
      "Step 2001 | Loss: 0.19521327316761017\n",
      "Step 2101 | Loss: 0.322490394115448\n",
      "Step 2201 | Loss: 0.3306659460067749\n",
      "Step 2301 | Loss: 0.21037980914115906\n",
      "Step 2401 | Loss: 0.23154082894325256\n",
      "0.2462363191630261 0.9775\n",
      "Step 1 | Loss: 0.983511209487915\n",
      "Step 101 | Loss: 0.8114966154098511\n",
      "Step 201 | Loss: 0.6781626343727112\n",
      "Step 301 | Loss: 0.6535115838050842\n",
      "Step 401 | Loss: 0.6516118049621582\n",
      "Step 501 | Loss: 0.5269533395767212\n",
      "Step 601 | Loss: 0.6049473881721497\n",
      "Step 701 | Loss: 0.7348089814186096\n",
      "Step 801 | Loss: 0.5939392447471619\n",
      "Step 901 | Loss: 0.5653342008590698\n",
      "Step 1001 | Loss: 0.5575657486915588\n",
      "Step 1101 | Loss: 0.4733852744102478\n",
      "Step 1201 | Loss: 0.5264785885810852\n",
      "Step 1301 | Loss: 0.38990867137908936\n",
      "Step 1401 | Loss: 0.308684378862381\n",
      "Step 1501 | Loss: 0.40282532572746277\n",
      "Step 1601 | Loss: 0.4994697868824005\n",
      "Step 1701 | Loss: 0.3535090386867523\n",
      "Step 1801 | Loss: 0.4807926416397095\n",
      "Step 1901 | Loss: 0.45503509044647217\n",
      "Step 2001 | Loss: 0.3654561936855316\n",
      "Step 2101 | Loss: 0.3544369041919708\n",
      "Step 2201 | Loss: 0.4951656460762024\n",
      "Step 2301 | Loss: 0.3137553632259369\n",
      "Step 2401 | Loss: 0.32631775736808777\n",
      "0.4447222632032481 0.88\n",
      "Step 1 | Loss: 1.0864477157592773\n",
      "Step 101 | Loss: 0.7483171224594116\n",
      "Step 201 | Loss: 0.6124908924102783\n",
      "Step 301 | Loss: 0.6124604940414429\n",
      "Step 401 | Loss: 0.668278694152832\n",
      "Step 501 | Loss: 0.5016556978225708\n",
      "Step 601 | Loss: 0.5550283789634705\n",
      "Step 701 | Loss: 0.39083796739578247\n",
      "Step 801 | Loss: 0.4077625274658203\n",
      "Step 901 | Loss: 0.3818817436695099\n",
      "Step 1001 | Loss: 0.38667774200439453\n",
      "Step 1101 | Loss: 0.4369768798351288\n",
      "Step 1201 | Loss: 0.41151952743530273\n",
      "Step 1301 | Loss: 0.3076893985271454\n",
      "Step 1401 | Loss: 0.3969089090824127\n",
      "Step 1501 | Loss: 0.44766169786453247\n",
      "Step 1601 | Loss: 0.4461398124694824\n",
      "Step 1701 | Loss: 0.40436258912086487\n",
      "Step 1801 | Loss: 0.4360591471195221\n",
      "Step 1901 | Loss: 0.5511544346809387\n",
      "Step 2001 | Loss: 0.45038050413131714\n",
      "Step 2101 | Loss: 0.35909855365753174\n",
      "Step 2201 | Loss: 0.46311917901039124\n",
      "Step 2301 | Loss: 0.48455002903938293\n",
      "Step 2401 | Loss: 0.3805057108402252\n",
      "0.4384591977981928 0.8975\n",
      "Step 1 | Loss: 1.1454743146896362\n",
      "Step 101 | Loss: 0.7774709463119507\n",
      "Step 201 | Loss: 0.6236180663108826\n",
      "Step 301 | Loss: 0.5506751537322998\n",
      "Step 401 | Loss: 0.6588225364685059\n",
      "Step 501 | Loss: 0.43926388025283813\n",
      "Step 601 | Loss: 0.4077368974685669\n",
      "Step 701 | Loss: 0.5454128980636597\n",
      "Step 801 | Loss: 0.531026303768158\n",
      "Step 901 | Loss: 0.3999802768230438\n",
      "Step 1001 | Loss: 0.48294350504875183\n",
      "Step 1101 | Loss: 0.5838108658790588\n",
      "Step 1201 | Loss: 0.5027732849121094\n",
      "Step 1301 | Loss: 0.25472450256347656\n",
      "Step 1401 | Loss: 0.45716044306755066\n",
      "Step 1501 | Loss: 0.30353039503097534\n",
      "Step 1601 | Loss: 0.28633373975753784\n",
      "Step 1701 | Loss: 0.4570591449737549\n",
      "Step 1801 | Loss: 0.4497148394584656\n",
      "Step 1901 | Loss: 0.34627991914749146\n",
      "Step 2001 | Loss: 0.46576446294784546\n",
      "Step 2101 | Loss: 0.43202877044677734\n",
      "Step 2201 | Loss: 0.3920965790748596\n",
      "Step 2301 | Loss: 0.3891221284866333\n",
      "Step 2401 | Loss: 0.2515900731086731\n",
      "0.40544242782777523 0.8925\n",
      "Step 1 | Loss: 1.1174101829528809\n",
      "Step 101 | Loss: 0.5900558233261108\n",
      "Step 201 | Loss: 0.5358393788337708\n",
      "Step 301 | Loss: 0.41078466176986694\n",
      "Step 401 | Loss: 0.4654308557510376\n",
      "Step 501 | Loss: 0.5384397506713867\n",
      "Step 601 | Loss: 0.4802302122116089\n",
      "Step 701 | Loss: 0.40182504057884216\n",
      "Step 801 | Loss: 0.3232870101928711\n",
      "Step 901 | Loss: 0.3911435008049011\n",
      "Step 1001 | Loss: 0.5376938581466675\n",
      "Step 1101 | Loss: 0.3747071325778961\n",
      "Step 1201 | Loss: 0.43528634309768677\n",
      "Step 1301 | Loss: 0.46042895317077637\n",
      "Step 1401 | Loss: 0.41100242733955383\n",
      "Step 1501 | Loss: 0.5064451098442078\n",
      "Step 1601 | Loss: 0.5349132418632507\n",
      "Step 1701 | Loss: 0.3580770194530487\n",
      "Step 1801 | Loss: 0.4529174864292145\n",
      "Step 1901 | Loss: 0.4084942638874054\n",
      "Step 2001 | Loss: 0.3824528157711029\n",
      "Step 2101 | Loss: 0.42602571845054626\n",
      "Step 2201 | Loss: 0.41297805309295654\n",
      "Step 2301 | Loss: 0.35008612275123596\n",
      "Step 2401 | Loss: 0.4186601936817169\n",
      "0.426996415155262 0.9025\n",
      "Step 1 | Loss: 1.2042843103408813\n",
      "Step 101 | Loss: 0.8370941877365112\n",
      "Step 201 | Loss: 0.8044632077217102\n",
      "Step 301 | Loss: 0.6899296045303345\n",
      "Step 401 | Loss: 0.7289316654205322\n",
      "Step 501 | Loss: 0.5908195972442627\n",
      "Step 601 | Loss: 0.6841391921043396\n",
      "Step 701 | Loss: 0.5795526504516602\n",
      "Step 801 | Loss: 0.5002157092094421\n",
      "Step 901 | Loss: 0.48488831520080566\n",
      "Step 1001 | Loss: 0.5711411833763123\n",
      "Step 1101 | Loss: 0.5991201400756836\n",
      "Step 1201 | Loss: 0.49305352568626404\n",
      "Step 1301 | Loss: 0.4555419385433197\n",
      "Step 1401 | Loss: 0.4381345510482788\n",
      "Step 1501 | Loss: 0.4947070777416229\n",
      "Step 1601 | Loss: 0.4814658761024475\n",
      "Step 1701 | Loss: 0.5301313400268555\n",
      "Step 1801 | Loss: 0.5055556893348694\n",
      "Step 1901 | Loss: 0.373250812292099\n",
      "Step 2001 | Loss: 0.3061378598213196\n",
      "Step 2101 | Loss: 0.303095281124115\n",
      "Step 2201 | Loss: 0.6111809015274048\n",
      "Step 2301 | Loss: 0.5183048248291016\n",
      "Step 2401 | Loss: 0.2676917016506195\n",
      "0.48242107160180164 0.8725\n",
      "Step 1 | Loss: 1.4347368478775024\n",
      "Step 101 | Loss: 0.7873515486717224\n",
      "Step 201 | Loss: 0.49600735306739807\n",
      "Step 301 | Loss: 0.398120641708374\n",
      "Step 401 | Loss: 0.24530190229415894\n",
      "Step 501 | Loss: 0.5643520951271057\n",
      "Step 601 | Loss: 0.36740168929100037\n",
      "Step 701 | Loss: 0.49508342146873474\n",
      "Step 801 | Loss: 0.3094462752342224\n",
      "Step 901 | Loss: 0.5287972688674927\n",
      "Step 1001 | Loss: 0.3009778559207916\n",
      "Step 1101 | Loss: 0.25978800654411316\n",
      "Step 1201 | Loss: 0.3915579915046692\n",
      "Step 1301 | Loss: 0.22893770039081573\n",
      "Step 1401 | Loss: 0.37139835953712463\n",
      "Step 1501 | Loss: 0.463344931602478\n",
      "Step 1601 | Loss: 0.23154708743095398\n",
      "Step 1701 | Loss: 0.3731667697429657\n",
      "Step 1801 | Loss: 0.2798295319080353\n",
      "Step 1901 | Loss: 0.49840742349624634\n",
      "Step 2001 | Loss: 0.3016311228275299\n",
      "Step 2101 | Loss: 0.3296734690666199\n",
      "Step 2201 | Loss: 0.22481684386730194\n",
      "Step 2301 | Loss: 0.1314728856086731\n",
      "Step 2401 | Loss: 0.39849305152893066\n",
      "0.2991770270771296 0.885\n",
      "Step 1 | Loss: 1.3721815347671509\n",
      "Step 101 | Loss: 0.753860354423523\n",
      "Step 201 | Loss: 0.7969107627868652\n",
      "Step 301 | Loss: 0.4144093990325928\n",
      "Step 401 | Loss: 0.2375660389661789\n",
      "Step 501 | Loss: 0.5331673622131348\n",
      "Step 601 | Loss: 0.3795620799064636\n",
      "Step 701 | Loss: 0.3240741193294525\n",
      "Step 801 | Loss: 0.1501622349023819\n",
      "Step 901 | Loss: 0.38026413321495056\n",
      "Step 1001 | Loss: 0.5568942427635193\n",
      "Step 1101 | Loss: 0.31342843174934387\n",
      "Step 1201 | Loss: 0.35536879301071167\n",
      "Step 1301 | Loss: 0.24440830945968628\n",
      "Step 1401 | Loss: 0.6225558519363403\n",
      "Step 1501 | Loss: 0.23412606120109558\n",
      "Step 1601 | Loss: 0.35796043276786804\n",
      "Step 1701 | Loss: 0.2961146831512451\n",
      "Step 1801 | Loss: 0.5538960695266724\n",
      "Step 1901 | Loss: 0.39114660024642944\n",
      "Step 2001 | Loss: 0.3017018735408783\n",
      "Step 2101 | Loss: 0.43855518102645874\n",
      "Step 2201 | Loss: 0.23927247524261475\n",
      "Step 2301 | Loss: 0.40683501958847046\n",
      "Step 2401 | Loss: 0.40854448080062866\n",
      "0.2944715905441863 0.8875\n",
      "Step 1 | Loss: 1.0589669942855835\n",
      "Step 101 | Loss: 0.6336843967437744\n",
      "Step 201 | Loss: 0.3344740867614746\n",
      "Step 301 | Loss: 0.35446515679359436\n",
      "Step 401 | Loss: 0.1531166136264801\n",
      "Step 501 | Loss: 0.4319975674152374\n",
      "Step 601 | Loss: 0.2181912660598755\n",
      "Step 701 | Loss: 0.46095728874206543\n",
      "Step 801 | Loss: 0.39387017488479614\n",
      "Step 901 | Loss: 0.4526647925376892\n",
      "Step 1001 | Loss: 0.4583497941493988\n",
      "Step 1101 | Loss: 0.17327801883220673\n",
      "Step 1201 | Loss: 0.12627610564231873\n",
      "Step 1301 | Loss: 0.2361432909965515\n",
      "Step 1401 | Loss: 0.26194947957992554\n",
      "Step 1501 | Loss: 0.41048532724380493\n",
      "Step 1601 | Loss: 0.33952564001083374\n",
      "Step 1701 | Loss: 0.5419282913208008\n",
      "Step 1801 | Loss: 0.24397628009319305\n",
      "Step 1901 | Loss: 0.3377687335014343\n",
      "Step 2001 | Loss: 0.27435502409935\n",
      "Step 2101 | Loss: 0.4429108500480652\n",
      "Step 2201 | Loss: 0.15250390768051147\n",
      "Step 2301 | Loss: 0.15659597516059875\n",
      "Step 2401 | Loss: 0.30403825640678406\n",
      "0.2951129658016293 0.8875\n",
      "Step 1 | Loss: 0.7391414642333984\n",
      "Step 101 | Loss: 0.4690686762332916\n",
      "Step 201 | Loss: 0.4036804437637329\n",
      "Step 301 | Loss: 0.2759154438972473\n",
      "Step 401 | Loss: 0.35701262950897217\n",
      "Step 501 | Loss: 0.3821134567260742\n",
      "Step 601 | Loss: 0.4580865204334259\n",
      "Step 701 | Loss: 0.652116596698761\n",
      "Step 801 | Loss: 0.37883636355400085\n",
      "Step 901 | Loss: 0.30428457260131836\n",
      "Step 1001 | Loss: 0.307882696390152\n",
      "Step 1101 | Loss: 0.27641892433166504\n",
      "Step 1201 | Loss: 0.34121352434158325\n",
      "Step 1301 | Loss: 0.41975516080856323\n",
      "Step 1401 | Loss: 0.20778286457061768\n",
      "Step 1501 | Loss: 0.4578060805797577\n",
      "Step 1601 | Loss: 0.5009782314300537\n",
      "Step 1701 | Loss: 0.2932506203651428\n",
      "Step 1801 | Loss: 0.42788010835647583\n",
      "Step 1901 | Loss: 0.2210308015346527\n",
      "Step 2001 | Loss: 0.34380900859832764\n",
      "Step 2101 | Loss: 0.2200562208890915\n",
      "Step 2201 | Loss: 0.1293216049671173\n",
      "Step 2301 | Loss: 0.3117406964302063\n",
      "Step 2401 | Loss: 0.16572244465351105\n",
      "0.3200572005401435 0.8775\n",
      "Step 1 | Loss: 1.593470573425293\n",
      "Step 101 | Loss: 1.0182113647460938\n",
      "Step 201 | Loss: 0.43364599347114563\n",
      "Step 301 | Loss: 0.36554956436157227\n",
      "Step 401 | Loss: 0.14378297328948975\n",
      "Step 501 | Loss: 0.2834605276584625\n",
      "Step 601 | Loss: 0.5075055360794067\n",
      "Step 701 | Loss: 0.307758629322052\n",
      "Step 801 | Loss: 0.32325276732444763\n",
      "Step 901 | Loss: 0.18186385929584503\n",
      "Step 1001 | Loss: 0.35488879680633545\n",
      "Step 1101 | Loss: 0.1838887631893158\n",
      "Step 1201 | Loss: 0.3068154454231262\n",
      "Step 1301 | Loss: 0.47064387798309326\n",
      "Step 1401 | Loss: 0.3433229327201843\n",
      "Step 1501 | Loss: 0.41099628806114197\n",
      "Step 1601 | Loss: 0.31089067459106445\n",
      "Step 1701 | Loss: 0.25770971179008484\n",
      "Step 1801 | Loss: 0.4848499000072479\n",
      "Step 1901 | Loss: 0.26086029410362244\n",
      "Step 2001 | Loss: 0.49099963903427124\n",
      "Step 2101 | Loss: 0.24277999997138977\n",
      "Step 2201 | Loss: 0.3124925196170807\n",
      "Step 2301 | Loss: 0.30549705028533936\n",
      "Step 2401 | Loss: 0.2512749433517456\n",
      "0.29712809864318007 0.885\n",
      "Step 1 | Loss: 1.0829806327819824\n",
      "Step 101 | Loss: 0.9010403752326965\n",
      "Step 201 | Loss: 0.6301546096801758\n",
      "Step 301 | Loss: 0.6619325280189514\n",
      "Step 401 | Loss: 0.6434264183044434\n",
      "Step 501 | Loss: 0.6273838877677917\n",
      "Step 601 | Loss: 0.47703447937965393\n",
      "Step 701 | Loss: 0.6628671884536743\n",
      "Step 801 | Loss: 0.6175926923751831\n",
      "Step 901 | Loss: 0.5979753732681274\n",
      "Step 1001 | Loss: 0.3541245758533478\n",
      "Step 1101 | Loss: 0.3569508194923401\n",
      "Step 1201 | Loss: 0.4069908559322357\n",
      "Step 1301 | Loss: 0.332148939371109\n",
      "Step 1401 | Loss: 0.39531269669532776\n",
      "Step 1501 | Loss: 0.43078702688217163\n",
      "Step 1601 | Loss: 0.4479371905326843\n",
      "Step 1701 | Loss: 0.4210912883281708\n",
      "Step 1801 | Loss: 0.46296775341033936\n",
      "Step 1901 | Loss: 0.3072168827056885\n",
      "Step 2001 | Loss: 0.4460037648677826\n",
      "Step 2101 | Loss: 0.39185798168182373\n",
      "Step 2201 | Loss: 0.4744117856025696\n",
      "Step 2301 | Loss: 0.33354452252388\n",
      "Step 2401 | Loss: 0.35840705037117004\n",
      "0.34143611114467787 0.9375\n",
      "Step 1 | Loss: 1.1143828630447388\n",
      "Step 101 | Loss: 0.7411912679672241\n",
      "Step 201 | Loss: 0.5082945823669434\n",
      "Step 301 | Loss: 0.5078003406524658\n",
      "Step 401 | Loss: 0.581078052520752\n",
      "Step 501 | Loss: 0.6120434403419495\n",
      "Step 601 | Loss: 0.52024906873703\n",
      "Step 701 | Loss: 0.7465521097183228\n",
      "Step 801 | Loss: 0.4465107023715973\n",
      "Step 901 | Loss: 0.5515201091766357\n",
      "Step 1001 | Loss: 0.45064830780029297\n",
      "Step 1101 | Loss: 0.4454955756664276\n",
      "Step 1201 | Loss: 0.48737335205078125\n",
      "Step 1301 | Loss: 0.6151336431503296\n",
      "Step 1401 | Loss: 0.6070213317871094\n",
      "Step 1501 | Loss: 0.6340796947479248\n",
      "Step 1601 | Loss: 0.377238392829895\n",
      "Step 1701 | Loss: 0.434067040681839\n",
      "Step 1801 | Loss: 0.4307784140110016\n",
      "Step 1901 | Loss: 0.5227793455123901\n",
      "Step 2001 | Loss: 0.48042070865631104\n",
      "Step 2101 | Loss: 0.42288196086883545\n",
      "Step 2201 | Loss: 0.4369094669818878\n",
      "Step 2301 | Loss: 0.465121328830719\n",
      "Step 2401 | Loss: 0.6449102759361267\n",
      "0.4891948734179557 0.855\n",
      "Step 1 | Loss: 1.324326992034912\n",
      "Step 101 | Loss: 0.824068546295166\n",
      "Step 201 | Loss: 0.8175432682037354\n",
      "Step 301 | Loss: 0.46971654891967773\n",
      "Step 401 | Loss: 0.4934098422527313\n",
      "Step 501 | Loss: 0.48138943314552307\n",
      "Step 601 | Loss: 0.35273250937461853\n",
      "Step 701 | Loss: 0.3574710190296173\n",
      "Step 801 | Loss: 0.41001570224761963\n",
      "Step 901 | Loss: 0.3394222855567932\n",
      "Step 1001 | Loss: 0.38416850566864014\n",
      "Step 1101 | Loss: 0.34793007373809814\n",
      "Step 1201 | Loss: 0.3117802143096924\n",
      "Step 1301 | Loss: 0.25609827041625977\n",
      "Step 1401 | Loss: 0.43585139513015747\n",
      "Step 1501 | Loss: 0.2973867952823639\n",
      "Step 1601 | Loss: 0.3586823344230652\n",
      "Step 1701 | Loss: 0.3852730691432953\n",
      "Step 1801 | Loss: 0.28603506088256836\n",
      "Step 1901 | Loss: 0.3957608938217163\n",
      "Step 2001 | Loss: 0.35566121339797974\n",
      "Step 2101 | Loss: 0.5481588244438171\n",
      "Step 2201 | Loss: 0.3546445965766907\n",
      "Step 2301 | Loss: 0.30429041385650635\n",
      "Step 2401 | Loss: 0.4682552218437195\n",
      "0.3356876375660208 0.9525\n",
      "Step 1 | Loss: 1.2279772758483887\n",
      "Step 101 | Loss: 0.9116942286491394\n",
      "Step 201 | Loss: 0.7208719253540039\n",
      "Step 301 | Loss: 0.6402285099029541\n",
      "Step 401 | Loss: 0.6189976930618286\n",
      "Step 501 | Loss: 0.5613558292388916\n",
      "Step 601 | Loss: 0.4472638964653015\n",
      "Step 701 | Loss: 0.43495917320251465\n",
      "Step 801 | Loss: 0.48817503452301025\n",
      "Step 901 | Loss: 0.42410963773727417\n",
      "Step 1001 | Loss: 0.3513529896736145\n",
      "Step 1101 | Loss: 0.43340110778808594\n",
      "Step 1201 | Loss: 0.47559458017349243\n",
      "Step 1301 | Loss: 0.33055436611175537\n",
      "Step 1401 | Loss: 0.3465019464492798\n",
      "Step 1501 | Loss: 0.45564770698547363\n",
      "Step 1601 | Loss: 0.36532628536224365\n",
      "Step 1701 | Loss: 0.42204171419143677\n",
      "Step 1801 | Loss: 0.4688936173915863\n",
      "Step 1901 | Loss: 0.38622725009918213\n",
      "Step 2001 | Loss: 0.3516920506954193\n",
      "Step 2101 | Loss: 0.3794107437133789\n",
      "Step 2201 | Loss: 0.30069419741630554\n",
      "Step 2301 | Loss: 0.5276912450790405\n",
      "Step 2401 | Loss: 0.34777340292930603\n",
      "0.31620433884625593 0.945\n",
      "Step 1 | Loss: 0.9280657768249512\n",
      "Step 101 | Loss: 0.7016777992248535\n",
      "Step 201 | Loss: 0.6630027294158936\n",
      "Step 301 | Loss: 0.5464779138565063\n",
      "Step 401 | Loss: 0.4240707457065582\n",
      "Step 501 | Loss: 0.4041028916835785\n",
      "Step 601 | Loss: 0.477398157119751\n",
      "Step 701 | Loss: 0.41831332445144653\n",
      "Step 801 | Loss: 0.5137454271316528\n",
      "Step 901 | Loss: 0.4515581727027893\n",
      "Step 1001 | Loss: 0.5671621561050415\n",
      "Step 1101 | Loss: 0.4184850752353668\n",
      "Step 1201 | Loss: 0.358704537153244\n",
      "Step 1301 | Loss: 0.3284090757369995\n",
      "Step 1401 | Loss: 0.31224650144577026\n",
      "Step 1501 | Loss: 0.42392978072166443\n",
      "Step 1601 | Loss: 0.3804037272930145\n",
      "Step 1701 | Loss: 0.4840533137321472\n",
      "Step 1801 | Loss: 0.32012656331062317\n",
      "Step 1901 | Loss: 0.3253118693828583\n",
      "Step 2001 | Loss: 0.36379674077033997\n",
      "Step 2101 | Loss: 0.3185870349407196\n",
      "Step 2201 | Loss: 0.3349238336086273\n",
      "Step 2301 | Loss: 0.36489686369895935\n",
      "Step 2401 | Loss: 0.25616025924682617\n",
      "0.3223082691668738 0.935\n",
      "Step 1 | Loss: 1.6706935167312622\n",
      "Step 101 | Loss: 0.4663189947605133\n",
      "Step 201 | Loss: 0.3598669171333313\n",
      "Step 301 | Loss: 0.6304389238357544\n",
      "Step 401 | Loss: 0.5789356231689453\n",
      "Step 501 | Loss: 0.5947823524475098\n",
      "Step 601 | Loss: 0.3228568136692047\n",
      "Step 701 | Loss: 0.46492284536361694\n",
      "Step 801 | Loss: 0.43832892179489136\n",
      "Step 901 | Loss: 0.4216960072517395\n",
      "Step 1001 | Loss: 0.35047948360443115\n",
      "Step 1101 | Loss: 0.5687140822410583\n",
      "Step 1201 | Loss: 0.4883357584476471\n",
      "Step 1301 | Loss: 0.5489748120307922\n",
      "Step 1401 | Loss: 0.4718213379383087\n",
      "Step 1501 | Loss: 0.42708665132522583\n",
      "Step 1601 | Loss: 0.5699417591094971\n",
      "Step 1701 | Loss: 0.40765130519866943\n",
      "Step 1801 | Loss: 0.38371461629867554\n",
      "Step 1901 | Loss: 0.5016595721244812\n",
      "Step 2001 | Loss: 0.48968952894210815\n",
      "Step 2101 | Loss: 0.3941175937652588\n",
      "Step 2201 | Loss: 0.5153005123138428\n",
      "Step 2301 | Loss: 0.4104228913784027\n",
      "Step 2401 | Loss: 0.38025909662246704\n",
      "0.48917067232670447 0.8325\n",
      "Step 1 | Loss: 1.98488450050354\n",
      "Step 101 | Loss: 0.956254243850708\n",
      "Step 201 | Loss: 0.5947735905647278\n",
      "Step 301 | Loss: 0.4196060597896576\n",
      "Step 401 | Loss: 0.61798495054245\n",
      "Step 501 | Loss: 0.45383715629577637\n",
      "Step 601 | Loss: 0.4314994812011719\n",
      "Step 701 | Loss: 0.5599963068962097\n",
      "Step 801 | Loss: 0.5400271415710449\n",
      "Step 901 | Loss: 0.3918527662754059\n",
      "Step 1001 | Loss: 0.4700682759284973\n",
      "Step 1101 | Loss: 0.27710211277008057\n",
      "Step 1201 | Loss: 0.33076557517051697\n",
      "Step 1301 | Loss: 0.3441339135169983\n",
      "Step 1401 | Loss: 0.3636629581451416\n",
      "Step 1501 | Loss: 0.4788445234298706\n",
      "Step 1601 | Loss: 0.32389581203460693\n",
      "Step 1701 | Loss: 0.23881353437900543\n",
      "Step 1801 | Loss: 0.38257867097854614\n",
      "Step 1901 | Loss: 0.5319384336471558\n",
      "Step 2001 | Loss: 0.3430635929107666\n",
      "Step 2101 | Loss: 0.3316081464290619\n",
      "Step 2201 | Loss: 0.44522225856781006\n",
      "Step 2301 | Loss: 0.26181113719940186\n",
      "Step 2401 | Loss: 0.5217178463935852\n",
      "0.4535732180247478 0.86\n",
      "Step 1 | Loss: 2.011815309524536\n",
      "Step 101 | Loss: 0.5703380703926086\n",
      "Step 201 | Loss: 0.6312272548675537\n",
      "Step 301 | Loss: 0.6319974064826965\n",
      "Step 401 | Loss: 0.4497339129447937\n",
      "Step 501 | Loss: 0.5312700271606445\n",
      "Step 601 | Loss: 0.30319371819496155\n",
      "Step 701 | Loss: 0.6924792528152466\n",
      "Step 801 | Loss: 0.44006240367889404\n",
      "Step 901 | Loss: 0.36483290791511536\n",
      "Step 1001 | Loss: 0.529495894908905\n",
      "Step 1101 | Loss: 0.25899559259414673\n",
      "Step 1201 | Loss: 0.4619556963443756\n",
      "Step 1301 | Loss: 0.28804731369018555\n",
      "Step 1401 | Loss: 0.6190788149833679\n",
      "Step 1501 | Loss: 0.34734252095222473\n",
      "Step 1601 | Loss: 0.3907109200954437\n",
      "Step 1701 | Loss: 0.6387360692024231\n",
      "Step 1801 | Loss: 0.4029962718486786\n",
      "Step 1901 | Loss: 0.39856424927711487\n",
      "Step 2001 | Loss: 0.44444358348846436\n",
      "Step 2101 | Loss: 0.5202458500862122\n",
      "Step 2201 | Loss: 0.3997995853424072\n",
      "Step 2301 | Loss: 0.41808846592903137\n",
      "Step 2401 | Loss: 0.36302345991134644\n",
      "0.4533641809961915 0.86\n",
      "Step 1 | Loss: 1.864126443862915\n",
      "Step 101 | Loss: 0.7631599307060242\n",
      "Step 201 | Loss: 0.7312687635421753\n",
      "Step 301 | Loss: 0.39978429675102234\n",
      "Step 401 | Loss: 0.7792577743530273\n",
      "Step 501 | Loss: 0.6355641484260559\n",
      "Step 601 | Loss: 0.3918606638908386\n",
      "Step 701 | Loss: 0.6509953737258911\n",
      "Step 801 | Loss: 0.49915000796318054\n",
      "Step 901 | Loss: 0.49032121896743774\n",
      "Step 1001 | Loss: 0.6288367509841919\n",
      "Step 1101 | Loss: 0.6044623851776123\n",
      "Step 1201 | Loss: 0.4415024220943451\n",
      "Step 1301 | Loss: 0.42566922307014465\n",
      "Step 1401 | Loss: 0.6231831312179565\n",
      "Step 1501 | Loss: 0.29602327942848206\n",
      "Step 1601 | Loss: 0.6587801575660706\n",
      "Step 1701 | Loss: 0.4716164469718933\n",
      "Step 1801 | Loss: 0.6012511253356934\n",
      "Step 1901 | Loss: 0.48258429765701294\n",
      "Step 2001 | Loss: 0.6279573440551758\n",
      "Step 2101 | Loss: 0.4530036449432373\n",
      "Step 2201 | Loss: 0.3227115273475647\n",
      "Step 2301 | Loss: 0.5296044945716858\n",
      "Step 2401 | Loss: 0.5052785873413086\n",
      "0.49077292605609846 0.835\n",
      "Step 1 | Loss: 0.711921215057373\n",
      "Step 101 | Loss: 0.40876421332359314\n",
      "Step 201 | Loss: 0.49880215525627136\n",
      "Step 301 | Loss: 0.2747887372970581\n",
      "Step 401 | Loss: 0.5164428949356079\n",
      "Step 501 | Loss: 0.34883996844291687\n",
      "Step 601 | Loss: 0.4077998697757721\n",
      "Step 701 | Loss: 0.7103878855705261\n",
      "Step 801 | Loss: 0.3303816020488739\n",
      "Step 901 | Loss: 0.745787501335144\n",
      "Step 1001 | Loss: 0.34976518154144287\n",
      "Step 1101 | Loss: 0.4830968379974365\n",
      "Step 1201 | Loss: 0.6006876230239868\n",
      "Step 1301 | Loss: 0.6044179201126099\n",
      "Step 1401 | Loss: 0.3793274164199829\n",
      "Step 1501 | Loss: 0.3817519545555115\n",
      "Step 1601 | Loss: 0.5380626320838928\n",
      "Step 1701 | Loss: 0.28834617137908936\n",
      "Step 1801 | Loss: 0.5235013365745544\n",
      "Step 1901 | Loss: 0.4004303514957428\n",
      "Step 2001 | Loss: 0.35775071382522583\n",
      "Step 2101 | Loss: 0.4022407531738281\n",
      "Step 2201 | Loss: 0.4929299056529999\n",
      "Step 2301 | Loss: 0.3416420817375183\n",
      "Step 2401 | Loss: 0.5616508722305298\n",
      "0.4538491660510079 0.86\n",
      "Step 1 | Loss: 0.9681907892227173\n",
      "Step 101 | Loss: 0.8990413546562195\n",
      "Step 201 | Loss: 0.7707198262214661\n",
      "Step 301 | Loss: 0.7331944108009338\n",
      "Step 401 | Loss: 0.8847954273223877\n",
      "Step 501 | Loss: 0.7074284553527832\n",
      "Step 601 | Loss: 0.8287537097930908\n",
      "Step 701 | Loss: 0.8454774618148804\n",
      "Step 801 | Loss: 0.711931586265564\n",
      "Step 901 | Loss: 0.6949672698974609\n",
      "Step 1001 | Loss: 0.8470425605773926\n",
      "Step 1101 | Loss: 0.7220772504806519\n",
      "Step 1201 | Loss: 0.734285295009613\n",
      "Step 1301 | Loss: 0.7712949514389038\n",
      "Step 1401 | Loss: 0.7033818364143372\n",
      "Step 1501 | Loss: 0.6928284764289856\n",
      "Step 1601 | Loss: 0.7499037981033325\n",
      "Step 1701 | Loss: 0.8471519351005554\n",
      "Step 1801 | Loss: 0.713069498538971\n",
      "Step 1901 | Loss: 0.7879025340080261\n",
      "Step 2001 | Loss: 0.6098963618278503\n",
      "Step 2101 | Loss: 0.7666735053062439\n",
      "Step 2201 | Loss: 0.886342465877533\n",
      "Step 2301 | Loss: 0.8590348958969116\n",
      "Step 2401 | Loss: 0.7987029552459717\n",
      "0.7189109017051807 0.72\n",
      "Step 1 | Loss: 0.8450263142585754\n",
      "Step 101 | Loss: 0.8683720827102661\n",
      "Step 201 | Loss: 0.8596054315567017\n",
      "Step 301 | Loss: 0.7139050364494324\n",
      "Step 401 | Loss: 0.7286797761917114\n",
      "Step 501 | Loss: 0.7938452959060669\n",
      "Step 601 | Loss: 0.8213488459587097\n",
      "Step 701 | Loss: 0.7509002685546875\n",
      "Step 801 | Loss: 0.7799689769744873\n",
      "Step 901 | Loss: 0.8095166087150574\n",
      "Step 1001 | Loss: 0.8283947110176086\n",
      "Step 1101 | Loss: 0.6145473718643188\n",
      "Step 1201 | Loss: 0.5559025406837463\n",
      "Step 1301 | Loss: 0.8040951490402222\n",
      "Step 1401 | Loss: 0.7103614807128906\n",
      "Step 1501 | Loss: 0.8508503437042236\n",
      "Step 1601 | Loss: 0.7608507871627808\n",
      "Step 1701 | Loss: 0.7476925849914551\n",
      "Step 1801 | Loss: 0.670569896697998\n",
      "Step 1901 | Loss: 0.7074404954910278\n",
      "Step 2001 | Loss: 0.6820258498191833\n",
      "Step 2101 | Loss: 0.7850827574729919\n",
      "Step 2201 | Loss: 0.7688419818878174\n",
      "Step 2301 | Loss: 0.7781519889831543\n",
      "Step 2401 | Loss: 0.8344262838363647\n",
      "0.7159791832211003 0.6825\n",
      "Step 1 | Loss: 1.0677430629730225\n",
      "Step 101 | Loss: 0.7872366905212402\n",
      "Step 201 | Loss: 0.8423488736152649\n",
      "Step 301 | Loss: 0.5865284204483032\n",
      "Step 401 | Loss: 0.8586146831512451\n",
      "Step 501 | Loss: 0.7667719721794128\n",
      "Step 601 | Loss: 0.7682377099990845\n",
      "Step 701 | Loss: 0.760831892490387\n",
      "Step 801 | Loss: 0.7354758977890015\n",
      "Step 901 | Loss: 0.6830418109893799\n",
      "Step 1001 | Loss: 0.5638640522956848\n",
      "Step 1101 | Loss: 0.817338228225708\n",
      "Step 1201 | Loss: 0.880801796913147\n",
      "Step 1301 | Loss: 0.7632650136947632\n",
      "Step 1401 | Loss: 0.6787111759185791\n",
      "Step 1501 | Loss: 0.5878371596336365\n",
      "Step 1601 | Loss: 0.7304007411003113\n",
      "Step 1701 | Loss: 0.7679421901702881\n",
      "Step 1801 | Loss: 0.8571210503578186\n",
      "Step 1901 | Loss: 0.7746412754058838\n",
      "Step 2001 | Loss: 0.8492390513420105\n",
      "Step 2101 | Loss: 0.7643877267837524\n",
      "Step 2201 | Loss: 0.8552202582359314\n",
      "Step 2301 | Loss: 0.6741288304328918\n",
      "Step 2401 | Loss: 0.7453792691230774\n",
      "0.7169597436067572 0.685\n",
      "Step 1 | Loss: 1.394338607788086\n",
      "Step 101 | Loss: 0.9387381076812744\n",
      "Step 201 | Loss: 0.8128259778022766\n",
      "Step 301 | Loss: 0.7888100147247314\n",
      "Step 401 | Loss: 0.5742318630218506\n",
      "Step 501 | Loss: 0.7426444888114929\n",
      "Step 601 | Loss: 0.8132162094116211\n",
      "Step 701 | Loss: 0.8933846950531006\n",
      "Step 801 | Loss: 0.7906618714332581\n",
      "Step 901 | Loss: 0.715378999710083\n",
      "Step 1001 | Loss: 0.6600088477134705\n",
      "Step 1101 | Loss: 0.663467526435852\n",
      "Step 1201 | Loss: 0.6625065207481384\n",
      "Step 1301 | Loss: 0.8574690818786621\n",
      "Step 1401 | Loss: 0.5370757579803467\n",
      "Step 1501 | Loss: 0.782630205154419\n",
      "Step 1601 | Loss: 0.6338907480239868\n",
      "Step 1701 | Loss: 0.7241138219833374\n",
      "Step 1801 | Loss: 0.8048667311668396\n",
      "Step 1901 | Loss: 0.7658286094665527\n",
      "Step 2001 | Loss: 0.7011654376983643\n",
      "Step 2101 | Loss: 0.5604538917541504\n",
      "Step 2201 | Loss: 0.8188303709030151\n",
      "Step 2301 | Loss: 0.6839331984519958\n",
      "Step 2401 | Loss: 0.6913291811943054\n",
      "0.7086901069637149 0.695\n",
      "Step 1 | Loss: 1.1900075674057007\n",
      "Step 101 | Loss: 0.8504492044448853\n",
      "Step 201 | Loss: 0.7581770420074463\n",
      "Step 301 | Loss: 0.7284091711044312\n",
      "Step 401 | Loss: 0.6598173975944519\n",
      "Step 501 | Loss: 0.7924667596817017\n",
      "Step 601 | Loss: 0.8031535148620605\n",
      "Step 701 | Loss: 0.6980407238006592\n",
      "Step 801 | Loss: 0.8113135099411011\n",
      "Step 901 | Loss: 0.7738317847251892\n",
      "Step 1001 | Loss: 0.7678418755531311\n",
      "Step 1101 | Loss: 0.7096405029296875\n",
      "Step 1201 | Loss: 0.6979145407676697\n",
      "Step 1301 | Loss: 0.7126471996307373\n",
      "Step 1401 | Loss: 0.7785324454307556\n",
      "Step 1501 | Loss: 0.6436903476715088\n",
      "Step 1601 | Loss: 0.7268438935279846\n",
      "Step 1701 | Loss: 0.7221078276634216\n",
      "Step 1801 | Loss: 0.667145311832428\n",
      "Step 1901 | Loss: 0.8549562692642212\n",
      "Step 2001 | Loss: 0.7506549954414368\n",
      "Step 2101 | Loss: 0.6914889216423035\n",
      "Step 2201 | Loss: 0.5899185538291931\n",
      "Step 2301 | Loss: 0.8180719614028931\n",
      "Step 2401 | Loss: 0.6404446959495544\n",
      "0.7201095881353379 0.6675\n",
      "Step 1 | Loss: 1.6814640760421753\n",
      "Step 101 | Loss: 0.6122815608978271\n",
      "Step 201 | Loss: 0.6041313409805298\n",
      "Step 301 | Loss: 0.5841038227081299\n",
      "Step 401 | Loss: 0.5732588171958923\n",
      "Step 501 | Loss: 0.524282693862915\n",
      "Step 601 | Loss: 0.48265358805656433\n",
      "Step 701 | Loss: 0.5146982669830322\n",
      "Step 801 | Loss: 0.6169880628585815\n",
      "Step 901 | Loss: 0.6254169940948486\n",
      "Step 1001 | Loss: 0.5297340750694275\n",
      "Step 1101 | Loss: 0.7456327080726624\n",
      "Step 1201 | Loss: 0.7500907182693481\n",
      "Step 1301 | Loss: 0.561613142490387\n",
      "Step 1401 | Loss: 0.5458798408508301\n",
      "Step 1501 | Loss: 0.5356205105781555\n",
      "Step 1601 | Loss: 0.46529868245124817\n",
      "Step 1701 | Loss: 0.6863260865211487\n",
      "Step 1801 | Loss: 0.4339858591556549\n",
      "Step 1901 | Loss: 0.4773332178592682\n",
      "Step 2001 | Loss: 0.6797276735305786\n",
      "Step 2101 | Loss: 0.5439038872718811\n",
      "Step 2201 | Loss: 0.5206825137138367\n",
      "Step 2301 | Loss: 0.6093858480453491\n",
      "Step 2401 | Loss: 0.5881357789039612\n",
      "0.5220776813154291 0.85\n",
      "Step 1 | Loss: 1.6316626071929932\n",
      "Step 101 | Loss: 0.6149771809577942\n",
      "Step 201 | Loss: 0.5982965230941772\n",
      "Step 301 | Loss: 0.5865316390991211\n",
      "Step 401 | Loss: 0.7720022797584534\n",
      "Step 501 | Loss: 0.5373784899711609\n",
      "Step 601 | Loss: 0.7311774492263794\n",
      "Step 701 | Loss: 0.6153308749198914\n",
      "Step 801 | Loss: 0.46977436542510986\n",
      "Step 901 | Loss: 0.5118440985679626\n",
      "Step 1001 | Loss: 0.577336847782135\n",
      "Step 1101 | Loss: 0.5960839986801147\n",
      "Step 1201 | Loss: 0.4792230725288391\n",
      "Step 1301 | Loss: 0.6635921001434326\n",
      "Step 1401 | Loss: 0.5391558408737183\n",
      "Step 1501 | Loss: 0.7612764239311218\n",
      "Step 1601 | Loss: 0.6249195337295532\n",
      "Step 1701 | Loss: 0.6112053394317627\n",
      "Step 1801 | Loss: 0.6100633144378662\n",
      "Step 1901 | Loss: 0.5074574947357178\n",
      "Step 2001 | Loss: 0.6469424962997437\n",
      "Step 2101 | Loss: 0.526154637336731\n",
      "Step 2201 | Loss: 0.5594785809516907\n",
      "Step 2301 | Loss: 0.5072566270828247\n",
      "Step 2401 | Loss: 0.6190493702888489\n",
      "0.5208401150989461 0.845\n",
      "Step 1 | Loss: 1.6568355560302734\n",
      "Step 101 | Loss: 0.5771915316581726\n",
      "Step 201 | Loss: 0.6185141801834106\n",
      "Step 301 | Loss: 0.4352051317691803\n",
      "Step 401 | Loss: 0.4900254011154175\n",
      "Step 501 | Loss: 0.42339029908180237\n",
      "Step 601 | Loss: 0.5412441492080688\n",
      "Step 701 | Loss: 0.49614736437797546\n",
      "Step 801 | Loss: 0.5174216032028198\n",
      "Step 901 | Loss: 0.5902436375617981\n",
      "Step 1001 | Loss: 0.6487125158309937\n",
      "Step 1101 | Loss: 0.6772316098213196\n",
      "Step 1201 | Loss: 0.6501047015190125\n",
      "Step 1301 | Loss: 0.5028096437454224\n",
      "Step 1401 | Loss: 0.46735692024230957\n",
      "Step 1501 | Loss: 0.5743904113769531\n",
      "Step 1601 | Loss: 0.5154274106025696\n",
      "Step 1701 | Loss: 0.6015588045120239\n",
      "Step 1801 | Loss: 0.5600861310958862\n",
      "Step 1901 | Loss: 0.8359171748161316\n",
      "Step 2001 | Loss: 0.37713050842285156\n",
      "Step 2101 | Loss: 0.4964885711669922\n",
      "Step 2201 | Loss: 0.7091033458709717\n",
      "Step 2301 | Loss: 0.5107787847518921\n",
      "Step 2401 | Loss: 0.4957202970981598\n",
      "0.521389145818719 0.85\n",
      "Step 1 | Loss: 1.2626982927322388\n",
      "Step 101 | Loss: 0.543205976486206\n",
      "Step 201 | Loss: 0.3853192925453186\n",
      "Step 301 | Loss: 0.5883334875106812\n",
      "Step 401 | Loss: 0.574944794178009\n",
      "Step 501 | Loss: 0.39599746465682983\n",
      "Step 601 | Loss: 0.6352778673171997\n",
      "Step 701 | Loss: 0.5160679221153259\n",
      "Step 801 | Loss: 0.4367431104183197\n",
      "Step 901 | Loss: 0.5372841954231262\n",
      "Step 1001 | Loss: 0.5239927172660828\n",
      "Step 1101 | Loss: 0.5615558624267578\n",
      "Step 1201 | Loss: 0.7941450476646423\n",
      "Step 1301 | Loss: 0.6343801021575928\n",
      "Step 1401 | Loss: 0.5584481954574585\n",
      "Step 1501 | Loss: 0.4061294198036194\n",
      "Step 1601 | Loss: 0.5328819751739502\n",
      "Step 1701 | Loss: 0.7621856927871704\n",
      "Step 1801 | Loss: 0.4224202334880829\n",
      "Step 1901 | Loss: 0.545254647731781\n",
      "Step 2001 | Loss: 0.46915411949157715\n",
      "Step 2101 | Loss: 0.4119556248188019\n",
      "Step 2201 | Loss: 0.8398222327232361\n",
      "Step 2301 | Loss: 0.5869622826576233\n",
      "Step 2401 | Loss: 0.44807931780815125\n",
      "0.5223557058486518 0.8525\n",
      "Step 1 | Loss: 0.9885379076004028\n",
      "Step 101 | Loss: 0.44809186458587646\n",
      "Step 201 | Loss: 0.5973039865493774\n",
      "Step 301 | Loss: 0.6369820237159729\n",
      "Step 401 | Loss: 0.48106011748313904\n",
      "Step 501 | Loss: 0.5583779811859131\n",
      "Step 601 | Loss: 0.5036316514015198\n",
      "Step 701 | Loss: 0.3720705211162567\n",
      "Step 801 | Loss: 0.5000059008598328\n",
      "Step 901 | Loss: 0.552880048751831\n",
      "Step 1001 | Loss: 0.4986626207828522\n",
      "Step 1101 | Loss: 0.5585375428199768\n",
      "Step 1201 | Loss: 0.5497459173202515\n",
      "Step 1301 | Loss: 0.5731385350227356\n",
      "Step 1401 | Loss: 0.5476827621459961\n",
      "Step 1501 | Loss: 0.7880581617355347\n",
      "Step 1601 | Loss: 0.3923090696334839\n",
      "Step 1701 | Loss: 0.6014468669891357\n",
      "Step 1801 | Loss: 0.582263171672821\n",
      "Step 1901 | Loss: 0.657262921333313\n",
      "Step 2001 | Loss: 0.5066098570823669\n",
      "Step 2101 | Loss: 0.5970140099525452\n",
      "Step 2201 | Loss: 0.5111759305000305\n",
      "Step 2301 | Loss: 0.6310533285140991\n",
      "Step 2401 | Loss: 0.5157455205917358\n",
      "0.5214811488083051 0.85\n",
      "Step 1 | Loss: 0.806734025478363\n",
      "Step 101 | Loss: 0.7733224630355835\n",
      "Step 201 | Loss: 0.6575629711151123\n",
      "Step 301 | Loss: 0.6298193335533142\n",
      "Step 401 | Loss: 0.5883899927139282\n",
      "Step 501 | Loss: 0.6489459276199341\n",
      "Step 601 | Loss: 0.46426892280578613\n",
      "Step 701 | Loss: 0.5499262809753418\n",
      "Step 801 | Loss: 0.5554116368293762\n",
      "Step 901 | Loss: 0.5425881147384644\n",
      "Step 1001 | Loss: 0.6482153534889221\n",
      "Step 1101 | Loss: 0.46382468938827515\n",
      "Step 1201 | Loss: 0.548194169998169\n",
      "Step 1301 | Loss: 0.6578781604766846\n",
      "Step 1401 | Loss: 0.5374472737312317\n",
      "Step 1501 | Loss: 0.7044503688812256\n",
      "Step 1601 | Loss: 0.4890778362751007\n",
      "Step 1701 | Loss: 0.5237533450126648\n",
      "Step 1801 | Loss: 0.5729082822799683\n",
      "Step 1901 | Loss: 0.4378286600112915\n",
      "Step 2001 | Loss: 0.6093769669532776\n",
      "Step 2101 | Loss: 0.49066001176834106\n",
      "Step 2201 | Loss: 0.5194787979125977\n",
      "Step 2301 | Loss: 0.5740917325019836\n",
      "Step 2401 | Loss: 0.6385399699211121\n",
      "0.5050869688577426 0.88\n",
      "Step 1 | Loss: 1.2006027698516846\n",
      "Step 101 | Loss: 0.7375955581665039\n",
      "Step 201 | Loss: 0.7494202256202698\n",
      "Step 301 | Loss: 0.666343629360199\n",
      "Step 401 | Loss: 0.6022706031799316\n",
      "Step 501 | Loss: 0.5308396816253662\n",
      "Step 601 | Loss: 0.5996007919311523\n",
      "Step 701 | Loss: 0.6517027020454407\n",
      "Step 801 | Loss: 0.5455387830734253\n",
      "Step 901 | Loss: 0.8430102467536926\n",
      "Step 1001 | Loss: 0.6016318798065186\n",
      "Step 1101 | Loss: 0.6140235662460327\n",
      "Step 1201 | Loss: 0.5083969831466675\n",
      "Step 1301 | Loss: 0.585468590259552\n",
      "Step 1401 | Loss: 0.5541419386863708\n",
      "Step 1501 | Loss: 0.8534694910049438\n",
      "Step 1601 | Loss: 0.6262484192848206\n",
      "Step 1701 | Loss: 0.6388848423957825\n",
      "Step 1801 | Loss: 0.6757673025131226\n",
      "Step 1901 | Loss: 0.5675654411315918\n",
      "Step 2001 | Loss: 0.6194554567337036\n",
      "Step 2101 | Loss: 0.6034751534461975\n",
      "Step 2201 | Loss: 0.5693901777267456\n",
      "Step 2301 | Loss: 0.6280921697616577\n",
      "Step 2401 | Loss: 0.6788151264190674\n",
      "0.6129061680784136 0.85\n",
      "Step 1 | Loss: 0.9968333840370178\n",
      "Step 101 | Loss: 0.7650708556175232\n",
      "Step 201 | Loss: 0.5675560235977173\n",
      "Step 301 | Loss: 0.5190572738647461\n",
      "Step 401 | Loss: 0.5927286148071289\n",
      "Step 501 | Loss: 0.4159319996833801\n",
      "Step 601 | Loss: 0.5829554796218872\n",
      "Step 701 | Loss: 0.49758636951446533\n",
      "Step 801 | Loss: 0.5177674293518066\n",
      "Step 901 | Loss: 0.4278112053871155\n",
      "Step 1001 | Loss: 0.42410922050476074\n",
      "Step 1101 | Loss: 0.5078395009040833\n",
      "Step 1201 | Loss: 0.42306557297706604\n",
      "Step 1301 | Loss: 0.47445863485336304\n",
      "Step 1401 | Loss: 0.4715960919857025\n",
      "Step 1501 | Loss: 0.38867080211639404\n",
      "Step 1601 | Loss: 0.532270073890686\n",
      "Step 1701 | Loss: 0.459042489528656\n",
      "Step 1801 | Loss: 0.4689021408557892\n",
      "Step 1901 | Loss: 0.4218030273914337\n",
      "Step 2001 | Loss: 0.4842776656150818\n",
      "Step 2101 | Loss: 0.4359217882156372\n",
      "Step 2201 | Loss: 0.5209324955940247\n",
      "Step 2301 | Loss: 0.4590275287628174\n",
      "Step 2401 | Loss: 0.44282475113868713\n",
      "0.4514788674906009 0.9225\n",
      "Step 1 | Loss: 1.0568357706069946\n",
      "Step 101 | Loss: 0.5774903297424316\n",
      "Step 201 | Loss: 0.5960904359817505\n",
      "Step 301 | Loss: 0.5570986866950989\n",
      "Step 401 | Loss: 0.5796641111373901\n",
      "Step 501 | Loss: 0.5946394205093384\n",
      "Step 601 | Loss: 0.5374261140823364\n",
      "Step 701 | Loss: 0.6574994325637817\n",
      "Step 801 | Loss: 0.558479368686676\n",
      "Step 901 | Loss: 0.5537571310997009\n",
      "Step 1001 | Loss: 0.536310076713562\n",
      "Step 1101 | Loss: 0.5941668152809143\n",
      "Step 1201 | Loss: 0.5443330407142639\n",
      "Step 1301 | Loss: 0.5264803767204285\n",
      "Step 1401 | Loss: 0.5168614983558655\n",
      "Step 1501 | Loss: 0.6170098185539246\n",
      "Step 1601 | Loss: 0.5202493071556091\n",
      "Step 1701 | Loss: 0.6919077634811401\n",
      "Step 1801 | Loss: 0.44394466280937195\n",
      "Step 1901 | Loss: 0.5106291174888611\n",
      "Step 2001 | Loss: 0.5238118171691895\n",
      "Step 2101 | Loss: 0.6927837133407593\n",
      "Step 2201 | Loss: 0.6333611011505127\n",
      "Step 2301 | Loss: 0.49989262223243713\n",
      "Step 2401 | Loss: 0.6280919909477234\n",
      "0.5510957297287761 0.875\n",
      "Step 1 | Loss: 1.2617194652557373\n",
      "Step 101 | Loss: 0.6727218627929688\n",
      "Step 201 | Loss: 0.9071165919303894\n",
      "Step 301 | Loss: 0.604077160358429\n",
      "Step 401 | Loss: 0.6434298157691956\n",
      "Step 501 | Loss: 0.7464898228645325\n",
      "Step 601 | Loss: 0.5863403081893921\n",
      "Step 701 | Loss: 0.6823083162307739\n",
      "Step 801 | Loss: 0.5848402976989746\n",
      "Step 901 | Loss: 0.4750002920627594\n",
      "Step 1001 | Loss: 0.6021255254745483\n",
      "Step 1101 | Loss: 0.682388424873352\n",
      "Step 1201 | Loss: 0.5525707006454468\n",
      "Step 1301 | Loss: 0.5725618004798889\n",
      "Step 1401 | Loss: 0.6185756325721741\n",
      "Step 1501 | Loss: 0.5273904204368591\n",
      "Step 1601 | Loss: 0.5352585315704346\n",
      "Step 1701 | Loss: 0.517115592956543\n",
      "Step 1801 | Loss: 0.6336053609848022\n",
      "Step 1901 | Loss: 0.5584767460823059\n",
      "Step 2001 | Loss: 0.6589007377624512\n",
      "Step 2101 | Loss: 0.5869218111038208\n",
      "Step 2201 | Loss: 0.5793285369873047\n",
      "Step 2301 | Loss: 0.583763599395752\n",
      "Step 2401 | Loss: 0.5797535181045532\n",
      "0.5809363505052588 0.83\n",
      "Step 1 | Loss: 1.301554560661316\n",
      "Step 101 | Loss: 0.5637280344963074\n",
      "Step 201 | Loss: 0.4897499680519104\n",
      "Step 301 | Loss: 0.5809042453765869\n",
      "Step 401 | Loss: 0.5216455459594727\n",
      "Step 501 | Loss: 0.5978339910507202\n",
      "Step 601 | Loss: 0.5670445561408997\n",
      "Step 701 | Loss: 0.509795069694519\n",
      "Step 801 | Loss: 0.5567666292190552\n",
      "Step 901 | Loss: 0.582588255405426\n",
      "Step 1001 | Loss: 0.7743039727210999\n",
      "Step 1101 | Loss: 0.506122887134552\n",
      "Step 1201 | Loss: 0.5195504426956177\n",
      "Step 1301 | Loss: 0.610403299331665\n",
      "Step 1401 | Loss: 0.6252413988113403\n",
      "Step 1501 | Loss: 0.6205768585205078\n",
      "Step 1601 | Loss: 0.4032760262489319\n",
      "Step 1701 | Loss: 0.4883292615413666\n",
      "Step 1801 | Loss: 0.7030695080757141\n",
      "Step 1901 | Loss: 0.5526506900787354\n",
      "Step 2001 | Loss: 0.6492454409599304\n",
      "Step 2101 | Loss: 0.5098265409469604\n",
      "Step 2201 | Loss: 0.5989603996276855\n",
      "Step 2301 | Loss: 0.5585386753082275\n",
      "Step 2401 | Loss: 0.43143564462661743\n",
      "0.5921607356892188 0.79\n",
      "Step 1 | Loss: 0.7539086937904358\n",
      "Step 101 | Loss: 0.5665283799171448\n",
      "Step 201 | Loss: 0.4786372184753418\n",
      "Step 301 | Loss: 0.44802242517471313\n",
      "Step 401 | Loss: 0.6182505488395691\n",
      "Step 501 | Loss: 0.4152515232563019\n",
      "Step 601 | Loss: 0.5184788703918457\n",
      "Step 701 | Loss: 0.4474616050720215\n",
      "Step 801 | Loss: 0.4742368757724762\n",
      "Step 901 | Loss: 0.6348077058792114\n",
      "Step 1001 | Loss: 0.6534978151321411\n",
      "Step 1101 | Loss: 0.3452187478542328\n",
      "Step 1201 | Loss: 0.5749271512031555\n",
      "Step 1301 | Loss: 0.6139416098594666\n",
      "Step 1401 | Loss: 0.4644264876842499\n",
      "Step 1501 | Loss: 0.550191342830658\n",
      "Step 1601 | Loss: 0.46793484687805176\n",
      "Step 1701 | Loss: 0.5987533926963806\n",
      "Step 1801 | Loss: 0.6002711057662964\n",
      "Step 1901 | Loss: 0.42579784989356995\n",
      "Step 2001 | Loss: 0.3635142743587494\n",
      "Step 2101 | Loss: 0.45337897539138794\n",
      "Step 2201 | Loss: 0.5013772249221802\n",
      "Step 2301 | Loss: 0.5253722667694092\n",
      "Step 2401 | Loss: 0.5989824533462524\n",
      "0.5898908113147049 0.8\n",
      "Step 1 | Loss: 1.309021234512329\n",
      "Step 101 | Loss: 0.8084338307380676\n",
      "Step 201 | Loss: 0.6027940511703491\n",
      "Step 301 | Loss: 0.46550852060317993\n",
      "Step 401 | Loss: 0.4884703457355499\n",
      "Step 501 | Loss: 0.4903426468372345\n",
      "Step 601 | Loss: 0.5632126331329346\n",
      "Step 701 | Loss: 0.5279920697212219\n",
      "Step 801 | Loss: 0.5219852328300476\n",
      "Step 901 | Loss: 0.5259802341461182\n",
      "Step 1001 | Loss: 0.49628207087516785\n",
      "Step 1101 | Loss: 0.5253071784973145\n",
      "Step 1201 | Loss: 0.2790989279747009\n",
      "Step 1301 | Loss: 0.7632086277008057\n",
      "Step 1401 | Loss: 0.5376497507095337\n",
      "Step 1501 | Loss: 0.4903619587421417\n",
      "Step 1601 | Loss: 0.3211165964603424\n",
      "Step 1701 | Loss: 0.6152837872505188\n",
      "Step 1801 | Loss: 0.4652947783470154\n",
      "Step 1901 | Loss: 0.5446582436561584\n",
      "Step 2001 | Loss: 0.6529310345649719\n",
      "Step 2101 | Loss: 0.45678412914276123\n",
      "Step 2201 | Loss: 0.7539108395576477\n",
      "Step 2301 | Loss: 0.41037556529045105\n",
      "Step 2401 | Loss: 0.419964462518692\n",
      "0.59525874284794 0.7925\n",
      "Step 1 | Loss: 1.0863597393035889\n",
      "Step 101 | Loss: 0.8957881927490234\n",
      "Step 201 | Loss: 0.5336384177207947\n",
      "Step 301 | Loss: 0.5760486721992493\n",
      "Step 401 | Loss: 0.48404136300086975\n",
      "Step 501 | Loss: 0.5261186361312866\n",
      "Step 601 | Loss: 0.5839105844497681\n",
      "Step 701 | Loss: 0.38291093707084656\n",
      "Step 801 | Loss: 0.5420911312103271\n",
      "Step 901 | Loss: 0.5915849804878235\n",
      "Step 1001 | Loss: 0.4175272285938263\n",
      "Step 1101 | Loss: 0.5764883160591125\n",
      "Step 1201 | Loss: 0.4701535999774933\n",
      "Step 1301 | Loss: 0.46410197019577026\n",
      "Step 1401 | Loss: 0.49925854802131653\n",
      "Step 1501 | Loss: 0.5193285346031189\n",
      "Step 1601 | Loss: 0.3781055808067322\n",
      "Step 1701 | Loss: 0.5455324053764343\n",
      "Step 1801 | Loss: 0.6137853264808655\n",
      "Step 1901 | Loss: 0.3144129812717438\n",
      "Step 2001 | Loss: 0.3979586362838745\n",
      "Step 2101 | Loss: 0.5161697864532471\n",
      "Step 2201 | Loss: 0.5413822531700134\n",
      "Step 2301 | Loss: 0.6052444577217102\n",
      "Step 2401 | Loss: 0.6276131272315979\n",
      "0.5877331397826284 0.7925\n",
      "Step 1 | Loss: 1.2292720079421997\n",
      "Step 101 | Loss: 0.8445663452148438\n",
      "Step 201 | Loss: 0.6588730812072754\n",
      "Step 301 | Loss: 0.4327350854873657\n",
      "Step 401 | Loss: 0.42239975929260254\n",
      "Step 501 | Loss: 0.5618662238121033\n",
      "Step 601 | Loss: 0.5138610005378723\n",
      "Step 701 | Loss: 0.5050370693206787\n",
      "Step 801 | Loss: 0.42754125595092773\n",
      "Step 901 | Loss: 0.45629313588142395\n",
      "Step 1001 | Loss: 0.5457720160484314\n",
      "Step 1101 | Loss: 0.4920235276222229\n",
      "Step 1201 | Loss: 0.36269956827163696\n",
      "Step 1301 | Loss: 0.5841768980026245\n",
      "Step 1401 | Loss: 0.5175270438194275\n",
      "Step 1501 | Loss: 0.4926612675189972\n",
      "Step 1601 | Loss: 0.5975237488746643\n",
      "Step 1701 | Loss: 0.5375653505325317\n",
      "Step 1801 | Loss: 0.5904719829559326\n",
      "Step 1901 | Loss: 0.5073593258857727\n",
      "Step 2001 | Loss: 0.43800362944602966\n",
      "Step 2101 | Loss: 0.4026111364364624\n",
      "Step 2201 | Loss: 0.43725520372390747\n",
      "Step 2301 | Loss: 0.6028591394424438\n",
      "Step 2401 | Loss: 0.4785069525241852\n",
      "0.5891012351982428 0.8\n",
      "Step 1 | Loss: 1.8890355825424194\n",
      "Step 101 | Loss: 1.8484735488891602\n",
      "Step 201 | Loss: 1.4263262748718262\n",
      "Step 301 | Loss: 1.0811959505081177\n",
      "Step 401 | Loss: 1.4018867015838623\n",
      "Step 501 | Loss: 1.0971782207489014\n",
      "Step 601 | Loss: 1.4069762229919434\n",
      "Step 701 | Loss: 1.3421375751495361\n",
      "Step 801 | Loss: 1.4872713088989258\n",
      "Step 901 | Loss: 1.4058737754821777\n",
      "Step 1001 | Loss: 1.286094307899475\n",
      "Step 1101 | Loss: 1.2589893341064453\n",
      "Step 1201 | Loss: 1.3707985877990723\n",
      "Step 1301 | Loss: 1.2809076309204102\n",
      "Step 1401 | Loss: 1.4209794998168945\n",
      "Step 1501 | Loss: 0.9031018018722534\n",
      "Step 1601 | Loss: 1.05734384059906\n",
      "Step 1701 | Loss: 1.3085488080978394\n",
      "Step 1801 | Loss: 1.417236328125\n",
      "Step 1901 | Loss: 1.5153385400772095\n",
      "Step 2001 | Loss: 1.4782249927520752\n",
      "Step 2101 | Loss: 1.2942253351211548\n",
      "Step 2201 | Loss: 1.2918132543563843\n",
      "Step 2301 | Loss: 1.105053424835205\n",
      "Step 2401 | Loss: 1.3898520469665527\n",
      "1.3605430630168227 0.4625\n",
      "Step 1 | Loss: 1.7800850868225098\n",
      "Step 101 | Loss: 1.3640152215957642\n",
      "Step 201 | Loss: 1.2053276300430298\n",
      "Step 301 | Loss: 1.3109486103057861\n",
      "Step 401 | Loss: 1.2483322620391846\n",
      "Step 501 | Loss: 0.9795219898223877\n",
      "Step 601 | Loss: 1.5406156778335571\n",
      "Step 701 | Loss: 1.2034056186676025\n",
      "Step 801 | Loss: 1.3539249897003174\n",
      "Step 901 | Loss: 0.9558945894241333\n",
      "Step 1001 | Loss: 1.2105365991592407\n",
      "Step 1101 | Loss: 1.2993125915527344\n",
      "Step 1201 | Loss: 1.232345461845398\n",
      "Step 1301 | Loss: 1.0646600723266602\n",
      "Step 1401 | Loss: 1.2696881294250488\n",
      "Step 1501 | Loss: 1.2518043518066406\n",
      "Step 1601 | Loss: 1.5704402923583984\n",
      "Step 1701 | Loss: 1.2639541625976562\n",
      "Step 1801 | Loss: 1.0917013883590698\n",
      "Step 1901 | Loss: 1.2496765851974487\n",
      "Step 2001 | Loss: 1.0769658088684082\n",
      "Step 2101 | Loss: 1.738710880279541\n",
      "Step 2201 | Loss: 1.273283839225769\n",
      "Step 2301 | Loss: 1.3761906623840332\n",
      "Step 2401 | Loss: 1.330628514289856\n",
      "1.3625550610928814 0.465\n",
      "Step 1 | Loss: 1.2539703845977783\n",
      "Step 101 | Loss: 1.3078678846359253\n",
      "Step 201 | Loss: 1.1683608293533325\n",
      "Step 301 | Loss: 1.237541675567627\n",
      "Step 401 | Loss: 1.6543662548065186\n",
      "Step 501 | Loss: 1.491072177886963\n",
      "Step 601 | Loss: 0.9774129986763\n",
      "Step 701 | Loss: 1.1321754455566406\n",
      "Step 801 | Loss: 1.6343270540237427\n",
      "Step 901 | Loss: 1.0433610677719116\n",
      "Step 1001 | Loss: 1.227670431137085\n",
      "Step 1101 | Loss: 1.1931849718093872\n",
      "Step 1201 | Loss: 1.5012527704238892\n",
      "Step 1301 | Loss: 0.9814174175262451\n",
      "Step 1401 | Loss: 1.12346613407135\n",
      "Step 1501 | Loss: 1.4473356008529663\n",
      "Step 1601 | Loss: 1.240390419960022\n",
      "Step 1701 | Loss: 1.4715802669525146\n",
      "Step 1801 | Loss: 1.108660340309143\n",
      "Step 1901 | Loss: 1.3768168687820435\n",
      "Step 2001 | Loss: 1.2507888078689575\n",
      "Step 2101 | Loss: 1.4175885915756226\n",
      "Step 2201 | Loss: 1.2740612030029297\n",
      "Step 2301 | Loss: 1.0119246244430542\n",
      "Step 2401 | Loss: 1.1156738996505737\n",
      "1.3629062613843068 0.47\n",
      "Step 1 | Loss: 1.52854585647583\n",
      "Step 101 | Loss: 1.4411237239837646\n",
      "Step 201 | Loss: 1.6745010614395142\n",
      "Step 301 | Loss: 0.9833813905715942\n",
      "Step 401 | Loss: 1.2601532936096191\n",
      "Step 501 | Loss: 1.2610851526260376\n",
      "Step 601 | Loss: 1.3367717266082764\n",
      "Step 701 | Loss: 0.8983358144760132\n",
      "Step 801 | Loss: 1.086739182472229\n",
      "Step 901 | Loss: 0.9559736847877502\n",
      "Step 1001 | Loss: 1.6616430282592773\n",
      "Step 1101 | Loss: 1.0852153301239014\n",
      "Step 1201 | Loss: 1.655982494354248\n",
      "Step 1301 | Loss: 1.3516778945922852\n",
      "Step 1401 | Loss: 1.1606855392456055\n",
      "Step 1501 | Loss: 1.2257477045059204\n",
      "Step 1601 | Loss: 1.4886796474456787\n",
      "Step 1701 | Loss: 1.2275031805038452\n",
      "Step 1801 | Loss: 1.054478406906128\n",
      "Step 1901 | Loss: 1.5508368015289307\n",
      "Step 2001 | Loss: 1.6276191473007202\n",
      "Step 2101 | Loss: 1.5501723289489746\n",
      "Step 2201 | Loss: 1.391906499862671\n",
      "Step 2301 | Loss: 1.0592917203903198\n",
      "Step 2401 | Loss: 1.4380065202713013\n",
      "1.362049769508534 0.465\n",
      "Step 1 | Loss: 1.1259844303131104\n",
      "Step 101 | Loss: 1.3969792127609253\n",
      "Step 201 | Loss: 1.1349763870239258\n",
      "Step 301 | Loss: 1.3523753881454468\n",
      "Step 401 | Loss: 1.3894832134246826\n",
      "Step 501 | Loss: 1.5518724918365479\n",
      "Step 601 | Loss: 1.294152021408081\n",
      "Step 701 | Loss: 1.1503572463989258\n",
      "Step 801 | Loss: 1.3716177940368652\n",
      "Step 901 | Loss: 1.1545462608337402\n",
      "Step 1001 | Loss: 1.3350015878677368\n",
      "Step 1101 | Loss: 1.1880574226379395\n",
      "Step 1201 | Loss: 1.1280794143676758\n",
      "Step 1301 | Loss: 1.1575819253921509\n",
      "Step 1401 | Loss: 1.4757719039916992\n",
      "Step 1501 | Loss: 1.5246697664260864\n",
      "Step 1601 | Loss: 1.4575668573379517\n",
      "Step 1701 | Loss: 1.191929817199707\n",
      "Step 1801 | Loss: 1.0855693817138672\n",
      "Step 1901 | Loss: 1.4416065216064453\n",
      "Step 2001 | Loss: 1.1540045738220215\n",
      "Step 2101 | Loss: 1.1935770511627197\n",
      "Step 2201 | Loss: 1.3656965494155884\n",
      "Step 2301 | Loss: 1.385031819343567\n",
      "Step 2401 | Loss: 1.3623322248458862\n",
      "1.360676171288032 0.4625\n",
      "Step 1 | Loss: 1.1667770147323608\n",
      "Step 101 | Loss: 0.6864203214645386\n",
      "Step 201 | Loss: 0.6724168062210083\n",
      "Step 301 | Loss: 0.6243842840194702\n",
      "Step 401 | Loss: 0.6159728765487671\n",
      "Step 501 | Loss: 0.6058652400970459\n",
      "Step 601 | Loss: 0.5980659127235413\n",
      "Step 701 | Loss: 0.5960038900375366\n",
      "Step 801 | Loss: 0.6533370018005371\n",
      "Step 901 | Loss: 0.6009660363197327\n",
      "Step 1001 | Loss: 0.5598961114883423\n",
      "Step 1101 | Loss: 0.6039574146270752\n",
      "Step 1201 | Loss: 0.6933663487434387\n",
      "Step 1301 | Loss: 0.5345597267150879\n",
      "Step 1401 | Loss: 0.5800644159317017\n",
      "Step 1501 | Loss: 0.4753936529159546\n",
      "Step 1601 | Loss: 0.5596981644630432\n",
      "Step 1701 | Loss: 0.5230550765991211\n",
      "Step 1801 | Loss: 0.5488578677177429\n",
      "Step 1901 | Loss: 0.590838611125946\n",
      "Step 2001 | Loss: 0.5504438877105713\n",
      "Step 2101 | Loss: 0.5919342041015625\n",
      "Step 2201 | Loss: 0.49123549461364746\n",
      "Step 2301 | Loss: 0.635725200176239\n",
      "Step 2401 | Loss: 0.5940334796905518\n",
      "0.534359713399229 0.9225\n",
      "Step 1 | Loss: 2.271583318710327\n",
      "Step 101 | Loss: 0.5021789073944092\n",
      "Step 201 | Loss: 0.5394464135169983\n",
      "Step 301 | Loss: 0.48510855436325073\n",
      "Step 401 | Loss: 0.46916353702545166\n",
      "Step 501 | Loss: 0.39559274911880493\n",
      "Step 601 | Loss: 0.3790636360645294\n",
      "Step 701 | Loss: 0.3920327425003052\n",
      "Step 801 | Loss: 0.33076751232147217\n",
      "Step 901 | Loss: 0.41208529472351074\n",
      "Step 1001 | Loss: 0.41534778475761414\n",
      "Step 1101 | Loss: 0.4789629578590393\n",
      "Step 1201 | Loss: 0.4113594591617584\n",
      "Step 1301 | Loss: 0.3893003463745117\n",
      "Step 1401 | Loss: 0.36721086502075195\n",
      "Step 1501 | Loss: 0.4014780521392822\n",
      "Step 1601 | Loss: 0.3746778964996338\n",
      "Step 1701 | Loss: 0.3025704026222229\n",
      "Step 1801 | Loss: 0.4377347528934479\n",
      "Step 1901 | Loss: 0.23062680661678314\n",
      "Step 2001 | Loss: 0.4247354567050934\n",
      "Step 2101 | Loss: 0.4376078248023987\n",
      "Step 2201 | Loss: 0.47602710127830505\n",
      "Step 2301 | Loss: 0.3421137034893036\n",
      "Step 2401 | Loss: 0.40374109148979187\n",
      "0.42154092291417533 0.895\n",
      "Step 1 | Loss: 1.1460187435150146\n",
      "Step 101 | Loss: 0.600436806678772\n",
      "Step 201 | Loss: 0.3637735843658447\n",
      "Step 301 | Loss: 0.2941066324710846\n",
      "Step 401 | Loss: 0.35163524746894836\n",
      "Step 501 | Loss: 0.5671786665916443\n",
      "Step 601 | Loss: 0.3416290879249573\n",
      "Step 701 | Loss: 0.3868257999420166\n",
      "Step 801 | Loss: 0.4419855773448944\n",
      "Step 901 | Loss: 0.2577684819698334\n",
      "Step 1001 | Loss: 0.29573649168014526\n",
      "Step 1101 | Loss: 0.4075953960418701\n",
      "Step 1201 | Loss: 0.5032166242599487\n",
      "Step 1301 | Loss: 0.5714830160140991\n",
      "Step 1401 | Loss: 0.4791785478591919\n",
      "Step 1501 | Loss: 0.4258435070514679\n",
      "Step 1601 | Loss: 0.5197758674621582\n",
      "Step 1701 | Loss: 0.444522887468338\n",
      "Step 1801 | Loss: 0.5369351506233215\n",
      "Step 1901 | Loss: 0.42028552293777466\n",
      "Step 2001 | Loss: 0.4380224347114563\n",
      "Step 2101 | Loss: 0.37143850326538086\n",
      "Step 2201 | Loss: 0.5271063446998596\n",
      "Step 2301 | Loss: 0.42028748989105225\n",
      "Step 2401 | Loss: 0.46049076318740845\n",
      "0.43962615583775555 0.8825\n",
      "Step 1 | Loss: 0.6812902092933655\n",
      "Step 101 | Loss: 0.5177050828933716\n",
      "Step 201 | Loss: 0.5042949318885803\n",
      "Step 301 | Loss: 0.3752415180206299\n",
      "Step 401 | Loss: 0.3266824781894684\n",
      "Step 501 | Loss: 0.4646834135055542\n",
      "Step 601 | Loss: 0.4300883412361145\n",
      "Step 701 | Loss: 0.33468717336654663\n",
      "Step 801 | Loss: 0.45036330819129944\n",
      "Step 901 | Loss: 0.3709799647331238\n",
      "Step 1001 | Loss: 0.42186859250068665\n",
      "Step 1101 | Loss: 0.3830617368221283\n",
      "Step 1201 | Loss: 0.3731957674026489\n",
      "Step 1301 | Loss: 0.3865801692008972\n",
      "Step 1401 | Loss: 0.3497891128063202\n",
      "Step 1501 | Loss: 0.44824326038360596\n",
      "Step 1601 | Loss: 0.5287252068519592\n",
      "Step 1701 | Loss: 0.49790358543395996\n",
      "Step 1801 | Loss: 0.36247557401657104\n",
      "Step 1901 | Loss: 0.4924018085002899\n",
      "Step 2001 | Loss: 0.3260918855667114\n",
      "Step 2101 | Loss: 0.3868066966533661\n",
      "Step 2201 | Loss: 0.3236261010169983\n",
      "Step 2301 | Loss: 0.4508745074272156\n",
      "Step 2401 | Loss: 0.6067017912864685\n",
      "0.4070425523862011 0.9\n",
      "Step 1 | Loss: 1.0992529392242432\n",
      "Step 101 | Loss: 0.6676289439201355\n",
      "Step 201 | Loss: 0.5017722249031067\n",
      "Step 301 | Loss: 0.5233127474784851\n",
      "Step 401 | Loss: 0.4742574393749237\n",
      "Step 501 | Loss: 0.621488094329834\n",
      "Step 601 | Loss: 0.3631449043750763\n",
      "Step 701 | Loss: 0.4755837321281433\n",
      "Step 801 | Loss: 0.44576495885849\n",
      "Step 901 | Loss: 0.3840167820453644\n",
      "Step 1001 | Loss: 0.5132241249084473\n",
      "Step 1101 | Loss: 0.4052906334400177\n",
      "Step 1201 | Loss: 0.34359675645828247\n",
      "Step 1301 | Loss: 0.3640469014644623\n",
      "Step 1401 | Loss: 0.47145915031433105\n",
      "Step 1501 | Loss: 0.48892393708229065\n",
      "Step 1601 | Loss: 0.4209229350090027\n",
      "Step 1701 | Loss: 0.6664698123931885\n",
      "Step 1801 | Loss: 0.5968721508979797\n",
      "Step 1901 | Loss: 0.4393412470817566\n",
      "Step 2001 | Loss: 0.4465460777282715\n",
      "Step 2101 | Loss: 0.5952615141868591\n",
      "Step 2201 | Loss: 0.4717317819595337\n",
      "Step 2301 | Loss: 0.48786938190460205\n",
      "Step 2401 | Loss: 0.45382848381996155\n",
      "0.4386122892317209 0.8825\n",
      "Step 1 | Loss: 0.851309597492218\n",
      "Step 101 | Loss: 0.6379585266113281\n",
      "Step 201 | Loss: 0.42084458470344543\n",
      "Step 301 | Loss: 0.5231314301490784\n",
      "Step 401 | Loss: 0.4382897615432739\n",
      "Step 501 | Loss: 0.42716097831726074\n",
      "Step 601 | Loss: 0.421517014503479\n",
      "Step 701 | Loss: 0.5617775321006775\n",
      "Step 801 | Loss: 0.4219961166381836\n",
      "Step 901 | Loss: 0.4026937782764435\n",
      "Step 1001 | Loss: 0.3313737213611603\n",
      "Step 1101 | Loss: 0.49491336941719055\n",
      "Step 1201 | Loss: 0.3140833377838135\n",
      "Step 1301 | Loss: 0.44086533784866333\n",
      "Step 1401 | Loss: 0.4274437725543976\n",
      "Step 1501 | Loss: 0.4945637881755829\n",
      "Step 1601 | Loss: 0.4165438413619995\n",
      "Step 1701 | Loss: 0.5519858002662659\n",
      "Step 1801 | Loss: 0.34675833582878113\n",
      "Step 1901 | Loss: 0.3002619445323944\n",
      "Step 2001 | Loss: 0.32305556535720825\n",
      "Step 2101 | Loss: 0.4845196008682251\n",
      "Step 2201 | Loss: 0.5019139051437378\n",
      "Step 2301 | Loss: 0.5184838175773621\n",
      "Step 2401 | Loss: 0.4275883436203003\n",
      "0.42618895837264775 0.895\n",
      "Step 1 | Loss: 1.4089783430099487\n",
      "Step 101 | Loss: 0.7469682693481445\n",
      "Step 201 | Loss: 0.6587011814117432\n",
      "Step 301 | Loss: 0.7012917399406433\n",
      "Step 401 | Loss: 0.6651231646537781\n",
      "Step 501 | Loss: 0.29828837513923645\n",
      "Step 601 | Loss: 0.6236570477485657\n",
      "Step 701 | Loss: 0.3471386432647705\n",
      "Step 801 | Loss: 0.4877076745033264\n",
      "Step 901 | Loss: 0.42283111810684204\n",
      "Step 1001 | Loss: 0.5545234084129333\n",
      "Step 1101 | Loss: 0.5745078325271606\n",
      "Step 1201 | Loss: 0.5634225010871887\n",
      "Step 1301 | Loss: 0.4127611815929413\n",
      "Step 1401 | Loss: 0.4798266589641571\n",
      "Step 1501 | Loss: 0.2808741331100464\n",
      "Step 1601 | Loss: 0.4312746822834015\n",
      "Step 1701 | Loss: 0.5970488786697388\n",
      "Step 1801 | Loss: 0.4956982731819153\n",
      "Step 1901 | Loss: 0.3226720988750458\n",
      "Step 2001 | Loss: 0.49724915623664856\n",
      "Step 2101 | Loss: 0.3594682216644287\n",
      "Step 2201 | Loss: 0.32211339473724365\n",
      "Step 2301 | Loss: 0.4896358847618103\n",
      "Step 2401 | Loss: 0.4316169321537018\n",
      "0.4571508045693475 0.8475\n",
      "Step 1 | Loss: 1.0453777313232422\n",
      "Step 101 | Loss: 0.7625516653060913\n",
      "Step 201 | Loss: 0.5591756105422974\n",
      "Step 301 | Loss: 0.5393218994140625\n",
      "Step 401 | Loss: 0.598378598690033\n",
      "Step 501 | Loss: 0.5450323820114136\n",
      "Step 601 | Loss: 0.4089459776878357\n",
      "Step 701 | Loss: 0.38165175914764404\n",
      "Step 801 | Loss: 0.32527875900268555\n",
      "Step 901 | Loss: 0.3919457197189331\n",
      "Step 1001 | Loss: 0.5014700889587402\n",
      "Step 1101 | Loss: 0.4311386048793793\n",
      "Step 1201 | Loss: 0.563322126865387\n",
      "Step 1301 | Loss: 0.5393269062042236\n",
      "Step 1401 | Loss: 0.471264511346817\n",
      "Step 1501 | Loss: 0.6162270307540894\n",
      "Step 1601 | Loss: 0.3643166422843933\n",
      "Step 1701 | Loss: 0.3931891620159149\n",
      "Step 1801 | Loss: 0.3993719518184662\n",
      "Step 1901 | Loss: 0.46153101325035095\n",
      "Step 2001 | Loss: 0.5542189478874207\n",
      "Step 2101 | Loss: 0.33858174085617065\n",
      "Step 2201 | Loss: 0.5547664761543274\n",
      "Step 2301 | Loss: 0.387899786233902\n",
      "Step 2401 | Loss: 0.4487270712852478\n",
      "0.47958205311967483 0.845\n",
      "Step 1 | Loss: 0.9803562164306641\n",
      "Step 101 | Loss: 0.6636225581169128\n",
      "Step 201 | Loss: 0.677086591720581\n",
      "Step 301 | Loss: 0.5666025876998901\n",
      "Step 401 | Loss: 0.6117376089096069\n",
      "Step 501 | Loss: 0.6565800309181213\n",
      "Step 601 | Loss: 0.5969060063362122\n",
      "Step 701 | Loss: 0.424277126789093\n",
      "Step 801 | Loss: 0.4190668761730194\n",
      "Step 901 | Loss: 0.5349336862564087\n",
      "Step 1001 | Loss: 0.5368155241012573\n",
      "Step 1101 | Loss: 0.3157474100589752\n",
      "Step 1201 | Loss: 0.592138409614563\n",
      "Step 1301 | Loss: 0.42934340238571167\n",
      "Step 1401 | Loss: 0.600595235824585\n",
      "Step 1501 | Loss: 0.512015163898468\n",
      "Step 1601 | Loss: 0.3603721261024475\n",
      "Step 1701 | Loss: 0.5186599493026733\n",
      "Step 1801 | Loss: 0.33777758479118347\n",
      "Step 1901 | Loss: 0.38771888613700867\n",
      "Step 2001 | Loss: 0.3878657817840576\n",
      "Step 2101 | Loss: 0.3056861460208893\n",
      "Step 2201 | Loss: 0.4194242060184479\n",
      "Step 2301 | Loss: 0.46307283639907837\n",
      "Step 2401 | Loss: 0.3425470292568207\n",
      "0.389070754502305 0.92\n",
      "Step 1 | Loss: 1.5569654703140259\n",
      "Step 101 | Loss: 0.5437411069869995\n",
      "Step 201 | Loss: 0.4258459806442261\n",
      "Step 301 | Loss: 0.46580812335014343\n",
      "Step 401 | Loss: 0.4540659785270691\n",
      "Step 501 | Loss: 0.3926151990890503\n",
      "Step 601 | Loss: 0.36739206314086914\n",
      "Step 701 | Loss: 0.3504530191421509\n",
      "Step 801 | Loss: 0.4657392203807831\n",
      "Step 901 | Loss: 0.38017570972442627\n",
      "Step 1001 | Loss: 0.42728444933891296\n",
      "Step 1101 | Loss: 0.37131640315055847\n",
      "Step 1201 | Loss: 0.36787110567092896\n",
      "Step 1301 | Loss: 0.3126382827758789\n",
      "Step 1401 | Loss: 0.6159268021583557\n",
      "Step 1501 | Loss: 0.3333461582660675\n",
      "Step 1601 | Loss: 0.3928983807563782\n",
      "Step 1701 | Loss: 0.29271382093429565\n",
      "Step 1801 | Loss: 0.3069564998149872\n",
      "Step 1901 | Loss: 0.30908262729644775\n",
      "Step 2001 | Loss: 0.28738778829574585\n",
      "Step 2101 | Loss: 0.2955572307109833\n",
      "Step 2201 | Loss: 0.40719956159591675\n",
      "Step 2301 | Loss: 0.31124091148376465\n",
      "Step 2401 | Loss: 0.31116175651550293\n",
      "0.36123418546875674 0.915\n",
      "Step 1 | Loss: 1.7763681411743164\n",
      "Step 101 | Loss: 2.3191726207733154\n",
      "Step 201 | Loss: 2.1422746181488037\n",
      "Step 301 | Loss: 2.0333786010742188\n",
      "Step 401 | Loss: 2.3354954719543457\n",
      "Step 501 | Loss: 1.941068410873413\n",
      "Step 601 | Loss: 2.3190271854400635\n",
      "Step 701 | Loss: 1.906388282775879\n",
      "Step 801 | Loss: 2.3398032188415527\n",
      "Step 901 | Loss: 1.7878097295761108\n",
      "Step 1001 | Loss: 2.090118885040283\n",
      "Step 1101 | Loss: 1.873787760734558\n",
      "Step 1201 | Loss: 1.7532514333724976\n",
      "Step 1301 | Loss: 1.922026515007019\n",
      "Step 1401 | Loss: 1.9260607957839966\n",
      "Step 1501 | Loss: 2.390833854675293\n",
      "Step 1601 | Loss: 2.197415590286255\n",
      "Step 1701 | Loss: 2.377372980117798\n",
      "Step 1801 | Loss: 2.0719051361083984\n",
      "Step 1901 | Loss: 1.828721046447754\n",
      "Step 2001 | Loss: 2.3368935585021973\n",
      "Step 2101 | Loss: 2.3750951290130615\n",
      "Step 2201 | Loss: 1.9197171926498413\n",
      "Step 2301 | Loss: 2.6214897632598877\n",
      "Step 2401 | Loss: 1.9505536556243896\n",
      "2.1505376109161127 0.3\n",
      "Step 1 | Loss: 1.7780441045761108\n",
      "Step 101 | Loss: 2.0833566188812256\n",
      "Step 201 | Loss: 2.1762657165527344\n",
      "Step 301 | Loss: 2.0189948081970215\n",
      "Step 401 | Loss: 1.9578542709350586\n",
      "Step 501 | Loss: 2.351872205734253\n",
      "Step 601 | Loss: 2.256690740585327\n",
      "Step 701 | Loss: 2.2032761573791504\n",
      "Step 801 | Loss: 2.112454891204834\n",
      "Step 901 | Loss: 1.7061232328414917\n",
      "Step 1001 | Loss: 1.8163385391235352\n",
      "Step 1101 | Loss: 2.1089038848876953\n",
      "Step 1201 | Loss: 2.234212875366211\n",
      "Step 1301 | Loss: 2.1481645107269287\n",
      "Step 1401 | Loss: 2.268756628036499\n",
      "Step 1501 | Loss: 2.1155848503112793\n",
      "Step 1601 | Loss: 1.8063558340072632\n",
      "Step 1701 | Loss: 2.2642197608947754\n",
      "Step 1801 | Loss: 2.3227291107177734\n",
      "Step 1901 | Loss: 2.180652618408203\n",
      "Step 2001 | Loss: 2.219048023223877\n",
      "Step 2101 | Loss: 2.2884557247161865\n",
      "Step 2201 | Loss: 1.5848438739776611\n",
      "Step 2301 | Loss: 2.2200512886047363\n",
      "Step 2401 | Loss: 1.9013559818267822\n",
      "2.1505373622618165 0.3\n",
      "Step 1 | Loss: 2.026857614517212\n",
      "Step 101 | Loss: 2.233100175857544\n",
      "Step 201 | Loss: 2.1085877418518066\n",
      "Step 301 | Loss: 1.5895565748214722\n",
      "Step 401 | Loss: 2.1513173580169678\n",
      "Step 501 | Loss: 2.2868990898132324\n",
      "Step 601 | Loss: 2.1533727645874023\n",
      "Step 701 | Loss: 2.0602076053619385\n",
      "Step 801 | Loss: 1.9784622192382812\n",
      "Step 901 | Loss: 2.1562440395355225\n",
      "Step 1001 | Loss: 2.1786389350891113\n",
      "Step 1101 | Loss: 2.0698599815368652\n",
      "Step 1201 | Loss: 1.9996144771575928\n",
      "Step 1301 | Loss: 1.7745475769042969\n",
      "Step 1401 | Loss: 1.846777081489563\n",
      "Step 1501 | Loss: 1.913536787033081\n",
      "Step 1601 | Loss: 1.8661757707595825\n",
      "Step 1701 | Loss: 2.299175500869751\n",
      "Step 1801 | Loss: 2.3003222942352295\n",
      "Step 1901 | Loss: 1.7477712631225586\n",
      "Step 2001 | Loss: 2.1211483478546143\n",
      "Step 2101 | Loss: 1.9691052436828613\n",
      "Step 2201 | Loss: 1.8509972095489502\n",
      "Step 2301 | Loss: 2.0020105838775635\n",
      "Step 2401 | Loss: 2.2915287017822266\n",
      "2.150537622343843 0.3\n",
      "Step 1 | Loss: 2.2287521362304688\n",
      "Step 101 | Loss: 2.0894947052001953\n",
      "Step 201 | Loss: 1.673985242843628\n",
      "Step 301 | Loss: 1.7892959117889404\n",
      "Step 401 | Loss: 1.7352417707443237\n",
      "Step 501 | Loss: 2.051776170730591\n",
      "Step 601 | Loss: 1.804871678352356\n",
      "Step 701 | Loss: 2.0355641841888428\n",
      "Step 801 | Loss: 2.142184019088745\n",
      "Step 901 | Loss: 2.138500690460205\n",
      "Step 1001 | Loss: 1.9912309646606445\n",
      "Step 1101 | Loss: 1.9806857109069824\n",
      "Step 1201 | Loss: 1.5346362590789795\n",
      "Step 1301 | Loss: 2.3173904418945312\n",
      "Step 1401 | Loss: 2.4792494773864746\n",
      "Step 1501 | Loss: 1.8369125127792358\n",
      "Step 1601 | Loss: 2.2380664348602295\n",
      "Step 1701 | Loss: 2.3299574851989746\n",
      "Step 1801 | Loss: 2.2784504890441895\n",
      "Step 1901 | Loss: 2.190406084060669\n",
      "Step 2001 | Loss: 2.09086012840271\n",
      "Step 2101 | Loss: 1.908400297164917\n",
      "Step 2201 | Loss: 2.0194506645202637\n",
      "Step 2301 | Loss: 2.475090503692627\n",
      "Step 2401 | Loss: 1.7390570640563965\n",
      "2.1505376044260562 0.3\n",
      "Step 1 | Loss: 2.182863235473633\n",
      "Step 101 | Loss: 1.9634428024291992\n",
      "Step 201 | Loss: 2.413949966430664\n",
      "Step 301 | Loss: 1.9466092586517334\n",
      "Step 401 | Loss: 2.335779905319214\n",
      "Step 501 | Loss: 2.5726070404052734\n",
      "Step 601 | Loss: 1.9446885585784912\n",
      "Step 701 | Loss: 2.437112331390381\n",
      "Step 801 | Loss: 2.075660228729248\n",
      "Step 901 | Loss: 2.2579100131988525\n",
      "Step 1001 | Loss: 2.1800148487091064\n",
      "Step 1101 | Loss: 1.7713708877563477\n",
      "Step 1201 | Loss: 2.0389599800109863\n",
      "Step 1301 | Loss: 1.9994685649871826\n",
      "Step 1401 | Loss: 2.194638729095459\n",
      "Step 1501 | Loss: 1.8049437999725342\n",
      "Step 1601 | Loss: 2.0364880561828613\n",
      "Step 1701 | Loss: 2.1279189586639404\n",
      "Step 1801 | Loss: 1.9843406677246094\n",
      "Step 1901 | Loss: 2.1497561931610107\n",
      "Step 2001 | Loss: 1.9537663459777832\n",
      "Step 2101 | Loss: 2.1098556518554688\n",
      "Step 2201 | Loss: 2.0813498497009277\n",
      "Step 2301 | Loss: 1.6668686866760254\n",
      "Step 2401 | Loss: 2.284670352935791\n",
      "2.1505377138598654 0.3\n",
      "Step 1 | Loss: 0.9975448846817017\n",
      "Step 101 | Loss: 1.0741382837295532\n",
      "Step 201 | Loss: 1.0508644580841064\n",
      "Step 301 | Loss: 1.4513216018676758\n",
      "Step 401 | Loss: 0.921390175819397\n",
      "Step 501 | Loss: 0.9555947780609131\n",
      "Step 601 | Loss: 1.3060349225997925\n",
      "Step 701 | Loss: 1.7077178955078125\n",
      "Step 801 | Loss: 1.2828750610351562\n",
      "Step 901 | Loss: 1.3393547534942627\n",
      "Step 1001 | Loss: 1.4330031871795654\n",
      "Step 1101 | Loss: 0.831569492816925\n",
      "Step 1201 | Loss: 1.311481237411499\n",
      "Step 1301 | Loss: 0.9320625066757202\n",
      "Step 1401 | Loss: 1.0801092386245728\n",
      "Step 1501 | Loss: 1.0064213275909424\n",
      "Step 1601 | Loss: 1.2036335468292236\n",
      "Step 1701 | Loss: 1.5576870441436768\n",
      "Step 1801 | Loss: 1.0431033372879028\n",
      "Step 1901 | Loss: 1.2192308902740479\n",
      "Step 2001 | Loss: 1.1969530582427979\n",
      "Step 2101 | Loss: 1.5712302923202515\n",
      "Step 2201 | Loss: 1.3248997926712036\n",
      "Step 2301 | Loss: 1.6171281337738037\n",
      "Step 2401 | Loss: 1.0876559019088745\n",
      "1.2472508219227743 0.6175\n",
      "Step 1 | Loss: 2.17521333694458\n",
      "Step 101 | Loss: 1.825822353363037\n",
      "Step 201 | Loss: 1.5987024307250977\n",
      "Step 301 | Loss: 1.0718715190887451\n",
      "Step 401 | Loss: 1.6070587635040283\n",
      "Step 501 | Loss: 1.4834108352661133\n",
      "Step 601 | Loss: 1.0996756553649902\n",
      "Step 701 | Loss: 1.2927758693695068\n",
      "Step 801 | Loss: 1.246252417564392\n",
      "Step 901 | Loss: 1.2456648349761963\n",
      "Step 1001 | Loss: 1.3822916746139526\n",
      "Step 1101 | Loss: 1.4955127239227295\n",
      "Step 1201 | Loss: 1.42374587059021\n",
      "Step 1301 | Loss: 1.6131672859191895\n",
      "Step 1401 | Loss: 1.2812960147857666\n",
      "Step 1501 | Loss: 1.6798455715179443\n",
      "Step 1601 | Loss: 1.3836700916290283\n",
      "Step 1701 | Loss: 1.1045514345169067\n",
      "Step 1801 | Loss: 1.3701854944229126\n",
      "Step 1901 | Loss: 1.0969829559326172\n",
      "Step 2001 | Loss: 1.6693921089172363\n",
      "Step 2101 | Loss: 1.4294337034225464\n",
      "Step 2201 | Loss: 1.4188896417617798\n",
      "Step 2301 | Loss: 0.670424222946167\n",
      "Step 2401 | Loss: 1.2067296504974365\n",
      "1.2220412281060116 0.6125\n",
      "Step 1 | Loss: 1.5883511304855347\n",
      "Step 101 | Loss: 1.6022573709487915\n",
      "Step 201 | Loss: 1.3456532955169678\n",
      "Step 301 | Loss: 1.9480957984924316\n",
      "Step 401 | Loss: 1.2956379652023315\n",
      "Step 501 | Loss: 1.3515880107879639\n",
      "Step 601 | Loss: 1.1101068258285522\n",
      "Step 701 | Loss: 1.7685574293136597\n",
      "Step 801 | Loss: 1.2667416334152222\n",
      "Step 901 | Loss: 1.6030970811843872\n",
      "Step 1001 | Loss: 0.8886823654174805\n",
      "Step 1101 | Loss: 1.4739315509796143\n",
      "Step 1201 | Loss: 1.2855273485183716\n",
      "Step 1301 | Loss: 1.3880597352981567\n",
      "Step 1401 | Loss: 1.0770149230957031\n",
      "Step 1501 | Loss: 1.2177656888961792\n",
      "Step 1601 | Loss: 1.3555474281311035\n",
      "Step 1701 | Loss: 1.4223573207855225\n",
      "Step 1801 | Loss: 1.1565066576004028\n",
      "Step 1901 | Loss: 0.9405499696731567\n",
      "Step 2001 | Loss: 1.511063575744629\n",
      "Step 2101 | Loss: 1.30605149269104\n",
      "Step 2201 | Loss: 1.4072232246398926\n",
      "Step 2301 | Loss: 1.2726664543151855\n",
      "Step 2401 | Loss: 1.341198444366455\n",
      "1.2257254831096303 0.615\n",
      "Step 1 | Loss: 1.8032112121582031\n",
      "Step 101 | Loss: 1.6697849035263062\n",
      "Step 201 | Loss: 1.086137294769287\n",
      "Step 301 | Loss: 2.1135072708129883\n",
      "Step 401 | Loss: 0.9734984636306763\n",
      "Step 501 | Loss: 1.4387919902801514\n",
      "Step 601 | Loss: 1.387555480003357\n",
      "Step 701 | Loss: 1.3777841329574585\n",
      "Step 801 | Loss: 1.735597848892212\n",
      "Step 901 | Loss: 1.3770090341567993\n",
      "Step 1001 | Loss: 1.151879906654358\n",
      "Step 1101 | Loss: 1.7452445030212402\n",
      "Step 1201 | Loss: 1.3847452402114868\n",
      "Step 1301 | Loss: 1.1991784572601318\n",
      "Step 1401 | Loss: 1.4635939598083496\n",
      "Step 1501 | Loss: 0.8115211725234985\n",
      "Step 1601 | Loss: 1.5597350597381592\n",
      "Step 1701 | Loss: 1.1454317569732666\n",
      "Step 1801 | Loss: 1.5327236652374268\n",
      "Step 1901 | Loss: 1.0570712089538574\n",
      "Step 2001 | Loss: 1.7380943298339844\n",
      "Step 2101 | Loss: 1.180556297302246\n",
      "Step 2201 | Loss: 0.9683108925819397\n",
      "Step 2301 | Loss: 1.3027777671813965\n",
      "Step 2401 | Loss: 0.8682422637939453\n",
      "1.230729588487964 0.62\n",
      "Step 1 | Loss: 2.0238232612609863\n",
      "Step 101 | Loss: 2.1529910564422607\n",
      "Step 201 | Loss: 1.5017595291137695\n",
      "Step 301 | Loss: 1.4215061664581299\n",
      "Step 401 | Loss: 1.674972653388977\n",
      "Step 501 | Loss: 1.927003026008606\n",
      "Step 601 | Loss: 1.560639500617981\n",
      "Step 701 | Loss: 1.6532394886016846\n",
      "Step 801 | Loss: 1.7643043994903564\n",
      "Step 901 | Loss: 1.5587210655212402\n",
      "Step 1001 | Loss: 1.433795690536499\n",
      "Step 1101 | Loss: 1.8203623294830322\n",
      "Step 1201 | Loss: 1.2952598333358765\n",
      "Step 1301 | Loss: 1.3896573781967163\n",
      "Step 1401 | Loss: 1.6491093635559082\n",
      "Step 1501 | Loss: 1.2157176733016968\n",
      "Step 1601 | Loss: 1.8180255889892578\n",
      "Step 1701 | Loss: 1.3226773738861084\n",
      "Step 1801 | Loss: 1.2461580038070679\n",
      "Step 1901 | Loss: 1.4560811519622803\n",
      "Step 2001 | Loss: 1.0197365283966064\n",
      "Step 2101 | Loss: 1.5198071002960205\n",
      "Step 2201 | Loss: 1.6045933961868286\n",
      "Step 2301 | Loss: 1.1746140718460083\n",
      "Step 2401 | Loss: 1.0680859088897705\n",
      "1.2870630521365916 0.595\n",
      "Step 1 | Loss: 1.0827116966247559\n",
      "Step 101 | Loss: 0.7002542018890381\n",
      "Step 201 | Loss: 0.561198890209198\n",
      "Step 301 | Loss: 0.4843607544898987\n",
      "Step 401 | Loss: 0.41664406657218933\n",
      "Step 501 | Loss: 0.3543144762516022\n",
      "Step 601 | Loss: 0.4202616512775421\n",
      "Step 701 | Loss: 0.3777320384979248\n",
      "Step 801 | Loss: 0.29612794518470764\n",
      "Step 901 | Loss: 0.23262304067611694\n",
      "Step 1001 | Loss: 0.1909347027540207\n",
      "Step 1101 | Loss: 0.28045323491096497\n",
      "Step 1201 | Loss: 0.20521681010723114\n",
      "Step 1301 | Loss: 0.23390279710292816\n",
      "Step 1401 | Loss: 0.4272431433200836\n",
      "Step 1501 | Loss: 0.3088350296020508\n",
      "Step 1601 | Loss: 0.3003443479537964\n",
      "Step 1701 | Loss: 0.32186099886894226\n",
      "Step 1801 | Loss: 0.21214930713176727\n",
      "Step 1901 | Loss: 0.17522457242012024\n",
      "Step 2001 | Loss: 0.3235648274421692\n",
      "Step 2101 | Loss: 0.26397767663002014\n",
      "Step 2201 | Loss: 0.29133474826812744\n",
      "Step 2301 | Loss: 0.2438863217830658\n",
      "Step 2401 | Loss: 0.28905367851257324\n",
      "0.2656761794674934 0.9575\n",
      "Step 1 | Loss: 1.1604424715042114\n",
      "Step 101 | Loss: 0.6122990250587463\n",
      "Step 201 | Loss: 0.5429072976112366\n",
      "Step 301 | Loss: 0.5094238519668579\n",
      "Step 401 | Loss: 0.497062087059021\n",
      "Step 501 | Loss: 0.3895248770713806\n",
      "Step 601 | Loss: 0.4723331034183502\n",
      "Step 701 | Loss: 0.4718625843524933\n",
      "Step 801 | Loss: 0.598716139793396\n",
      "Step 901 | Loss: 0.5237163305282593\n",
      "Step 1001 | Loss: 0.7449487447738647\n",
      "Step 1101 | Loss: 0.4868343770503998\n",
      "Step 1201 | Loss: 0.42529046535491943\n",
      "Step 1301 | Loss: 0.3576880097389221\n",
      "Step 1401 | Loss: 0.43994277715682983\n",
      "Step 1501 | Loss: 0.35211920738220215\n",
      "Step 1601 | Loss: 0.3804475963115692\n",
      "Step 1701 | Loss: 0.3637735843658447\n",
      "Step 1801 | Loss: 0.4634426236152649\n",
      "Step 1901 | Loss: 0.38037005066871643\n",
      "Step 2001 | Loss: 0.21612140536308289\n",
      "Step 2101 | Loss: 0.22465743124485016\n",
      "Step 2201 | Loss: 0.2532197833061218\n",
      "Step 2301 | Loss: 0.23443546891212463\n",
      "Step 2401 | Loss: 0.30374184250831604\n",
      "0.263663385595441 0.955\n",
      "Step 1 | Loss: 1.0916376113891602\n",
      "Step 101 | Loss: 0.5246542692184448\n",
      "Step 201 | Loss: 0.6563395261764526\n",
      "Step 301 | Loss: 0.6859719753265381\n",
      "Step 401 | Loss: 0.5203371047973633\n",
      "Step 501 | Loss: 0.500037670135498\n",
      "Step 601 | Loss: 0.5458909273147583\n",
      "Step 701 | Loss: 0.47031864523887634\n",
      "Step 801 | Loss: 0.39755314588546753\n",
      "Step 901 | Loss: 0.5404998660087585\n",
      "Step 1001 | Loss: 0.41818201541900635\n",
      "Step 1101 | Loss: 0.25887006521224976\n",
      "Step 1201 | Loss: 0.49485597014427185\n",
      "Step 1301 | Loss: 0.4602760374546051\n",
      "Step 1401 | Loss: 0.49511095881462097\n",
      "Step 1501 | Loss: 0.3891862630844116\n",
      "Step 1601 | Loss: 0.3677140772342682\n",
      "Step 1701 | Loss: 0.4629676043987274\n",
      "Step 1801 | Loss: 0.47507545351982117\n",
      "Step 1901 | Loss: 0.3572130799293518\n",
      "Step 2001 | Loss: 0.31604766845703125\n",
      "Step 2101 | Loss: 0.4249509572982788\n",
      "Step 2201 | Loss: 0.39164066314697266\n",
      "Step 2301 | Loss: 0.5004962086677551\n",
      "Step 2401 | Loss: 0.5826553106307983\n",
      "0.4067580410013344 0.915\n",
      "Step 1 | Loss: 1.0887410640716553\n",
      "Step 101 | Loss: 0.5891426801681519\n",
      "Step 201 | Loss: 0.3978699743747711\n",
      "Step 301 | Loss: 0.38825976848602295\n",
      "Step 401 | Loss: 0.41181594133377075\n",
      "Step 501 | Loss: 0.438071072101593\n",
      "Step 601 | Loss: 0.4256797432899475\n",
      "Step 701 | Loss: 0.26468950510025024\n",
      "Step 801 | Loss: 0.3623158633708954\n",
      "Step 901 | Loss: 0.2539423406124115\n",
      "Step 1001 | Loss: 0.28819963335990906\n",
      "Step 1101 | Loss: 0.2541661858558655\n",
      "Step 1201 | Loss: 0.267867773771286\n",
      "Step 1301 | Loss: 0.2379905879497528\n",
      "Step 1401 | Loss: 0.2034285068511963\n",
      "Step 1501 | Loss: 0.257006973028183\n",
      "Step 1601 | Loss: 0.2268296182155609\n",
      "Step 1701 | Loss: 0.26701316237449646\n",
      "Step 1801 | Loss: 0.25897571444511414\n",
      "Step 1901 | Loss: 0.40622538328170776\n",
      "Step 2001 | Loss: 0.24532413482666016\n",
      "Step 2101 | Loss: 0.1741916835308075\n",
      "Step 2201 | Loss: 0.3440931737422943\n",
      "Step 2301 | Loss: 0.3020186424255371\n",
      "Step 2401 | Loss: 0.3036247491836548\n",
      "0.2633166803589059 0.96\n",
      "Step 1 | Loss: 1.1043363809585571\n",
      "Step 101 | Loss: 0.7245827317237854\n",
      "Step 201 | Loss: 0.7667628526687622\n",
      "Step 301 | Loss: 0.39960306882858276\n",
      "Step 401 | Loss: 0.49678391218185425\n",
      "Step 501 | Loss: 0.5367483496665955\n",
      "Step 601 | Loss: 0.38712459802627563\n",
      "Step 701 | Loss: 0.4435794949531555\n",
      "Step 801 | Loss: 0.37863707542419434\n",
      "Step 901 | Loss: 0.6311318874359131\n",
      "Step 1001 | Loss: 0.3921962082386017\n",
      "Step 1101 | Loss: 0.3879666328430176\n",
      "Step 1201 | Loss: 0.3539869487285614\n",
      "Step 1301 | Loss: 0.4103492200374603\n",
      "Step 1401 | Loss: 0.3513283431529999\n",
      "Step 1501 | Loss: 0.20063535869121552\n",
      "Step 1601 | Loss: 0.34924906492233276\n",
      "Step 1701 | Loss: 0.2453368604183197\n",
      "Step 1801 | Loss: 0.26617270708084106\n",
      "Step 1901 | Loss: 0.31863221526145935\n",
      "Step 2001 | Loss: 0.22318273782730103\n",
      "Step 2101 | Loss: 0.23358027637004852\n",
      "Step 2201 | Loss: 0.1956462413072586\n",
      "Step 2301 | Loss: 0.3753260374069214\n",
      "Step 2401 | Loss: 0.2271537333726883\n",
      "0.2724768741818825 0.9475\n",
      "Step 1 | Loss: 0.7843259572982788\n",
      "Step 101 | Loss: 0.5029540061950684\n",
      "Step 201 | Loss: 0.2377585768699646\n",
      "Step 301 | Loss: 0.32634103298187256\n",
      "Step 401 | Loss: 0.31019362807273865\n",
      "Step 501 | Loss: 0.22314417362213135\n",
      "Step 601 | Loss: 0.22582900524139404\n",
      "Step 701 | Loss: 0.31040409207344055\n",
      "Step 801 | Loss: 0.33201926946640015\n",
      "Step 901 | Loss: 0.3659333288669586\n",
      "Step 1001 | Loss: 0.20216244459152222\n",
      "Step 1101 | Loss: 0.23474112153053284\n",
      "Step 1201 | Loss: 0.26912054419517517\n",
      "Step 1301 | Loss: 0.319609135389328\n",
      "Step 1401 | Loss: 0.17295579612255096\n",
      "Step 1501 | Loss: 0.2159763127565384\n",
      "Step 1601 | Loss: 0.14132559299468994\n",
      "Step 1701 | Loss: 0.14203740656375885\n",
      "Step 1801 | Loss: 0.2456265389919281\n",
      "Step 1901 | Loss: 0.25776031613349915\n",
      "Step 2001 | Loss: 0.27805763483047485\n",
      "Step 2101 | Loss: 0.2396887093782425\n",
      "Step 2201 | Loss: 0.2616499066352844\n",
      "Step 2301 | Loss: 0.2961593270301819\n",
      "Step 2401 | Loss: 0.33765947818756104\n",
      "0.21959349033887005 0.945\n",
      "Step 1 | Loss: 0.7879225015640259\n",
      "Step 101 | Loss: 0.6202558875083923\n",
      "Step 201 | Loss: 0.5051090717315674\n",
      "Step 301 | Loss: 0.32299962639808655\n",
      "Step 401 | Loss: 0.4833078384399414\n",
      "Step 501 | Loss: 0.32365113496780396\n",
      "Step 601 | Loss: 0.4514157176017761\n",
      "Step 701 | Loss: 0.1951729953289032\n",
      "Step 801 | Loss: 0.222038134932518\n",
      "Step 901 | Loss: 0.23211190104484558\n",
      "Step 1001 | Loss: 0.2088548243045807\n",
      "Step 1101 | Loss: 0.3102641701698303\n",
      "Step 1201 | Loss: 0.21671155095100403\n",
      "Step 1301 | Loss: 0.3196377158164978\n",
      "Step 1401 | Loss: 0.25765460729599\n",
      "Step 1501 | Loss: 0.2709418535232544\n",
      "Step 1601 | Loss: 0.2385386973619461\n",
      "Step 1701 | Loss: 0.21158431470394135\n",
      "Step 1801 | Loss: 0.1699630469083786\n",
      "Step 1901 | Loss: 0.1883162260055542\n",
      "Step 2001 | Loss: 0.22623443603515625\n",
      "Step 2101 | Loss: 0.17604674398899078\n",
      "Step 2201 | Loss: 0.2171306312084198\n",
      "Step 2301 | Loss: 0.23060530424118042\n",
      "Step 2401 | Loss: 0.20285804569721222\n",
      "0.2304926478358332 0.945\n",
      "Step 1 | Loss: 1.5912492275238037\n",
      "Step 101 | Loss: 0.4074406027793884\n",
      "Step 201 | Loss: 0.31765976548194885\n",
      "Step 301 | Loss: 0.252997487783432\n",
      "Step 401 | Loss: 0.27957019209861755\n",
      "Step 501 | Loss: 0.3146741986274719\n",
      "Step 601 | Loss: 0.2811139225959778\n",
      "Step 701 | Loss: 0.3953665494918823\n",
      "Step 801 | Loss: 0.26830416917800903\n",
      "Step 901 | Loss: 0.20350100100040436\n",
      "Step 1001 | Loss: 0.22426539659500122\n",
      "Step 1101 | Loss: 0.2381780743598938\n",
      "Step 1201 | Loss: 0.3330191373825073\n",
      "Step 1301 | Loss: 0.2896419167518616\n",
      "Step 1401 | Loss: 0.14722904562950134\n",
      "Step 1501 | Loss: 0.1435641348361969\n",
      "Step 1601 | Loss: 0.21090592443943024\n",
      "Step 1701 | Loss: 0.24504336714744568\n",
      "Step 1801 | Loss: 0.19199542701244354\n",
      "Step 1901 | Loss: 0.23910097777843475\n",
      "Step 2001 | Loss: 0.2824401557445526\n",
      "Step 2101 | Loss: 0.2675975561141968\n",
      "Step 2201 | Loss: 0.19292762875556946\n",
      "Step 2301 | Loss: 0.17391787469387054\n",
      "Step 2401 | Loss: 0.24769063293933868\n",
      "0.25205778339056023 0.975\n",
      "Step 1 | Loss: 0.6521665453910828\n",
      "Step 101 | Loss: 0.43350616097450256\n",
      "Step 201 | Loss: 0.26932740211486816\n",
      "Step 301 | Loss: 0.3104150891304016\n",
      "Step 401 | Loss: 0.4116505980491638\n",
      "Step 501 | Loss: 0.2530674636363983\n",
      "Step 601 | Loss: 0.38175249099731445\n",
      "Step 701 | Loss: 0.3290892243385315\n",
      "Step 801 | Loss: 0.2074541598558426\n",
      "Step 901 | Loss: 0.47443491220474243\n",
      "Step 1001 | Loss: 0.22672702372074127\n",
      "Step 1101 | Loss: 0.28571417927742004\n",
      "Step 1201 | Loss: 0.31757915019989014\n",
      "Step 1301 | Loss: 0.32144203782081604\n",
      "Step 1401 | Loss: 0.4758017063140869\n",
      "Step 1501 | Loss: 0.3327324092388153\n",
      "Step 1601 | Loss: 0.2876322269439697\n",
      "Step 1701 | Loss: 0.3288343846797943\n",
      "Step 1801 | Loss: 0.2907586097717285\n",
      "Step 1901 | Loss: 0.3262771666049957\n",
      "Step 2001 | Loss: 0.22790518403053284\n",
      "Step 2101 | Loss: 0.2599644362926483\n",
      "Step 2201 | Loss: 0.3656902313232422\n",
      "Step 2301 | Loss: 0.24172091484069824\n",
      "Step 2401 | Loss: 0.17570142447948456\n",
      "0.296869187580274 0.9525\n",
      "Step 1 | Loss: 0.7464028596878052\n",
      "Step 101 | Loss: 0.573860764503479\n",
      "Step 201 | Loss: 0.4132190942764282\n",
      "Step 301 | Loss: 0.46673399209976196\n",
      "Step 401 | Loss: 0.6202026605606079\n",
      "Step 501 | Loss: 0.5312115550041199\n",
      "Step 601 | Loss: 0.5349573493003845\n",
      "Step 701 | Loss: 0.5582598447799683\n",
      "Step 801 | Loss: 0.3961874544620514\n",
      "Step 901 | Loss: 0.4670484662055969\n",
      "Step 1001 | Loss: 0.4500003457069397\n",
      "Step 1101 | Loss: 0.4111390709877014\n",
      "Step 1201 | Loss: 0.4468318223953247\n",
      "Step 1301 | Loss: 0.3855328857898712\n",
      "Step 1401 | Loss: 0.380523681640625\n",
      "Step 1501 | Loss: 0.3982202410697937\n",
      "Step 1601 | Loss: 0.4110872149467468\n",
      "Step 1701 | Loss: 0.5154467821121216\n",
      "Step 1801 | Loss: 0.46972891688346863\n",
      "Step 1901 | Loss: 0.40696683526039124\n",
      "Step 2001 | Loss: 0.350818008184433\n",
      "Step 2101 | Loss: 0.5830233097076416\n",
      "Step 2201 | Loss: 0.47768983244895935\n",
      "Step 2301 | Loss: 0.44385629892349243\n",
      "Step 2401 | Loss: 0.4112784266471863\n",
      "0.41398414284734575 0.925\n",
      "Step 1 | Loss: 2.155710220336914\n",
      "Step 101 | Loss: 2.3847463130950928\n",
      "Step 201 | Loss: 2.593001127243042\n",
      "Step 301 | Loss: 2.016649007797241\n",
      "Step 401 | Loss: 1.9174693822860718\n",
      "Step 501 | Loss: 2.3170785903930664\n",
      "Step 601 | Loss: 1.8556571006774902\n",
      "Step 701 | Loss: 2.0699996948242188\n",
      "Step 801 | Loss: 2.1028449535369873\n",
      "Step 901 | Loss: 1.7476565837860107\n",
      "Step 1001 | Loss: 2.0058958530426025\n",
      "Step 1101 | Loss: 2.320338010787964\n",
      "Step 1201 | Loss: 1.8315517902374268\n",
      "Step 1301 | Loss: 1.9980828762054443\n",
      "Step 1401 | Loss: 2.222771167755127\n",
      "Step 1501 | Loss: 1.7805633544921875\n",
      "Step 1601 | Loss: 2.033576488494873\n",
      "Step 1701 | Loss: 1.5880050659179688\n",
      "Step 1801 | Loss: 1.7662562131881714\n",
      "Step 1901 | Loss: 1.7118785381317139\n",
      "Step 2001 | Loss: 2.0929298400878906\n",
      "Step 2101 | Loss: 2.030111312866211\n",
      "Step 2201 | Loss: 2.015928030014038\n",
      "Step 2301 | Loss: 2.0398895740509033\n",
      "Step 2401 | Loss: 1.9667481184005737\n",
      "2.044191683634377 0.3675\n",
      "Step 1 | Loss: 1.8856134414672852\n",
      "Step 101 | Loss: 2.1632678508758545\n",
      "Step 201 | Loss: 1.9712069034576416\n",
      "Step 301 | Loss: 2.0386857986450195\n",
      "Step 401 | Loss: 2.3070168495178223\n",
      "Step 501 | Loss: 2.2302744388580322\n",
      "Step 601 | Loss: 2.117161273956299\n",
      "Step 701 | Loss: 2.196380615234375\n",
      "Step 801 | Loss: 1.6242475509643555\n",
      "Step 901 | Loss: 1.9508297443389893\n",
      "Step 1001 | Loss: 1.5865223407745361\n",
      "Step 1101 | Loss: 1.7922090291976929\n",
      "Step 1201 | Loss: 2.380026340484619\n",
      "Step 1301 | Loss: 2.110050678253174\n",
      "Step 1401 | Loss: 2.0902905464172363\n",
      "Step 1501 | Loss: 2.300220251083374\n",
      "Step 1601 | Loss: 1.977174997329712\n",
      "Step 1701 | Loss: 2.181121587753296\n",
      "Step 1801 | Loss: 1.9770758152008057\n",
      "Step 1901 | Loss: 1.7353538274765015\n",
      "Step 2001 | Loss: 2.132749319076538\n",
      "Step 2101 | Loss: 1.6977877616882324\n",
      "Step 2201 | Loss: 2.2711660861968994\n",
      "Step 2301 | Loss: 2.073436737060547\n",
      "Step 2401 | Loss: 1.755715012550354\n",
      "2.044191505027145 0.3675\n",
      "Step 1 | Loss: 1.8966336250305176\n",
      "Step 101 | Loss: 2.098484992980957\n",
      "Step 201 | Loss: 2.2472429275512695\n",
      "Step 301 | Loss: 1.8486390113830566\n",
      "Step 401 | Loss: 1.5667864084243774\n",
      "Step 501 | Loss: 2.235260248184204\n",
      "Step 601 | Loss: 2.067204713821411\n",
      "Step 701 | Loss: 1.9004918336868286\n",
      "Step 801 | Loss: 2.3209500312805176\n",
      "Step 901 | Loss: 2.0026800632476807\n",
      "Step 1001 | Loss: 1.910524606704712\n",
      "Step 1101 | Loss: 1.8277195692062378\n",
      "Step 1201 | Loss: 2.393714666366577\n",
      "Step 1301 | Loss: 2.1836957931518555\n",
      "Step 1401 | Loss: 2.0028297901153564\n",
      "Step 1501 | Loss: 2.0314414501190186\n",
      "Step 1601 | Loss: 1.7261474132537842\n",
      "Step 1701 | Loss: 1.748473048210144\n",
      "Step 1801 | Loss: 1.7149735689163208\n",
      "Step 1901 | Loss: 1.6504688262939453\n",
      "Step 2001 | Loss: 1.607985019683838\n",
      "Step 2101 | Loss: 1.8780242204666138\n",
      "Step 2201 | Loss: 2.045746088027954\n",
      "Step 2301 | Loss: 2.1505653858184814\n",
      "Step 2401 | Loss: 2.024076223373413\n",
      "2.044191730108273 0.3675\n",
      "Step 1 | Loss: 1.8421976566314697\n",
      "Step 101 | Loss: 2.064157724380493\n",
      "Step 201 | Loss: 2.168165683746338\n",
      "Step 301 | Loss: 1.595442533493042\n",
      "Step 401 | Loss: 1.838716745376587\n",
      "Step 501 | Loss: 1.9480116367340088\n",
      "Step 601 | Loss: 2.0704519748687744\n",
      "Step 701 | Loss: 2.164508581161499\n",
      "Step 801 | Loss: 2.160072088241577\n",
      "Step 901 | Loss: 2.323906421661377\n",
      "Step 1001 | Loss: 2.225374698638916\n",
      "Step 1101 | Loss: 2.0057079792022705\n",
      "Step 1201 | Loss: 1.8096210956573486\n",
      "Step 1301 | Loss: 2.2907161712646484\n",
      "Step 1401 | Loss: 2.231822967529297\n",
      "Step 1501 | Loss: 1.6334710121154785\n",
      "Step 1601 | Loss: 2.188570022583008\n",
      "Step 1701 | Loss: 2.4549429416656494\n",
      "Step 1801 | Loss: 2.164625644683838\n",
      "Step 1901 | Loss: 1.8664329051971436\n",
      "Step 2001 | Loss: 1.8175878524780273\n",
      "Step 2101 | Loss: 2.1145718097686768\n",
      "Step 2201 | Loss: 1.7971827983856201\n",
      "Step 2301 | Loss: 1.9654779434204102\n",
      "Step 2401 | Loss: 2.4511241912841797\n",
      "2.0441919313866164 0.3675\n",
      "Step 1 | Loss: 2.280061960220337\n",
      "Step 101 | Loss: 1.7775534391403198\n",
      "Step 201 | Loss: 2.113236904144287\n",
      "Step 301 | Loss: 2.093869209289551\n",
      "Step 401 | Loss: 2.3316879272460938\n",
      "Step 501 | Loss: 1.7513096332550049\n",
      "Step 601 | Loss: 2.404867649078369\n",
      "Step 701 | Loss: 1.9339779615402222\n",
      "Step 801 | Loss: 1.5387704372406006\n",
      "Step 901 | Loss: 2.398212194442749\n",
      "Step 1001 | Loss: 1.6137086153030396\n",
      "Step 1101 | Loss: 2.1425089836120605\n",
      "Step 1201 | Loss: 2.2480990886688232\n",
      "Step 1301 | Loss: 2.2883169651031494\n",
      "Step 1401 | Loss: 2.0733110904693604\n",
      "Step 1501 | Loss: 1.8773796558380127\n",
      "Step 1601 | Loss: 1.7366541624069214\n",
      "Step 1701 | Loss: 1.856335163116455\n",
      "Step 1801 | Loss: 1.7105076313018799\n",
      "Step 1901 | Loss: 2.1590638160705566\n",
      "Step 2001 | Loss: 2.2353475093841553\n",
      "Step 2101 | Loss: 2.2860629558563232\n",
      "Step 2201 | Loss: 2.375965118408203\n",
      "Step 2301 | Loss: 2.088066339492798\n",
      "Step 2401 | Loss: 1.6060566902160645\n",
      "2.0441918911855006 0.3675\n",
      "Step 1 | Loss: 1.1492230892181396\n",
      "Step 101 | Loss: 1.0556068420410156\n",
      "Step 201 | Loss: 0.8729790449142456\n",
      "Step 301 | Loss: 0.5132185220718384\n",
      "Step 401 | Loss: 0.400471568107605\n",
      "Step 501 | Loss: 0.476807564496994\n",
      "Step 601 | Loss: 0.5121795535087585\n",
      "Step 701 | Loss: 0.25231608748435974\n",
      "Step 801 | Loss: 0.38467830419540405\n",
      "Step 901 | Loss: 0.33502647280693054\n",
      "Step 1001 | Loss: 0.42084941267967224\n",
      "Step 1101 | Loss: 0.557305634021759\n",
      "Step 1201 | Loss: 0.4841984212398529\n",
      "Step 1301 | Loss: 0.43591028451919556\n",
      "Step 1401 | Loss: 0.33598610758781433\n",
      "Step 1501 | Loss: 0.40494173765182495\n",
      "Step 1601 | Loss: 0.3330276608467102\n",
      "Step 1701 | Loss: 0.41697782278060913\n",
      "Step 1801 | Loss: 0.5480256676673889\n",
      "Step 1901 | Loss: 0.42202410101890564\n",
      "Step 2001 | Loss: 0.1991802155971527\n",
      "Step 2101 | Loss: 0.2323078066110611\n",
      "Step 2201 | Loss: 0.37400656938552856\n",
      "Step 2301 | Loss: 0.18477611243724823\n",
      "Step 2401 | Loss: 0.3380982279777527\n",
      "0.39085872426119955 0.8625\n",
      "Step 1 | Loss: 1.3985304832458496\n",
      "Step 101 | Loss: 0.44824838638305664\n",
      "Step 201 | Loss: 0.4836549758911133\n",
      "Step 301 | Loss: 0.4977041482925415\n",
      "Step 401 | Loss: 0.188792422413826\n",
      "Step 501 | Loss: 0.3018123209476471\n",
      "Step 601 | Loss: 0.3711787760257721\n",
      "Step 701 | Loss: 0.4411809742450714\n",
      "Step 801 | Loss: 0.4407583475112915\n",
      "Step 901 | Loss: 0.31274905800819397\n",
      "Step 1001 | Loss: 0.43952322006225586\n",
      "Step 1101 | Loss: 0.4302501380443573\n",
      "Step 1201 | Loss: 0.2931827902793884\n",
      "Step 1301 | Loss: 0.5906415581703186\n",
      "Step 1401 | Loss: 0.46942445635795593\n",
      "Step 1501 | Loss: 0.47306445240974426\n",
      "Step 1601 | Loss: 0.5480383634567261\n",
      "Step 1701 | Loss: 0.5303071141242981\n",
      "Step 1801 | Loss: 0.29016783833503723\n",
      "Step 1901 | Loss: 0.3244239389896393\n",
      "Step 2001 | Loss: 0.1561213731765747\n",
      "Step 2101 | Loss: 0.3751014769077301\n",
      "Step 2201 | Loss: 0.5439803600311279\n",
      "Step 2301 | Loss: 0.16733326017856598\n",
      "Step 2401 | Loss: 0.5110167264938354\n",
      "0.42537508946204056 0.8675\n",
      "Step 1 | Loss: 0.9796044826507568\n",
      "Step 101 | Loss: 0.8547453880310059\n",
      "Step 201 | Loss: 0.49655526876449585\n",
      "Step 301 | Loss: 0.4635370373725891\n",
      "Step 401 | Loss: 0.42643246054649353\n",
      "Step 501 | Loss: 0.4131900370121002\n",
      "Step 601 | Loss: 0.3434728980064392\n",
      "Step 701 | Loss: 0.3037831783294678\n",
      "Step 801 | Loss: 0.20703795552253723\n",
      "Step 901 | Loss: 0.3057876527309418\n",
      "Step 1001 | Loss: 0.3826292157173157\n",
      "Step 1101 | Loss: 0.3486189544200897\n",
      "Step 1201 | Loss: 0.3470517098903656\n",
      "Step 1301 | Loss: 0.5028994083404541\n",
      "Step 1401 | Loss: 0.3506217300891876\n",
      "Step 1501 | Loss: 0.2975950539112091\n",
      "Step 1601 | Loss: 0.365120530128479\n",
      "Step 1701 | Loss: 0.37231889367103577\n",
      "Step 1801 | Loss: 0.24994635581970215\n",
      "Step 1901 | Loss: 0.28071853518486023\n",
      "Step 2001 | Loss: 0.314780592918396\n",
      "Step 2101 | Loss: 0.30649328231811523\n",
      "Step 2201 | Loss: 0.36245301365852356\n",
      "Step 2301 | Loss: 0.4817197918891907\n",
      "Step 2401 | Loss: 0.42923372983932495\n",
      "0.31884266920092047 0.895\n",
      "Step 1 | Loss: 1.1752021312713623\n",
      "Step 101 | Loss: 0.7213740348815918\n",
      "Step 201 | Loss: 0.5192400813102722\n",
      "Step 301 | Loss: 0.5257999300956726\n",
      "Step 401 | Loss: 0.374703586101532\n",
      "Step 501 | Loss: 0.29594704508781433\n",
      "Step 601 | Loss: 0.406918466091156\n",
      "Step 701 | Loss: 0.5361834168434143\n",
      "Step 801 | Loss: 0.5424835085868835\n",
      "Step 901 | Loss: 0.33505457639694214\n",
      "Step 1001 | Loss: 0.5151989459991455\n",
      "Step 1101 | Loss: 0.3083501160144806\n",
      "Step 1201 | Loss: 0.3556922674179077\n",
      "Step 1301 | Loss: 0.41439327597618103\n",
      "Step 1401 | Loss: 0.40671613812446594\n",
      "Step 1501 | Loss: 0.28632545471191406\n",
      "Step 1601 | Loss: 0.3318333327770233\n",
      "Step 1701 | Loss: 0.38342931866645813\n",
      "Step 1801 | Loss: 0.2707144021987915\n",
      "Step 1901 | Loss: 0.12447188049554825\n",
      "Step 2001 | Loss: 0.3009284436702728\n",
      "Step 2101 | Loss: 0.19742703437805176\n",
      "Step 2201 | Loss: 0.20536676049232483\n",
      "Step 2301 | Loss: 0.2814815640449524\n",
      "Step 2401 | Loss: 0.4480370283126831\n",
      "0.3017746721804504 0.895\n",
      "Step 1 | Loss: 1.2994883060455322\n",
      "Step 101 | Loss: 0.7730695009231567\n",
      "Step 201 | Loss: 0.6985723376274109\n",
      "Step 301 | Loss: 0.5535308718681335\n",
      "Step 401 | Loss: 0.636253297328949\n",
      "Step 501 | Loss: 0.4993491470813751\n",
      "Step 601 | Loss: 0.47887060046195984\n",
      "Step 701 | Loss: 0.3456229865550995\n",
      "Step 801 | Loss: 0.29981788992881775\n",
      "Step 901 | Loss: 0.2969641089439392\n",
      "Step 1001 | Loss: 0.20211103558540344\n",
      "Step 1101 | Loss: 0.3436151146888733\n",
      "Step 1201 | Loss: 0.3328254222869873\n",
      "Step 1301 | Loss: 0.4338321387767792\n",
      "Step 1401 | Loss: 0.35266122221946716\n",
      "Step 1501 | Loss: 0.1973143368959427\n",
      "Step 1601 | Loss: 0.39029935002326965\n",
      "Step 1701 | Loss: 0.2948070168495178\n",
      "Step 1801 | Loss: 0.21810080111026764\n",
      "Step 1901 | Loss: 0.2035336047410965\n",
      "Step 2001 | Loss: 0.29862362146377563\n",
      "Step 2101 | Loss: 0.26129597425460815\n",
      "Step 2201 | Loss: 0.38187700510025024\n",
      "Step 2301 | Loss: 0.5185905694961548\n",
      "Step 2401 | Loss: 0.24750716984272003\n",
      "0.3204769219091467 0.885\n",
      "Step 1 | Loss: 0.7351484894752502\n",
      "Step 101 | Loss: 0.41925692558288574\n",
      "Step 201 | Loss: 0.5044582486152649\n",
      "Step 301 | Loss: 0.3479715585708618\n",
      "Step 401 | Loss: 0.28897371888160706\n",
      "Step 501 | Loss: 0.3399215340614319\n",
      "Step 601 | Loss: 0.3644595742225647\n",
      "Step 701 | Loss: 0.25951918959617615\n",
      "Step 801 | Loss: 0.29469871520996094\n",
      "Step 901 | Loss: 0.3619726896286011\n",
      "Step 1001 | Loss: 0.4322749376296997\n",
      "Step 1101 | Loss: 0.38920342922210693\n",
      "Step 1201 | Loss: 0.32874882221221924\n",
      "Step 1301 | Loss: 0.31559082865715027\n",
      "Step 1401 | Loss: 0.254868745803833\n",
      "Step 1501 | Loss: 0.2554319202899933\n",
      "Step 1601 | Loss: 0.4902913570404053\n",
      "Step 1701 | Loss: 0.42733830213546753\n",
      "Step 1801 | Loss: 0.34068208932876587\n",
      "Step 1901 | Loss: 0.3702147305011749\n",
      "Step 2001 | Loss: 0.4372711479663849\n",
      "Step 2101 | Loss: 0.39000990986824036\n",
      "Step 2201 | Loss: 0.3108682930469513\n",
      "Step 2301 | Loss: 0.2128187119960785\n",
      "Step 2401 | Loss: 0.34362491965293884\n",
      "0.3127149220925189 0.945\n",
      "Step 1 | Loss: 1.7658861875534058\n",
      "Step 101 | Loss: 0.43466073274612427\n",
      "Step 201 | Loss: 0.4893523156642914\n",
      "Step 301 | Loss: 0.24236777424812317\n",
      "Step 401 | Loss: 0.35872069001197815\n",
      "Step 501 | Loss: 0.3975052833557129\n",
      "Step 601 | Loss: 0.38011234998703003\n",
      "Step 701 | Loss: 0.323025107383728\n",
      "Step 801 | Loss: 0.3050844967365265\n",
      "Step 901 | Loss: 0.40529099106788635\n",
      "Step 1001 | Loss: 0.2807013988494873\n",
      "Step 1101 | Loss: 0.316547691822052\n",
      "Step 1201 | Loss: 0.34576427936553955\n",
      "Step 1301 | Loss: 0.37979716062545776\n",
      "Step 1401 | Loss: 0.43549686670303345\n",
      "Step 1501 | Loss: 0.39284569025039673\n",
      "Step 1601 | Loss: 0.32630839943885803\n",
      "Step 1701 | Loss: 0.3371398448944092\n",
      "Step 1801 | Loss: 0.23592659831047058\n",
      "Step 1901 | Loss: 0.37201404571533203\n",
      "Step 2001 | Loss: 0.36952266097068787\n",
      "Step 2101 | Loss: 0.22915597259998322\n",
      "Step 2201 | Loss: 0.2473212033510208\n",
      "Step 2301 | Loss: 0.2639634907245636\n",
      "Step 2401 | Loss: 0.3126929700374603\n",
      "0.30310624306753065 0.9575\n",
      "Step 1 | Loss: 0.770227313041687\n",
      "Step 101 | Loss: 0.24953557550907135\n",
      "Step 201 | Loss: 0.4460693895816803\n",
      "Step 301 | Loss: 0.4033750295639038\n",
      "Step 401 | Loss: 0.29478931427001953\n",
      "Step 501 | Loss: 0.3718045949935913\n",
      "Step 601 | Loss: 0.20129355788230896\n",
      "Step 701 | Loss: 0.3493240475654602\n",
      "Step 801 | Loss: 0.3777958154678345\n",
      "Step 901 | Loss: 0.3343144357204437\n",
      "Step 1001 | Loss: 0.43567955493927\n",
      "Step 1101 | Loss: 0.4524688720703125\n",
      "Step 1201 | Loss: 0.2938401699066162\n",
      "Step 1301 | Loss: 0.33177995681762695\n",
      "Step 1401 | Loss: 0.34532493352890015\n",
      "Step 1501 | Loss: 0.3422706723213196\n",
      "Step 1601 | Loss: 0.33459174633026123\n",
      "Step 1701 | Loss: 0.3311430513858795\n",
      "Step 1801 | Loss: 0.5448145270347595\n",
      "Step 1901 | Loss: 0.3829483389854431\n",
      "Step 2001 | Loss: 0.5414469838142395\n",
      "Step 2101 | Loss: 0.35259002447128296\n",
      "Step 2201 | Loss: 0.2585133910179138\n",
      "Step 2301 | Loss: 0.3935609757900238\n",
      "Step 2401 | Loss: 0.3238270580768585\n",
      "0.30949554184564476 0.94\n",
      "Step 1 | Loss: 1.1124463081359863\n",
      "Step 101 | Loss: 0.7558364868164062\n",
      "Step 201 | Loss: 0.625446081161499\n",
      "Step 301 | Loss: 0.6070197820663452\n",
      "Step 401 | Loss: 0.632409393787384\n",
      "Step 501 | Loss: 0.5992311239242554\n",
      "Step 601 | Loss: 0.32947105169296265\n",
      "Step 701 | Loss: 0.27326059341430664\n",
      "Step 801 | Loss: 0.3558443486690521\n",
      "Step 901 | Loss: 0.33873480558395386\n",
      "Step 1001 | Loss: 0.398183137178421\n",
      "Step 1101 | Loss: 0.3407145142555237\n",
      "Step 1201 | Loss: 0.36972761154174805\n",
      "Step 1301 | Loss: 0.26305708289146423\n",
      "Step 1401 | Loss: 0.3612704873085022\n",
      "Step 1501 | Loss: 0.2852536141872406\n",
      "Step 1601 | Loss: 0.43957802653312683\n",
      "Step 1701 | Loss: 0.35744768381118774\n",
      "Step 1801 | Loss: 0.37942859530448914\n",
      "Step 1901 | Loss: 0.4013901352882385\n",
      "Step 2001 | Loss: 0.44049307703971863\n",
      "Step 2101 | Loss: 0.39047494530677795\n",
      "Step 2201 | Loss: 0.23550742864608765\n",
      "Step 2301 | Loss: 0.38890257477760315\n",
      "Step 2401 | Loss: 0.2695785164833069\n",
      "0.3046900961487245 0.95\n",
      "Step 1 | Loss: 0.7348246574401855\n",
      "Step 101 | Loss: 0.5943918228149414\n",
      "Step 201 | Loss: 0.598829448223114\n",
      "Step 301 | Loss: 0.4674852192401886\n",
      "Step 401 | Loss: 0.36398372054100037\n",
      "Step 501 | Loss: 0.33629274368286133\n",
      "Step 601 | Loss: 0.2971920669078827\n",
      "Step 701 | Loss: 0.2958427965641022\n",
      "Step 801 | Loss: 0.2964268922805786\n",
      "Step 901 | Loss: 0.38094592094421387\n",
      "Step 1001 | Loss: 0.2667912244796753\n",
      "Step 1101 | Loss: 0.43949806690216064\n",
      "Step 1201 | Loss: 0.30054205656051636\n",
      "Step 1301 | Loss: 0.20648115873336792\n",
      "Step 1401 | Loss: 0.27559271454811096\n",
      "Step 1501 | Loss: 0.2962222695350647\n",
      "Step 1601 | Loss: 0.34611451625823975\n",
      "Step 1701 | Loss: 0.34061986207962036\n",
      "Step 1801 | Loss: 0.3471733331680298\n",
      "Step 1901 | Loss: 0.5185301899909973\n",
      "Step 2001 | Loss: 0.41394591331481934\n",
      "Step 2101 | Loss: 0.3971567153930664\n",
      "Step 2201 | Loss: 0.4594174921512604\n",
      "Step 2301 | Loss: 0.31249484419822693\n",
      "Step 2401 | Loss: 0.44484269618988037\n",
      "0.3017283940009731 0.94\n",
      "Step 1 | Loss: 1.3889154195785522\n",
      "Step 101 | Loss: 0.9902029037475586\n",
      "Step 201 | Loss: 0.8057944774627686\n",
      "Step 301 | Loss: 0.6773503422737122\n",
      "Step 401 | Loss: 0.5722503066062927\n",
      "Step 501 | Loss: 0.664519190788269\n",
      "Step 601 | Loss: 0.4350980222225189\n",
      "Step 701 | Loss: 0.540458619594574\n",
      "Step 801 | Loss: 0.4455253481864929\n",
      "Step 901 | Loss: 0.5664604902267456\n",
      "Step 1001 | Loss: 0.45510774850845337\n",
      "Step 1101 | Loss: 0.6420527100563049\n",
      "Step 1201 | Loss: 0.6303849220275879\n",
      "Step 1301 | Loss: 0.6906405687332153\n",
      "Step 1401 | Loss: 0.41590023040771484\n",
      "Step 1501 | Loss: 0.4967196583747864\n",
      "Step 1601 | Loss: 0.5165826678276062\n",
      "Step 1701 | Loss: 0.5877550840377808\n",
      "Step 1801 | Loss: 0.5698443651199341\n",
      "Step 1901 | Loss: 0.5860711336135864\n",
      "Step 2001 | Loss: 0.5786949396133423\n",
      "Step 2101 | Loss: 0.4565894603729248\n",
      "Step 2201 | Loss: 0.5131755471229553\n",
      "Step 2301 | Loss: 0.4887925088405609\n",
      "Step 2401 | Loss: 0.49159151315689087\n",
      "0.5124327271522297 0.8275\n",
      "Step 1 | Loss: 1.3413641452789307\n",
      "Step 101 | Loss: 0.6003850698471069\n",
      "Step 201 | Loss: 0.3712938129901886\n",
      "Step 301 | Loss: 0.5539069771766663\n",
      "Step 401 | Loss: 0.5902772545814514\n",
      "Step 501 | Loss: 0.4207063615322113\n",
      "Step 601 | Loss: 0.35804617404937744\n",
      "Step 701 | Loss: 0.5150505900382996\n",
      "Step 801 | Loss: 0.5066395998001099\n",
      "Step 901 | Loss: 0.3805495500564575\n",
      "Step 1001 | Loss: 0.5471193790435791\n",
      "Step 1101 | Loss: 0.6207960844039917\n",
      "Step 1201 | Loss: 0.48247212171554565\n",
      "Step 1301 | Loss: 0.6707113981246948\n",
      "Step 1401 | Loss: 0.535254716873169\n",
      "Step 1501 | Loss: 0.5454326868057251\n",
      "Step 1601 | Loss: 0.4293690323829651\n",
      "Step 1701 | Loss: 0.5122296214103699\n",
      "Step 1801 | Loss: 0.5033459067344666\n",
      "Step 1901 | Loss: 0.4925861656665802\n",
      "Step 2001 | Loss: 0.6202412247657776\n",
      "Step 2101 | Loss: 0.47760364413261414\n",
      "Step 2201 | Loss: 0.5115712881088257\n",
      "Step 2301 | Loss: 0.5891754031181335\n",
      "Step 2401 | Loss: 0.3976554870605469\n",
      "0.5047296079562061 0.835\n",
      "Step 1 | Loss: 1.5509213209152222\n",
      "Step 101 | Loss: 0.8980618119239807\n",
      "Step 201 | Loss: 0.703830897808075\n",
      "Step 301 | Loss: 0.8787298798561096\n",
      "Step 401 | Loss: 0.8832348585128784\n",
      "Step 501 | Loss: 0.7914953827857971\n",
      "Step 601 | Loss: 0.7207838296890259\n",
      "Step 701 | Loss: 0.7920505404472351\n",
      "Step 801 | Loss: 0.7697342038154602\n",
      "Step 901 | Loss: 0.8495097160339355\n",
      "Step 1001 | Loss: 0.8126225471496582\n",
      "Step 1101 | Loss: 0.8266886472702026\n",
      "Step 1201 | Loss: 0.8319822549819946\n",
      "Step 1301 | Loss: 0.6627633571624756\n",
      "Step 1401 | Loss: 0.8349149227142334\n",
      "Step 1501 | Loss: 0.9208925366401672\n",
      "Step 1601 | Loss: 0.793146550655365\n",
      "Step 1701 | Loss: 0.8128470778465271\n",
      "Step 1801 | Loss: 0.8374634385108948\n",
      "Step 1901 | Loss: 0.8745071291923523\n",
      "Step 2001 | Loss: 0.8214878439903259\n",
      "Step 2101 | Loss: 0.7797509431838989\n",
      "Step 2201 | Loss: 0.8388827443122864\n",
      "Step 2301 | Loss: 0.7955235838890076\n",
      "Step 2401 | Loss: 0.9426954984664917\n",
      "0.7866372733930901 0.7525\n",
      "Step 1 | Loss: 1.163077712059021\n",
      "Step 101 | Loss: 0.9537728428840637\n",
      "Step 201 | Loss: 0.8553733229637146\n",
      "Step 301 | Loss: 0.7886231541633606\n",
      "Step 401 | Loss: 0.8171634674072266\n",
      "Step 501 | Loss: 0.7730834484100342\n",
      "Step 601 | Loss: 0.8878232836723328\n",
      "Step 701 | Loss: 0.7964292764663696\n",
      "Step 801 | Loss: 0.8759579062461853\n",
      "Step 901 | Loss: 0.6938245296478271\n",
      "Step 1001 | Loss: 0.8529230356216431\n",
      "Step 1101 | Loss: 0.8662920594215393\n",
      "Step 1201 | Loss: 0.8830044865608215\n",
      "Step 1301 | Loss: 0.7480796575546265\n",
      "Step 1401 | Loss: 0.7869381308555603\n",
      "Step 1501 | Loss: 0.7721726298332214\n",
      "Step 1601 | Loss: 0.7618645429611206\n",
      "Step 1701 | Loss: 0.7943595051765442\n",
      "Step 1801 | Loss: 0.8539280891418457\n",
      "Step 1901 | Loss: 0.7967826724052429\n",
      "Step 2001 | Loss: 0.7695195078849792\n",
      "Step 2101 | Loss: 0.8764113783836365\n",
      "Step 2201 | Loss: 0.8030319809913635\n",
      "Step 2301 | Loss: 0.8672506809234619\n",
      "Step 2401 | Loss: 0.7276296019554138\n",
      "0.8174309036440224 0.73\n",
      "Step 1 | Loss: 1.3768478631973267\n",
      "Step 101 | Loss: 0.8148479461669922\n",
      "Step 201 | Loss: 0.5009281635284424\n",
      "Step 301 | Loss: 0.575347900390625\n",
      "Step 401 | Loss: 0.41056498885154724\n",
      "Step 501 | Loss: 0.46792072057724\n",
      "Step 601 | Loss: 0.36031216382980347\n",
      "Step 701 | Loss: 0.46411171555519104\n",
      "Step 801 | Loss: 0.456877201795578\n",
      "Step 901 | Loss: 0.5788375735282898\n",
      "Step 1001 | Loss: 0.4746806025505066\n",
      "Step 1101 | Loss: 0.6161797046661377\n",
      "Step 1201 | Loss: 0.5354620814323425\n",
      "Step 1301 | Loss: 0.6632247567176819\n",
      "Step 1401 | Loss: 0.37066471576690674\n",
      "Step 1501 | Loss: 0.4897770881652832\n",
      "Step 1601 | Loss: 0.6090739965438843\n",
      "Step 1701 | Loss: 0.4298872947692871\n",
      "Step 1801 | Loss: 0.4631781280040741\n",
      "Step 1901 | Loss: 0.44790422916412354\n",
      "Step 2001 | Loss: 0.5834708213806152\n",
      "Step 2101 | Loss: 0.5914281606674194\n",
      "Step 2201 | Loss: 0.4869431257247925\n",
      "Step 2301 | Loss: 0.551215410232544\n",
      "Step 2401 | Loss: 0.6714540719985962\n",
      "0.5025814774774998 0.8325\n",
      "Step 1 | Loss: 1.624999761581421\n",
      "Step 101 | Loss: 2.124999761581421\n",
      "Step 201 | Loss: 1.749999761581421\n",
      "Step 301 | Loss: 2.374999523162842\n",
      "Step 401 | Loss: 1.749999761581421\n",
      "Step 501 | Loss: 1.999999761581421\n",
      "Step 601 | Loss: 1.8749996423721313\n",
      "Step 701 | Loss: 2.124999761581421\n",
      "Step 801 | Loss: 2.874999761581421\n",
      "Step 901 | Loss: 2.124999523162842\n",
      "Step 1001 | Loss: 2.249999761581421\n",
      "Step 1101 | Loss: 1.499999761581421\n",
      "Step 1201 | Loss: 2.499999761581421\n",
      "Step 1301 | Loss: 2.249999523162842\n",
      "Step 1401 | Loss: 1.624999761581421\n",
      "Step 1501 | Loss: 1.8749996423721313\n",
      "Step 1601 | Loss: 2.249999523162842\n",
      "Step 1701 | Loss: 1.999999761581421\n",
      "Step 1801 | Loss: 1.499999761581421\n",
      "Step 1901 | Loss: 2.499999523162842\n",
      "Step 2001 | Loss: 1.6249995231628418\n",
      "Step 2101 | Loss: 2.499999523162842\n",
      "Step 2201 | Loss: 2.249999761581421\n",
      "Step 2301 | Loss: 1.999999761581421\n",
      "Step 2401 | Loss: 2.124999761581421\n",
      "1.9711534570043396 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.375\n",
      "Step 401 | Loss: 2.125\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 2.25\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 2.25\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 1.625\n",
      "Step 1201 | Loss: 2.25\n",
      "Step 1301 | Loss: 2.375\n",
      "Step 1401 | Loss: 1.4999998807907104\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 1.625\n",
      "Step 1701 | Loss: 2.375\n",
      "Step 1801 | Loss: 2.25\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 1.375\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 1.625\n",
      "Step 2301 | Loss: 2.375\n",
      "Step 2401 | Loss: 2.0\n",
      "1.9711537939997896 0.5\n",
      "Step 1 | Loss: 1.875\n",
      "Step 101 | Loss: 2.25\n",
      "Step 201 | Loss: 1.75\n",
      "Step 301 | Loss: 1.75\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.875\n",
      "Step 601 | Loss: 0.8749999403953552\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 1.25\n",
      "Step 1001 | Loss: 2.875\n",
      "Step 1101 | Loss: 1.875\n",
      "Step 1201 | Loss: 1.625\n",
      "Step 1301 | Loss: 1.125\n",
      "Step 1401 | Loss: 1.875\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 1.625\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 1.875\n",
      "Step 2001 | Loss: 2.125\n",
      "Step 2101 | Loss: 2.25\n",
      "Step 2201 | Loss: 2.125\n",
      "Step 2301 | Loss: 1.875\n",
      "Step 2401 | Loss: 2.125\n",
      "1.9711539980310806 0.5\n",
      "Step 1 | Loss: 2.249999761581421\n",
      "Step 101 | Loss: 1.749999761581421\n",
      "Step 201 | Loss: 1.75\n",
      "Step 301 | Loss: 2.249999761581421\n",
      "Step 401 | Loss: 1.9999998807907104\n",
      "Step 501 | Loss: 1.125\n",
      "Step 601 | Loss: 1.6249998807907104\n",
      "Step 701 | Loss: 1.875\n",
      "Step 801 | Loss: 2.249999761581421\n",
      "Step 901 | Loss: 1.5\n",
      "Step 1001 | Loss: 1.8749998807907104\n",
      "Step 1101 | Loss: 2.125\n",
      "Step 1201 | Loss: 2.249999761581421\n",
      "Step 1301 | Loss: 2.25\n",
      "Step 1401 | Loss: 1.8749998807907104\n",
      "Step 1501 | Loss: 2.124999761581421\n",
      "Step 1601 | Loss: 2.125\n",
      "Step 1701 | Loss: 1.9999998807907104\n",
      "Step 1801 | Loss: 1.7499998807907104\n",
      "Step 1901 | Loss: 1.5\n",
      "Step 2001 | Loss: 2.749999761581421\n",
      "Step 2101 | Loss: 1.9999996423721313\n",
      "Step 2201 | Loss: 2.124999761581421\n",
      "Step 2301 | Loss: 2.249999761581421\n",
      "Step 2401 | Loss: 2.625\n",
      "1.9711536736442645 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.875\n",
      "Step 201 | Loss: 1.875\n",
      "Step 301 | Loss: 2.375\n",
      "Step 401 | Loss: 2.5\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 1.75\n",
      "Step 701 | Loss: 1.875\n",
      "Step 801 | Loss: 2.0\n",
      "Step 901 | Loss: 1.75\n",
      "Step 1001 | Loss: 1.625\n",
      "Step 1101 | Loss: 2.375\n",
      "Step 1201 | Loss: 2.375\n",
      "Step 1301 | Loss: 2.125\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 1.875\n",
      "Step 1601 | Loss: 1.875\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 1.625\n",
      "Step 1901 | Loss: 2.625\n",
      "Step 2001 | Loss: 1.75\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 2.625\n",
      "Step 2301 | Loss: 1.75\n",
      "Step 2401 | Loss: 1.5\n",
      "1.9711538255214753 0.5\n",
      "Step 1 | Loss: 1.0780763626098633\n",
      "Step 101 | Loss: 1.0067784786224365\n",
      "Step 201 | Loss: 0.617112398147583\n",
      "Step 301 | Loss: 0.696040689945221\n",
      "Step 401 | Loss: 0.5707624554634094\n",
      "Step 501 | Loss: 0.4590224027633667\n",
      "Step 601 | Loss: 0.5146547555923462\n",
      "Step 701 | Loss: 0.5774791240692139\n",
      "Step 801 | Loss: 0.5688464045524597\n",
      "Step 901 | Loss: 0.5193998217582703\n",
      "Step 1001 | Loss: 1.0275318622589111\n",
      "Step 1101 | Loss: 0.8287717700004578\n",
      "Step 1201 | Loss: 0.8812015652656555\n",
      "Step 1301 | Loss: 0.6064823865890503\n",
      "Step 1401 | Loss: 0.7791633605957031\n",
      "Step 1501 | Loss: 0.6840805411338806\n",
      "Step 1601 | Loss: 0.4737028479576111\n",
      "Step 1701 | Loss: 0.8053736686706543\n",
      "Step 1801 | Loss: 0.6408547759056091\n",
      "Step 1901 | Loss: 0.6627933382987976\n",
      "Step 2001 | Loss: 0.7212073802947998\n",
      "Step 2101 | Loss: 0.6947895288467407\n",
      "Step 2201 | Loss: 0.6098694205284119\n",
      "Step 2301 | Loss: 0.5557329058647156\n",
      "Step 2401 | Loss: 0.7380167841911316\n",
      "0.6273874735264773 0.795\n",
      "Step 1 | Loss: 1.175012469291687\n",
      "Step 101 | Loss: 1.0025155544281006\n",
      "Step 201 | Loss: 0.8012861609458923\n",
      "Step 301 | Loss: 0.4252149164676666\n",
      "Step 401 | Loss: 0.6783881187438965\n",
      "Step 501 | Loss: 0.8222010731697083\n",
      "Step 601 | Loss: 0.5612514019012451\n",
      "Step 701 | Loss: 0.5690342783927917\n",
      "Step 801 | Loss: 0.892627477645874\n",
      "Step 901 | Loss: 0.6265965104103088\n",
      "Step 1001 | Loss: 0.6239053010940552\n",
      "Step 1101 | Loss: 0.7541651129722595\n",
      "Step 1201 | Loss: 0.655785083770752\n",
      "Step 1301 | Loss: 0.6944292783737183\n",
      "Step 1401 | Loss: 0.6191401481628418\n",
      "Step 1501 | Loss: 0.5257810950279236\n",
      "Step 1601 | Loss: 0.622718334197998\n",
      "Step 1701 | Loss: 0.5936395525932312\n",
      "Step 1801 | Loss: 0.8033158779144287\n",
      "Step 1901 | Loss: 0.5447072982788086\n",
      "Step 2001 | Loss: 0.4636417329311371\n",
      "Step 2101 | Loss: 0.552374541759491\n",
      "Step 2201 | Loss: 0.6008902788162231\n",
      "Step 2301 | Loss: 0.624427318572998\n",
      "Step 2401 | Loss: 0.5626181364059448\n",
      "0.6301540842644955 0.7975\n",
      "Step 1 | Loss: 1.1942920684814453\n",
      "Step 101 | Loss: 0.8206157684326172\n",
      "Step 201 | Loss: 0.7301498055458069\n",
      "Step 301 | Loss: 0.8364561796188354\n",
      "Step 401 | Loss: 0.6640339493751526\n",
      "Step 501 | Loss: 0.8559351563453674\n",
      "Step 601 | Loss: 0.5720059275627136\n",
      "Step 701 | Loss: 0.547745943069458\n",
      "Step 801 | Loss: 0.59987473487854\n",
      "Step 901 | Loss: 0.5615662932395935\n",
      "Step 1001 | Loss: 0.4813304543495178\n",
      "Step 1101 | Loss: 0.6976308822631836\n",
      "Step 1201 | Loss: 0.8252100348472595\n",
      "Step 1301 | Loss: 0.6679036021232605\n",
      "Step 1401 | Loss: 0.3966040015220642\n",
      "Step 1501 | Loss: 0.5455037355422974\n",
      "Step 1601 | Loss: 0.8574440479278564\n",
      "Step 1701 | Loss: 0.5695194005966187\n",
      "Step 1801 | Loss: 0.6408071517944336\n",
      "Step 1901 | Loss: 0.6096019744873047\n",
      "Step 2001 | Loss: 0.6226701736450195\n",
      "Step 2101 | Loss: 0.6247906684875488\n",
      "Step 2201 | Loss: 0.6106118559837341\n",
      "Step 2301 | Loss: 0.777333676815033\n",
      "Step 2401 | Loss: 0.7203937768936157\n",
      "0.616242644419664 0.8075\n",
      "Step 1 | Loss: 0.9253526329994202\n",
      "Step 101 | Loss: 0.5632430911064148\n",
      "Step 201 | Loss: 0.6070234179496765\n",
      "Step 301 | Loss: 0.7895437479019165\n",
      "Step 401 | Loss: 0.5310289263725281\n",
      "Step 501 | Loss: 0.8648548722267151\n",
      "Step 601 | Loss: 0.7169002890586853\n",
      "Step 701 | Loss: 0.5946928858757019\n",
      "Step 801 | Loss: 0.759739339351654\n",
      "Step 901 | Loss: 0.6777721643447876\n",
      "Step 1001 | Loss: 0.7344068884849548\n",
      "Step 1101 | Loss: 0.7399190664291382\n",
      "Step 1201 | Loss: 0.7561804056167603\n",
      "Step 1301 | Loss: 0.7252373695373535\n",
      "Step 1401 | Loss: 0.40900713205337524\n",
      "Step 1501 | Loss: 0.5412988662719727\n",
      "Step 1601 | Loss: 1.1068224906921387\n",
      "Step 1701 | Loss: 0.6754234433174133\n",
      "Step 1801 | Loss: 0.44727638363838196\n",
      "Step 1901 | Loss: 0.8453534245491028\n",
      "Step 2001 | Loss: 0.42015179991722107\n",
      "Step 2101 | Loss: 0.6223419904708862\n",
      "Step 2201 | Loss: 0.6390915513038635\n",
      "Step 2301 | Loss: 0.4491036832332611\n",
      "Step 2401 | Loss: 0.6211289763450623\n",
      "0.6218680011400016 0.795\n",
      "Step 1 | Loss: 1.0674912929534912\n",
      "Step 101 | Loss: 0.9114066362380981\n",
      "Step 201 | Loss: 0.8278772830963135\n",
      "Step 301 | Loss: 0.8276496529579163\n",
      "Step 401 | Loss: 0.7966519594192505\n",
      "Step 501 | Loss: 0.7944059371948242\n",
      "Step 601 | Loss: 0.8208950757980347\n",
      "Step 701 | Loss: 0.5484283566474915\n",
      "Step 801 | Loss: 0.41185078024864197\n",
      "Step 901 | Loss: 0.5962913632392883\n",
      "Step 1001 | Loss: 0.6794869899749756\n",
      "Step 1101 | Loss: 0.639845609664917\n",
      "Step 1201 | Loss: 0.5997185707092285\n",
      "Step 1301 | Loss: 0.6594089269638062\n",
      "Step 1401 | Loss: 0.4848819971084595\n",
      "Step 1501 | Loss: 0.5976703763008118\n",
      "Step 1601 | Loss: 0.498039573431015\n",
      "Step 1701 | Loss: 0.6338346004486084\n",
      "Step 1801 | Loss: 0.6701006889343262\n",
      "Step 1901 | Loss: 0.7201232314109802\n",
      "Step 2001 | Loss: 0.7884748578071594\n",
      "Step 2101 | Loss: 0.8975048065185547\n",
      "Step 2201 | Loss: 0.6268215179443359\n",
      "Step 2301 | Loss: 0.6284313201904297\n",
      "Step 2401 | Loss: 0.7992894649505615\n",
      "0.6225983386651104 0.7975\n",
      "Step 1 | Loss: 0.8922796845436096\n",
      "Step 101 | Loss: 0.5671015381813049\n",
      "Step 201 | Loss: 0.443757027387619\n",
      "Step 301 | Loss: 0.4319292902946472\n",
      "Step 401 | Loss: 0.3542692959308624\n",
      "Step 501 | Loss: 0.31024688482284546\n",
      "Step 601 | Loss: 0.26234593987464905\n",
      "Step 701 | Loss: 0.3420453667640686\n",
      "Step 801 | Loss: 0.27936115860939026\n",
      "Step 901 | Loss: 0.3130497336387634\n",
      "Step 1001 | Loss: 0.316486656665802\n",
      "Step 1101 | Loss: 0.21842488646507263\n",
      "Step 1201 | Loss: 0.282106876373291\n",
      "Step 1301 | Loss: 0.21420496702194214\n",
      "Step 1401 | Loss: 0.3190917670726776\n",
      "Step 1501 | Loss: 0.31222230195999146\n",
      "Step 1601 | Loss: 0.21223175525665283\n",
      "Step 1701 | Loss: 0.25233176350593567\n",
      "Step 1801 | Loss: 0.26211386919021606\n",
      "Step 1901 | Loss: 0.35494354367256165\n",
      "Step 2001 | Loss: 0.2684950530529022\n",
      "Step 2101 | Loss: 0.2475813627243042\n",
      "Step 2201 | Loss: 0.2237817645072937\n",
      "Step 2301 | Loss: 0.30956652760505676\n",
      "Step 2401 | Loss: 0.29499804973602295\n",
      "0.2678513579826434 0.95\n",
      "Step 1 | Loss: 1.1678228378295898\n",
      "Step 101 | Loss: 0.708544135093689\n",
      "Step 201 | Loss: 0.4766286015510559\n",
      "Step 301 | Loss: 0.280160129070282\n",
      "Step 401 | Loss: 0.35636746883392334\n",
      "Step 501 | Loss: 0.37937667965888977\n",
      "Step 601 | Loss: 0.32537660002708435\n",
      "Step 701 | Loss: 0.2987910211086273\n",
      "Step 801 | Loss: 0.23634696006774902\n",
      "Step 901 | Loss: 0.2727539837360382\n",
      "Step 1001 | Loss: 0.23225755989551544\n",
      "Step 1101 | Loss: 0.27891889214515686\n",
      "Step 1201 | Loss: 0.27412256598472595\n",
      "Step 1301 | Loss: 0.4000968337059021\n",
      "Step 1401 | Loss: 0.276725709438324\n",
      "Step 1501 | Loss: 0.16756129264831543\n",
      "Step 1601 | Loss: 0.22186991572380066\n",
      "Step 1701 | Loss: 0.2629747986793518\n",
      "Step 1801 | Loss: 0.21728281676769257\n",
      "Step 1901 | Loss: 0.18408143520355225\n",
      "Step 2001 | Loss: 0.26858532428741455\n",
      "Step 2101 | Loss: 0.25900694727897644\n",
      "Step 2201 | Loss: 0.18535000085830688\n",
      "Step 2301 | Loss: 0.2002948820590973\n",
      "Step 2401 | Loss: 0.3214120864868164\n",
      "0.2614331649782104 0.955\n",
      "Step 1 | Loss: 0.7187724113464355\n",
      "Step 101 | Loss: 0.37873756885528564\n",
      "Step 201 | Loss: 0.32321545481681824\n",
      "Step 301 | Loss: 0.41799062490463257\n",
      "Step 401 | Loss: 0.2966476082801819\n",
      "Step 501 | Loss: 0.2413308173418045\n",
      "Step 601 | Loss: 0.20100708305835724\n",
      "Step 701 | Loss: 0.27889490127563477\n",
      "Step 801 | Loss: 0.2441861480474472\n",
      "Step 901 | Loss: 0.24541623890399933\n",
      "Step 1001 | Loss: 0.24542993307113647\n",
      "Step 1101 | Loss: 0.21068190038204193\n",
      "Step 1201 | Loss: 0.2852821946144104\n",
      "Step 1301 | Loss: 0.14762073755264282\n",
      "Step 1401 | Loss: 0.21599026024341583\n",
      "Step 1501 | Loss: 0.1835845559835434\n",
      "Step 1601 | Loss: 0.14604610204696655\n",
      "Step 1701 | Loss: 0.18497268855571747\n",
      "Step 1801 | Loss: 0.1124948039650917\n",
      "Step 1901 | Loss: 0.18339967727661133\n",
      "Step 2001 | Loss: 0.1869555562734604\n",
      "Step 2101 | Loss: 0.15049998462200165\n",
      "Step 2201 | Loss: 0.19988511502742767\n",
      "Step 2301 | Loss: 0.14312128722667694\n",
      "Step 2401 | Loss: 0.21545696258544922\n",
      "0.1798882758856008 0.985\n",
      "Step 1 | Loss: 0.7956809401512146\n",
      "Step 101 | Loss: 0.6645668148994446\n",
      "Step 201 | Loss: 0.4054042100906372\n",
      "Step 301 | Loss: 0.4126611053943634\n",
      "Step 401 | Loss: 0.3751943111419678\n",
      "Step 501 | Loss: 0.2778105139732361\n",
      "Step 601 | Loss: 0.30585813522338867\n",
      "Step 701 | Loss: 0.2595272958278656\n",
      "Step 801 | Loss: 0.19019454717636108\n",
      "Step 901 | Loss: 0.1976124346256256\n",
      "Step 1001 | Loss: 0.3380433917045593\n",
      "Step 1101 | Loss: 0.28611457347869873\n",
      "Step 1201 | Loss: 0.29540371894836426\n",
      "Step 1301 | Loss: 0.2435053288936615\n",
      "Step 1401 | Loss: 0.302855908870697\n",
      "Step 1501 | Loss: 0.22157947719097137\n",
      "Step 1601 | Loss: 0.3291747570037842\n",
      "Step 1701 | Loss: 0.2955334186553955\n",
      "Step 1801 | Loss: 0.28056660294532776\n",
      "Step 1901 | Loss: 0.2047799676656723\n",
      "Step 2001 | Loss: 0.1881445348262787\n",
      "Step 2101 | Loss: 0.2682737112045288\n",
      "Step 2201 | Loss: 0.23361651599407196\n",
      "Step 2301 | Loss: 0.19319461286067963\n",
      "Step 2401 | Loss: 0.2542344331741333\n",
      "0.2264795628534523 0.96\n",
      "Step 1 | Loss: 1.0546706914901733\n",
      "Step 101 | Loss: 0.7355847954750061\n",
      "Step 201 | Loss: 0.4535423815250397\n",
      "Step 301 | Loss: 0.48127028346061707\n",
      "Step 401 | Loss: 0.3371622562408447\n",
      "Step 501 | Loss: 0.25035929679870605\n",
      "Step 601 | Loss: 0.26802858710289\n",
      "Step 701 | Loss: 0.208757683634758\n",
      "Step 801 | Loss: 0.20789238810539246\n",
      "Step 901 | Loss: 0.2669934034347534\n",
      "Step 1001 | Loss: 0.268010675907135\n",
      "Step 1101 | Loss: 0.2766168713569641\n",
      "Step 1201 | Loss: 0.2800535261631012\n",
      "Step 1301 | Loss: 0.3242494761943817\n",
      "Step 1401 | Loss: 0.21823401749134064\n",
      "Step 1501 | Loss: 0.24352745711803436\n",
      "Step 1601 | Loss: 0.17861497402191162\n",
      "Step 1701 | Loss: 0.2426244616508484\n",
      "Step 1801 | Loss: 0.17707377672195435\n",
      "Step 1901 | Loss: 0.22289268672466278\n",
      "Step 2001 | Loss: 0.23287908732891083\n",
      "Step 2101 | Loss: 0.389915406703949\n",
      "Step 2201 | Loss: 0.20665323734283447\n",
      "Step 2301 | Loss: 0.26349398493766785\n",
      "Step 2401 | Loss: 0.14896705746650696\n",
      "0.21535892360707792 0.965\n",
      "Step 1 | Loss: 0.9634382128715515\n",
      "Step 101 | Loss: 0.5913376808166504\n",
      "Step 201 | Loss: 0.5278483033180237\n",
      "Step 301 | Loss: 0.5712253451347351\n",
      "Step 401 | Loss: 0.45143425464630127\n",
      "Step 501 | Loss: 0.4513145685195923\n",
      "Step 601 | Loss: 0.49921953678131104\n",
      "Step 701 | Loss: 0.5282689332962036\n",
      "Step 801 | Loss: 0.47493648529052734\n",
      "Step 901 | Loss: 0.3550945222377777\n",
      "Step 1001 | Loss: 0.43551209568977356\n",
      "Step 1101 | Loss: 0.479920357465744\n",
      "Step 1201 | Loss: 0.3551985025405884\n",
      "Step 1301 | Loss: 0.5166311860084534\n",
      "Step 1401 | Loss: 0.5930284857749939\n",
      "Step 1501 | Loss: 0.45390015840530396\n",
      "Step 1601 | Loss: 0.5137024521827698\n",
      "Step 1701 | Loss: 0.5903534889221191\n",
      "Step 1801 | Loss: 0.3996252417564392\n",
      "Step 1901 | Loss: 0.4987936019897461\n",
      "Step 2001 | Loss: 0.42134279012680054\n",
      "Step 2101 | Loss: 0.5514106154441833\n",
      "Step 2201 | Loss: 0.48004475235939026\n",
      "Step 2301 | Loss: 0.5244437456130981\n",
      "Step 2401 | Loss: 0.4916783571243286\n",
      "0.5045169016577399 0.8925\n",
      "Step 1 | Loss: 1.6809240579605103\n",
      "Step 101 | Loss: 0.710923969745636\n",
      "Step 201 | Loss: 0.586036205291748\n",
      "Step 301 | Loss: 0.5814789533615112\n",
      "Step 401 | Loss: 0.598958432674408\n",
      "Step 501 | Loss: 0.5608601570129395\n",
      "Step 601 | Loss: 0.6089580059051514\n",
      "Step 701 | Loss: 0.8030716776847839\n",
      "Step 801 | Loss: 0.6844660043716431\n",
      "Step 901 | Loss: 0.5991635918617249\n",
      "Step 1001 | Loss: 0.5852080583572388\n",
      "Step 1101 | Loss: 0.5411948561668396\n",
      "Step 1201 | Loss: 0.450489342212677\n",
      "Step 1301 | Loss: 0.7713721394538879\n",
      "Step 1401 | Loss: 0.725629985332489\n",
      "Step 1501 | Loss: 0.5211688876152039\n",
      "Step 1601 | Loss: 0.5876325368881226\n",
      "Step 1701 | Loss: 0.4759272336959839\n",
      "Step 1801 | Loss: 0.39018017053604126\n",
      "Step 1901 | Loss: 0.4729776680469513\n",
      "Step 2001 | Loss: 0.5479558706283569\n",
      "Step 2101 | Loss: 0.5750008821487427\n",
      "Step 2201 | Loss: 0.49046558141708374\n",
      "Step 2301 | Loss: 0.5238822102546692\n",
      "Step 2401 | Loss: 0.6655254364013672\n",
      "0.5020433187309368 0.89\n",
      "Step 1 | Loss: 1.1594194173812866\n",
      "Step 101 | Loss: 0.9055675864219666\n",
      "Step 201 | Loss: 0.6591465473175049\n",
      "Step 301 | Loss: 0.4662681818008423\n",
      "Step 401 | Loss: 0.5205310583114624\n",
      "Step 501 | Loss: 0.5193408727645874\n",
      "Step 601 | Loss: 0.4974270761013031\n",
      "Step 701 | Loss: 0.3754614591598511\n",
      "Step 801 | Loss: 0.508228600025177\n",
      "Step 901 | Loss: 0.49423345923423767\n",
      "Step 1001 | Loss: 0.49983009696006775\n",
      "Step 1101 | Loss: 0.5164152979850769\n",
      "Step 1201 | Loss: 0.5244223475456238\n",
      "Step 1301 | Loss: 0.4929388761520386\n",
      "Step 1401 | Loss: 0.3919624388217926\n",
      "Step 1501 | Loss: 0.6192343235015869\n",
      "Step 1601 | Loss: 0.4517329931259155\n",
      "Step 1701 | Loss: 0.3996376395225525\n",
      "Step 1801 | Loss: 0.4933144748210907\n",
      "Step 1901 | Loss: 0.4905948042869568\n",
      "Step 2001 | Loss: 0.4414021968841553\n",
      "Step 2101 | Loss: 0.44126200675964355\n",
      "Step 2201 | Loss: 0.5615561008453369\n",
      "Step 2301 | Loss: 0.494931697845459\n",
      "Step 2401 | Loss: 0.5600239634513855\n",
      "0.5083143598694246 0.905\n",
      "Step 1 | Loss: 1.0041002035140991\n",
      "Step 101 | Loss: 0.7806404829025269\n",
      "Step 201 | Loss: 0.5522599220275879\n",
      "Step 301 | Loss: 0.6516383290290833\n",
      "Step 401 | Loss: 0.6745067834854126\n",
      "Step 501 | Loss: 0.47127649188041687\n",
      "Step 601 | Loss: 0.5968831777572632\n",
      "Step 701 | Loss: 0.46792367100715637\n",
      "Step 801 | Loss: 0.6181718707084656\n",
      "Step 901 | Loss: 0.6089999675750732\n",
      "Step 1001 | Loss: 0.40463000535964966\n",
      "Step 1101 | Loss: 0.5609285235404968\n",
      "Step 1201 | Loss: 0.49698883295059204\n",
      "Step 1301 | Loss: 0.5679044127464294\n",
      "Step 1401 | Loss: 0.5149930119514465\n",
      "Step 1501 | Loss: 0.5451673865318298\n",
      "Step 1601 | Loss: 0.5570544004440308\n",
      "Step 1701 | Loss: 0.42535972595214844\n",
      "Step 1801 | Loss: 0.5317730903625488\n",
      "Step 1901 | Loss: 0.4551225006580353\n",
      "Step 2001 | Loss: 0.3275921940803528\n",
      "Step 2101 | Loss: 0.5951333045959473\n",
      "Step 2201 | Loss: 0.44792449474334717\n",
      "Step 2301 | Loss: 0.4230231046676636\n",
      "Step 2401 | Loss: 0.5015029907226562\n",
      "0.5049458609838229 0.8925\n",
      "Step 1 | Loss: 1.1001421213150024\n",
      "Step 101 | Loss: 0.7616429328918457\n",
      "Step 201 | Loss: 0.6611747741699219\n",
      "Step 301 | Loss: 0.9537051916122437\n",
      "Step 401 | Loss: 0.6173242330551147\n",
      "Step 501 | Loss: 0.822254478931427\n",
      "Step 601 | Loss: 0.6155341863632202\n",
      "Step 701 | Loss: 0.6460238099098206\n",
      "Step 801 | Loss: 0.5399758815765381\n",
      "Step 901 | Loss: 0.4788713753223419\n",
      "Step 1001 | Loss: 0.6243988275527954\n",
      "Step 1101 | Loss: 0.5406510233879089\n",
      "Step 1201 | Loss: 0.7270894646644592\n",
      "Step 1301 | Loss: 0.7405008673667908\n",
      "Step 1401 | Loss: 0.5600199103355408\n",
      "Step 1501 | Loss: 0.5799797773361206\n",
      "Step 1601 | Loss: 0.4992679953575134\n",
      "Step 1701 | Loss: 0.4262598752975464\n",
      "Step 1801 | Loss: 0.5981390476226807\n",
      "Step 1901 | Loss: 0.5412693023681641\n",
      "Step 2001 | Loss: 0.6272041201591492\n",
      "Step 2101 | Loss: 0.5929394960403442\n",
      "Step 2201 | Loss: 0.5812515020370483\n",
      "Step 2301 | Loss: 0.6170844435691833\n",
      "Step 2401 | Loss: 0.5801575779914856\n",
      "0.5337030892310745 0.8825\n",
      "Step 1 | Loss: 0.7793487310409546\n",
      "Step 101 | Loss: 0.3299636244773865\n",
      "Step 201 | Loss: 0.3710951805114746\n",
      "Step 301 | Loss: 0.14659035205841064\n",
      "Step 401 | Loss: 0.3505140542984009\n",
      "Step 501 | Loss: 0.20332218706607819\n",
      "Step 601 | Loss: 0.1465606540441513\n",
      "Step 701 | Loss: 0.22365689277648926\n",
      "Step 801 | Loss: 0.24301138520240784\n",
      "Step 901 | Loss: 0.14542198181152344\n",
      "Step 1001 | Loss: 0.18754620850086212\n",
      "Step 1101 | Loss: 0.18207627534866333\n",
      "Step 1201 | Loss: 0.09458456188440323\n",
      "Step 1301 | Loss: 0.12769083678722382\n",
      "Step 1401 | Loss: 0.20461881160736084\n",
      "Step 1501 | Loss: 0.1419266313314438\n",
      "Step 1601 | Loss: 0.13399378955364227\n",
      "Step 1701 | Loss: 0.1565731018781662\n",
      "Step 1801 | Loss: 0.19777880609035492\n",
      "Step 1901 | Loss: 0.28883033990859985\n",
      "Step 2001 | Loss: 0.15325237810611725\n",
      "Step 2101 | Loss: 0.29507941007614136\n",
      "Step 2201 | Loss: 0.2082996964454651\n",
      "Step 2301 | Loss: 0.20142731070518494\n",
      "Step 2401 | Loss: 0.12400181591510773\n",
      "0.18602683438908602 0.96\n",
      "Step 1 | Loss: 0.8556464314460754\n",
      "Step 101 | Loss: 0.4396582841873169\n",
      "Step 201 | Loss: 0.27855196595191956\n",
      "Step 301 | Loss: 0.19552402198314667\n",
      "Step 401 | Loss: 0.15313518047332764\n",
      "Step 501 | Loss: 0.23861244320869446\n",
      "Step 601 | Loss: 0.19889187812805176\n",
      "Step 701 | Loss: 0.2017689198255539\n",
      "Step 801 | Loss: 0.10494153946638107\n",
      "Step 901 | Loss: 0.18569599092006683\n",
      "Step 1001 | Loss: 0.23080827295780182\n",
      "Step 1101 | Loss: 0.21995839476585388\n",
      "Step 1201 | Loss: 0.11072234809398651\n",
      "Step 1301 | Loss: 0.16630500555038452\n",
      "Step 1401 | Loss: 0.10743292421102524\n",
      "Step 1501 | Loss: 0.1989373117685318\n",
      "Step 1601 | Loss: 0.2886042594909668\n",
      "Step 1701 | Loss: 0.12201225757598877\n",
      "Step 1801 | Loss: 0.08622809499502182\n",
      "Step 1901 | Loss: 0.08367014676332474\n",
      "Step 2001 | Loss: 0.23238742351531982\n",
      "Step 2101 | Loss: 0.18326352536678314\n",
      "Step 2201 | Loss: 0.13160692155361176\n",
      "Step 2301 | Loss: 0.18974173069000244\n",
      "Step 2401 | Loss: 0.11003875732421875\n",
      "0.15977464580736428 0.9625\n",
      "Step 1 | Loss: 0.8098836541175842\n",
      "Step 101 | Loss: 0.3075186014175415\n",
      "Step 201 | Loss: 0.27524492144584656\n",
      "Step 301 | Loss: 0.27203309535980225\n",
      "Step 401 | Loss: 0.13388261198997498\n",
      "Step 501 | Loss: 0.22687098383903503\n",
      "Step 601 | Loss: 0.22650283575057983\n",
      "Step 701 | Loss: 0.17165227234363556\n",
      "Step 801 | Loss: 0.18704113364219666\n",
      "Step 901 | Loss: 0.13900187611579895\n",
      "Step 1001 | Loss: 0.15343070030212402\n",
      "Step 1101 | Loss: 0.17635931074619293\n",
      "Step 1201 | Loss: 0.18109779059886932\n",
      "Step 1301 | Loss: 0.131134033203125\n",
      "Step 1401 | Loss: 0.14446520805358887\n",
      "Step 1501 | Loss: 0.22249634563922882\n",
      "Step 1601 | Loss: 0.267604261636734\n",
      "Step 1701 | Loss: 0.18099315464496613\n",
      "Step 1801 | Loss: 0.2057722508907318\n",
      "Step 1901 | Loss: 0.16353744268417358\n",
      "Step 2001 | Loss: 0.12987282872200012\n",
      "Step 2101 | Loss: 0.29400238394737244\n",
      "Step 2201 | Loss: 0.16190828382968903\n",
      "Step 2301 | Loss: 0.14126941561698914\n",
      "Step 2401 | Loss: 0.12639325857162476\n",
      "0.16151905395129854 0.9725\n",
      "Step 1 | Loss: 0.6432939767837524\n",
      "Step 101 | Loss: 0.36319535970687866\n",
      "Step 201 | Loss: 0.17583975195884705\n",
      "Step 301 | Loss: 0.27007946372032166\n",
      "Step 401 | Loss: 0.21094383299350739\n",
      "Step 501 | Loss: 0.26626265048980713\n",
      "Step 601 | Loss: 0.14345954358577728\n",
      "Step 701 | Loss: 0.1600075364112854\n",
      "Step 801 | Loss: 0.16035868227481842\n",
      "Step 901 | Loss: 0.17671717703342438\n",
      "Step 1001 | Loss: 0.22290430963039398\n",
      "Step 1101 | Loss: 0.2886568307876587\n",
      "Step 1201 | Loss: 0.17474693059921265\n",
      "Step 1301 | Loss: 0.223470076918602\n",
      "Step 1401 | Loss: 0.15143662691116333\n",
      "Step 1501 | Loss: 0.11746000498533249\n",
      "Step 1601 | Loss: 0.14131399989128113\n",
      "Step 1701 | Loss: 0.15640783309936523\n",
      "Step 1801 | Loss: 0.15233466029167175\n",
      "Step 1901 | Loss: 0.11877302080392838\n",
      "Step 2001 | Loss: 0.12036556005477905\n",
      "Step 2101 | Loss: 0.18570548295974731\n",
      "Step 2201 | Loss: 0.14301906526088715\n",
      "Step 2301 | Loss: 0.22556206583976746\n",
      "Step 2401 | Loss: 0.13032390177249908\n",
      "0.1573672762931192 0.9725\n",
      "Step 1 | Loss: 1.0084781646728516\n",
      "Step 101 | Loss: 0.18033917248249054\n",
      "Step 201 | Loss: 0.26925593614578247\n",
      "Step 301 | Loss: 0.1656249612569809\n",
      "Step 401 | Loss: 0.34504860639572144\n",
      "Step 501 | Loss: 0.34829941391944885\n",
      "Step 601 | Loss: 0.31564807891845703\n",
      "Step 701 | Loss: 0.14559100568294525\n",
      "Step 801 | Loss: 0.1211208701133728\n",
      "Step 901 | Loss: 0.08000893145799637\n",
      "Step 1001 | Loss: 0.18210944533348083\n",
      "Step 1101 | Loss: 0.1393483281135559\n",
      "Step 1201 | Loss: 0.120744489133358\n",
      "Step 1301 | Loss: 0.1482572853565216\n",
      "Step 1401 | Loss: 0.2167035937309265\n",
      "Step 1501 | Loss: 0.11075082421302795\n",
      "Step 1601 | Loss: 0.2718752920627594\n",
      "Step 1701 | Loss: 0.19421818852424622\n",
      "Step 1801 | Loss: 0.2453595995903015\n",
      "Step 1901 | Loss: 0.15538598597049713\n",
      "Step 2001 | Loss: 0.1370675414800644\n",
      "Step 2101 | Loss: 0.16003099083900452\n",
      "Step 2201 | Loss: 0.17171233892440796\n",
      "Step 2301 | Loss: 0.10195271670818329\n",
      "Step 2401 | Loss: 0.2167263776063919\n",
      "0.15544346813622367 0.9725\n",
      "Step 1 | Loss: 1.3864855766296387\n",
      "Step 101 | Loss: 0.7042065858840942\n",
      "Step 201 | Loss: 0.44102969765663147\n",
      "Step 301 | Loss: 0.6944177150726318\n",
      "Step 401 | Loss: 0.6099258065223694\n",
      "Step 501 | Loss: 0.7562304735183716\n",
      "Step 601 | Loss: 0.6413726806640625\n",
      "Step 701 | Loss: 0.5130476355552673\n",
      "Step 801 | Loss: 0.6142041087150574\n",
      "Step 901 | Loss: 0.5433704257011414\n",
      "Step 1001 | Loss: 0.33128324151039124\n",
      "Step 1101 | Loss: 0.6808221340179443\n",
      "Step 1201 | Loss: 0.5776641368865967\n",
      "Step 1301 | Loss: 0.2597562074661255\n",
      "Step 1401 | Loss: 0.847933828830719\n",
      "Step 1501 | Loss: 0.7753064632415771\n",
      "Step 1601 | Loss: 0.6228786706924438\n",
      "Step 1701 | Loss: 0.5090097188949585\n",
      "Step 1801 | Loss: 0.42609691619873047\n",
      "Step 1901 | Loss: 0.6312503218650818\n",
      "Step 2001 | Loss: 0.5219965577125549\n",
      "Step 2101 | Loss: 0.4227094352245331\n",
      "Step 2201 | Loss: 0.7872799038887024\n",
      "Step 2301 | Loss: 0.6074676513671875\n",
      "Step 2401 | Loss: 0.3802507817745209\n",
      "0.6492180935664446 0.785\n",
      "Step 1 | Loss: 2.337080717086792\n",
      "Step 101 | Loss: 0.7396399974822998\n",
      "Step 201 | Loss: 0.8210576772689819\n",
      "Step 301 | Loss: 0.8232782483100891\n",
      "Step 401 | Loss: 0.565607488155365\n",
      "Step 501 | Loss: 0.8295344114303589\n",
      "Step 601 | Loss: 0.548379123210907\n",
      "Step 701 | Loss: 0.5763821005821228\n",
      "Step 801 | Loss: 0.5534836649894714\n",
      "Step 901 | Loss: 0.3037703037261963\n",
      "Step 1001 | Loss: 0.38971003890037537\n",
      "Step 1101 | Loss: 0.5682696104049683\n",
      "Step 1201 | Loss: 0.4645126461982727\n",
      "Step 1301 | Loss: 0.6103415489196777\n",
      "Step 1401 | Loss: 0.5715060830116272\n",
      "Step 1501 | Loss: 0.6204818487167358\n",
      "Step 1601 | Loss: 0.8950339555740356\n",
      "Step 1701 | Loss: 0.5722982883453369\n",
      "Step 1801 | Loss: 0.5664798617362976\n",
      "Step 1901 | Loss: 0.9595801830291748\n",
      "Step 2001 | Loss: 0.7443945407867432\n",
      "Step 2101 | Loss: 0.6713080406188965\n",
      "Step 2201 | Loss: 0.5740848779678345\n",
      "Step 2301 | Loss: 0.44669345021247864\n",
      "Step 2401 | Loss: 0.7875947952270508\n",
      "0.6484885394163532 0.7875\n",
      "Step 1 | Loss: 1.3921217918395996\n",
      "Step 101 | Loss: 0.4332679212093353\n",
      "Step 201 | Loss: 0.22029826045036316\n",
      "Step 301 | Loss: 0.662110447883606\n",
      "Step 401 | Loss: 0.68988436460495\n",
      "Step 501 | Loss: 0.6042911410331726\n",
      "Step 601 | Loss: 0.461849570274353\n",
      "Step 701 | Loss: 0.4837024211883545\n",
      "Step 801 | Loss: 0.5889566540718079\n",
      "Step 901 | Loss: 0.2309107780456543\n",
      "Step 1001 | Loss: 0.5038003325462341\n",
      "Step 1101 | Loss: 0.7608857154846191\n",
      "Step 1201 | Loss: 0.5119803547859192\n",
      "Step 1301 | Loss: 0.7040555477142334\n",
      "Step 1401 | Loss: 0.5806831121444702\n",
      "Step 1501 | Loss: 0.6022546291351318\n",
      "Step 1601 | Loss: 0.8409528732299805\n",
      "Step 1701 | Loss: 0.5729207992553711\n",
      "Step 1801 | Loss: 0.4129595160484314\n",
      "Step 1901 | Loss: 0.48492133617401123\n",
      "Step 2001 | Loss: 0.3603549599647522\n",
      "Step 2101 | Loss: 0.8694405555725098\n",
      "Step 2201 | Loss: 0.47241395711898804\n",
      "Step 2301 | Loss: 0.5653183460235596\n",
      "Step 2401 | Loss: 0.8665377497673035\n",
      "0.63262748008891 0.7875\n",
      "Step 1 | Loss: 0.8597900867462158\n",
      "Step 101 | Loss: 0.584194540977478\n",
      "Step 201 | Loss: 0.5936157703399658\n",
      "Step 301 | Loss: 0.6236870884895325\n",
      "Step 401 | Loss: 0.6475124955177307\n",
      "Step 501 | Loss: 0.7025535106658936\n",
      "Step 601 | Loss: 0.4096129536628723\n",
      "Step 701 | Loss: 0.5155794620513916\n",
      "Step 801 | Loss: 0.6811878085136414\n",
      "Step 901 | Loss: 0.652221143245697\n",
      "Step 1001 | Loss: 0.4368283748626709\n",
      "Step 1101 | Loss: 0.9017379879951477\n",
      "Step 1201 | Loss: 0.8665196895599365\n",
      "Step 1301 | Loss: 0.5593681335449219\n",
      "Step 1401 | Loss: 0.5625701546669006\n",
      "Step 1501 | Loss: 0.4182693660259247\n",
      "Step 1601 | Loss: 0.47652652859687805\n",
      "Step 1701 | Loss: 0.3846457004547119\n",
      "Step 1801 | Loss: 0.35066837072372437\n",
      "Step 1901 | Loss: 0.4401477873325348\n",
      "Step 2001 | Loss: 0.4669930338859558\n",
      "Step 2101 | Loss: 0.7244683504104614\n",
      "Step 2201 | Loss: 0.8749257922172546\n",
      "Step 2301 | Loss: 0.689964234828949\n",
      "Step 2401 | Loss: 0.41931504011154175\n",
      "0.6337400503092917 0.7775\n",
      "Step 1 | Loss: 1.122778296470642\n",
      "Step 101 | Loss: 0.5697696805000305\n",
      "Step 201 | Loss: 0.6633970737457275\n",
      "Step 301 | Loss: 0.6068245768547058\n",
      "Step 401 | Loss: 0.6835823059082031\n",
      "Step 501 | Loss: 0.4618518352508545\n",
      "Step 601 | Loss: 0.407947838306427\n",
      "Step 701 | Loss: 0.5259260535240173\n",
      "Step 801 | Loss: 0.9810084700584412\n",
      "Step 901 | Loss: 0.6746158599853516\n",
      "Step 1001 | Loss: 0.5287330150604248\n",
      "Step 1101 | Loss: 0.5237782001495361\n",
      "Step 1201 | Loss: 0.517081618309021\n",
      "Step 1301 | Loss: 0.6337295174598694\n",
      "Step 1401 | Loss: 0.7842323780059814\n",
      "Step 1501 | Loss: 0.3258765935897827\n",
      "Step 1601 | Loss: 0.818941593170166\n",
      "Step 1701 | Loss: 0.5589242577552795\n",
      "Step 1801 | Loss: 0.4858914911746979\n",
      "Step 1901 | Loss: 0.623146653175354\n",
      "Step 2001 | Loss: 0.701947808265686\n",
      "Step 2101 | Loss: 0.9668881297111511\n",
      "Step 2201 | Loss: 0.8215001821517944\n",
      "Step 2301 | Loss: 0.6281057000160217\n",
      "Step 2401 | Loss: 0.5577674508094788\n",
      "0.6337976942172663 0.7775\n",
      "Step 1 | Loss: 1.1402809619903564\n",
      "Step 101 | Loss: 0.4835033714771271\n",
      "Step 201 | Loss: 0.4181581139564514\n",
      "Step 301 | Loss: 0.4048488140106201\n",
      "Step 401 | Loss: 0.29518991708755493\n",
      "Step 501 | Loss: 0.42814767360687256\n",
      "Step 601 | Loss: 0.43100786209106445\n",
      "Step 701 | Loss: 0.3940701484680176\n",
      "Step 801 | Loss: 0.34822043776512146\n",
      "Step 901 | Loss: 0.3147212862968445\n",
      "Step 1001 | Loss: 0.3373672366142273\n",
      "Step 1101 | Loss: 0.3422567546367645\n",
      "Step 1201 | Loss: 0.34844574332237244\n",
      "Step 1301 | Loss: 0.5252802968025208\n",
      "Step 1401 | Loss: 0.3250351548194885\n",
      "Step 1501 | Loss: 0.3146606683731079\n",
      "Step 1601 | Loss: 0.4605323374271393\n",
      "Step 1701 | Loss: 0.37537023425102234\n",
      "Step 1801 | Loss: 0.5158182978630066\n",
      "Step 1901 | Loss: 0.533504068851471\n",
      "Step 2001 | Loss: 0.311477929353714\n",
      "Step 2101 | Loss: 0.4026731550693512\n",
      "Step 2201 | Loss: 0.3607786297798157\n",
      "Step 2301 | Loss: 0.2937166094779968\n",
      "Step 2401 | Loss: 0.336937814950943\n",
      "0.4389624714771187 0.8925\n",
      "Step 1 | Loss: 1.690804123878479\n",
      "Step 101 | Loss: 0.45541125535964966\n",
      "Step 201 | Loss: 0.45390942692756653\n",
      "Step 301 | Loss: 0.3952058255672455\n",
      "Step 401 | Loss: 0.3467874825000763\n",
      "Step 501 | Loss: 0.4399327337741852\n",
      "Step 601 | Loss: 0.32029488682746887\n",
      "Step 701 | Loss: 0.38579699397087097\n",
      "Step 801 | Loss: 0.40895789861679077\n",
      "Step 901 | Loss: 0.2968898117542267\n",
      "Step 1001 | Loss: 0.39128634333610535\n",
      "Step 1101 | Loss: 0.4580484628677368\n",
      "Step 1201 | Loss: 0.2920285165309906\n",
      "Step 1301 | Loss: 0.3442889451980591\n",
      "Step 1401 | Loss: 0.36808347702026367\n",
      "Step 1501 | Loss: 0.5299076437950134\n",
      "Step 1601 | Loss: 0.6124638915061951\n",
      "Step 1701 | Loss: 0.42729634046554565\n",
      "Step 1801 | Loss: 0.3494103252887726\n",
      "Step 1901 | Loss: 0.3220675587654114\n",
      "Step 2001 | Loss: 0.274852991104126\n",
      "Step 2101 | Loss: 0.4838307499885559\n",
      "Step 2201 | Loss: 0.44825953245162964\n",
      "Step 2301 | Loss: 0.4179493188858032\n",
      "Step 2401 | Loss: 0.33022645115852356\n",
      "0.4387498380378283 0.8875\n",
      "Step 1 | Loss: 1.2266420125961304\n",
      "Step 101 | Loss: 0.5662220120429993\n",
      "Step 201 | Loss: 0.687288761138916\n",
      "Step 301 | Loss: 0.22601792216300964\n",
      "Step 401 | Loss: 0.38915812969207764\n",
      "Step 501 | Loss: 0.5357558131217957\n",
      "Step 601 | Loss: 0.514320969581604\n",
      "Step 701 | Loss: 0.404070645570755\n",
      "Step 801 | Loss: 0.3820194900035858\n",
      "Step 901 | Loss: 0.36495405435562134\n",
      "Step 1001 | Loss: 0.5589618682861328\n",
      "Step 1101 | Loss: 0.4300774931907654\n",
      "Step 1201 | Loss: 0.5064021944999695\n",
      "Step 1301 | Loss: 0.4215509295463562\n",
      "Step 1401 | Loss: 0.36445873975753784\n",
      "Step 1501 | Loss: 0.31827524304389954\n",
      "Step 1601 | Loss: 0.34745702147483826\n",
      "Step 1701 | Loss: 0.5054250955581665\n",
      "Step 1801 | Loss: 0.43168503046035767\n",
      "Step 1901 | Loss: 0.4572519361972809\n",
      "Step 2001 | Loss: 0.510552167892456\n",
      "Step 2101 | Loss: 0.3132031559944153\n",
      "Step 2201 | Loss: 0.5173252820968628\n",
      "Step 2301 | Loss: 0.47250762581825256\n",
      "Step 2401 | Loss: 0.600975751876831\n",
      "0.459325162606552 0.8675\n",
      "Step 1 | Loss: 0.9318087100982666\n",
      "Step 101 | Loss: 0.44199129939079285\n",
      "Step 201 | Loss: 0.41763415932655334\n",
      "Step 301 | Loss: 0.5438994765281677\n",
      "Step 401 | Loss: 0.47389352321624756\n",
      "Step 501 | Loss: 0.4897415041923523\n",
      "Step 601 | Loss: 0.5229752659797668\n",
      "Step 701 | Loss: 0.3997102379798889\n",
      "Step 801 | Loss: 0.44363418221473694\n",
      "Step 901 | Loss: 0.33553624153137207\n",
      "Step 1001 | Loss: 0.48209452629089355\n",
      "Step 1101 | Loss: 0.4786829650402069\n",
      "Step 1201 | Loss: 0.38530346751213074\n",
      "Step 1301 | Loss: 0.34720584750175476\n",
      "Step 1401 | Loss: 0.4045732915401459\n",
      "Step 1501 | Loss: 0.46586015820503235\n",
      "Step 1601 | Loss: 0.34094056487083435\n",
      "Step 1701 | Loss: 0.44237107038497925\n",
      "Step 1801 | Loss: 0.5915876626968384\n",
      "Step 1901 | Loss: 0.4712817072868347\n",
      "Step 2001 | Loss: 0.5037298798561096\n",
      "Step 2101 | Loss: 0.5409258008003235\n",
      "Step 2201 | Loss: 0.20851564407348633\n",
      "Step 2301 | Loss: 0.6629378795623779\n",
      "Step 2401 | Loss: 0.452022910118103\n",
      "0.4592877149602428 0.8675\n",
      "Step 1 | Loss: 2.0962228775024414\n",
      "Step 101 | Loss: 0.5450965762138367\n",
      "Step 201 | Loss: 0.40309804677963257\n",
      "Step 301 | Loss: 0.40299662947654724\n",
      "Step 401 | Loss: 0.6117045879364014\n",
      "Step 501 | Loss: 0.3562200665473938\n",
      "Step 601 | Loss: 0.4783850908279419\n",
      "Step 701 | Loss: 0.5103094577789307\n",
      "Step 801 | Loss: 0.4467725157737732\n",
      "Step 901 | Loss: 0.3930889666080475\n",
      "Step 1001 | Loss: 0.5252591967582703\n",
      "Step 1101 | Loss: 0.3449263572692871\n",
      "Step 1201 | Loss: 0.5820077657699585\n",
      "Step 1301 | Loss: 0.3600863814353943\n",
      "Step 1401 | Loss: 0.34425076842308044\n",
      "Step 1501 | Loss: 0.3860097825527191\n",
      "Step 1601 | Loss: 0.32313188910484314\n",
      "Step 1701 | Loss: 0.5105780363082886\n",
      "Step 1801 | Loss: 0.38725101947784424\n",
      "Step 1901 | Loss: 0.4197958707809448\n",
      "Step 2001 | Loss: 0.373418927192688\n",
      "Step 2101 | Loss: 0.35882115364074707\n",
      "Step 2201 | Loss: 0.49174824357032776\n",
      "Step 2301 | Loss: 0.410444974899292\n",
      "Step 2401 | Loss: 0.4929261803627014\n",
      "0.4386983852305606 0.89\n",
      "Step 1 | Loss: 1.0171468257904053\n",
      "Step 101 | Loss: 0.9722426533699036\n",
      "Step 201 | Loss: 0.9915501475334167\n",
      "Step 301 | Loss: 0.9827412366867065\n",
      "Step 401 | Loss: 0.9899447560310364\n",
      "Step 501 | Loss: 0.9252029657363892\n",
      "Step 601 | Loss: 1.0231022834777832\n",
      "Step 701 | Loss: 1.028214693069458\n",
      "Step 801 | Loss: 0.9732348918914795\n",
      "Step 901 | Loss: 0.9639953374862671\n",
      "Step 1001 | Loss: 0.9990758895874023\n",
      "Step 1101 | Loss: 0.9767752289772034\n",
      "Step 1201 | Loss: 1.0271484851837158\n",
      "Step 1301 | Loss: 1.0092731714248657\n",
      "Step 1401 | Loss: 0.9949122071266174\n",
      "Step 1501 | Loss: 0.9891676306724548\n",
      "Step 1601 | Loss: 0.9640064239501953\n",
      "Step 1701 | Loss: 1.009092092514038\n",
      "Step 1801 | Loss: 1.0011109113693237\n",
      "Step 1901 | Loss: 1.030609130859375\n",
      "Step 2001 | Loss: 0.9662824273109436\n",
      "Step 2101 | Loss: 0.946284294128418\n",
      "Step 2201 | Loss: 0.9451779127120972\n",
      "Step 2301 | Loss: 0.9884655475616455\n",
      "Step 2401 | Loss: 0.9123401641845703\n",
      "0.9521459345248324 0.5725\n",
      "Step 1 | Loss: 1.3629182577133179\n",
      "Step 101 | Loss: 0.8164262175559998\n",
      "Step 201 | Loss: 0.6450648903846741\n",
      "Step 301 | Loss: 0.7110416889190674\n",
      "Step 401 | Loss: 0.6919139623641968\n",
      "Step 501 | Loss: 0.6345566511154175\n",
      "Step 601 | Loss: 0.5898243188858032\n",
      "Step 701 | Loss: 0.6428678035736084\n",
      "Step 801 | Loss: 0.6553040146827698\n",
      "Step 901 | Loss: 0.6275917887687683\n",
      "Step 1001 | Loss: 0.7019368410110474\n",
      "Step 1101 | Loss: 0.5972211360931396\n",
      "Step 1201 | Loss: 0.6015072464942932\n",
      "Step 1301 | Loss: 0.6085589528083801\n",
      "Step 1401 | Loss: 0.7003755569458008\n",
      "Step 1501 | Loss: 0.667912483215332\n",
      "Step 1601 | Loss: 0.5475524067878723\n",
      "Step 1701 | Loss: 0.6678417325019836\n",
      "Step 1801 | Loss: 0.5983912348747253\n",
      "Step 1901 | Loss: 0.5149085521697998\n",
      "Step 2001 | Loss: 0.6612287759780884\n",
      "Step 2101 | Loss: 0.5500383973121643\n",
      "Step 2201 | Loss: 0.6698215007781982\n",
      "Step 2301 | Loss: 0.6889840364456177\n",
      "Step 2401 | Loss: 0.5432420372962952\n",
      "0.6237680615938778 0.865\n",
      "Step 1 | Loss: 1.1221051216125488\n",
      "Step 101 | Loss: 1.0319920778274536\n",
      "Step 201 | Loss: 0.8005999326705933\n",
      "Step 301 | Loss: 0.7649157047271729\n",
      "Step 401 | Loss: 0.6835806369781494\n",
      "Step 501 | Loss: 0.6597216129302979\n",
      "Step 601 | Loss: 0.7081989049911499\n",
      "Step 701 | Loss: 0.6781247854232788\n",
      "Step 801 | Loss: 0.7064788937568665\n",
      "Step 901 | Loss: 0.6185721755027771\n",
      "Step 1001 | Loss: 0.6800071597099304\n",
      "Step 1101 | Loss: 0.6761483550071716\n",
      "Step 1201 | Loss: 0.528942346572876\n",
      "Step 1301 | Loss: 0.6058732271194458\n",
      "Step 1401 | Loss: 0.6702205538749695\n",
      "Step 1501 | Loss: 0.6270995736122131\n",
      "Step 1601 | Loss: 0.7204373478889465\n",
      "Step 1701 | Loss: 0.6375845074653625\n",
      "Step 1801 | Loss: 0.6746342778205872\n",
      "Step 1901 | Loss: 0.5922342538833618\n",
      "Step 2001 | Loss: 0.6286819577217102\n",
      "Step 2101 | Loss: 0.5381158590316772\n",
      "Step 2201 | Loss: 0.6619412899017334\n",
      "Step 2301 | Loss: 0.6677722930908203\n",
      "Step 2401 | Loss: 0.6115070581436157\n",
      "0.627487616070246 0.89\n",
      "Step 1 | Loss: 1.2373576164245605\n",
      "Step 101 | Loss: 0.7224711775779724\n",
      "Step 201 | Loss: 0.5562556385993958\n",
      "Step 301 | Loss: 0.5535944700241089\n",
      "Step 401 | Loss: 0.6022470593452454\n",
      "Step 501 | Loss: 0.7201716899871826\n",
      "Step 601 | Loss: 0.6943981647491455\n",
      "Step 701 | Loss: 0.6295875310897827\n",
      "Step 801 | Loss: 0.6184031963348389\n",
      "Step 901 | Loss: 0.602923572063446\n",
      "Step 1001 | Loss: 0.7312917113304138\n",
      "Step 1101 | Loss: 0.5789069533348083\n",
      "Step 1201 | Loss: 0.512589156627655\n",
      "Step 1301 | Loss: 0.626508891582489\n",
      "Step 1401 | Loss: 0.7888420224189758\n",
      "Step 1501 | Loss: 0.6174660921096802\n",
      "Step 1601 | Loss: 0.5635657906532288\n",
      "Step 1701 | Loss: 0.6402273774147034\n",
      "Step 1801 | Loss: 0.6240980625152588\n",
      "Step 1901 | Loss: 0.5780413150787354\n",
      "Step 2001 | Loss: 0.6829584836959839\n",
      "Step 2101 | Loss: 0.7376527786254883\n",
      "Step 2201 | Loss: 0.6256453394889832\n",
      "Step 2301 | Loss: 0.6142444610595703\n",
      "Step 2401 | Loss: 0.5139234662055969\n",
      "0.6242018508652818 0.88\n",
      "Step 1 | Loss: 1.173144817352295\n",
      "Step 101 | Loss: 0.8032287955284119\n",
      "Step 201 | Loss: 0.8653532862663269\n",
      "Step 301 | Loss: 1.138297200202942\n",
      "Step 401 | Loss: 1.0792369842529297\n",
      "Step 501 | Loss: 1.0383541584014893\n",
      "Step 601 | Loss: 1.0867903232574463\n",
      "Step 701 | Loss: 0.9905414581298828\n",
      "Step 801 | Loss: 1.0194138288497925\n",
      "Step 901 | Loss: 0.9809922575950623\n",
      "Step 1001 | Loss: 1.038711428642273\n",
      "Step 1101 | Loss: 0.9994131326675415\n",
      "Step 1201 | Loss: 1.0252211093902588\n",
      "Step 1301 | Loss: 1.034655213356018\n",
      "Step 1401 | Loss: 0.9414710998535156\n",
      "Step 1501 | Loss: 1.0283266305923462\n",
      "Step 1601 | Loss: 0.9495724439620972\n",
      "Step 1701 | Loss: 1.0480347871780396\n",
      "Step 1801 | Loss: 0.9472007751464844\n",
      "Step 1901 | Loss: 0.9982970952987671\n",
      "Step 2001 | Loss: 0.9823191165924072\n",
      "Step 2101 | Loss: 0.9235724210739136\n",
      "Step 2201 | Loss: 0.8818300366401672\n",
      "Step 2301 | Loss: 1.0268386602401733\n",
      "Step 2401 | Loss: 1.009622573852539\n",
      "0.9548618497308685 0.5725\n",
      "Step 1 | Loss: 1.2650728225708008\n",
      "Step 101 | Loss: 0.9792863726615906\n",
      "Step 201 | Loss: 0.8589293360710144\n",
      "Step 301 | Loss: 0.9161470532417297\n",
      "Step 401 | Loss: 1.065482497215271\n",
      "Step 501 | Loss: 0.7534016370773315\n",
      "Step 601 | Loss: 1.140552043914795\n",
      "Step 701 | Loss: 1.1177113056182861\n",
      "Step 801 | Loss: 1.07265043258667\n",
      "Step 901 | Loss: 0.7496220469474792\n",
      "Step 1001 | Loss: 0.6646406650543213\n",
      "Step 1101 | Loss: 0.9445434212684631\n",
      "Step 1201 | Loss: 1.116532802581787\n",
      "Step 1301 | Loss: 0.6295867562294006\n",
      "Step 1401 | Loss: 1.0382884740829468\n",
      "Step 1501 | Loss: 0.9787452816963196\n",
      "Step 1601 | Loss: 0.8692615032196045\n",
      "Step 1701 | Loss: 1.1865339279174805\n",
      "Step 1801 | Loss: 0.7181935906410217\n",
      "Step 1901 | Loss: 1.161433219909668\n",
      "Step 2001 | Loss: 1.1811882257461548\n",
      "Step 2101 | Loss: 0.7782272696495056\n",
      "Step 2201 | Loss: 1.2035681009292603\n",
      "Step 2301 | Loss: 1.238775610923767\n",
      "Step 2401 | Loss: 0.9271661043167114\n",
      "0.9202938769619091 0.6775\n",
      "Step 1 | Loss: 1.4698032140731812\n",
      "Step 101 | Loss: 0.8524415493011475\n",
      "Step 201 | Loss: 1.0708019733428955\n",
      "Step 301 | Loss: 1.1301851272583008\n",
      "Step 401 | Loss: 0.7882468104362488\n",
      "Step 501 | Loss: 0.8350667357444763\n",
      "Step 601 | Loss: 1.1315525770187378\n",
      "Step 701 | Loss: 0.8425253629684448\n",
      "Step 801 | Loss: 0.6740648746490479\n",
      "Step 901 | Loss: 0.5466599464416504\n",
      "Step 1001 | Loss: 1.2056795358657837\n",
      "Step 1101 | Loss: 1.238700270652771\n",
      "Step 1201 | Loss: 0.8720189929008484\n",
      "Step 1301 | Loss: 1.0800211429595947\n",
      "Step 1401 | Loss: 1.0799245834350586\n",
      "Step 1501 | Loss: 1.0971274375915527\n",
      "Step 1601 | Loss: 1.0066763162612915\n",
      "Step 1701 | Loss: 1.2302210330963135\n",
      "Step 1801 | Loss: 0.7118542790412903\n",
      "Step 1901 | Loss: 1.147431492805481\n",
      "Step 2001 | Loss: 1.2795647382736206\n",
      "Step 2101 | Loss: 1.1002016067504883\n",
      "Step 2201 | Loss: 0.8346235156059265\n",
      "Step 2301 | Loss: 1.283647894859314\n",
      "Step 2401 | Loss: 1.04109787940979\n",
      "0.9103993173381334 0.68\n",
      "Step 1 | Loss: 1.827599287033081\n",
      "Step 101 | Loss: 0.9555374979972839\n",
      "Step 201 | Loss: 1.2434250116348267\n",
      "Step 301 | Loss: 0.8546021580696106\n",
      "Step 401 | Loss: 0.7023751139640808\n",
      "Step 501 | Loss: 1.1854832172393799\n",
      "Step 601 | Loss: 1.0256240367889404\n",
      "Step 701 | Loss: 0.6614643335342407\n",
      "Step 801 | Loss: 0.8812556266784668\n",
      "Step 901 | Loss: 1.3019753694534302\n",
      "Step 1001 | Loss: 1.1277332305908203\n",
      "Step 1101 | Loss: 0.8793049454689026\n",
      "Step 1201 | Loss: 0.9231804013252258\n",
      "Step 1301 | Loss: 1.2830275297164917\n",
      "Step 1401 | Loss: 1.0671457052230835\n",
      "Step 1501 | Loss: 1.116300106048584\n",
      "Step 1601 | Loss: 0.6471349000930786\n",
      "Step 1701 | Loss: 0.7863744497299194\n",
      "Step 1801 | Loss: 0.7533671855926514\n",
      "Step 1901 | Loss: 0.906014084815979\n",
      "Step 2001 | Loss: 1.1604506969451904\n",
      "Step 2101 | Loss: 0.7087382674217224\n",
      "Step 2201 | Loss: 0.8311653137207031\n",
      "Step 2301 | Loss: 1.049152135848999\n",
      "Step 2401 | Loss: 1.0234614610671997\n",
      "0.8989180063185379 0.6925\n",
      "Step 1 | Loss: 1.0807510614395142\n",
      "Step 101 | Loss: 0.8986683487892151\n",
      "Step 201 | Loss: 1.2116248607635498\n",
      "Step 301 | Loss: 1.2879372835159302\n",
      "Step 401 | Loss: 0.9737699031829834\n",
      "Step 501 | Loss: 1.465739130973816\n",
      "Step 601 | Loss: 0.9592592120170593\n",
      "Step 701 | Loss: 0.9785457253456116\n",
      "Step 801 | Loss: 1.0736782550811768\n",
      "Step 901 | Loss: 1.2427637577056885\n",
      "Step 1001 | Loss: 0.8025065660476685\n",
      "Step 1101 | Loss: 0.5773138999938965\n",
      "Step 1201 | Loss: 0.7193876504898071\n",
      "Step 1301 | Loss: 0.7544000744819641\n",
      "Step 1401 | Loss: 0.7944141030311584\n",
      "Step 1501 | Loss: 0.6985830068588257\n",
      "Step 1601 | Loss: 0.9730406403541565\n",
      "Step 1701 | Loss: 0.928769588470459\n",
      "Step 1801 | Loss: 0.9521893262863159\n",
      "Step 1901 | Loss: 0.934745192527771\n",
      "Step 2001 | Loss: 1.1301902532577515\n",
      "Step 2101 | Loss: 1.0168941020965576\n",
      "Step 2201 | Loss: 1.0169754028320312\n",
      "Step 2301 | Loss: 1.1618300676345825\n",
      "Step 2401 | Loss: 1.0001171827316284\n",
      "0.8961630749810781 0.685\n",
      "Step 1 | Loss: 1.6157841682434082\n",
      "Step 101 | Loss: 0.9930754899978638\n",
      "Step 201 | Loss: 0.8684184551239014\n",
      "Step 301 | Loss: 0.9477795362472534\n",
      "Step 401 | Loss: 0.9488758444786072\n",
      "Step 501 | Loss: 1.0080726146697998\n",
      "Step 601 | Loss: 1.1486119031906128\n",
      "Step 701 | Loss: 1.0716564655303955\n",
      "Step 801 | Loss: 1.0246312618255615\n",
      "Step 901 | Loss: 1.1512209177017212\n",
      "Step 1001 | Loss: 0.9387218356132507\n",
      "Step 1101 | Loss: 0.9511460065841675\n",
      "Step 1201 | Loss: 0.9882831573486328\n",
      "Step 1301 | Loss: 0.9862639307975769\n",
      "Step 1401 | Loss: 0.9769741296768188\n",
      "Step 1501 | Loss: 1.367255449295044\n",
      "Step 1601 | Loss: 1.0156843662261963\n",
      "Step 1701 | Loss: 1.2852513790130615\n",
      "Step 1801 | Loss: 0.8422466516494751\n",
      "Step 1901 | Loss: 0.9125148057937622\n",
      "Step 2001 | Loss: 1.0240007638931274\n",
      "Step 2101 | Loss: 1.3054265975952148\n",
      "Step 2201 | Loss: 0.873055100440979\n",
      "Step 2301 | Loss: 0.9199553728103638\n",
      "Step 2401 | Loss: 1.0784735679626465\n",
      "0.9158910399446621 0.67\n",
      "Step 1 | Loss: 1.186059594154358\n",
      "Step 101 | Loss: 0.6934856176376343\n",
      "Step 201 | Loss: 0.5109155178070068\n",
      "Step 301 | Loss: 0.4179795980453491\n",
      "Step 401 | Loss: 0.5396665930747986\n",
      "Step 501 | Loss: 0.43158870935440063\n",
      "Step 601 | Loss: 0.5440022945404053\n",
      "Step 701 | Loss: 0.47486525774002075\n",
      "Step 801 | Loss: 0.36313948035240173\n",
      "Step 901 | Loss: 0.4960096478462219\n",
      "Step 1001 | Loss: 0.29833200573921204\n",
      "Step 1101 | Loss: 0.487842857837677\n",
      "Step 1201 | Loss: 0.4740793704986572\n",
      "Step 1301 | Loss: 0.5683742165565491\n",
      "Step 1401 | Loss: 0.5386481285095215\n",
      "Step 1501 | Loss: 0.4531843364238739\n",
      "Step 1601 | Loss: 0.3475126028060913\n",
      "Step 1701 | Loss: 0.4186408221721649\n",
      "Step 1801 | Loss: 0.5148305296897888\n",
      "Step 1901 | Loss: 0.404107928276062\n",
      "Step 2001 | Loss: 0.3669417202472687\n",
      "Step 2101 | Loss: 0.4763999879360199\n",
      "Step 2201 | Loss: 0.3217308819293976\n",
      "Step 2301 | Loss: 0.381341814994812\n",
      "Step 2401 | Loss: 0.4700009524822235\n",
      "0.4349650006103667 0.91\n",
      "Step 1 | Loss: 1.1815721988677979\n",
      "Step 101 | Loss: 0.5316187143325806\n",
      "Step 201 | Loss: 0.561763346195221\n",
      "Step 301 | Loss: 0.6273496747016907\n",
      "Step 401 | Loss: 0.6336894631385803\n",
      "Step 501 | Loss: 0.5042102932929993\n",
      "Step 601 | Loss: 0.7106746435165405\n",
      "Step 701 | Loss: 0.3464069962501526\n",
      "Step 801 | Loss: 0.4675878882408142\n",
      "Step 901 | Loss: 0.6001042127609253\n",
      "Step 1001 | Loss: 0.5389808416366577\n",
      "Step 1101 | Loss: 0.5402445793151855\n",
      "Step 1201 | Loss: 0.4880877733230591\n",
      "Step 1301 | Loss: 0.5386325120925903\n",
      "Step 1401 | Loss: 0.520480215549469\n",
      "Step 1501 | Loss: 0.4021841287612915\n",
      "Step 1601 | Loss: 0.4107876718044281\n",
      "Step 1701 | Loss: 0.47528356313705444\n",
      "Step 1801 | Loss: 0.547173023223877\n",
      "Step 1901 | Loss: 0.43571802973747253\n",
      "Step 2001 | Loss: 0.5203818082809448\n",
      "Step 2101 | Loss: 0.5005372166633606\n",
      "Step 2201 | Loss: 0.552300751209259\n",
      "Step 2301 | Loss: 0.5425559282302856\n",
      "Step 2401 | Loss: 0.45816555619239807\n",
      "0.4611187888011648 0.885\n",
      "Step 1 | Loss: 1.225852131843567\n",
      "Step 101 | Loss: 0.5821787714958191\n",
      "Step 201 | Loss: 0.3746179938316345\n",
      "Step 301 | Loss: 0.5165404081344604\n",
      "Step 401 | Loss: 0.5177693367004395\n",
      "Step 501 | Loss: 0.5104614496231079\n",
      "Step 601 | Loss: 0.5182694792747498\n",
      "Step 701 | Loss: 0.4469423294067383\n",
      "Step 801 | Loss: 0.45593076944351196\n",
      "Step 901 | Loss: 0.46900445222854614\n",
      "Step 1001 | Loss: 0.5620207786560059\n",
      "Step 1101 | Loss: 0.40741366147994995\n",
      "Step 1201 | Loss: 0.4802800714969635\n",
      "Step 1301 | Loss: 0.6711046695709229\n",
      "Step 1401 | Loss: 0.4630957245826721\n",
      "Step 1501 | Loss: 0.49069949984550476\n",
      "Step 1601 | Loss: 0.5600908994674683\n",
      "Step 1701 | Loss: 0.37350308895111084\n",
      "Step 1801 | Loss: 0.8036864399909973\n",
      "Step 1901 | Loss: 0.4155207872390747\n",
      "Step 2001 | Loss: 0.5033693313598633\n",
      "Step 2101 | Loss: 0.29051148891448975\n",
      "Step 2201 | Loss: 0.34180349111557007\n",
      "Step 2301 | Loss: 0.2683866620063782\n",
      "Step 2401 | Loss: 0.5563967823982239\n",
      "0.44290052134909674 0.8925\n",
      "Step 1 | Loss: 0.9784187078475952\n",
      "Step 101 | Loss: 0.5591470003128052\n",
      "Step 201 | Loss: 0.5672070980072021\n",
      "Step 301 | Loss: 0.6819397211074829\n",
      "Step 401 | Loss: 0.8004578351974487\n",
      "Step 501 | Loss: 0.5422325730323792\n",
      "Step 601 | Loss: 0.43876391649246216\n",
      "Step 701 | Loss: 0.34446439146995544\n",
      "Step 801 | Loss: 0.49884673953056335\n",
      "Step 901 | Loss: 0.502798318862915\n",
      "Step 1001 | Loss: 0.3976009488105774\n",
      "Step 1101 | Loss: 0.3300495147705078\n",
      "Step 1201 | Loss: 0.32291024923324585\n",
      "Step 1301 | Loss: 0.6023677587509155\n",
      "Step 1401 | Loss: 0.34815943241119385\n",
      "Step 1501 | Loss: 0.46785590052604675\n",
      "Step 1601 | Loss: 0.4124903678894043\n",
      "Step 1701 | Loss: 0.581332802772522\n",
      "Step 1801 | Loss: 0.45003560185432434\n",
      "Step 1901 | Loss: 0.49886733293533325\n",
      "Step 2001 | Loss: 0.39509862661361694\n",
      "Step 2101 | Loss: 0.4509231746196747\n",
      "Step 2201 | Loss: 0.48079782724380493\n",
      "Step 2301 | Loss: 0.5197287797927856\n",
      "Step 2401 | Loss: 0.47725826501846313\n",
      "0.4440631776650994 0.895\n",
      "Step 1 | Loss: 1.122404932975769\n",
      "Step 101 | Loss: 0.8100544214248657\n",
      "Step 201 | Loss: 0.5078887939453125\n",
      "Step 301 | Loss: 0.3802480697631836\n",
      "Step 401 | Loss: 0.43719765543937683\n",
      "Step 501 | Loss: 0.5031928420066833\n",
      "Step 601 | Loss: 0.36531442403793335\n",
      "Step 701 | Loss: 0.4283044934272766\n",
      "Step 801 | Loss: 0.6078135371208191\n",
      "Step 901 | Loss: 0.585144579410553\n",
      "Step 1001 | Loss: 0.4969802498817444\n",
      "Step 1101 | Loss: 0.524069607257843\n",
      "Step 1201 | Loss: 0.3774041533470154\n",
      "Step 1301 | Loss: 0.48590829968452454\n",
      "Step 1401 | Loss: 0.4696384370326996\n",
      "Step 1501 | Loss: 0.6382269859313965\n",
      "Step 1601 | Loss: 0.40196865797042847\n",
      "Step 1701 | Loss: 0.3973637521266937\n",
      "Step 1801 | Loss: 0.4302675127983093\n",
      "Step 1901 | Loss: 0.5432669520378113\n",
      "Step 2001 | Loss: 0.5206786394119263\n",
      "Step 2101 | Loss: 0.5006003379821777\n",
      "Step 2201 | Loss: 0.3833383023738861\n",
      "Step 2301 | Loss: 0.4929996728897095\n",
      "Step 2401 | Loss: 0.5392552018165588\n",
      "0.44346260809682125 0.895\n",
      "Step 1 | Loss: 1.0853809118270874\n",
      "Step 101 | Loss: 0.7466230988502502\n",
      "Step 201 | Loss: 0.687862753868103\n",
      "Step 301 | Loss: 0.6169869303703308\n",
      "Step 401 | Loss: 0.5829664468765259\n",
      "Step 501 | Loss: 0.707198441028595\n",
      "Step 601 | Loss: 0.6269861459732056\n",
      "Step 701 | Loss: 0.7205091714859009\n",
      "Step 801 | Loss: 0.6669290065765381\n",
      "Step 901 | Loss: 0.7100704908370972\n",
      "Step 1001 | Loss: 0.6211210489273071\n",
      "Step 1101 | Loss: 0.5992725491523743\n",
      "Step 1201 | Loss: 0.6001885533332825\n",
      "Step 1301 | Loss: 0.6061517596244812\n",
      "Step 1401 | Loss: 0.6180974245071411\n",
      "Step 1501 | Loss: 0.7221848964691162\n",
      "Step 1601 | Loss: 0.5583195090293884\n",
      "Step 1701 | Loss: 0.6312302947044373\n",
      "Step 1801 | Loss: 0.6876966953277588\n",
      "Step 1901 | Loss: 0.537546694278717\n",
      "Step 2001 | Loss: 0.5461058020591736\n",
      "Step 2101 | Loss: 0.5953244566917419\n",
      "Step 2201 | Loss: 0.6477941274642944\n",
      "Step 2301 | Loss: 0.5017729997634888\n",
      "Step 2401 | Loss: 0.517664909362793\n",
      "0.5442178069963723 0.8875\n",
      "Step 1 | Loss: 1.1818746328353882\n",
      "Step 101 | Loss: 0.8891973495483398\n",
      "Step 201 | Loss: 0.8157658576965332\n",
      "Step 301 | Loss: 0.698317289352417\n",
      "Step 401 | Loss: 0.7497004866600037\n",
      "Step 501 | Loss: 0.6271902918815613\n",
      "Step 601 | Loss: 0.6102312207221985\n",
      "Step 701 | Loss: 0.6974923610687256\n",
      "Step 801 | Loss: 0.6568963527679443\n",
      "Step 901 | Loss: 0.5772738456726074\n",
      "Step 1001 | Loss: 0.5258368253707886\n",
      "Step 1101 | Loss: 0.6712575554847717\n",
      "Step 1201 | Loss: 0.6461827754974365\n",
      "Step 1301 | Loss: 0.6135926246643066\n",
      "Step 1401 | Loss: 0.6076428890228271\n",
      "Step 1501 | Loss: 0.7457259893417358\n",
      "Step 1601 | Loss: 0.6780766248703003\n",
      "Step 1701 | Loss: 0.473132848739624\n",
      "Step 1801 | Loss: 0.7190406322479248\n",
      "Step 1901 | Loss: 0.5188582539558411\n",
      "Step 2001 | Loss: 0.5255218148231506\n",
      "Step 2101 | Loss: 0.5238380432128906\n",
      "Step 2201 | Loss: 0.5989744663238525\n",
      "Step 2301 | Loss: 0.7174564599990845\n",
      "Step 2401 | Loss: 0.5914509296417236\n",
      "0.6049100839540309 0.8675\n",
      "Step 1 | Loss: 0.6966420412063599\n",
      "Step 101 | Loss: 0.6569539904594421\n",
      "Step 201 | Loss: 0.47948262095451355\n",
      "Step 301 | Loss: 0.5130302309989929\n",
      "Step 401 | Loss: 0.5639779567718506\n",
      "Step 501 | Loss: 0.469282329082489\n",
      "Step 601 | Loss: 0.4795750379562378\n",
      "Step 701 | Loss: 0.4239504337310791\n",
      "Step 801 | Loss: 0.4414210915565491\n",
      "Step 901 | Loss: 0.5003570318222046\n",
      "Step 1001 | Loss: 0.5105942487716675\n",
      "Step 1101 | Loss: 0.4317900836467743\n",
      "Step 1201 | Loss: 0.5444115400314331\n",
      "Step 1301 | Loss: 0.39809921383857727\n",
      "Step 1401 | Loss: 0.4172923266887665\n",
      "Step 1501 | Loss: 0.5525021553039551\n",
      "Step 1601 | Loss: 0.40619438886642456\n",
      "Step 1701 | Loss: 0.5257712602615356\n",
      "Step 1801 | Loss: 0.3740946650505066\n",
      "Step 1901 | Loss: 0.42670586705207825\n",
      "Step 2001 | Loss: 0.5352905988693237\n",
      "Step 2101 | Loss: 0.47514021396636963\n",
      "Step 2201 | Loss: 0.45383819937705994\n",
      "Step 2301 | Loss: 0.4886775016784668\n",
      "Step 2401 | Loss: 0.43817418813705444\n",
      "0.4338369601663171 0.9475\n",
      "Step 1 | Loss: 1.0805643796920776\n",
      "Step 101 | Loss: 0.6706398129463196\n",
      "Step 201 | Loss: 0.6270245313644409\n",
      "Step 301 | Loss: 0.5809934139251709\n",
      "Step 401 | Loss: 0.5543214678764343\n",
      "Step 501 | Loss: 0.6515612602233887\n",
      "Step 601 | Loss: 0.6010854840278625\n",
      "Step 701 | Loss: 0.7250831723213196\n",
      "Step 801 | Loss: 0.5306415557861328\n",
      "Step 901 | Loss: 0.40429288148880005\n",
      "Step 1001 | Loss: 0.515674352645874\n",
      "Step 1101 | Loss: 0.528084397315979\n",
      "Step 1201 | Loss: 0.47771820425987244\n",
      "Step 1301 | Loss: 0.4671216905117035\n",
      "Step 1401 | Loss: 0.5728088617324829\n",
      "Step 1501 | Loss: 0.6132230162620544\n",
      "Step 1601 | Loss: 0.6903916597366333\n",
      "Step 1701 | Loss: 0.44618725776672363\n",
      "Step 1801 | Loss: 0.42376720905303955\n",
      "Step 1901 | Loss: 0.5977086424827576\n",
      "Step 2001 | Loss: 0.47284817695617676\n",
      "Step 2101 | Loss: 0.45990440249443054\n",
      "Step 2201 | Loss: 0.4832667112350464\n",
      "Step 2301 | Loss: 0.5091504454612732\n",
      "Step 2401 | Loss: 0.4638150930404663\n",
      "0.5153041380277678 0.91\n",
      "Step 1 | Loss: 1.01176118850708\n",
      "Step 101 | Loss: 0.6127942800521851\n",
      "Step 201 | Loss: 0.42939114570617676\n",
      "Step 301 | Loss: 0.5504159927368164\n",
      "Step 401 | Loss: 0.4646925926208496\n",
      "Step 501 | Loss: 0.3866822123527527\n",
      "Step 601 | Loss: 0.5869728326797485\n",
      "Step 701 | Loss: 0.49667859077453613\n",
      "Step 801 | Loss: 0.5284921526908875\n",
      "Step 901 | Loss: 0.48356181383132935\n",
      "Step 1001 | Loss: 0.4533574879169464\n",
      "Step 1101 | Loss: 0.4727734923362732\n",
      "Step 1201 | Loss: 0.48073774576187134\n",
      "Step 1301 | Loss: 0.401576429605484\n",
      "Step 1401 | Loss: 0.4145975410938263\n",
      "Step 1501 | Loss: 0.4176979660987854\n",
      "Step 1601 | Loss: 0.4644387364387512\n",
      "Step 1701 | Loss: 0.3963196575641632\n",
      "Step 1801 | Loss: 0.44755324721336365\n",
      "Step 1901 | Loss: 0.4400085210800171\n",
      "Step 2001 | Loss: 0.4650069773197174\n",
      "Step 2101 | Loss: 0.5288640856742859\n",
      "Step 2201 | Loss: 0.477931410074234\n",
      "Step 2301 | Loss: 0.3930080831050873\n",
      "Step 2401 | Loss: 0.4167190492153168\n",
      "0.42909078308639564 0.9525\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 1.625\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.875\n",
      "Step 601 | Loss: 2.0\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 2.5\n",
      "Step 901 | Loss: 1.875\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 1.875\n",
      "Step 1301 | Loss: 2.375\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 1.625\n",
      "Step 1601 | Loss: 2.0\n",
      "Step 1701 | Loss: 1.25\n",
      "Step 1801 | Loss: 2.25\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 1.875\n",
      "Step 2101 | Loss: 2.375\n",
      "Step 2201 | Loss: 2.0\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 2.0\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.625\n",
      "Step 201 | Loss: 1.75\n",
      "Step 301 | Loss: 2.25\n",
      "Step 401 | Loss: 2.25\n",
      "Step 501 | Loss: 1.5\n",
      "Step 601 | Loss: 2.375\n",
      "Step 701 | Loss: 2.125\n",
      "Step 801 | Loss: 1.5\n",
      "Step 901 | Loss: 2.625\n",
      "Step 1001 | Loss: 2.25\n",
      "Step 1101 | Loss: 2.75\n",
      "Step 1201 | Loss: 2.25\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 1.5\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 1.875\n",
      "Step 1701 | Loss: 2.375\n",
      "Step 1801 | Loss: 2.0\n",
      "Step 1901 | Loss: 1.75\n",
      "Step 2001 | Loss: 1.5\n",
      "Step 2101 | Loss: 2.125\n",
      "Step 2201 | Loss: 2.375\n",
      "Step 2301 | Loss: 1.875\n",
      "Step 2401 | Loss: 1.625\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 2.5\n",
      "Step 201 | Loss: 1.75\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 2.0\n",
      "Step 701 | Loss: 2.625\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 1.875\n",
      "Step 1001 | Loss: 2.375\n",
      "Step 1101 | Loss: 1.375\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 1.75\n",
      "Step 1401 | Loss: 2.25\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 2.625\n",
      "Step 1701 | Loss: 2.5\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 1.375\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.5\n",
      "Step 2401 | Loss: 2.125\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 1.625\n",
      "Step 101 | Loss: 1.5\n",
      "Step 201 | Loss: 2.625\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.25\n",
      "Step 501 | Loss: 1.5\n",
      "Step 601 | Loss: 1.875\n",
      "Step 701 | Loss: 1.625\n",
      "Step 801 | Loss: 2.625\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 1.875\n",
      "Step 1101 | Loss: 1.875\n",
      "Step 1201 | Loss: 2.375\n",
      "Step 1301 | Loss: 1.25\n",
      "Step 1401 | Loss: 2.0\n",
      "Step 1501 | Loss: 1.875\n",
      "Step 1601 | Loss: 2.75\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 1.875\n",
      "Step 2001 | Loss: 1.75\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.25\n",
      "Step 2401 | Loss: 1.75\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 2.375\n",
      "Step 101 | Loss: 2.25\n",
      "Step 201 | Loss: 2.5\n",
      "Step 301 | Loss: 2.25\n",
      "Step 401 | Loss: 1.75\n",
      "Step 501 | Loss: 2.75\n",
      "Step 601 | Loss: 2.75\n",
      "Step 701 | Loss: 1.5\n",
      "Step 801 | Loss: 1.75\n",
      "Step 901 | Loss: 2.25\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.75\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 1.75\n",
      "Step 1401 | Loss: 2.75\n",
      "Step 1501 | Loss: 2.0\n",
      "Step 1601 | Loss: 1.75\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 2.375\n",
      "Step 1901 | Loss: 2.375\n",
      "Step 2001 | Loss: 1.875\n",
      "Step 2101 | Loss: 2.0\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.25\n",
      "Step 2401 | Loss: 1.625\n",
      "1.9711538461538463 0.5\n",
      "Step 1 | Loss: 0.5269195437431335\n",
      "Step 101 | Loss: 0.4012027382850647\n",
      "Step 201 | Loss: 0.4631689190864563\n",
      "Step 301 | Loss: 0.4788847863674164\n",
      "Step 401 | Loss: 0.511896550655365\n",
      "Step 501 | Loss: 0.18384183943271637\n",
      "Step 601 | Loss: 0.2582589387893677\n",
      "Step 701 | Loss: 0.43249058723449707\n",
      "Step 801 | Loss: 0.34444692730903625\n",
      "Step 901 | Loss: 0.3885679244995117\n",
      "Step 1001 | Loss: 0.3309266269207001\n",
      "Step 1101 | Loss: 0.3529432415962219\n",
      "Step 1201 | Loss: 0.26055023074150085\n",
      "Step 1301 | Loss: 0.29795578122138977\n",
      "Step 1401 | Loss: 0.35206174850463867\n",
      "Step 1501 | Loss: 0.3755742609500885\n",
      "Step 1601 | Loss: 0.3668488562107086\n",
      "Step 1701 | Loss: 0.24361155927181244\n",
      "Step 1801 | Loss: 0.4347347617149353\n",
      "Step 1901 | Loss: 0.23939785361289978\n",
      "Step 2001 | Loss: 0.3420580327510834\n",
      "Step 2101 | Loss: 0.2903625965118408\n",
      "Step 2201 | Loss: 0.4503536820411682\n",
      "Step 2301 | Loss: 0.32215893268585205\n",
      "Step 2401 | Loss: 0.4336680471897125\n",
      "0.3170553407133847 0.9325\n",
      "Step 1 | Loss: 0.4505600333213806\n",
      "Step 101 | Loss: 0.49538731575012207\n",
      "Step 201 | Loss: 0.4175279438495636\n",
      "Step 301 | Loss: 0.5252476930618286\n",
      "Step 401 | Loss: 0.2235717475414276\n",
      "Step 501 | Loss: 0.2657184302806854\n",
      "Step 601 | Loss: 0.3269651532173157\n",
      "Step 701 | Loss: 0.3486531376838684\n",
      "Step 801 | Loss: 0.6059083938598633\n",
      "Step 901 | Loss: 0.45980507135391235\n",
      "Step 1001 | Loss: 0.32290226221084595\n",
      "Step 1101 | Loss: 0.2416745126247406\n",
      "Step 1201 | Loss: 0.4449235796928406\n",
      "Step 1301 | Loss: 0.4016892910003662\n",
      "Step 1401 | Loss: 0.3375694751739502\n",
      "Step 1501 | Loss: 0.3284507691860199\n",
      "Step 1601 | Loss: 0.2822076678276062\n",
      "Step 1701 | Loss: 0.3808894455432892\n",
      "Step 1801 | Loss: 0.29202985763549805\n",
      "Step 1901 | Loss: 0.4446974992752075\n",
      "Step 2001 | Loss: 0.47596362233161926\n",
      "Step 2101 | Loss: 0.27995172142982483\n",
      "Step 2201 | Loss: 0.24351821839809418\n",
      "Step 2301 | Loss: 0.39190661907196045\n",
      "Step 2401 | Loss: 0.3489266335964203\n",
      "0.3995810710982496 0.86\n",
      "Step 1 | Loss: 0.9992430806159973\n",
      "Step 101 | Loss: 0.3688643276691437\n",
      "Step 201 | Loss: 0.44174471497535706\n",
      "Step 301 | Loss: 0.29731449484825134\n",
      "Step 401 | Loss: 0.29408472776412964\n",
      "Step 501 | Loss: 0.3184177279472351\n",
      "Step 601 | Loss: 0.15539446473121643\n",
      "Step 701 | Loss: 0.2640957236289978\n",
      "Step 801 | Loss: 0.36386242508888245\n",
      "Step 901 | Loss: 0.31150105595588684\n",
      "Step 1001 | Loss: 0.36315780878067017\n",
      "Step 1101 | Loss: 0.33580198884010315\n",
      "Step 1201 | Loss: 0.3914005756378174\n",
      "Step 1301 | Loss: 0.29087966680526733\n",
      "Step 1401 | Loss: 0.2271881103515625\n",
      "Step 1501 | Loss: 0.4052090644836426\n",
      "Step 1601 | Loss: 0.3040875792503357\n",
      "Step 1701 | Loss: 0.5168748497962952\n",
      "Step 1801 | Loss: 0.3096364736557007\n",
      "Step 1901 | Loss: 0.265171080827713\n",
      "Step 2001 | Loss: 0.34222644567489624\n",
      "Step 2101 | Loss: 0.29756927490234375\n",
      "Step 2201 | Loss: 0.3386920094490051\n",
      "Step 2301 | Loss: 0.2870888411998749\n",
      "Step 2401 | Loss: 0.3177551031112671\n",
      "0.3163615673041918 0.9325\n",
      "Step 1 | Loss: 0.3994981050491333\n",
      "Step 101 | Loss: 0.35612061619758606\n",
      "Step 201 | Loss: 0.3508948087692261\n",
      "Step 301 | Loss: 0.3431379795074463\n",
      "Step 401 | Loss: 0.45765578746795654\n",
      "Step 501 | Loss: 0.5070322155952454\n",
      "Step 601 | Loss: 0.34584885835647583\n",
      "Step 701 | Loss: 0.24490708112716675\n",
      "Step 801 | Loss: 0.35816389322280884\n",
      "Step 901 | Loss: 0.5914487838745117\n",
      "Step 1001 | Loss: 0.2013266533613205\n",
      "Step 1101 | Loss: 0.2588382363319397\n",
      "Step 1201 | Loss: 0.3170081675052643\n",
      "Step 1301 | Loss: 0.24311241507530212\n",
      "Step 1401 | Loss: 0.35167351365089417\n",
      "Step 1501 | Loss: 0.31227314472198486\n",
      "Step 1601 | Loss: 0.30778563022613525\n",
      "Step 1701 | Loss: 0.25828006863594055\n",
      "Step 1801 | Loss: 0.2498427778482437\n",
      "Step 1901 | Loss: 0.2966488003730774\n",
      "Step 2001 | Loss: 0.31588464975357056\n",
      "Step 2101 | Loss: 0.35873913764953613\n",
      "Step 2201 | Loss: 0.43405503034591675\n",
      "Step 2301 | Loss: 0.20579883456230164\n",
      "Step 2401 | Loss: 0.30413100123405457\n",
      "0.296518711397824 0.94\n",
      "Step 1 | Loss: 0.34945839643478394\n",
      "Step 101 | Loss: 0.4805592894554138\n",
      "Step 201 | Loss: 0.27779296040534973\n",
      "Step 301 | Loss: 0.3335925042629242\n",
      "Step 401 | Loss: 0.47014138102531433\n",
      "Step 501 | Loss: 0.30900242924690247\n",
      "Step 601 | Loss: 0.4321476221084595\n",
      "Step 701 | Loss: 0.4902603328227997\n",
      "Step 801 | Loss: 0.447475790977478\n",
      "Step 901 | Loss: 0.33678898215293884\n",
      "Step 1001 | Loss: 0.28101930022239685\n",
      "Step 1101 | Loss: 0.39575299620628357\n",
      "Step 1201 | Loss: 0.2946840226650238\n",
      "Step 1301 | Loss: 0.17632892727851868\n",
      "Step 1401 | Loss: 0.3896912932395935\n",
      "Step 1501 | Loss: 0.43993398547172546\n",
      "Step 1601 | Loss: 0.5000637769699097\n",
      "Step 1701 | Loss: 0.3577498197555542\n",
      "Step 1801 | Loss: 0.28635820746421814\n",
      "Step 1901 | Loss: 0.4924219250679016\n",
      "Step 2001 | Loss: 0.3457348048686981\n",
      "Step 2101 | Loss: 0.3599149286746979\n",
      "Step 2201 | Loss: 0.3085613250732422\n",
      "Step 2301 | Loss: 0.3036506474018097\n",
      "Step 2401 | Loss: 0.2764573097229004\n",
      "0.31714298820487563 0.935\n",
      "Step 1 | Loss: 0.541858434677124\n",
      "Step 101 | Loss: 0.39226311445236206\n",
      "Step 201 | Loss: 0.4654545485973358\n",
      "Step 301 | Loss: 0.5101591944694519\n",
      "Step 401 | Loss: 0.43182241916656494\n",
      "Step 501 | Loss: 0.50951087474823\n",
      "Step 601 | Loss: 0.3627982437610626\n",
      "Step 701 | Loss: 0.30914637446403503\n",
      "Step 801 | Loss: 0.382356733083725\n",
      "Step 901 | Loss: 0.42932409048080444\n",
      "Step 1001 | Loss: 0.5389618873596191\n",
      "Step 1101 | Loss: 0.35214513540267944\n",
      "Step 1201 | Loss: 0.3234539330005646\n",
      "Step 1301 | Loss: 0.3002166450023651\n",
      "Step 1401 | Loss: 0.4922265410423279\n",
      "Step 1501 | Loss: 0.4294355809688568\n",
      "Step 1601 | Loss: 0.32790660858154297\n",
      "Step 1701 | Loss: 0.47058749198913574\n",
      "Step 1801 | Loss: 0.3410756587982178\n",
      "Step 1901 | Loss: 0.3488753139972687\n",
      "Step 2001 | Loss: 0.37986961007118225\n",
      "Step 2101 | Loss: 0.3400017321109772\n",
      "Step 2201 | Loss: 0.30105239152908325\n",
      "Step 2301 | Loss: 0.40849971771240234\n",
      "Step 2401 | Loss: 0.40647992491722107\n",
      "0.3401487803167189 0.925\n",
      "Step 1 | Loss: 0.46945494413375854\n",
      "Step 101 | Loss: 0.3507610261440277\n",
      "Step 201 | Loss: 0.6494588851928711\n",
      "Step 301 | Loss: 0.3814389705657959\n",
      "Step 401 | Loss: 0.3812248110771179\n",
      "Step 501 | Loss: 0.3555227816104889\n",
      "Step 601 | Loss: 0.3277128040790558\n",
      "Step 701 | Loss: 0.5879362225532532\n",
      "Step 801 | Loss: 0.33543503284454346\n",
      "Step 901 | Loss: 0.38886910676956177\n",
      "Step 1001 | Loss: 0.289126992225647\n",
      "Step 1101 | Loss: 0.5495821833610535\n",
      "Step 1201 | Loss: 0.4642275273799896\n",
      "Step 1301 | Loss: 0.266345351934433\n",
      "Step 1401 | Loss: 0.3683732748031616\n",
      "Step 1501 | Loss: 0.520377516746521\n",
      "Step 1601 | Loss: 0.2820471525192261\n",
      "Step 1701 | Loss: 0.42303866147994995\n",
      "Step 1801 | Loss: 0.4591297507286072\n",
      "Step 1901 | Loss: 0.4810170531272888\n",
      "Step 2001 | Loss: 0.2995883524417877\n",
      "Step 2101 | Loss: 0.4343627989292145\n",
      "Step 2201 | Loss: 0.42577481269836426\n",
      "Step 2301 | Loss: 0.44922786951065063\n",
      "Step 2401 | Loss: 0.38379502296447754\n",
      "0.3401487859354874 0.925\n",
      "Step 1 | Loss: 0.5516650676727295\n",
      "Step 101 | Loss: 0.4286442995071411\n",
      "Step 201 | Loss: 0.4436185359954834\n",
      "Step 301 | Loss: 0.46557682752609253\n",
      "Step 401 | Loss: 0.3371284008026123\n",
      "Step 501 | Loss: 0.43922722339630127\n",
      "Step 601 | Loss: 0.4354036748409271\n",
      "Step 701 | Loss: 0.40264037251472473\n",
      "Step 801 | Loss: 0.39145368337631226\n",
      "Step 901 | Loss: 0.2837897837162018\n",
      "Step 1001 | Loss: 0.36054912209510803\n",
      "Step 1101 | Loss: 0.3637009859085083\n",
      "Step 1201 | Loss: 0.6117157936096191\n",
      "Step 1301 | Loss: 0.3517933487892151\n",
      "Step 1401 | Loss: 0.39629024267196655\n",
      "Step 1501 | Loss: 0.37689268589019775\n",
      "Step 1601 | Loss: 0.24039779603481293\n",
      "Step 1701 | Loss: 0.3464394509792328\n",
      "Step 1801 | Loss: 0.28825652599334717\n",
      "Step 1901 | Loss: 0.4594433307647705\n",
      "Step 2001 | Loss: 0.32642829418182373\n",
      "Step 2101 | Loss: 0.3864000141620636\n",
      "Step 2201 | Loss: 0.40329816937446594\n",
      "Step 2301 | Loss: 0.24076750874519348\n",
      "Step 2401 | Loss: 0.38237807154655457\n",
      "0.3401488052597218 0.925\n",
      "Step 1 | Loss: 0.4129457473754883\n",
      "Step 101 | Loss: 0.34765470027923584\n",
      "Step 201 | Loss: 0.44114458560943604\n",
      "Step 301 | Loss: 0.45832547545433044\n",
      "Step 401 | Loss: 0.3969738185405731\n",
      "Step 501 | Loss: 0.42176711559295654\n",
      "Step 601 | Loss: 0.36911797523498535\n",
      "Step 701 | Loss: 0.45561209321022034\n",
      "Step 801 | Loss: 0.45963796973228455\n",
      "Step 901 | Loss: 0.30667534470558167\n",
      "Step 1001 | Loss: 0.4765315055847168\n",
      "Step 1101 | Loss: 0.3743475675582886\n",
      "Step 1201 | Loss: 0.612833559513092\n",
      "Step 1301 | Loss: 0.3497268557548523\n",
      "Step 1401 | Loss: 0.38402774930000305\n",
      "Step 1501 | Loss: 0.43942025303840637\n",
      "Step 1601 | Loss: 0.40288305282592773\n",
      "Step 1701 | Loss: 0.3629564642906189\n",
      "Step 1801 | Loss: 0.4153551459312439\n",
      "Step 1901 | Loss: 0.5306835770606995\n",
      "Step 2001 | Loss: 0.39285388588905334\n",
      "Step 2101 | Loss: 0.3556782007217407\n",
      "Step 2201 | Loss: 0.34798663854599\n",
      "Step 2301 | Loss: 0.3151032328605652\n",
      "Step 2401 | Loss: 0.3659408688545227\n",
      "0.34014881825988297 0.925\n",
      "Step 1 | Loss: 0.412582665681839\n",
      "Step 101 | Loss: 0.42985600233078003\n",
      "Step 201 | Loss: 0.36471453309059143\n",
      "Step 301 | Loss: 0.6091603636741638\n",
      "Step 401 | Loss: 0.3496253490447998\n",
      "Step 501 | Loss: 0.3969501554965973\n",
      "Step 601 | Loss: 0.5281013250350952\n",
      "Step 701 | Loss: 0.35495424270629883\n",
      "Step 801 | Loss: 0.4642678499221802\n",
      "Step 901 | Loss: 0.5234872698783875\n",
      "Step 1001 | Loss: 0.2864465117454529\n",
      "Step 1101 | Loss: 0.4150044322013855\n",
      "Step 1201 | Loss: 0.3321352005004883\n",
      "Step 1301 | Loss: 0.2944038510322571\n",
      "Step 1401 | Loss: 0.44946908950805664\n",
      "Step 1501 | Loss: 0.4505167007446289\n",
      "Step 1601 | Loss: 0.3987015187740326\n",
      "Step 1701 | Loss: 0.34414902329444885\n",
      "Step 1801 | Loss: 0.33733293414115906\n",
      "Step 1901 | Loss: 0.40194106101989746\n",
      "Step 2001 | Loss: 0.38503947854042053\n",
      "Step 2101 | Loss: 0.3494449555873871\n",
      "Step 2201 | Loss: 0.47523584961891174\n",
      "Step 2301 | Loss: 0.26772341132164\n",
      "Step 2401 | Loss: 0.43898874521255493\n",
      "0.34014881757842935 0.925\n",
      "Step 1 | Loss: 0.9941843748092651\n",
      "Step 101 | Loss: 0.7279855608940125\n",
      "Step 201 | Loss: 0.6702239513397217\n",
      "Step 301 | Loss: 0.634576678276062\n",
      "Step 401 | Loss: 0.7312331199645996\n",
      "Step 501 | Loss: 0.5815000534057617\n",
      "Step 601 | Loss: 0.5315017700195312\n",
      "Step 701 | Loss: 0.36366239190101624\n",
      "Step 801 | Loss: 0.3706182837486267\n",
      "Step 901 | Loss: 0.28826916217803955\n",
      "Step 1001 | Loss: 0.3844422399997711\n",
      "Step 1101 | Loss: 0.44403231143951416\n",
      "Step 1201 | Loss: 0.44850587844848633\n",
      "Step 1301 | Loss: 0.30176740884780884\n",
      "Step 1401 | Loss: 0.3326139748096466\n",
      "Step 1501 | Loss: 0.39054208993911743\n",
      "Step 1601 | Loss: 0.3244887590408325\n",
      "Step 1701 | Loss: 0.5008154511451721\n",
      "Step 1801 | Loss: 0.5039629936218262\n",
      "Step 1901 | Loss: 0.43254977464675903\n",
      "Step 2001 | Loss: 0.49840933084487915\n",
      "Step 2101 | Loss: 0.39246517419815063\n",
      "Step 2201 | Loss: 0.2514616847038269\n",
      "Step 2301 | Loss: 0.4632797837257385\n",
      "Step 2401 | Loss: 0.38825684785842896\n",
      "0.3901800147755075 0.91\n",
      "Step 1 | Loss: 1.0016117095947266\n",
      "Step 101 | Loss: 0.9725080728530884\n",
      "Step 201 | Loss: 0.6671643853187561\n",
      "Step 301 | Loss: 0.6641517877578735\n",
      "Step 401 | Loss: 0.5053132176399231\n",
      "Step 501 | Loss: 0.6323974132537842\n",
      "Step 601 | Loss: 0.5080509185791016\n",
      "Step 701 | Loss: 0.3728733956813812\n",
      "Step 801 | Loss: 0.5607824921607971\n",
      "Step 901 | Loss: 0.6205456256866455\n",
      "Step 1001 | Loss: 0.6170489192008972\n",
      "Step 1101 | Loss: 0.6833411455154419\n",
      "Step 1201 | Loss: 0.5354794263839722\n",
      "Step 1301 | Loss: 0.5948110222816467\n",
      "Step 1401 | Loss: 0.632522702217102\n",
      "Step 1501 | Loss: 0.5966355800628662\n",
      "Step 1601 | Loss: 0.5812954902648926\n",
      "Step 1701 | Loss: 0.5721074342727661\n",
      "Step 1801 | Loss: 0.5539034008979797\n",
      "Step 1901 | Loss: 0.7970083951950073\n",
      "Step 2001 | Loss: 0.6566981077194214\n",
      "Step 2101 | Loss: 0.4275827407836914\n",
      "Step 2201 | Loss: 0.8916468024253845\n",
      "Step 2301 | Loss: 0.8757868409156799\n",
      "Step 2401 | Loss: 0.3908170163631439\n",
      "0.5696496424667976 0.7875\n",
      "Step 1 | Loss: 1.0454446077346802\n",
      "Step 101 | Loss: 0.6349466443061829\n",
      "Step 201 | Loss: 0.3853451907634735\n",
      "Step 301 | Loss: 0.4199361801147461\n",
      "Step 401 | Loss: 0.4269525110721588\n",
      "Step 501 | Loss: 0.42976102232933044\n",
      "Step 601 | Loss: 0.4992843568325043\n",
      "Step 701 | Loss: 0.31602463126182556\n",
      "Step 801 | Loss: 0.4581194519996643\n",
      "Step 901 | Loss: 0.5127662420272827\n",
      "Step 1001 | Loss: 0.4282536804676056\n",
      "Step 1101 | Loss: 0.35813626646995544\n",
      "Step 1201 | Loss: 0.43119144439697266\n",
      "Step 1301 | Loss: 0.38124364614486694\n",
      "Step 1401 | Loss: 0.5154544115066528\n",
      "Step 1501 | Loss: 0.5454270243644714\n",
      "Step 1601 | Loss: 0.3514547348022461\n",
      "Step 1701 | Loss: 0.4860766530036926\n",
      "Step 1801 | Loss: 0.5215772390365601\n",
      "Step 1901 | Loss: 0.4611836373806\n",
      "Step 2001 | Loss: 0.30228960514068604\n",
      "Step 2101 | Loss: 0.4114597737789154\n",
      "Step 2201 | Loss: 0.5031574964523315\n",
      "Step 2301 | Loss: 0.760427713394165\n",
      "Step 2401 | Loss: 0.24908745288848877\n",
      "0.4713471468060366 0.8625\n",
      "Step 1 | Loss: 1.3270158767700195\n",
      "Step 101 | Loss: 1.2407681941986084\n",
      "Step 201 | Loss: 0.6604819297790527\n",
      "Step 301 | Loss: 0.36896783113479614\n",
      "Step 401 | Loss: 0.46334108710289\n",
      "Step 501 | Loss: 0.49152693152427673\n",
      "Step 601 | Loss: 0.36037975549697876\n",
      "Step 701 | Loss: 0.4263319671154022\n",
      "Step 801 | Loss: 0.5640345215797424\n",
      "Step 901 | Loss: 0.2781282067298889\n",
      "Step 1001 | Loss: 0.32904475927352905\n",
      "Step 1101 | Loss: 0.514162003993988\n",
      "Step 1201 | Loss: 0.3793039321899414\n",
      "Step 1301 | Loss: 0.45063379406929016\n",
      "Step 1401 | Loss: 0.5715288519859314\n",
      "Step 1501 | Loss: 0.45676907896995544\n",
      "Step 1601 | Loss: 0.3394183814525604\n",
      "Step 1701 | Loss: 0.604078471660614\n",
      "Step 1801 | Loss: 0.49646180868148804\n",
      "Step 1901 | Loss: 0.41836264729499817\n",
      "Step 2001 | Loss: 0.6334273219108582\n",
      "Step 2101 | Loss: 0.552649974822998\n",
      "Step 2201 | Loss: 0.5345617532730103\n",
      "Step 2301 | Loss: 0.3297611474990845\n",
      "Step 2401 | Loss: 0.43115949630737305\n",
      "0.4770521634992141 0.86\n",
      "Step 1 | Loss: 0.8657065629959106\n",
      "Step 101 | Loss: 0.8032969832420349\n",
      "Step 201 | Loss: 0.9336423873901367\n",
      "Step 301 | Loss: 0.5752288103103638\n",
      "Step 401 | Loss: 0.341961145401001\n",
      "Step 501 | Loss: 0.48146188259124756\n",
      "Step 601 | Loss: 0.3964551091194153\n",
      "Step 701 | Loss: 0.39001137018203735\n",
      "Step 801 | Loss: 0.46436625719070435\n",
      "Step 901 | Loss: 0.4378698170185089\n",
      "Step 1001 | Loss: 0.32078438997268677\n",
      "Step 1101 | Loss: 0.5479556322097778\n",
      "Step 1201 | Loss: 0.37927478551864624\n",
      "Step 1301 | Loss: 0.4623706340789795\n",
      "Step 1401 | Loss: 0.2746323049068451\n",
      "Step 1501 | Loss: 0.2828661799430847\n",
      "Step 1601 | Loss: 0.44111454486846924\n",
      "Step 1701 | Loss: 0.31193795800209045\n",
      "Step 1801 | Loss: 0.41368746757507324\n",
      "Step 1901 | Loss: 0.33614879846572876\n",
      "Step 2001 | Loss: 0.3250848054885864\n",
      "Step 2101 | Loss: 0.4166453778743744\n",
      "Step 2201 | Loss: 0.3306313455104828\n",
      "Step 2301 | Loss: 0.24785476922988892\n",
      "Step 2401 | Loss: 0.3307790756225586\n",
      "0.3881149536646134 0.9025\n",
      "Step 1 | Loss: 0.9705047607421875\n",
      "Step 101 | Loss: 0.42452821135520935\n",
      "Step 201 | Loss: 0.38496917486190796\n",
      "Step 301 | Loss: 0.3617497682571411\n",
      "Step 401 | Loss: 0.2354082614183426\n",
      "Step 501 | Loss: 0.3176661431789398\n",
      "Step 601 | Loss: 0.186527281999588\n",
      "Step 701 | Loss: 0.24332886934280396\n",
      "Step 801 | Loss: 0.30724477767944336\n",
      "Step 901 | Loss: 0.2996932566165924\n",
      "Step 1001 | Loss: 0.2322091907262802\n",
      "Step 1101 | Loss: 0.2428104430437088\n",
      "Step 1201 | Loss: 0.19037635624408722\n",
      "Step 1301 | Loss: 0.3530280590057373\n",
      "Step 1401 | Loss: 0.3406487703323364\n",
      "Step 1501 | Loss: 0.30661922693252563\n",
      "Step 1601 | Loss: 0.3693932294845581\n",
      "Step 1701 | Loss: 0.3899818956851959\n",
      "Step 1801 | Loss: 0.3140566051006317\n",
      "Step 1901 | Loss: 0.37007948756217957\n",
      "Step 2001 | Loss: 0.33146771788597107\n",
      "Step 2101 | Loss: 0.2833153307437897\n",
      "Step 2201 | Loss: 0.18413977324962616\n",
      "Step 2301 | Loss: 0.4081195294857025\n",
      "Step 2401 | Loss: 0.4376158118247986\n",
      "0.30153370548155434 0.8975\n",
      "Step 1 | Loss: 0.9203928112983704\n",
      "Step 101 | Loss: 0.5719760060310364\n",
      "Step 201 | Loss: 0.4313162863254547\n",
      "Step 301 | Loss: 0.2873832583427429\n",
      "Step 401 | Loss: 0.5373157262802124\n",
      "Step 501 | Loss: 0.2717331647872925\n",
      "Step 601 | Loss: 0.2705516815185547\n",
      "Step 701 | Loss: 0.5511826276779175\n",
      "Step 801 | Loss: 0.14880073070526123\n",
      "Step 901 | Loss: 0.32270491123199463\n",
      "Step 1001 | Loss: 0.5476382970809937\n",
      "Step 1101 | Loss: 0.5027814507484436\n",
      "Step 1201 | Loss: 0.3669534921646118\n",
      "Step 1301 | Loss: 0.4678686857223511\n",
      "Step 1401 | Loss: 0.46678274869918823\n",
      "Step 1501 | Loss: 0.4199120104312897\n",
      "Step 1601 | Loss: 0.41572508215904236\n",
      "Step 1701 | Loss: 0.3534103035926819\n",
      "Step 1801 | Loss: 0.5434297919273376\n",
      "Step 1901 | Loss: 0.28391894698143005\n",
      "Step 2001 | Loss: 0.1899988055229187\n",
      "Step 2101 | Loss: 0.661527156829834\n",
      "Step 2201 | Loss: 0.5377798080444336\n",
      "Step 2301 | Loss: 0.5382270812988281\n",
      "Step 2401 | Loss: 0.5788545608520508\n",
      "0.40040195394732475 0.8575\n",
      "Step 1 | Loss: 0.7934913635253906\n",
      "Step 101 | Loss: 0.30907678604125977\n",
      "Step 201 | Loss: 0.28446030616760254\n",
      "Step 301 | Loss: 0.2639075517654419\n",
      "Step 401 | Loss: 0.2803920805454254\n",
      "Step 501 | Loss: 0.2862839996814728\n",
      "Step 601 | Loss: 0.3289012312889099\n",
      "Step 701 | Loss: 0.4229170083999634\n",
      "Step 801 | Loss: 0.3919995427131653\n",
      "Step 901 | Loss: 0.3803020715713501\n",
      "Step 1001 | Loss: 0.34351807832717896\n",
      "Step 1101 | Loss: 0.22676628828048706\n",
      "Step 1201 | Loss: 0.2935330271720886\n",
      "Step 1301 | Loss: 0.24602040648460388\n",
      "Step 1401 | Loss: 0.22410841286182404\n",
      "Step 1501 | Loss: 0.2448723018169403\n",
      "Step 1601 | Loss: 0.37316325306892395\n",
      "Step 1701 | Loss: 0.2920484244823456\n",
      "Step 1801 | Loss: 0.3326041102409363\n",
      "Step 1901 | Loss: 0.21825255453586578\n",
      "Step 2001 | Loss: 0.3663320541381836\n",
      "Step 2101 | Loss: 0.30093544721603394\n",
      "Step 2201 | Loss: 0.3396896421909332\n",
      "Step 2301 | Loss: 0.3676387071609497\n",
      "Step 2401 | Loss: 0.3808142840862274\n",
      "0.28663654586987863 0.915\n",
      "Step 1 | Loss: 1.6028449535369873\n",
      "Step 101 | Loss: 0.5623710751533508\n",
      "Step 201 | Loss: 0.6835821866989136\n",
      "Step 301 | Loss: 0.31312984228134155\n",
      "Step 401 | Loss: 0.3255564868450165\n",
      "Step 501 | Loss: 0.41006335616111755\n",
      "Step 601 | Loss: 0.28499752283096313\n",
      "Step 701 | Loss: 0.3218003511428833\n",
      "Step 801 | Loss: 0.36952224373817444\n",
      "Step 901 | Loss: 0.20492203533649445\n",
      "Step 1001 | Loss: 0.3136497437953949\n",
      "Step 1101 | Loss: 0.2575474679470062\n",
      "Step 1201 | Loss: 0.2554255425930023\n",
      "Step 1301 | Loss: 0.34837955236434937\n",
      "Step 1401 | Loss: 0.3429047763347626\n",
      "Step 1501 | Loss: 0.2211245745420456\n",
      "Step 1601 | Loss: 0.36054137349128723\n",
      "Step 1701 | Loss: 0.20962780714035034\n",
      "Step 1801 | Loss: 0.30982786417007446\n",
      "Step 1901 | Loss: 0.36662349104881287\n",
      "Step 2001 | Loss: 0.32737091183662415\n",
      "Step 2101 | Loss: 0.23642006516456604\n",
      "Step 2201 | Loss: 0.3197571933269501\n",
      "Step 2301 | Loss: 0.335355281829834\n",
      "Step 2401 | Loss: 0.3157519996166229\n",
      "0.2743177678422363 0.935\n",
      "Step 1 | Loss: 1.234116792678833\n",
      "Step 101 | Loss: 0.3834323287010193\n",
      "Step 201 | Loss: 0.3753836750984192\n",
      "Step 301 | Loss: 0.3544211983680725\n",
      "Step 401 | Loss: 0.34938398003578186\n",
      "Step 501 | Loss: 0.31042715907096863\n",
      "Step 601 | Loss: 0.31267327070236206\n",
      "Step 701 | Loss: 0.3160485625267029\n",
      "Step 801 | Loss: 0.260114461183548\n",
      "Step 901 | Loss: 0.2967044711112976\n",
      "Step 1001 | Loss: 0.3195810616016388\n",
      "Step 1101 | Loss: 0.3338923752307892\n",
      "Step 1201 | Loss: 0.3359852433204651\n",
      "Step 1301 | Loss: 0.3582756817340851\n",
      "Step 1401 | Loss: 0.2853838801383972\n",
      "Step 1501 | Loss: 0.32026606798171997\n",
      "Step 1601 | Loss: 0.3503192365169525\n",
      "Step 1701 | Loss: 0.31974437832832336\n",
      "Step 1801 | Loss: 0.32797420024871826\n",
      "Step 1901 | Loss: 0.49138760566711426\n",
      "Step 2001 | Loss: 0.2901741862297058\n",
      "Step 2101 | Loss: 0.298525869846344\n",
      "Step 2201 | Loss: 0.2512640058994293\n",
      "Step 2301 | Loss: 0.4046517312526703\n",
      "Step 2401 | Loss: 0.2685811221599579\n",
      "0.274322815279308 0.935\n",
      "Step 1 | Loss: 1.0169038772583008\n",
      "Step 101 | Loss: 0.753939151763916\n",
      "Step 201 | Loss: 0.6200292110443115\n",
      "Step 301 | Loss: 0.6987530589103699\n",
      "Step 401 | Loss: 0.7820912599563599\n",
      "Step 501 | Loss: 0.6792231202125549\n",
      "Step 601 | Loss: 0.6871387362480164\n",
      "Step 701 | Loss: 0.6646110415458679\n",
      "Step 801 | Loss: 0.7235374450683594\n",
      "Step 901 | Loss: 0.6421408653259277\n",
      "Step 1001 | Loss: 0.6480885148048401\n",
      "Step 1101 | Loss: 0.6410942077636719\n",
      "Step 1201 | Loss: 0.685039758682251\n",
      "Step 1301 | Loss: 0.8062256574630737\n",
      "Step 1401 | Loss: 0.7241209149360657\n",
      "Step 1501 | Loss: 0.5529624223709106\n",
      "Step 1601 | Loss: 0.729681670665741\n",
      "Step 1701 | Loss: 0.7552922368049622\n",
      "Step 1801 | Loss: 0.677134096622467\n",
      "Step 1901 | Loss: 0.6871212124824524\n",
      "Step 2001 | Loss: 0.7028898000717163\n",
      "Step 2101 | Loss: 0.6972872018814087\n",
      "Step 2201 | Loss: 0.7697474956512451\n",
      "Step 2301 | Loss: 0.6227614283561707\n",
      "Step 2401 | Loss: 0.4717891216278076\n",
      "0.6581349849566933 0.8175\n",
      "Step 1 | Loss: 1.4660886526107788\n",
      "Step 101 | Loss: 1.5627775192260742\n",
      "Step 201 | Loss: 1.1945019960403442\n",
      "Step 301 | Loss: 1.0862497091293335\n",
      "Step 401 | Loss: 1.0139273405075073\n",
      "Step 501 | Loss: 0.9994558095932007\n",
      "Step 601 | Loss: 1.0007354021072388\n",
      "Step 701 | Loss: 1.002511739730835\n",
      "Step 801 | Loss: 0.9979514479637146\n",
      "Step 901 | Loss: 0.9969507455825806\n",
      "Step 1001 | Loss: 0.9954423308372498\n",
      "Step 1101 | Loss: 0.9707549214363098\n",
      "Step 1201 | Loss: 0.9953548908233643\n",
      "Step 1301 | Loss: 0.9992411732673645\n",
      "Step 1401 | Loss: 1.0067083835601807\n",
      "Step 1501 | Loss: 1.0271378755569458\n",
      "Step 1601 | Loss: 0.9884805679321289\n",
      "Step 1701 | Loss: 1.0037627220153809\n",
      "Step 1801 | Loss: 0.9969857931137085\n",
      "Step 1901 | Loss: 0.9981623888015747\n",
      "Step 2001 | Loss: 0.9995508193969727\n",
      "Step 2101 | Loss: 1.0104155540466309\n",
      "Step 2201 | Loss: 0.9976784586906433\n",
      "Step 2301 | Loss: 1.0131722688674927\n",
      "Step 2401 | Loss: 1.0000337362289429\n",
      "1.0007055125112372 0.5\n",
      "Step 1 | Loss: 1.1670100688934326\n",
      "Step 101 | Loss: 0.6464875340461731\n",
      "Step 201 | Loss: 0.7081620693206787\n",
      "Step 301 | Loss: 0.6236237287521362\n",
      "Step 401 | Loss: 0.6902658939361572\n",
      "Step 501 | Loss: 0.5637252926826477\n",
      "Step 601 | Loss: 0.6081156134605408\n",
      "Step 701 | Loss: 0.6879238486289978\n",
      "Step 801 | Loss: 0.6951384544372559\n",
      "Step 901 | Loss: 0.6499744057655334\n",
      "Step 1001 | Loss: 0.564485490322113\n",
      "Step 1101 | Loss: 0.7139568328857422\n",
      "Step 1201 | Loss: 0.6402295827865601\n",
      "Step 1301 | Loss: 0.7483519315719604\n",
      "Step 1401 | Loss: 0.6529355645179749\n",
      "Step 1501 | Loss: 0.7048324346542358\n",
      "Step 1601 | Loss: 0.6415463089942932\n",
      "Step 1701 | Loss: 0.6727800369262695\n",
      "Step 1801 | Loss: 0.6262431740760803\n",
      "Step 1901 | Loss: 0.6614818572998047\n",
      "Step 2001 | Loss: 0.6291558146476746\n",
      "Step 2101 | Loss: 0.7298528552055359\n",
      "Step 2201 | Loss: 0.5525163412094116\n",
      "Step 2301 | Loss: 0.6322725415229797\n",
      "Step 2401 | Loss: 0.6265029311180115\n",
      "0.6583175960758527 0.805\n",
      "Step 1 | Loss: 1.07407808303833\n",
      "Step 101 | Loss: 0.7836970090866089\n",
      "Step 201 | Loss: 0.7640993595123291\n",
      "Step 301 | Loss: 0.6646241545677185\n",
      "Step 401 | Loss: 0.7941453456878662\n",
      "Step 501 | Loss: 0.624341607093811\n",
      "Step 601 | Loss: 0.6498271822929382\n",
      "Step 701 | Loss: 0.626669704914093\n",
      "Step 801 | Loss: 0.6312767267227173\n",
      "Step 901 | Loss: 0.5548378825187683\n",
      "Step 1001 | Loss: 0.5836028456687927\n",
      "Step 1101 | Loss: 0.663942277431488\n",
      "Step 1201 | Loss: 0.5811176300048828\n",
      "Step 1301 | Loss: 0.6727461218833923\n",
      "Step 1401 | Loss: 0.6814647912979126\n",
      "Step 1501 | Loss: 0.537455677986145\n",
      "Step 1601 | Loss: 0.5759099125862122\n",
      "Step 1701 | Loss: 0.6219059228897095\n",
      "Step 1801 | Loss: 0.514157772064209\n",
      "Step 1901 | Loss: 0.6236456632614136\n",
      "Step 2001 | Loss: 0.614406406879425\n",
      "Step 2101 | Loss: 0.575131893157959\n",
      "Step 2201 | Loss: 0.5185748338699341\n",
      "Step 2301 | Loss: 0.7233434319496155\n",
      "Step 2401 | Loss: 0.7978121042251587\n",
      "0.6580747143840502 0.8125\n",
      "Step 1 | Loss: 1.0838193893432617\n",
      "Step 101 | Loss: 1.0322657823562622\n",
      "Step 201 | Loss: 1.006170630455017\n",
      "Step 301 | Loss: 1.0010383129119873\n",
      "Step 401 | Loss: 0.957109808921814\n",
      "Step 501 | Loss: 0.8425408601760864\n",
      "Step 601 | Loss: 0.7942564487457275\n",
      "Step 701 | Loss: 0.6331065893173218\n",
      "Step 801 | Loss: 0.6811642050743103\n",
      "Step 901 | Loss: 0.6921935677528381\n",
      "Step 1001 | Loss: 0.5757660865783691\n",
      "Step 1101 | Loss: 0.7049702405929565\n",
      "Step 1201 | Loss: 0.5802931189537048\n",
      "Step 1301 | Loss: 0.7040004134178162\n",
      "Step 1401 | Loss: 0.6759248971939087\n",
      "Step 1501 | Loss: 0.6951173543930054\n",
      "Step 1601 | Loss: 0.5450767278671265\n",
      "Step 1701 | Loss: 0.7794597148895264\n",
      "Step 1801 | Loss: 0.6744086742401123\n",
      "Step 1901 | Loss: 0.6693423986434937\n",
      "Step 2001 | Loss: 0.6255388855934143\n",
      "Step 2101 | Loss: 0.6436151266098022\n",
      "Step 2201 | Loss: 0.8051424026489258\n",
      "Step 2301 | Loss: 0.6970314383506775\n",
      "Step 2401 | Loss: 0.7068349123001099\n",
      "0.6589732253656255 0.7925\n",
      "Step 1 | Loss: 1.5184177160263062\n",
      "Step 101 | Loss: 0.8118720650672913\n",
      "Step 201 | Loss: 0.7231675386428833\n",
      "Step 301 | Loss: 0.505365788936615\n",
      "Step 401 | Loss: 0.39545053243637085\n",
      "Step 501 | Loss: 0.603855311870575\n",
      "Step 601 | Loss: 0.5959013104438782\n",
      "Step 701 | Loss: 0.27304115891456604\n",
      "Step 801 | Loss: 0.593707799911499\n",
      "Step 901 | Loss: 0.3042566776275635\n",
      "Step 1001 | Loss: 0.33664751052856445\n",
      "Step 1101 | Loss: 0.41292741894721985\n",
      "Step 1201 | Loss: 0.3270062208175659\n",
      "Step 1301 | Loss: 0.23440976440906525\n",
      "Step 1401 | Loss: 0.28411728143692017\n",
      "Step 1501 | Loss: 0.4398227632045746\n",
      "Step 1601 | Loss: 0.24286267161369324\n",
      "Step 1701 | Loss: 0.35848182439804077\n",
      "Step 1801 | Loss: 0.44757604598999023\n",
      "Step 1901 | Loss: 0.3675605058670044\n",
      "Step 2001 | Loss: 0.3568994998931885\n",
      "Step 2101 | Loss: 0.288329154253006\n",
      "Step 2201 | Loss: 0.3122067451477051\n",
      "Step 2301 | Loss: 0.3653159439563751\n",
      "Step 2401 | Loss: 0.3201853930950165\n",
      "0.3086772150148018 0.93\n",
      "Step 1 | Loss: 0.9098937511444092\n",
      "Step 101 | Loss: 0.6140725016593933\n",
      "Step 201 | Loss: 0.5431210398674011\n",
      "Step 301 | Loss: 0.6127959489822388\n",
      "Step 401 | Loss: 0.4455223083496094\n",
      "Step 501 | Loss: 0.5601985454559326\n",
      "Step 601 | Loss: 0.682925283908844\n",
      "Step 701 | Loss: 0.6316289901733398\n",
      "Step 801 | Loss: 0.4954140782356262\n",
      "Step 901 | Loss: 0.549849271774292\n",
      "Step 1001 | Loss: 0.4806389808654785\n",
      "Step 1101 | Loss: 0.5224531888961792\n",
      "Step 1201 | Loss: 0.6175412535667419\n",
      "Step 1301 | Loss: 0.4424094557762146\n",
      "Step 1401 | Loss: 0.5383926630020142\n",
      "Step 1501 | Loss: 0.47933781147003174\n",
      "Step 1601 | Loss: 0.35045596957206726\n",
      "Step 1701 | Loss: 0.46225959062576294\n",
      "Step 1801 | Loss: 0.4651264548301697\n",
      "Step 1901 | Loss: 0.4763977825641632\n",
      "Step 2001 | Loss: 0.5122385025024414\n",
      "Step 2101 | Loss: 0.45289966464042664\n",
      "Step 2201 | Loss: 0.5514264106750488\n",
      "Step 2301 | Loss: 0.47495466470718384\n",
      "Step 2401 | Loss: 0.44969284534454346\n",
      "0.494286263048811 0.9\n",
      "Step 1 | Loss: 1.1047173738479614\n",
      "Step 101 | Loss: 0.8467506170272827\n",
      "Step 201 | Loss: 0.4439082741737366\n",
      "Step 301 | Loss: 0.5263867974281311\n",
      "Step 401 | Loss: 0.5198104977607727\n",
      "Step 501 | Loss: 0.5786479711532593\n",
      "Step 601 | Loss: 0.5764764547348022\n",
      "Step 701 | Loss: 0.6386112570762634\n",
      "Step 801 | Loss: 0.47199031710624695\n",
      "Step 901 | Loss: 0.5066778063774109\n",
      "Step 1001 | Loss: 0.47593462467193604\n",
      "Step 1101 | Loss: 0.5069612860679626\n",
      "Step 1201 | Loss: 0.5121033787727356\n",
      "Step 1301 | Loss: 0.5749058127403259\n",
      "Step 1401 | Loss: 0.4701652526855469\n",
      "Step 1501 | Loss: 0.47226837277412415\n",
      "Step 1601 | Loss: 0.4722920358181\n",
      "Step 1701 | Loss: 0.5494637489318848\n",
      "Step 1801 | Loss: 0.4652476906776428\n",
      "Step 1901 | Loss: 0.4799152910709381\n",
      "Step 2001 | Loss: 0.45711955428123474\n",
      "Step 2101 | Loss: 0.4792988896369934\n",
      "Step 2201 | Loss: 0.532806932926178\n",
      "Step 2301 | Loss: 0.4743208885192871\n",
      "Step 2401 | Loss: 0.5612969398498535\n",
      "0.5030328433552248 0.9175\n",
      "Step 1 | Loss: 0.7104418873786926\n",
      "Step 101 | Loss: 0.4984678626060486\n",
      "Step 201 | Loss: 0.4158725142478943\n",
      "Step 301 | Loss: 0.3478292226791382\n",
      "Step 401 | Loss: 0.3180813789367676\n",
      "Step 501 | Loss: 0.3230368494987488\n",
      "Step 601 | Loss: 0.281688928604126\n",
      "Step 701 | Loss: 0.300274133682251\n",
      "Step 801 | Loss: 0.3859706223011017\n",
      "Step 901 | Loss: 0.22034113109111786\n",
      "Step 1001 | Loss: 0.24867591261863708\n",
      "Step 1101 | Loss: 0.23376408219337463\n",
      "Step 1201 | Loss: 0.22672200202941895\n",
      "Step 1301 | Loss: 0.501430094242096\n",
      "Step 1401 | Loss: 0.5409502983093262\n",
      "Step 1501 | Loss: 0.34753772616386414\n",
      "Step 1601 | Loss: 0.23558180034160614\n",
      "Step 1701 | Loss: 0.2545710802078247\n",
      "Step 1801 | Loss: 0.4435020089149475\n",
      "Step 1901 | Loss: 0.3847544491291046\n",
      "Step 2001 | Loss: 0.2487441599369049\n",
      "Step 2101 | Loss: 0.3796732723712921\n",
      "Step 2201 | Loss: 0.37619608640670776\n",
      "Step 2301 | Loss: 0.26351067423820496\n",
      "Step 2401 | Loss: 0.18785540759563446\n",
      "0.2818613533894899 0.935\n",
      "Step 1 | Loss: 0.7048524618148804\n",
      "Step 101 | Loss: 0.48287564516067505\n",
      "Step 201 | Loss: 0.43577098846435547\n",
      "Step 301 | Loss: 0.5294517874717712\n",
      "Step 401 | Loss: 0.3504595160484314\n",
      "Step 501 | Loss: 0.34182116389274597\n",
      "Step 601 | Loss: 0.5105958580970764\n",
      "Step 701 | Loss: 0.3757433295249939\n",
      "Step 801 | Loss: 0.5324109792709351\n",
      "Step 901 | Loss: 0.28408247232437134\n",
      "Step 1001 | Loss: 0.4657730162143707\n",
      "Step 1101 | Loss: 0.3960658311843872\n",
      "Step 1201 | Loss: 0.32046717405319214\n",
      "Step 1301 | Loss: 0.4762938618659973\n",
      "Step 1401 | Loss: 0.531291127204895\n",
      "Step 1501 | Loss: 0.4042566418647766\n",
      "Step 1601 | Loss: 0.366391658782959\n",
      "Step 1701 | Loss: 0.31276682019233704\n",
      "Step 1801 | Loss: 0.2886219620704651\n",
      "Step 1901 | Loss: 0.34365350008010864\n",
      "Step 2001 | Loss: 0.32848554849624634\n",
      "Step 2101 | Loss: 0.22411438822746277\n",
      "Step 2201 | Loss: 0.25333845615386963\n",
      "Step 2301 | Loss: 0.2848263084888458\n",
      "Step 2401 | Loss: 0.30892810225486755\n",
      "0.2663631498662429 0.94\n",
      "Step 1 | Loss: 0.8324304819107056\n",
      "Step 101 | Loss: 1.017436146736145\n",
      "Step 201 | Loss: 1.1519004106521606\n",
      "Step 301 | Loss: 1.3894561529159546\n",
      "Step 401 | Loss: 1.1751987934112549\n",
      "Step 501 | Loss: 0.9093755483627319\n",
      "Step 601 | Loss: 0.8846448063850403\n",
      "Step 701 | Loss: 1.0240647792816162\n",
      "Step 801 | Loss: 0.9578914046287537\n",
      "Step 901 | Loss: 0.7096546292304993\n",
      "Step 1001 | Loss: 0.955206036567688\n",
      "Step 1101 | Loss: 1.2278025150299072\n",
      "Step 1201 | Loss: 0.8537722826004028\n",
      "Step 1301 | Loss: 1.2566907405853271\n",
      "Step 1401 | Loss: 0.7794815897941589\n",
      "Step 1501 | Loss: 0.5963524580001831\n",
      "Step 1601 | Loss: 0.8858139514923096\n",
      "Step 1701 | Loss: 1.310402750968933\n",
      "Step 1801 | Loss: 0.5807727575302124\n",
      "Step 1901 | Loss: 0.728748083114624\n",
      "Step 2001 | Loss: 1.0571478605270386\n",
      "Step 2101 | Loss: 0.9027671813964844\n",
      "Step 2201 | Loss: 0.9193011522293091\n",
      "Step 2301 | Loss: 1.2253837585449219\n",
      "Step 2401 | Loss: 0.834687352180481\n",
      "0.9715601007114812 0.6525\n",
      "Step 1 | Loss: 1.2085167169570923\n",
      "Step 101 | Loss: 0.8994481563568115\n",
      "Step 201 | Loss: 0.8736032843589783\n",
      "Step 301 | Loss: 1.1064165830612183\n",
      "Step 401 | Loss: 1.0744705200195312\n",
      "Step 501 | Loss: 0.8947527408599854\n",
      "Step 601 | Loss: 0.9307461380958557\n",
      "Step 701 | Loss: 0.8649867177009583\n",
      "Step 801 | Loss: 0.6629132628440857\n",
      "Step 901 | Loss: 1.256606101989746\n",
      "Step 1001 | Loss: 0.9963263273239136\n",
      "Step 1101 | Loss: 0.8652424812316895\n",
      "Step 1201 | Loss: 1.046419382095337\n",
      "Step 1301 | Loss: 1.129999041557312\n",
      "Step 1401 | Loss: 1.035578966140747\n",
      "Step 1501 | Loss: 1.3034610748291016\n",
      "Step 1601 | Loss: 1.1019467115402222\n",
      "Step 1701 | Loss: 0.9718777537345886\n",
      "Step 1801 | Loss: 0.9653950929641724\n",
      "Step 1901 | Loss: 0.9693451523780823\n",
      "Step 2001 | Loss: 0.9554605484008789\n",
      "Step 2101 | Loss: 1.0965094566345215\n",
      "Step 2201 | Loss: 0.6654655933380127\n",
      "Step 2301 | Loss: 0.9081326127052307\n",
      "Step 2401 | Loss: 0.9921091198921204\n",
      "0.9726607226413047 0.6525\n",
      "Step 1 | Loss: 1.7483420372009277\n",
      "Step 101 | Loss: 0.8297436237335205\n",
      "Step 201 | Loss: 1.5195327997207642\n",
      "Step 301 | Loss: 0.7054262161254883\n",
      "Step 401 | Loss: 0.9174578189849854\n",
      "Step 501 | Loss: 0.867933988571167\n",
      "Step 601 | Loss: 0.9222267866134644\n",
      "Step 701 | Loss: 1.1591086387634277\n",
      "Step 801 | Loss: 0.6986712217330933\n",
      "Step 901 | Loss: 1.115778923034668\n",
      "Step 1001 | Loss: 0.934054970741272\n",
      "Step 1101 | Loss: 0.8448582887649536\n",
      "Step 1201 | Loss: 0.759474515914917\n",
      "Step 1301 | Loss: 1.1640596389770508\n",
      "Step 1401 | Loss: 0.8333123922348022\n",
      "Step 1501 | Loss: 1.0332982540130615\n",
      "Step 1601 | Loss: 1.0005470514297485\n",
      "Step 1701 | Loss: 1.3298341035842896\n",
      "Step 1801 | Loss: 0.771593451499939\n",
      "Step 1901 | Loss: 0.9349079132080078\n",
      "Step 2001 | Loss: 1.0370839834213257\n",
      "Step 2101 | Loss: 0.6704041361808777\n",
      "Step 2201 | Loss: 1.1432223320007324\n",
      "Step 2301 | Loss: 1.2250581979751587\n",
      "Step 2401 | Loss: 0.834833025932312\n",
      "0.9806812584963858 0.6475\n",
      "Step 1 | Loss: 1.4878056049346924\n",
      "Step 101 | Loss: 0.9888269305229187\n",
      "Step 201 | Loss: 1.0253771543502808\n",
      "Step 301 | Loss: 0.9176461100578308\n",
      "Step 401 | Loss: 1.1094597578048706\n",
      "Step 501 | Loss: 0.6439659595489502\n",
      "Step 601 | Loss: 0.9318430423736572\n",
      "Step 701 | Loss: 0.7313334941864014\n",
      "Step 801 | Loss: 0.7752479910850525\n",
      "Step 901 | Loss: 1.00973379611969\n",
      "Step 1001 | Loss: 0.925534725189209\n",
      "Step 1101 | Loss: 0.7006226181983948\n",
      "Step 1201 | Loss: 1.2686831951141357\n",
      "Step 1301 | Loss: 0.7763499021530151\n",
      "Step 1401 | Loss: 0.9712508320808411\n",
      "Step 1501 | Loss: 1.0580930709838867\n",
      "Step 1601 | Loss: 0.9421358704566956\n",
      "Step 1701 | Loss: 1.3480149507522583\n",
      "Step 1801 | Loss: 0.8847368955612183\n",
      "Step 1901 | Loss: 0.792941153049469\n",
      "Step 2001 | Loss: 0.7934396266937256\n",
      "Step 2101 | Loss: 0.9812356233596802\n",
      "Step 2201 | Loss: 1.2514410018920898\n",
      "Step 2301 | Loss: 1.1484571695327759\n",
      "Step 2401 | Loss: 0.843750536441803\n",
      "0.9728780255615969 0.6525\n",
      "Step 1 | Loss: 1.353774070739746\n",
      "Step 101 | Loss: 0.7524247169494629\n",
      "Step 201 | Loss: 1.1515178680419922\n",
      "Step 301 | Loss: 1.02791428565979\n",
      "Step 401 | Loss: 0.8286684155464172\n",
      "Step 501 | Loss: 1.1966638565063477\n",
      "Step 601 | Loss: 1.3633410930633545\n",
      "Step 701 | Loss: 0.8076079487800598\n",
      "Step 801 | Loss: 0.8167449235916138\n",
      "Step 901 | Loss: 0.8555688261985779\n",
      "Step 1001 | Loss: 1.0058200359344482\n",
      "Step 1101 | Loss: 1.0128226280212402\n",
      "Step 1201 | Loss: 0.9535029530525208\n",
      "Step 1301 | Loss: 0.9944971799850464\n",
      "Step 1401 | Loss: 1.2541377544403076\n",
      "Step 1501 | Loss: 0.8480802774429321\n",
      "Step 1601 | Loss: 1.0278832912445068\n",
      "Step 1701 | Loss: 0.9258778691291809\n",
      "Step 1801 | Loss: 1.3252736330032349\n",
      "Step 1901 | Loss: 1.3293198347091675\n",
      "Step 2001 | Loss: 0.8107856512069702\n",
      "Step 2101 | Loss: 0.7795692682266235\n",
      "Step 2201 | Loss: 0.8698667287826538\n",
      "Step 2301 | Loss: 1.1577428579330444\n",
      "Step 2401 | Loss: 0.8433976769447327\n",
      "0.9714411076341997 0.65\n",
      "Step 1 | Loss: 1.2624900341033936\n",
      "Step 101 | Loss: 0.9236935377120972\n",
      "Step 201 | Loss: 1.0783894062042236\n",
      "Step 301 | Loss: 0.8066496849060059\n",
      "Step 401 | Loss: 0.9668700098991394\n",
      "Step 501 | Loss: 1.083849549293518\n",
      "Step 601 | Loss: 0.8491242527961731\n",
      "Step 701 | Loss: 0.8349382877349854\n",
      "Step 801 | Loss: 0.915813148021698\n",
      "Step 901 | Loss: 0.6701929569244385\n",
      "Step 1001 | Loss: 0.49883151054382324\n",
      "Step 1101 | Loss: 0.8430128693580627\n",
      "Step 1201 | Loss: 0.7341822385787964\n",
      "Step 1301 | Loss: 0.3863518238067627\n",
      "Step 1401 | Loss: 0.8616299033164978\n",
      "Step 1501 | Loss: 0.9274799227714539\n",
      "Step 1601 | Loss: 0.6724421977996826\n",
      "Step 1701 | Loss: 0.8014618754386902\n",
      "Step 1801 | Loss: 0.8213353753089905\n",
      "Step 1901 | Loss: 0.7528885006904602\n",
      "Step 2001 | Loss: 0.5442001223564148\n",
      "Step 2101 | Loss: 0.6873094439506531\n",
      "Step 2201 | Loss: 0.7851626873016357\n",
      "Step 2301 | Loss: 0.7992453575134277\n",
      "Step 2401 | Loss: 0.9183775186538696\n",
      "0.706823450206324 0.7775\n",
      "Step 1 | Loss: 1.6214547157287598\n",
      "Step 101 | Loss: 1.2618796825408936\n",
      "Step 201 | Loss: 1.0843334197998047\n",
      "Step 301 | Loss: 0.8605985641479492\n",
      "Step 401 | Loss: 0.8009482026100159\n",
      "Step 501 | Loss: 0.9665947556495667\n",
      "Step 601 | Loss: 0.9946702718734741\n",
      "Step 701 | Loss: 0.832457959651947\n",
      "Step 801 | Loss: 0.8246266841888428\n",
      "Step 901 | Loss: 0.7134541273117065\n",
      "Step 1001 | Loss: 0.4909805655479431\n",
      "Step 1101 | Loss: 0.6855096220970154\n",
      "Step 1201 | Loss: 0.9176129698753357\n",
      "Step 1301 | Loss: 0.5756583213806152\n",
      "Step 1401 | Loss: 0.7799218893051147\n",
      "Step 1501 | Loss: 0.8906253576278687\n",
      "Step 1601 | Loss: 0.5865913033485413\n",
      "Step 1701 | Loss: 0.7233113646507263\n",
      "Step 1801 | Loss: 0.7068901062011719\n",
      "Step 1901 | Loss: 0.7830073833465576\n",
      "Step 2001 | Loss: 0.8794893026351929\n",
      "Step 2101 | Loss: 0.5707764625549316\n",
      "Step 2201 | Loss: 0.8755900859832764\n",
      "Step 2301 | Loss: 0.6836874485015869\n",
      "Step 2401 | Loss: 1.0564982891082764\n",
      "0.7065616873749474 0.78\n",
      "Step 1 | Loss: 1.003990650177002\n",
      "Step 101 | Loss: 0.8609675765037537\n",
      "Step 201 | Loss: 0.7492200136184692\n",
      "Step 301 | Loss: 0.9262830018997192\n",
      "Step 401 | Loss: 0.6790130138397217\n",
      "Step 501 | Loss: 0.8300795555114746\n",
      "Step 601 | Loss: 0.6873957514762878\n",
      "Step 701 | Loss: 0.6667783856391907\n",
      "Step 801 | Loss: 0.6235517859458923\n",
      "Step 901 | Loss: 0.6598937511444092\n",
      "Step 1001 | Loss: 0.705644428730011\n",
      "Step 1101 | Loss: 0.7669514417648315\n",
      "Step 1201 | Loss: 0.4987777769565582\n",
      "Step 1301 | Loss: 0.48892927169799805\n",
      "Step 1401 | Loss: 0.9955017566680908\n",
      "Step 1501 | Loss: 0.7993124723434448\n",
      "Step 1601 | Loss: 0.8051150441169739\n",
      "Step 1701 | Loss: 0.7561823129653931\n",
      "Step 1801 | Loss: 1.0729551315307617\n",
      "Step 1901 | Loss: 0.6155800223350525\n",
      "Step 2001 | Loss: 0.8843610882759094\n",
      "Step 2101 | Loss: 0.7358907461166382\n",
      "Step 2201 | Loss: 0.8282432556152344\n",
      "Step 2301 | Loss: 0.6897019147872925\n",
      "Step 2401 | Loss: 0.8191579580307007\n",
      "0.7077184591575281 0.7725\n",
      "Step 1 | Loss: 0.9013084769248962\n",
      "Step 101 | Loss: 0.8979407548904419\n",
      "Step 201 | Loss: 0.7385314702987671\n",
      "Step 301 | Loss: 0.7931550741195679\n",
      "Step 401 | Loss: 0.6639847755432129\n",
      "Step 501 | Loss: 0.9149816036224365\n",
      "Step 601 | Loss: 0.7093639373779297\n",
      "Step 701 | Loss: 0.4453315734863281\n",
      "Step 801 | Loss: 0.6115503907203674\n",
      "Step 901 | Loss: 0.7772247195243835\n",
      "Step 1001 | Loss: 0.8821621537208557\n",
      "Step 1101 | Loss: 0.5551035404205322\n",
      "Step 1201 | Loss: 0.5338362455368042\n",
      "Step 1301 | Loss: 0.6305550336837769\n",
      "Step 1401 | Loss: 0.8106992840766907\n",
      "Step 1501 | Loss: 0.6281318664550781\n",
      "Step 1601 | Loss: 0.7403640747070312\n",
      "Step 1701 | Loss: 0.5530633330345154\n",
      "Step 1801 | Loss: 0.5245398283004761\n",
      "Step 1901 | Loss: 0.7200882434844971\n",
      "Step 2001 | Loss: 0.8581984639167786\n",
      "Step 2101 | Loss: 0.8387037515640259\n",
      "Step 2201 | Loss: 0.7315660715103149\n",
      "Step 2301 | Loss: 0.9031423330307007\n",
      "Step 2401 | Loss: 0.7082489132881165\n",
      "0.708551361746507 0.7725\n",
      "Step 1 | Loss: 1.136857271194458\n",
      "Step 101 | Loss: 1.0352972745895386\n",
      "Step 201 | Loss: 0.889347493648529\n",
      "Step 301 | Loss: 1.1998103857040405\n",
      "Step 401 | Loss: 1.0209624767303467\n",
      "Step 501 | Loss: 0.8329432010650635\n",
      "Step 601 | Loss: 1.0024980306625366\n",
      "Step 701 | Loss: 1.0253338813781738\n",
      "Step 801 | Loss: 0.928234875202179\n",
      "Step 901 | Loss: 0.7878924608230591\n",
      "Step 1001 | Loss: 0.8051522374153137\n",
      "Step 1101 | Loss: 0.660983681678772\n",
      "Step 1201 | Loss: 0.7265718579292297\n",
      "Step 1301 | Loss: 1.0197229385375977\n",
      "Step 1401 | Loss: 0.8625266551971436\n",
      "Step 1501 | Loss: 0.7496047616004944\n",
      "Step 1601 | Loss: 0.576690673828125\n",
      "Step 1701 | Loss: 0.7389939427375793\n",
      "Step 1801 | Loss: 0.5031685829162598\n",
      "Step 1901 | Loss: 0.6083429455757141\n",
      "Step 2001 | Loss: 0.8243343830108643\n",
      "Step 2101 | Loss: 0.7916068434715271\n",
      "Step 2201 | Loss: 0.754460871219635\n",
      "Step 2301 | Loss: 0.8325338959693909\n",
      "Step 2401 | Loss: 0.9235042333602905\n",
      "0.7071358984560489 0.775\n",
      "Step 1 | Loss: 1.3368459939956665\n",
      "Step 101 | Loss: 1.0724596977233887\n",
      "Step 201 | Loss: 0.629317581653595\n",
      "Step 301 | Loss: 0.5542872548103333\n",
      "Step 401 | Loss: 0.5994699597358704\n",
      "Step 501 | Loss: 0.5824260115623474\n",
      "Step 601 | Loss: 0.5498952269554138\n",
      "Step 701 | Loss: 0.4811294376850128\n",
      "Step 801 | Loss: 0.6264662742614746\n",
      "Step 901 | Loss: 0.6085643172264099\n",
      "Step 1001 | Loss: 0.5863223671913147\n",
      "Step 1101 | Loss: 0.6164270043373108\n",
      "Step 1201 | Loss: 0.4274027347564697\n",
      "Step 1301 | Loss: 0.47607120871543884\n",
      "Step 1401 | Loss: 0.5942907333374023\n",
      "Step 1501 | Loss: 0.6968602538108826\n",
      "Step 1601 | Loss: 0.5051636099815369\n",
      "Step 1701 | Loss: 0.40399712324142456\n",
      "Step 1801 | Loss: 0.469253808259964\n",
      "Step 1901 | Loss: 0.5779048204421997\n",
      "Step 2001 | Loss: 0.5455561280250549\n",
      "Step 2101 | Loss: 0.6230877637863159\n",
      "Step 2201 | Loss: 0.43288856744766235\n",
      "Step 2301 | Loss: 0.5093525052070618\n",
      "Step 2401 | Loss: 0.5151203870773315\n",
      "0.542332374987277 0.8825\n",
      "Step 1 | Loss: 1.1464258432388306\n",
      "Step 101 | Loss: 0.7273626923561096\n",
      "Step 201 | Loss: 0.9189451932907104\n",
      "Step 301 | Loss: 0.5143619179725647\n",
      "Step 401 | Loss: 0.6294710040092468\n",
      "Step 501 | Loss: 0.787438690662384\n",
      "Step 601 | Loss: 0.7515786290168762\n",
      "Step 701 | Loss: 0.7013249397277832\n",
      "Step 801 | Loss: 0.7484251260757446\n",
      "Step 901 | Loss: 0.779327392578125\n",
      "Step 1001 | Loss: 0.6163774728775024\n",
      "Step 1101 | Loss: 0.8503105044364929\n",
      "Step 1201 | Loss: 0.6365197896957397\n",
      "Step 1301 | Loss: 0.8294881582260132\n",
      "Step 1401 | Loss: 0.6835547685623169\n",
      "Step 1501 | Loss: 0.5601745247840881\n",
      "Step 1601 | Loss: 0.6857208609580994\n",
      "Step 1701 | Loss: 0.49730125069618225\n",
      "Step 1801 | Loss: 0.6795492768287659\n",
      "Step 1901 | Loss: 0.7717183232307434\n",
      "Step 2001 | Loss: 0.48149123787879944\n",
      "Step 2101 | Loss: 0.5768312215805054\n",
      "Step 2201 | Loss: 0.6262431144714355\n",
      "Step 2301 | Loss: 0.5635865926742554\n",
      "Step 2401 | Loss: 0.6454052329063416\n",
      "0.5914820284945057 0.845\n",
      "Step 1 | Loss: 0.9902536273002625\n",
      "Step 101 | Loss: 0.8324606418609619\n",
      "Step 201 | Loss: 0.734919548034668\n",
      "Step 301 | Loss: 0.671585738658905\n",
      "Step 401 | Loss: 0.8235561847686768\n",
      "Step 501 | Loss: 0.5541390776634216\n",
      "Step 601 | Loss: 0.4872150421142578\n",
      "Step 701 | Loss: 0.6705595850944519\n",
      "Step 801 | Loss: 0.5741320252418518\n",
      "Step 901 | Loss: 0.6934690475463867\n",
      "Step 1001 | Loss: 0.6458396911621094\n",
      "Step 1101 | Loss: 0.4445628523826599\n",
      "Step 1201 | Loss: 0.6399440765380859\n",
      "Step 1301 | Loss: 0.4284498691558838\n",
      "Step 1401 | Loss: 0.6139624714851379\n",
      "Step 1501 | Loss: 0.6129016280174255\n",
      "Step 1601 | Loss: 0.6349897980690002\n",
      "Step 1701 | Loss: 0.5598047375679016\n",
      "Step 1801 | Loss: 0.4957779049873352\n",
      "Step 1901 | Loss: 0.5242791771888733\n",
      "Step 2001 | Loss: 0.6224686503410339\n",
      "Step 2101 | Loss: 0.5319507122039795\n",
      "Step 2201 | Loss: 0.41513967514038086\n",
      "Step 2301 | Loss: 0.4565373659133911\n",
      "Step 2401 | Loss: 0.5836642384529114\n",
      "0.5798269635738669 0.835\n",
      "Step 1 | Loss: 1.2526856660842896\n",
      "Step 101 | Loss: 0.9786481261253357\n",
      "Step 201 | Loss: 0.8798458576202393\n",
      "Step 301 | Loss: 0.735713541507721\n",
      "Step 401 | Loss: 0.5434030294418335\n",
      "Step 501 | Loss: 0.5734920501708984\n",
      "Step 601 | Loss: 0.5197945237159729\n",
      "Step 701 | Loss: 0.48606806993484497\n",
      "Step 801 | Loss: 0.49223077297210693\n",
      "Step 901 | Loss: 0.6478835940361023\n",
      "Step 1001 | Loss: 0.34749695658683777\n",
      "Step 1101 | Loss: 0.529568076133728\n",
      "Step 1201 | Loss: 0.46981775760650635\n",
      "Step 1301 | Loss: 0.4670216143131256\n",
      "Step 1401 | Loss: 0.46528860926628113\n",
      "Step 1501 | Loss: 0.6646021008491516\n",
      "Step 1601 | Loss: 0.5506083965301514\n",
      "Step 1701 | Loss: 0.5089505910873413\n",
      "Step 1801 | Loss: 0.5344308018684387\n",
      "Step 1901 | Loss: 0.5405744314193726\n",
      "Step 2001 | Loss: 0.5800591707229614\n",
      "Step 2101 | Loss: 0.5816400051116943\n",
      "Step 2201 | Loss: 0.4859650731086731\n",
      "Step 2301 | Loss: 0.6195705533027649\n",
      "Step 2401 | Loss: 0.5171492099761963\n",
      "0.5412533604611763 0.8675\n",
      "Step 1 | Loss: 0.9549608826637268\n",
      "Step 101 | Loss: 0.8864887356758118\n",
      "Step 201 | Loss: 0.6050544381141663\n",
      "Step 301 | Loss: 0.6216239929199219\n",
      "Step 401 | Loss: 0.6685518622398376\n",
      "Step 501 | Loss: 0.7111610174179077\n",
      "Step 601 | Loss: 0.44816863536834717\n",
      "Step 701 | Loss: 0.6028836369514465\n",
      "Step 801 | Loss: 0.523701548576355\n",
      "Step 901 | Loss: 0.5421798825263977\n",
      "Step 1001 | Loss: 0.6605133414268494\n",
      "Step 1101 | Loss: 0.5692509412765503\n",
      "Step 1201 | Loss: 0.5352769494056702\n",
      "Step 1301 | Loss: 0.5438507795333862\n",
      "Step 1401 | Loss: 0.6867996454238892\n",
      "Step 1501 | Loss: 0.6404364705085754\n",
      "Step 1601 | Loss: 0.4792308211326599\n",
      "Step 1701 | Loss: 0.5072757601737976\n",
      "Step 1801 | Loss: 0.6390838623046875\n",
      "Step 1901 | Loss: 0.7183213233947754\n",
      "Step 2001 | Loss: 0.5378025770187378\n",
      "Step 2101 | Loss: 0.6278848052024841\n",
      "Step 2201 | Loss: 0.6048033833503723\n",
      "Step 2301 | Loss: 0.6170401573181152\n",
      "Step 2401 | Loss: 0.527873694896698\n",
      "0.5887347906811926 0.845\n",
      "Step 1 | Loss: 1.1574623584747314\n",
      "Step 101 | Loss: 0.8305676579475403\n",
      "Step 201 | Loss: 0.6016475558280945\n",
      "Step 301 | Loss: 0.5577636957168579\n",
      "Step 401 | Loss: 0.507470965385437\n",
      "Step 501 | Loss: 0.4700247645378113\n",
      "Step 601 | Loss: 0.49137264490127563\n",
      "Step 701 | Loss: 0.36134278774261475\n"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset\n",
    "from create_gate_circs_np import get_circ_params, TQCirc, generate_true_random_gate_circ\n",
    "from train_circ_np import train_tq_model, TQMseLoss\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset = 'fmnist_2'\n",
    "curr_dir = f'./experiment_data/{dataset}/trained_circuits/'\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "\n",
    "num_train_steps = 2500\n",
    "num_test_data = 400\n",
    "\n",
    "num_meas_qubits = 1\n",
    "\n",
    "loss = TQMseLoss(num_meas_qubits, 4)\n",
    "\n",
    "dev = qml.device('lightning.qubit', wires=4)\n",
    "device = 'cpu'\n",
    "\n",
    "train_data = TorchDataset(dataset, 'angle', 1, reshape_labels=True)\n",
    "test_data = TorchDataset(dataset, 'angle', 1, False, reshape_labels=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, sampler=torch.utils.data.RandomSampler(train_data))\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=32, sampler=torch.utils.data.SequentialSampler(test_data))\n",
    "\n",
    "for i in range(200, 400):    \n",
    "    circ_dir = curr_dir + f'circ_{i + 1}'\n",
    "    \n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(circ_dir)\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        curr_train_dir = circ_dir + '/run_{}'.format(j + 1)\n",
    "        \n",
    "        if os.path.exists(curr_train_dir):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(curr_train_dir)\n",
    "    \n",
    "        model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, False).to(device)\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "    \n",
    "        curr_loss, curr_acc = train_tq_model(model, num_meas_qubits, opt, loss, train_data_loader, test_data_loader, num_test_data, num_train_steps, 100, 10)\n",
    "        \n",
    "        print(curr_loss, curr_acc)\n",
    "        \n",
    "        torch.save(model.state_dict(), curr_train_dir + '/model.pt')\n",
    "\n",
    "        losses_list.append(curr_loss)\n",
    "        accs_list.append(curr_acc)\n",
    "        \n",
    "    np.savetxt(circ_dir + '/val_losses.txt', losses_list)\n",
    "    np.savetxt(circ_dir + '/accs.txt', accs_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-10-24 04:06:50,178: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0197088297770074 1.3827621891047674 401\n",
      "1.1272406146312668 1.4321240135449873 402\n",
      "1.3025608854258954 1.36873736933033 403\n",
      "1.218392629407745 1.2777175555019844 404\n"
     ]
    }
   ],
   "source": [
    "from create_noise_models import noisy_dev_from_backend\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ, get_circ_params\n",
    "from train_circ import train_qnn, mse_loss\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset = 'fmnist_4'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "num_params = 18\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "for i in range(400, 600):\n",
    "    curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                    weights_bounds, [0], 'exp')\n",
    "    \n",
    "    \n",
    "    noiseless_losses = np.genfromtxt(curr_dir + '/val_losses.txt')\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    curr_dev_dir = curr_dir + '/' + device_name\n",
    "\n",
    "#     if not os.path.exists(curr_dev_dir + '/accs_inference_only.txt'):\n",
    "    if True:\n",
    "        if not os.path.exists(curr_dev_dir):\n",
    "            os.mkdir(curr_dev_dir)\n",
    "\n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "            curr_params = np.genfromtxt(curr_train_dir + '/params_{}.txt'.format(j + 1))[-1]\n",
    "\n",
    "            val_exps = [circ(x_test[i], curr_params) for i in range(len(x_test))]\n",
    "            val_loss = np.array([mse_vec_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "            acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "#             acc = np.mean(val_loss < 1)\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        print(np.mean(noiseless_losses), np.mean(losses_list), i + 1)\n",
    "\n",
    "        np.save(curr_dev_dir + '/val_losses_inference_only.npy', losses_list)\n",
    "        np.savetxt(curr_dev_dir + '/accs_inference_only.txt', accs_list)\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## humandesgn circs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 1.059145450592041\n",
      "Step 101 | Loss: 1.018198013305664\n",
      "Step 201 | Loss: 1.1507503986358643\n",
      "Step 301 | Loss: 0.9303577542304993\n",
      "Step 401 | Loss: 0.9430369138717651\n",
      "Step 501 | Loss: 0.7633569240570068\n",
      "Step 601 | Loss: 0.9405395984649658\n",
      "Step 701 | Loss: 0.9032794237136841\n",
      "Step 801 | Loss: 0.9373906850814819\n",
      "Step 901 | Loss: 0.9722119569778442\n",
      "0.981750134136273 0.5666666666666667\n",
      "Step 1 | Loss: 1.1858423948287964\n",
      "Step 101 | Loss: 0.9825073480606079\n",
      "Step 201 | Loss: 0.976843535900116\n",
      "Step 301 | Loss: 1.0874971151351929\n",
      "Step 401 | Loss: 0.8870750665664673\n",
      "Step 501 | Loss: 1.0089274644851685\n",
      "Step 601 | Loss: 0.8543026447296143\n",
      "Step 701 | Loss: 0.9265348315238953\n",
      "Step 801 | Loss: 1.0485655069351196\n",
      "Step 901 | Loss: 0.8838073015213013\n",
      "0.9668062135163681 0.5416666666666666\n",
      "Step 1 | Loss: 0.8840807676315308\n",
      "Step 101 | Loss: 0.8197826147079468\n",
      "Step 201 | Loss: 0.7930959463119507\n",
      "Step 301 | Loss: 0.8757045269012451\n",
      "Step 401 | Loss: 0.8126855492591858\n",
      "Step 501 | Loss: 0.8517589569091797\n",
      "Step 601 | Loss: 1.0105088949203491\n",
      "Step 701 | Loss: 1.1028121709823608\n",
      "Step 801 | Loss: 0.956211268901825\n",
      "Step 901 | Loss: 0.7905033230781555\n",
      "0.9534289609642116 0.575\n",
      "Step 1 | Loss: 1.1579028367996216\n",
      "Step 101 | Loss: 0.9943180680274963\n",
      "Step 201 | Loss: 0.7641008496284485\n",
      "Step 301 | Loss: 0.9367945194244385\n",
      "Step 401 | Loss: 0.9217113256454468\n",
      "Step 501 | Loss: 0.8778225183486938\n",
      "Step 601 | Loss: 0.932136595249176\n",
      "Step 701 | Loss: 0.9501417279243469\n",
      "Step 801 | Loss: 0.8828171491622925\n",
      "Step 901 | Loss: 1.0450451374053955\n",
      "0.9942616173916117 0.575\n",
      "Step 1 | Loss: 1.0090587139129639\n",
      "Step 101 | Loss: 0.9891706705093384\n",
      "Step 201 | Loss: 1.076406717300415\n",
      "Step 301 | Loss: 0.9779428243637085\n",
      "Step 401 | Loss: 0.9464548230171204\n",
      "Step 501 | Loss: 1.0149561166763306\n",
      "Step 601 | Loss: 0.8179079294204712\n",
      "Step 701 | Loss: 1.0524258613586426\n",
      "Step 801 | Loss: 0.9815918207168579\n",
      "Step 901 | Loss: 0.9584450721740723\n",
      "0.9546152577655542 0.575\n",
      "Step 1 | Loss: 1.0914006233215332\n",
      "Step 101 | Loss: 1.0250933170318604\n",
      "Step 201 | Loss: 0.9246804714202881\n",
      "Step 301 | Loss: 1.049946665763855\n",
      "Step 401 | Loss: 1.021989345550537\n",
      "Step 501 | Loss: 1.0074888467788696\n",
      "Step 601 | Loss: 0.8835321664810181\n",
      "Step 701 | Loss: 0.9408807158470154\n",
      "Step 801 | Loss: 0.9129566550254822\n",
      "Step 901 | Loss: 0.8752156496047974\n",
      "0.8886378698243859 0.625\n",
      "Step 1 | Loss: 1.2923164367675781\n",
      "Step 101 | Loss: 1.0722135305404663\n",
      "Step 201 | Loss: 0.8979499936103821\n",
      "Step 301 | Loss: 0.8372975587844849\n",
      "Step 401 | Loss: 0.9780048727989197\n",
      "Step 501 | Loss: 0.8476297855377197\n",
      "Step 601 | Loss: 0.959704577922821\n",
      "Step 701 | Loss: 0.833452582359314\n",
      "Step 801 | Loss: 0.9932637214660645\n",
      "Step 901 | Loss: 0.9101564288139343\n",
      "0.9622081708324378 0.5916666666666667\n",
      "Step 1 | Loss: 0.9062502384185791\n",
      "Step 101 | Loss: 1.0228993892669678\n",
      "Step 201 | Loss: 1.122340440750122\n",
      "Step 301 | Loss: 0.8893492221832275\n",
      "Step 401 | Loss: 0.8758225440979004\n",
      "Step 501 | Loss: 0.9732030630111694\n",
      "Step 601 | Loss: 0.9966283440589905\n",
      "Step 701 | Loss: 0.9403555989265442\n",
      "Step 801 | Loss: 0.8797143697738647\n",
      "Step 901 | Loss: 0.8934804201126099\n",
      "0.9735771597146229 0.5583333333333333\n",
      "Step 1 | Loss: 1.0527980327606201\n",
      "Step 101 | Loss: 1.002219319343567\n",
      "Step 201 | Loss: 1.076690912246704\n",
      "Step 301 | Loss: 0.9652291536331177\n",
      "Step 401 | Loss: 0.9863150715827942\n",
      "Step 501 | Loss: 1.006091594696045\n",
      "Step 601 | Loss: 1.054111361503601\n",
      "Step 701 | Loss: 0.8658199906349182\n",
      "Step 801 | Loss: 0.9713611602783203\n",
      "Step 901 | Loss: 0.8473682403564453\n",
      "0.9345242795511504 0.6166666666666667\n",
      "Step 1 | Loss: 1.0600453615188599\n",
      "Step 101 | Loss: 1.0202525854110718\n",
      "Step 201 | Loss: 0.8926658034324646\n",
      "Step 301 | Loss: 0.9731240272521973\n",
      "Step 401 | Loss: 1.0479638576507568\n",
      "Step 501 | Loss: 0.9747339487075806\n",
      "Step 601 | Loss: 0.9369596838951111\n",
      "Step 701 | Loss: 0.8287259340286255\n",
      "Step 801 | Loss: 0.9914820194244385\n",
      "Step 901 | Loss: 1.041675329208374\n",
      "1.1026305663613525 0.4666666666666667\n",
      "Step 1 | Loss: 1.054240345954895\n",
      "Step 101 | Loss: 1.0616506338119507\n",
      "Step 201 | Loss: 0.9496263265609741\n",
      "Step 301 | Loss: 1.0707635879516602\n",
      "Step 401 | Loss: 0.9869474172592163\n",
      "Step 501 | Loss: 0.9195576906204224\n",
      "Step 601 | Loss: 0.9584998488426208\n",
      "Step 701 | Loss: 0.8817797899246216\n",
      "Step 801 | Loss: 0.875497579574585\n",
      "Step 901 | Loss: 0.8903630971908569\n",
      "1.0381912298634868 0.475\n",
      "Step 1 | Loss: 1.048235297203064\n",
      "Step 101 | Loss: 0.960679829120636\n",
      "Step 201 | Loss: 0.9793214797973633\n",
      "Step 301 | Loss: 0.8383647203445435\n",
      "Step 401 | Loss: 0.8824353814125061\n",
      "Step 501 | Loss: 0.782304584980011\n",
      "Step 601 | Loss: 0.8674821257591248\n",
      "Step 701 | Loss: 0.7729141116142273\n",
      "Step 801 | Loss: 0.8134332895278931\n",
      "Step 901 | Loss: 0.7914458513259888\n",
      "0.9020751450265033 0.6166666666666667\n",
      "Step 1 | Loss: 1.0681642293930054\n",
      "Step 101 | Loss: 0.9116237759590149\n",
      "Step 201 | Loss: 0.9476732611656189\n",
      "Step 301 | Loss: 0.8903018832206726\n",
      "Step 401 | Loss: 0.8821168541908264\n",
      "Step 501 | Loss: 0.7179107666015625\n",
      "Step 601 | Loss: 1.0245486497879028\n",
      "Step 701 | Loss: 0.8305113315582275\n",
      "Step 801 | Loss: 0.7324994802474976\n",
      "Step 901 | Loss: 0.7754951119422913\n",
      "1.0087872823089845 0.4666666666666667\n",
      "Step 1 | Loss: 0.8515210747718811\n",
      "Step 101 | Loss: 0.8454174995422363\n",
      "Step 201 | Loss: 0.9709571003913879\n",
      "Step 301 | Loss: 0.8422737121582031\n",
      "Step 401 | Loss: 0.8177648782730103\n",
      "Step 501 | Loss: 0.8794093132019043\n",
      "Step 601 | Loss: 0.8387603759765625\n",
      "Step 701 | Loss: 0.776286244392395\n",
      "Step 801 | Loss: 0.9521005153656006\n",
      "Step 901 | Loss: 0.8932561874389648\n",
      "0.9346631402122458 0.5583333333333333\n",
      "Step 1 | Loss: 1.0595169067382812\n",
      "Step 101 | Loss: 0.9562429189682007\n",
      "Step 201 | Loss: 0.833154559135437\n",
      "Step 301 | Loss: 0.6699366569519043\n",
      "Step 401 | Loss: 0.8279238343238831\n",
      "Step 501 | Loss: 0.9526081085205078\n",
      "Step 601 | Loss: 0.9447200894355774\n",
      "Step 701 | Loss: 0.9986807107925415\n",
      "Step 801 | Loss: 0.6636284589767456\n",
      "Step 901 | Loss: 1.000614881515503\n",
      "0.9285128294351065 0.6166666666666667\n",
      "Step 1 | Loss: 1.0706000328063965\n",
      "Step 101 | Loss: 1.0415666103363037\n",
      "Step 201 | Loss: 0.8856588006019592\n",
      "Step 301 | Loss: 1.073898196220398\n",
      "Step 401 | Loss: 0.960485577583313\n",
      "Step 501 | Loss: 0.830512523651123\n",
      "Step 601 | Loss: 0.8826311230659485\n",
      "Step 701 | Loss: 0.8933802843093872\n",
      "Step 801 | Loss: 0.9697239995002747\n",
      "Step 901 | Loss: 0.8123943209648132\n",
      "0.9778253009611979 0.5666666666666667\n",
      "Step 1 | Loss: 1.0146064758300781\n",
      "Step 101 | Loss: 1.1258245706558228\n",
      "Step 201 | Loss: 0.9824682474136353\n",
      "Step 301 | Loss: 0.9048068523406982\n",
      "Step 401 | Loss: 0.9257991909980774\n",
      "Step 501 | Loss: 0.9847937822341919\n",
      "Step 601 | Loss: 0.7791967391967773\n",
      "Step 701 | Loss: 0.8366345167160034\n",
      "Step 801 | Loss: 0.9626786708831787\n",
      "Step 901 | Loss: 0.8033057451248169\n",
      "0.9565557796514998 0.5833333333333334\n",
      "Step 1 | Loss: 1.2167295217514038\n",
      "Step 101 | Loss: 1.0976650714874268\n",
      "Step 201 | Loss: 1.0245541334152222\n",
      "Step 301 | Loss: 0.9430393576622009\n",
      "Step 401 | Loss: 0.9900662899017334\n",
      "Step 501 | Loss: 0.9910169243812561\n",
      "Step 601 | Loss: 0.8991502523422241\n",
      "Step 701 | Loss: 0.8968303203582764\n",
      "Step 801 | Loss: 0.9728534817695618\n",
      "Step 901 | Loss: 0.9147311449050903\n",
      "1.0037060135436107 0.55\n",
      "Step 1 | Loss: 0.9354174137115479\n",
      "Step 101 | Loss: 1.0378177165985107\n",
      "Step 201 | Loss: 1.0043506622314453\n",
      "Step 301 | Loss: 0.788804292678833\n",
      "Step 401 | Loss: 0.9568130970001221\n",
      "Step 501 | Loss: 0.7770588397979736\n",
      "Step 601 | Loss: 0.8833264112472534\n",
      "Step 701 | Loss: 0.7813950777053833\n",
      "Step 801 | Loss: 0.7959591150283813\n",
      "Step 901 | Loss: 0.7918447852134705\n",
      "0.935383581366278 0.5833333333333334\n",
      "Step 1 | Loss: 1.09380042552948\n",
      "Step 101 | Loss: 1.05696702003479\n",
      "Step 201 | Loss: 0.9164352416992188\n",
      "Step 301 | Loss: 0.8665717244148254\n",
      "Step 401 | Loss: 0.932994544506073\n",
      "Step 501 | Loss: 0.8078053593635559\n",
      "Step 601 | Loss: 0.8380227088928223\n",
      "Step 701 | Loss: 0.7457168698310852\n",
      "Step 801 | Loss: 0.8481204509735107\n",
      "Step 901 | Loss: 0.9255267381668091\n",
      "0.8729403374405518 0.6166666666666667\n",
      "Step 1 | Loss: 1.0270769596099854\n",
      "Step 101 | Loss: 0.9631702899932861\n",
      "Step 201 | Loss: 0.9451289772987366\n",
      "Step 301 | Loss: 0.8713452219963074\n",
      "Step 401 | Loss: 0.9783449172973633\n",
      "Step 501 | Loss: 0.900590181350708\n",
      "Step 601 | Loss: 0.8624853491783142\n",
      "Step 701 | Loss: 0.8926540613174438\n",
      "Step 801 | Loss: 0.9739173650741577\n",
      "Step 901 | Loss: 0.8277807235717773\n",
      "0.9724935149157268 0.575\n",
      "Step 1 | Loss: 1.012131690979004\n",
      "Step 101 | Loss: 1.0209310054779053\n",
      "Step 201 | Loss: 0.9956990480422974\n",
      "Step 301 | Loss: 0.9560327529907227\n",
      "Step 401 | Loss: 0.7916502952575684\n",
      "Step 501 | Loss: 0.8855243921279907\n",
      "Step 601 | Loss: 0.8254146575927734\n",
      "Step 701 | Loss: 0.9319682121276855\n",
      "Step 801 | Loss: 0.8687987327575684\n",
      "Step 901 | Loss: 0.8812302947044373\n",
      "0.9081485467916779 0.6166666666666667\n",
      "Step 1 | Loss: 1.0984076261520386\n",
      "Step 101 | Loss: 1.1243646144866943\n",
      "Step 201 | Loss: 0.8715164661407471\n",
      "Step 301 | Loss: 0.8211829662322998\n",
      "Step 401 | Loss: 0.816636860370636\n",
      "Step 501 | Loss: 0.8880678415298462\n",
      "Step 601 | Loss: 0.88047194480896\n",
      "Step 701 | Loss: 0.814092755317688\n",
      "Step 801 | Loss: 0.7968206405639648\n",
      "Step 901 | Loss: 0.8606733679771423\n",
      "0.9672264068361012 0.5916666666666667\n",
      "Step 1 | Loss: 1.2271560430526733\n",
      "Step 101 | Loss: 1.1149201393127441\n",
      "Step 201 | Loss: 0.9921756386756897\n",
      "Step 301 | Loss: 0.9824299216270447\n",
      "Step 401 | Loss: 0.8576852679252625\n",
      "Step 501 | Loss: 0.8257532119750977\n",
      "Step 601 | Loss: 0.7836117744445801\n",
      "Step 701 | Loss: 0.7702690362930298\n",
      "Step 801 | Loss: 0.8733991384506226\n",
      "Step 901 | Loss: 0.8451648950576782\n",
      "0.9292761921289477 0.625\n",
      "Step 1 | Loss: 1.0156906843185425\n",
      "Step 101 | Loss: 0.9354406595230103\n",
      "Step 201 | Loss: 0.8641582131385803\n",
      "Step 301 | Loss: 0.927664041519165\n",
      "Step 401 | Loss: 1.0122802257537842\n",
      "Step 501 | Loss: 0.9464417695999146\n",
      "Step 601 | Loss: 0.8781395554542542\n",
      "Step 701 | Loss: 0.8373303413391113\n",
      "Step 801 | Loss: 0.8874844312667847\n",
      "Step 901 | Loss: 0.761296272277832\n",
      "0.9975219097578649 0.55\n"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs_np import get_circ_params, TQCirc, generate_true_random_gate_circ\n",
    "from train_circ_np import train_tq_model, TQMseLoss\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "from create_human_design_circs import convert_human_design_circ_to_gate_circ\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "embed_method = 'iqp'\n",
    "\n",
    "dataset = 'vowel_2'\n",
    "curr_dir = f'./human_design/{embed_method}_basic/{dataset}'\n",
    "\n",
    "num_qubits = 4\n",
    "num_embed_layers = 3\n",
    "num_dataset_reps = 1\n",
    "\n",
    "num_train_steps = 1000\n",
    "num_test_data = 120\n",
    "\n",
    "num_meas_qubits = 1\n",
    "\n",
    "param_nums = [16, 20, 24, 28, 32]\n",
    "\n",
    "if not os.path.exists(curr_dir):\n",
    "    os.mkdir(curr_dir)\n",
    "\n",
    "loss = TQMseLoss(num_meas_qubits, 4)\n",
    "device = 'cpu'\n",
    "\n",
    "train_data = TorchDataset(dataset, embed_method, num_dataset_reps, reshape_labels=True)\n",
    "test_data = TorchDataset(dataset, embed_method, num_dataset_reps, False, reshape_labels=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, sampler=torch.utils.data.RandomSampler(train_data))\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=32, sampler=torch.utils.data.SequentialSampler(test_data))\n",
    "\n",
    "for param_num in param_nums:\n",
    "    curr_param_dir = curr_dir + f'/{param_num}_params/'\n",
    "    \n",
    "    if not os.path.exists(curr_param_dir):\n",
    "        os.mkdir(curr_param_dir)\n",
    "    \n",
    "    for i in range(1):    \n",
    "        circ_dir = curr_param_dir\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = convert_human_design_circ_to_gate_circ(num_qubits, embed_method, 'basic', num_embed_layers,\n",
    "                                                                                                        param_num // num_qubits)\n",
    "        \n",
    "        np.savetxt(curr_param_dir + '/gates.txt', circ_gates, fmt=\"%s\")\n",
    "        np.savetxt(curr_param_dir + '/gate_params.txt', gate_params, fmt=\"%s\")\n",
    "        np.savetxt(curr_param_dir + '/inputs_bounds.txt', inputs_bounds)\n",
    "        np.savetxt(curr_param_dir + '/weights_bounds.txt', weights_bounds)\n",
    "        \n",
    "        losses_list = []\n",
    "        accs_list = []\n",
    "\n",
    "        for j in range(5):\n",
    "            curr_train_dir = circ_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "            if os.path.exists(curr_train_dir):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir(curr_train_dir)\n",
    "\n",
    "            model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, False).to(device)\n",
    "            opt = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "\n",
    "            curr_loss, curr_acc = train_tq_model(model, num_meas_qubits, opt, loss, train_data_loader, test_data_loader, num_test_data, num_train_steps, 100, 10)\n",
    "\n",
    "            print(curr_loss, curr_acc)\n",
    "\n",
    "            losses_list.append(curr_loss)\n",
    "            accs_list.append(curr_acc)\n",
    "            \n",
    "            torch.save(model.state_dict(), curr_train_dir + '/model.pt')\n",
    "\n",
    "        np.savetxt(circ_dir + '/val_losses.txt', losses_list)\n",
    "        np.savetxt(circ_dir + '/accs.txt', accs_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets_nt' from '/root/datasets_nt.py'>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import elivagar.inference.tq_circ_noisy_sim\n",
    "import datasets_nt\n",
    "\n",
    "reload(elivagar.inference.tq_circ_noisy_sim)\n",
    "reload(datasets_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import elivagar.metric_computation.compute_clifford_nr\n",
    "\n",
    "reload(elivagar.metric_computation.compute_clifford_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train our circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201 | Loss: 1.067293405532837\n",
      "Step 301 | Loss: 1.1350668668746948\n",
      "Step 401 | Loss: 1.2388997077941895\n",
      "Step 501 | Loss: 1.2709476947784424\n",
      "Step 601 | Loss: 1.065375566482544\n",
      "Step 701 | Loss: 1.061983585357666\n",
      "Step 801 | Loss: 0.9946576952934265\n",
      "Step 901 | Loss: 1.2294495105743408\n",
      "1.1726146715658836 0.6666666666666666\n",
      "Step 1 | Loss: 1.9492011070251465\n",
      "Step 101 | Loss: 1.334709644317627\n",
      "Step 201 | Loss: 1.1559081077575684\n",
      "Step 301 | Loss: 1.3054108619689941\n",
      "Step 401 | Loss: 0.8566857576370239\n",
      "Step 501 | Loss: 0.8089755177497864\n",
      "Step 601 | Loss: 1.1627891063690186\n",
      "Step 701 | Loss: 1.0729025602340698\n",
      "Step 801 | Loss: 1.1424273252487183\n",
      "Step 901 | Loss: 1.027600646018982\n",
      "1.1446763477130883 0.6583333333333333\n",
      "Step 1 | Loss: 2.4663705825805664\n",
      "Step 101 | Loss: 1.4275672435760498\n",
      "Step 201 | Loss: 1.1782761812210083\n",
      "Step 301 | Loss: 1.1334788799285889\n",
      "Step 401 | Loss: 1.1472740173339844\n",
      "Step 501 | Loss: 0.9647291898727417\n",
      "Step 601 | Loss: 1.0607974529266357\n",
      "Step 701 | Loss: 1.2186176776885986\n",
      "Step 801 | Loss: 1.2820543050765991\n",
      "Step 901 | Loss: 1.013516902923584\n",
      "1.1551492621997432 0.6666666666666666\n",
      "Step 1 | Loss: 2.2500672340393066\n",
      "Step 101 | Loss: 1.4641146659851074\n",
      "Step 201 | Loss: 1.2492575645446777\n",
      "Step 301 | Loss: 0.8864758610725403\n",
      "Step 401 | Loss: 1.3571197986602783\n",
      "Step 501 | Loss: 0.8619060516357422\n",
      "Step 601 | Loss: 1.0422085523605347\n",
      "Step 701 | Loss: 1.0065959692001343\n",
      "Step 801 | Loss: 0.9518158435821533\n",
      "Step 901 | Loss: 1.3544211387634277\n",
      "1.1669290095882447 0.625\n",
      "1\n",
      "[1150 2287  931  863 2431  890  379   88 1929  555  134 2340 1712 2154\n",
      " 1282 1576 1774  665 1163 1357  625 1999 2455 2199  900 2045  170 2055\n",
      "  469 1672 1317  910 1785 1120 2259 1025 1537   10 2496  527 2297 1277\n",
      "  390  985  451 1506  834  360  268  157  221 2260  292 1450  223 1873\n",
      "  788 2108 2464   16 2461 1746  252  959 1789  564 1986 2080 1562  224\n",
      " 1079 1959 1635  495 2499 2169 2351  822 1587  757 1985  610   70  511\n",
      " 1690 2282 1963 1735 1369 2113  892 1001 1331 1074 2035    5 1303 1467\n",
      "  880 2135] 863\n",
      "0.7624937698346912\n",
      "Step 1 | Loss: 1.9702632427215576\n",
      "Step 101 | Loss: 1.2209919691085815\n",
      "Step 201 | Loss: 1.3804011344909668\n",
      "Step 301 | Loss: 1.5441763401031494\n",
      "Step 401 | Loss: 1.3585673570632935\n",
      "Step 501 | Loss: 1.4262244701385498\n",
      "Step 601 | Loss: 1.2627217769622803\n",
      "Step 701 | Loss: 1.2536495923995972\n",
      "Step 801 | Loss: 1.1043332815170288\n",
      "Step 901 | Loss: 1.3332862854003906\n",
      "1.1376028699153682 0.6833333333333333\n",
      "Step 1 | Loss: 2.2671713829040527\n",
      "Step 101 | Loss: 1.297845482826233\n",
      "Step 201 | Loss: 1.298554539680481\n",
      "Step 301 | Loss: 1.5688656568527222\n",
      "Step 401 | Loss: 1.29941987991333\n",
      "Step 501 | Loss: 1.4441105127334595\n",
      "Step 601 | Loss: 1.3345478773117065\n",
      "Step 701 | Loss: 1.160955786705017\n"
     ]
    }
   ],
   "source": [
    "from elivagar.training.train_circuits_from_predictors import train_elivagar_circuits\n",
    "\n",
    "dataset = 'vowel_4'\n",
    "device_name = 'ibmq_manila'\n",
    "num_train_steps = 1000\n",
    "num_params = 40\n",
    "num_test_data = 120\n",
    "num_qubits = 4\n",
    "num_meas_qubits = 2\n",
    "num_data_in_matrix = 64\n",
    "\n",
    "circ_dir = f'./ours/{dataset}/{num_params}_params'\n",
    "\n",
    "train_elivagar_circuits(circ_dir, dataset, device_name, num_train_steps, num_test_data, num_qubits, num_meas_qubits,\n",
    "                            num_data_in_matrix, num_candidates_per_circuit=100, num_circuits=2500, num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83158684 0.84251022 0.79097557 0.86144447 0.80526924 0.80498314\n",
      " 0.82521248 0.82475471 0.74424171 0.86441612 0.78021812 0.80946541\n",
      " 0.81977654 0.81616592 0.80131149 0.54882812 0.80265236 0.81085205\n",
      " 0.8251133  0.80967522 0.80595016 0.83556938 0.85645294 0.78894806\n",
      " 0.82696724]\n",
      "0\n",
      "[2313 1795  262 2370  881 1805  559  143  847 1244 1957 1563  604 1665\n",
      "  853 1938 1366 2486 2019 1644 2293  255 1905  785  531   55 2002  817\n",
      " 1296  323 1146 1672 2065  493 1164  111 1911 1068  529 2473  184 2319\n",
      "  458 2389 1348  607  763 2258 1025  889 2133 1962 1010 1827 2188 1208\n",
      "  153  409  968 1461 2471  996 1230 1693  231 1289 2226  904  730 1138\n",
      " 1463 1602 1525  186  740 2064 1086  318 2171 1399 1904  669  404  447\n",
      "  745  344 1151  449 1218 1658 1425 2330  897 1295 2310  849 1536 1780\n",
      " 1765 2009] 1025\n",
      "0.9101524353027344\n",
      "Step 1 | Loss: 0.9380895495414734\n",
      "Step 101 | Loss: 0.4010678827762604\n",
      "Step 201 | Loss: 0.2963978946208954\n",
      "Step 301 | Loss: 0.400852233171463\n",
      "Step 401 | Loss: 0.5547229647636414\n",
      "Step 501 | Loss: 0.3178776502609253\n",
      "Step 601 | Loss: 0.26720014214515686\n",
      "Step 701 | Loss: 0.41534313559532166\n",
      "Step 801 | Loss: 0.409954309463501\n",
      "Step 901 | Loss: 0.3059796392917633\n",
      "Step 1001 | Loss: 0.38916903734207153\n",
      "Step 1101 | Loss: 0.29752036929130554\n",
      "Step 1201 | Loss: 0.24657058715820312\n",
      "Step 1301 | Loss: 0.2797664701938629\n",
      "Step 1401 | Loss: 0.2835809886455536\n",
      "Step 1501 | Loss: 0.31060874462127686\n",
      "Step 1601 | Loss: 0.37912121415138245\n",
      "Step 1701 | Loss: 0.42970502376556396\n",
      "Step 1801 | Loss: 0.36664125323295593\n",
      "Step 1901 | Loss: 0.2861904203891754\n",
      "Step 2001 | Loss: 0.38748204708099365\n",
      "Step 2101 | Loss: 0.2966715693473816\n",
      "Step 2201 | Loss: 0.22421157360076904\n",
      "Step 2301 | Loss: 0.3456574082374573\n",
      "Step 2401 | Loss: 0.37443333864212036\n",
      "0.36856684556420394 0.925\n",
      "Step 1 | Loss: 1.5805530548095703\n",
      "Step 101 | Loss: 0.4480764865875244\n",
      "Step 201 | Loss: 0.42149990797042847\n",
      "Step 301 | Loss: 0.31507936120033264\n",
      "Step 401 | Loss: 0.3939788341522217\n",
      "Step 501 | Loss: 0.3865552246570587\n",
      "Step 601 | Loss: 0.4509461224079132\n",
      "Step 701 | Loss: 0.3171469271183014\n",
      "Step 801 | Loss: 0.3278040289878845\n",
      "Step 901 | Loss: 0.3503137230873108\n",
      "Step 1001 | Loss: 0.31632524728775024\n",
      "Step 1101 | Loss: 0.3475557565689087\n",
      "Step 1201 | Loss: 0.3536125421524048\n",
      "Step 1301 | Loss: 0.3479410409927368\n",
      "Step 1401 | Loss: 0.3323288559913635\n",
      "Step 1501 | Loss: 0.4447610378265381\n",
      "Step 1601 | Loss: 0.3760124146938324\n",
      "Step 1701 | Loss: 0.47338369488716125\n",
      "Step 1801 | Loss: 0.3587420880794525\n",
      "Step 1901 | Loss: 0.46620669960975647\n",
      "Step 2001 | Loss: 0.42871978878974915\n",
      "Step 2101 | Loss: 0.4514354169368744\n",
      "Step 2201 | Loss: 0.4239501655101776\n",
      "Step 2301 | Loss: 0.46485328674316406\n",
      "Step 2401 | Loss: 0.4571995139122009\n",
      "0.41876730717587285 0.9175\n",
      "Step 1 | Loss: 1.9269013404846191\n",
      "Step 101 | Loss: 0.38186588883399963\n",
      "Step 201 | Loss: 0.5151450634002686\n",
      "Step 301 | Loss: 0.41203245520591736\n",
      "Step 401 | Loss: 0.3840782642364502\n",
      "Step 501 | Loss: 0.3394250273704529\n",
      "Step 601 | Loss: 0.4171387553215027\n",
      "Step 701 | Loss: 0.4078207314014435\n",
      "Step 801 | Loss: 0.37401434779167175\n",
      "Step 901 | Loss: 0.46517467498779297\n",
      "Step 1001 | Loss: 0.39175188541412354\n",
      "Step 1101 | Loss: 0.3416016399860382\n",
      "Step 1201 | Loss: 0.33033129572868347\n",
      "Step 1301 | Loss: 0.39883217215538025\n",
      "Step 1401 | Loss: 0.34900808334350586\n",
      "Step 1501 | Loss: 0.370510995388031\n",
      "Step 1601 | Loss: 0.3494711220264435\n",
      "Step 1701 | Loss: 0.5611454844474792\n",
      "Step 1801 | Loss: 0.3180398941040039\n",
      "Step 1901 | Loss: 0.44689399003982544\n",
      "Step 2001 | Loss: 0.5020264387130737\n",
      "Step 2101 | Loss: 0.41142693161964417\n",
      "Step 2201 | Loss: 0.4044041931629181\n",
      "Step 2301 | Loss: 0.40713539719581604\n",
      "Step 2401 | Loss: 0.4382103383541107\n",
      "0.41941802727844085 0.9275\n",
      "Step 1 | Loss: 1.6661556959152222\n",
      "Step 101 | Loss: 0.5347273945808411\n",
      "Step 201 | Loss: 0.5060487389564514\n",
      "Step 301 | Loss: 0.43035072088241577\n",
      "Step 401 | Loss: 0.4805490970611572\n",
      "Step 501 | Loss: 0.4758916199207306\n",
      "Step 601 | Loss: 0.33045098185539246\n",
      "Step 701 | Loss: 0.3608764410018921\n",
      "Step 801 | Loss: 0.3693792223930359\n",
      "Step 901 | Loss: 0.42843499779701233\n",
      "Step 1001 | Loss: 0.35758256912231445\n",
      "Step 1101 | Loss: 0.3896010220050812\n",
      "Step 1201 | Loss: 0.36216384172439575\n",
      "Step 1301 | Loss: 0.4304933547973633\n",
      "Step 1401 | Loss: 0.38227829337120056\n",
      "Step 1501 | Loss: 0.4503358006477356\n",
      "Step 1601 | Loss: 0.3808281421661377\n",
      "Step 1701 | Loss: 0.3543540835380554\n",
      "Step 1801 | Loss: 0.4273780584335327\n",
      "Step 1901 | Loss: 0.3586422801017761\n",
      "Step 2001 | Loss: 0.3414052128791809\n",
      "Step 2101 | Loss: 0.42643028497695923\n",
      "Step 2201 | Loss: 0.2887030243873596\n",
      "Step 2301 | Loss: 0.40323829650878906\n",
      "Step 2401 | Loss: 0.3605634570121765\n",
      "0.4207830222028481 0.92\n",
      "Step 1 | Loss: 0.9523483514785767\n",
      "Step 101 | Loss: 0.3683011829853058\n",
      "Step 201 | Loss: 0.4494614601135254\n",
      "Step 301 | Loss: 0.42027944326400757\n",
      "Step 401 | Loss: 0.3412002623081207\n",
      "Step 501 | Loss: 0.4135558605194092\n",
      "Step 601 | Loss: 0.3802954852581024\n",
      "Step 701 | Loss: 0.303149551153183\n",
      "Step 801 | Loss: 0.4359844923019409\n",
      "Step 901 | Loss: 0.32194432616233826\n",
      "Step 1001 | Loss: 0.45130518078804016\n",
      "Step 1101 | Loss: 0.19821029901504517\n",
      "Step 1201 | Loss: 0.38336625695228577\n",
      "Step 1301 | Loss: 0.48526430130004883\n",
      "Step 1401 | Loss: 0.43240290880203247\n",
      "Step 1501 | Loss: 0.30134695768356323\n",
      "Step 1601 | Loss: 0.3418040871620178\n",
      "Step 1701 | Loss: 0.44336944818496704\n",
      "Step 1801 | Loss: 0.42387765645980835\n",
      "Step 1901 | Loss: 0.30017876625061035\n",
      "Step 2001 | Loss: 0.300241082906723\n",
      "Step 2101 | Loss: 0.40690746903419495\n",
      "Step 2201 | Loss: 0.27924656867980957\n",
      "Step 2301 | Loss: 0.3352539837360382\n",
      "Step 2401 | Loss: 0.6284707188606262\n",
      "0.386902160664556 0.925\n",
      "1\n",
      "[ 973  125 1808 2267  406  754  797 1157  446 2194  549 1131  694   50\n",
      " 1744 1618 2416  173  257 2394   51 2155 2453 2008 1347 1861  636 1354\n",
      " 2069  711 1370  807 1997  608 1973  128  515  654  471 1391 1381 1227\n",
      " 2184 1920 2256 2119 1362 1123 1752  772 1651 1788  947 2391 1830 1112\n",
      " 2149 1610 1925  891  330 1104   91  630  705 1493   18 1486 2228  922\n",
      "  724  774 2005 2141  945  964 1103  371 1722 2157  338  576 1996 2094\n",
      "  337  909 1041 1378 1959  545 2409 2262 2464 2365 2295 2052 1984 2333\n",
      " 2367 2373] 2069\n",
      "0.9411239624023438\n",
      "Step 1 | Loss: 1.4710490703582764\n",
      "Step 101 | Loss: 0.22212982177734375\n",
      "Step 201 | Loss: 0.14922407269477844\n",
      "Step 301 | Loss: 0.1713043749332428\n",
      "Step 401 | Loss: 0.0437755323946476\n",
      "Step 501 | Loss: 0.1330428421497345\n",
      "Step 601 | Loss: 0.07291671633720398\n",
      "Step 701 | Loss: 0.0826895534992218\n",
      "Step 801 | Loss: 0.12741798162460327\n",
      "Step 901 | Loss: 0.07416185736656189\n",
      "Step 1001 | Loss: 0.06940239667892456\n",
      "Step 1101 | Loss: 0.09622971713542938\n",
      "Step 1201 | Loss: 0.08707133680582047\n",
      "Step 1301 | Loss: 0.09859802573919296\n",
      "Step 1401 | Loss: 0.14761921763420105\n",
      "Step 1501 | Loss: 0.03908667713403702\n",
      "Step 1601 | Loss: 0.29008209705352783\n",
      "Step 1701 | Loss: 0.09152942895889282\n",
      "Step 1801 | Loss: 0.10037557780742645\n",
      "Step 1901 | Loss: 0.11645322293043137\n",
      "Step 2001 | Loss: 0.0892840325832367\n",
      "Step 2101 | Loss: 0.1596326231956482\n",
      "Step 2201 | Loss: 0.10156315565109253\n",
      "Step 2301 | Loss: 0.15947365760803223\n",
      "Step 2401 | Loss: 0.09565450251102448\n",
      "0.09774138055984621 0.975\n",
      "Step 1 | Loss: 0.84147709608078\n",
      "Step 101 | Loss: 0.3325730264186859\n",
      "Step 201 | Loss: 0.3225851058959961\n",
      "Step 301 | Loss: 0.22733157873153687\n",
      "Step 401 | Loss: 0.14009250700473785\n",
      "Step 501 | Loss: 0.13918781280517578\n",
      "Step 601 | Loss: 0.07152645289897919\n",
      "Step 701 | Loss: 0.06064235419034958\n",
      "Step 801 | Loss: 0.1303161233663559\n",
      "Step 901 | Loss: 0.18687137961387634\n",
      "Step 1001 | Loss: 0.15037520229816437\n",
      "Step 1101 | Loss: 0.08402948826551437\n",
      "Step 1201 | Loss: 0.11720074713230133\n",
      "Step 1301 | Loss: 0.17837095260620117\n",
      "Step 1401 | Loss: 0.08302263170480728\n",
      "Step 1501 | Loss: 0.08509606122970581\n",
      "Step 1601 | Loss: 0.11838018894195557\n",
      "Step 1701 | Loss: 0.028827646747231483\n",
      "Step 1801 | Loss: 0.09478943049907684\n",
      "Step 1901 | Loss: 0.05116801708936691\n",
      "Step 2001 | Loss: 0.03226885944604874\n",
      "Step 2101 | Loss: 0.06819658726453781\n",
      "Step 2201 | Loss: 0.05464671552181244\n",
      "Step 2301 | Loss: 0.16634079813957214\n",
      "Step 2401 | Loss: 0.05224171653389931\n",
      "0.09880119605088428 0.9775\n",
      "Step 1 | Loss: 2.0251286029815674\n",
      "Step 101 | Loss: 0.433149516582489\n",
      "Step 201 | Loss: 0.28672343492507935\n",
      "Step 301 | Loss: 0.10757920145988464\n",
      "Step 401 | Loss: 0.061990685760974884\n",
      "Step 501 | Loss: 0.15871816873550415\n",
      "Step 601 | Loss: 0.13271185755729675\n",
      "Step 701 | Loss: 0.15343686938285828\n",
      "Step 801 | Loss: 0.12154177576303482\n",
      "Step 901 | Loss: 0.14547599852085114\n",
      "Step 1001 | Loss: 0.14881840348243713\n",
      "Step 1101 | Loss: 0.08084038645029068\n",
      "Step 1201 | Loss: 0.15124033391475677\n",
      "Step 1301 | Loss: 0.03860709071159363\n",
      "Step 1401 | Loss: 0.14009903371334076\n",
      "Step 1501 | Loss: 0.12864802777767181\n",
      "Step 1601 | Loss: 0.2129322588443756\n",
      "Step 1701 | Loss: 0.09196794033050537\n",
      "Step 1801 | Loss: 0.060102786868810654\n",
      "Step 1901 | Loss: 0.0958641916513443\n",
      "Step 2001 | Loss: 0.1488509476184845\n",
      "Step 2101 | Loss: 0.20776818692684174\n",
      "Step 2201 | Loss: 0.18220646679401398\n",
      "Step 2301 | Loss: 0.0830676406621933\n",
      "Step 2401 | Loss: 0.055135056376457214\n",
      "0.10067794729249835 0.97\n",
      "Step 1 | Loss: 0.9620403051376343\n",
      "Step 101 | Loss: 0.3277530372142792\n",
      "Step 201 | Loss: 0.25390318036079407\n",
      "Step 301 | Loss: 0.1615077555179596\n",
      "Step 401 | Loss: 0.12354642152786255\n",
      "Step 501 | Loss: 0.10150307416915894\n",
      "Step 601 | Loss: 0.19571655988693237\n",
      "Step 701 | Loss: 0.06792198121547699\n",
      "Step 801 | Loss: 0.0966864824295044\n",
      "Step 901 | Loss: 0.06309071183204651\n",
      "Step 1001 | Loss: 0.16199710965156555\n",
      "Step 1101 | Loss: 0.12851771712303162\n",
      "Step 1201 | Loss: 0.21578577160835266\n",
      "Step 1301 | Loss: 0.10826817899942398\n",
      "Step 1401 | Loss: 0.10839589685201645\n",
      "Step 1501 | Loss: 0.07473411411046982\n",
      "Step 1601 | Loss: 0.09308052062988281\n",
      "Step 1701 | Loss: 0.11863885819911957\n",
      "Step 1801 | Loss: 0.07864388078451157\n",
      "Step 1901 | Loss: 0.0300426185131073\n",
      "Step 2001 | Loss: 0.06037179008126259\n",
      "Step 2101 | Loss: 0.0239579938352108\n",
      "Step 2201 | Loss: 0.030887894332408905\n",
      "Step 2301 | Loss: 0.09330227971076965\n",
      "Step 2401 | Loss: 0.15293101966381073\n",
      "0.08955156429587516 0.98\n",
      "Step 1 | Loss: 1.1668431758880615\n",
      "Step 101 | Loss: 0.10926948487758636\n",
      "Step 201 | Loss: 0.07520003616809845\n",
      "Step 301 | Loss: 0.06744753569364548\n",
      "Step 401 | Loss: 0.32792380452156067\n",
      "Step 501 | Loss: 0.1796022653579712\n",
      "Step 601 | Loss: 0.033506639301776886\n",
      "Step 701 | Loss: 0.2658926844596863\n",
      "Step 801 | Loss: 0.15206022560596466\n",
      "Step 901 | Loss: 0.09488841891288757\n",
      "Step 1001 | Loss: 0.16936179995536804\n",
      "Step 1101 | Loss: 0.051613472402095795\n",
      "Step 1201 | Loss: 0.050316642969846725\n",
      "Step 1301 | Loss: 0.2486390322446823\n",
      "Step 1401 | Loss: 0.240666002035141\n",
      "Step 1501 | Loss: 0.08360715210437775\n",
      "Step 1601 | Loss: 0.045457325875759125\n",
      "Step 1701 | Loss: 0.06275980174541473\n",
      "Step 1801 | Loss: 0.10717322677373886\n",
      "Step 1901 | Loss: 0.12572498619556427\n",
      "Step 2001 | Loss: 0.06603098660707474\n",
      "Step 2101 | Loss: 0.08520735055208206\n",
      "Step 2201 | Loss: 0.10513919591903687\n",
      "Step 2301 | Loss: 0.048919521272182465\n",
      "Step 2401 | Loss: 0.11387735605239868\n",
      "0.09433230197229354 0.97\n",
      "2\n",
      "[2160 1314 1969 2443   63  368 2463  920 2407 1277  374  476 1879  989\n",
      "  533  109  916 1423  287 2466 1355 1782 1627 1239 1268 2142 2000 1249\n",
      "  282  159 1323 2414  761  569  507 1978 2108  666 1669  844 2493  384\n",
      " 2401  653 2006  486 1109  146 1162 1985 1417 2331  823 2270  310 1798\n",
      " 2385 2222 2288 1228 1546   74 2358 1642 1990 1657 2480  860 2266  505\n",
      " 1926 1043  209 2465 1976  676 1591  615  988 1842  167   84  635  433\n",
      " 1214 2084 1564  286 1325 2075 2198  586 2011 1581 1470 1292 2474 2405\n",
      "   69 1631] 286\n",
      "0.9172191619873047\n",
      "Step 1 | Loss: 1.1047277450561523\n",
      "Step 101 | Loss: 0.4201493263244629\n",
      "Step 201 | Loss: 0.20712582767009735\n",
      "Step 301 | Loss: 0.15765677392482758\n",
      "Step 401 | Loss: 0.22890324890613556\n",
      "Step 501 | Loss: 0.20287257432937622\n",
      "Step 601 | Loss: 0.1156475767493248\n",
      "Step 701 | Loss: 0.17272263765335083\n",
      "Step 801 | Loss: 0.2511841356754303\n",
      "Step 901 | Loss: 0.16077792644500732\n",
      "Step 1001 | Loss: 0.21160617470741272\n",
      "Step 1101 | Loss: 0.19633269309997559\n",
      "Step 1201 | Loss: 0.09494171291589737\n",
      "Step 1301 | Loss: 0.2964174151420593\n",
      "Step 1401 | Loss: 0.3279913365840912\n",
      "Step 1501 | Loss: 0.2565270960330963\n",
      "Step 1601 | Loss: 0.25972262024879456\n",
      "Step 1701 | Loss: 0.22663739323616028\n",
      "Step 1801 | Loss: 0.1477256864309311\n",
      "Step 1901 | Loss: 0.117534339427948\n",
      "Step 2001 | Loss: 0.15541748702526093\n",
      "Step 2101 | Loss: 0.22546808421611786\n",
      "Step 2201 | Loss: 0.38291430473327637\n",
      "Step 2301 | Loss: 0.25400474667549133\n",
      "Step 2401 | Loss: 0.1519901603460312\n",
      "0.21077459709880633 0.935\n",
      "Step 1 | Loss: 1.3163872957229614\n",
      "Step 101 | Loss: 0.11974519491195679\n",
      "Step 201 | Loss: 0.3167360723018646\n",
      "Step 301 | Loss: 0.22413548827171326\n",
      "Step 401 | Loss: 0.2278827577829361\n",
      "Step 501 | Loss: 0.22836588323116302\n",
      "Step 601 | Loss: 0.1527346670627594\n",
      "Step 701 | Loss: 0.18436455726623535\n",
      "Step 801 | Loss: 0.307650625705719\n",
      "Step 901 | Loss: 0.225015327334404\n",
      "Step 1001 | Loss: 0.3010427951812744\n",
      "Step 1101 | Loss: 0.21256664395332336\n",
      "Step 1201 | Loss: 0.2482488751411438\n",
      "Step 1301 | Loss: 0.21713973581790924\n",
      "Step 1401 | Loss: 0.339150071144104\n",
      "Step 1501 | Loss: 0.15493665635585785\n",
      "Step 1601 | Loss: 0.22844591736793518\n",
      "Step 1701 | Loss: 0.27810138463974\n",
      "Step 1801 | Loss: 0.2390764057636261\n",
      "Step 1901 | Loss: 0.21599888801574707\n",
      "Step 2001 | Loss: 0.43355318903923035\n",
      "Step 2101 | Loss: 0.36703750491142273\n",
      "Step 2201 | Loss: 0.22076112031936646\n",
      "Step 2301 | Loss: 0.1561879813671112\n",
      "Step 2401 | Loss: 0.3002944588661194\n",
      "0.2120451303917538 0.9325\n",
      "Step 1 | Loss: 1.9634290933609009\n",
      "Step 101 | Loss: 0.1320917308330536\n",
      "Step 201 | Loss: 0.23731853067874908\n",
      "Step 301 | Loss: 0.3247600197792053\n",
      "Step 401 | Loss: 0.23482005298137665\n",
      "Step 501 | Loss: 0.23233254253864288\n",
      "Step 601 | Loss: 0.2576868236064911\n",
      "Step 701 | Loss: 0.2850092649459839\n",
      "Step 801 | Loss: 0.23101317882537842\n",
      "Step 901 | Loss: 0.3435492217540741\n",
      "Step 1001 | Loss: 0.23307695984840393\n",
      "Step 1101 | Loss: 0.28612664341926575\n",
      "Step 1201 | Loss: 0.12669163942337036\n",
      "Step 1301 | Loss: 0.22556009888648987\n",
      "Step 1401 | Loss: 0.16188959777355194\n",
      "Step 1501 | Loss: 0.15739254653453827\n",
      "Step 1601 | Loss: 0.15696720778942108\n",
      "Step 1701 | Loss: 0.31970131397247314\n",
      "Step 1801 | Loss: 0.22352102398872375\n",
      "Step 1901 | Loss: 0.2472352683544159\n",
      "Step 2001 | Loss: 0.2695639729499817\n",
      "Step 2101 | Loss: 0.35979485511779785\n",
      "Step 2201 | Loss: 0.11738994717597961\n",
      "Step 2301 | Loss: 0.2385396957397461\n",
      "Step 2401 | Loss: 0.16188547015190125\n",
      "0.2120181035664864 0.9325\n",
      "Step 1 | Loss: 2.0035240650177\n",
      "Step 101 | Loss: 0.22621512413024902\n",
      "Step 201 | Loss: 0.3495776057243347\n",
      "Step 301 | Loss: 0.3199680745601654\n",
      "Step 401 | Loss: 0.2640237510204315\n",
      "Step 501 | Loss: 0.2489340901374817\n",
      "Step 601 | Loss: 0.09994790703058243\n",
      "Step 701 | Loss: 0.21330904960632324\n",
      "Step 801 | Loss: 0.21839338541030884\n",
      "Step 901 | Loss: 0.17927999794483185\n",
      "Step 1001 | Loss: 0.2207646667957306\n",
      "Step 1101 | Loss: 0.17233215272426605\n",
      "Step 1201 | Loss: 0.2678580582141876\n",
      "Step 1301 | Loss: 0.20348626375198364\n",
      "Step 1401 | Loss: 0.2865438163280487\n",
      "Step 1501 | Loss: 0.15224391222000122\n",
      "Step 1601 | Loss: 0.21339836716651917\n",
      "Step 1701 | Loss: 0.22932115197181702\n",
      "Step 1801 | Loss: 0.2106960564851761\n",
      "Step 1901 | Loss: 0.30780458450317383\n",
      "Step 2001 | Loss: 0.13942109048366547\n",
      "Step 2101 | Loss: 0.2557956576347351\n",
      "Step 2201 | Loss: 0.22982589900493622\n",
      "Step 2301 | Loss: 0.4467988610267639\n",
      "Step 2401 | Loss: 0.16779501736164093\n",
      "0.2106577720558077 0.9375\n",
      "Step 1 | Loss: 0.7895818948745728\n",
      "Step 101 | Loss: 0.3531973958015442\n",
      "Step 201 | Loss: 0.1120661050081253\n",
      "Step 301 | Loss: 0.2429816573858261\n",
      "Step 401 | Loss: 0.18959496915340424\n",
      "Step 501 | Loss: 0.2789307236671448\n",
      "Step 601 | Loss: 0.2481229603290558\n",
      "Step 701 | Loss: 0.08560485392808914\n",
      "Step 801 | Loss: 0.17566795647144318\n",
      "Step 901 | Loss: 0.339081734418869\n",
      "Step 1001 | Loss: 0.3203811049461365\n",
      "Step 1101 | Loss: 0.2657761573791504\n",
      "Step 1201 | Loss: 0.22212988138198853\n",
      "Step 1301 | Loss: 0.17686432600021362\n",
      "Step 1401 | Loss: 0.1278730034828186\n",
      "Step 1501 | Loss: 0.16841614246368408\n",
      "Step 1601 | Loss: 0.2838030457496643\n",
      "Step 1701 | Loss: 0.12574663758277893\n",
      "Step 1801 | Loss: 0.09202728420495987\n",
      "Step 1901 | Loss: 0.19669687747955322\n",
      "Step 2001 | Loss: 0.13922487199306488\n",
      "Step 2101 | Loss: 0.2191106379032135\n",
      "Step 2201 | Loss: 0.22026990354061127\n",
      "Step 2301 | Loss: 0.19732262194156647\n",
      "Step 2401 | Loss: 0.14876636862754822\n",
      "0.21132888469251634 0.935\n",
      "3\n",
      "[1524 1858  815  137 1533 1016  358 1612 2475  673 1278 1954 2120 1855\n",
      "  777 1107 2125 1120 2437  377  115  775  598 1720  809 2396 1397  767\n",
      " 2054  701  151  743  391  108  801 1776 1312 2303 1098  657  605  834\n",
      "  251 1838 2294 1983  335   20 1149  952  838  858 1738  739 1787 2400\n",
      " 1810  552  570  432  511 2371  483  873 1596 1477 1696 1063 2023 2282\n",
      " 1500 1133  600  220  769 2291 1481 1141  940 1458  192  731  684  557\n",
      "  908 1294 1940  802 2083 2277 2182  331 2086 1000 2173 1306  241  189\n",
      " 1560  448] 1120\n",
      "0.9259700775146484\n",
      "Step 1 | Loss: 1.0408236980438232\n",
      "Step 101 | Loss: 0.21860447525978088\n",
      "Step 201 | Loss: 0.06718379259109497\n",
      "Step 301 | Loss: 0.15949733555316925\n",
      "Step 401 | Loss: 0.06583379954099655\n",
      "Step 501 | Loss: 0.07807914912700653\n",
      "Step 601 | Loss: 0.043078016489744186\n",
      "Step 701 | Loss: 0.16153281927108765\n",
      "Step 801 | Loss: 0.09221629053354263\n",
      "Step 901 | Loss: 0.10564663261175156\n",
      "Step 1001 | Loss: 0.21451503038406372\n",
      "Step 1101 | Loss: 0.20549790561199188\n",
      "Step 1201 | Loss: 0.09173661470413208\n",
      "Step 1301 | Loss: 0.04693971946835518\n",
      "Step 1401 | Loss: 0.07478125393390656\n",
      "Step 1501 | Loss: 0.17032752931118011\n",
      "Step 1601 | Loss: 0.2005830854177475\n",
      "Step 1701 | Loss: 0.15972232818603516\n",
      "Step 1801 | Loss: 0.15982212126255035\n",
      "Step 1901 | Loss: 0.0631597489118576\n",
      "Step 2001 | Loss: 0.07791803032159805\n",
      "Step 2101 | Loss: 0.052937064319849014\n",
      "Step 2201 | Loss: 0.23873279988765717\n",
      "Step 2301 | Loss: 0.028732096776366234\n",
      "Step 2401 | Loss: 0.3694085478782654\n",
      "0.1090713441852735 0.965\n",
      "Step 1 | Loss: 0.7884625792503357\n",
      "Step 101 | Loss: 0.03218315169215202\n",
      "Step 201 | Loss: 0.20061150193214417\n",
      "Step 301 | Loss: 0.08833951503038406\n",
      "Step 401 | Loss: 0.18252839148044586\n",
      "Step 501 | Loss: 0.07948613911867142\n",
      "Step 601 | Loss: 0.04336957260966301\n",
      "Step 701 | Loss: 0.11209779232740402\n",
      "Step 801 | Loss: 0.11050884425640106\n",
      "Step 901 | Loss: 0.06450482457876205\n",
      "Step 1001 | Loss: 0.048509109765291214\n",
      "Step 1101 | Loss: 0.051331546157598495\n",
      "Step 1201 | Loss: 0.04987375810742378\n",
      "Step 1301 | Loss: 0.07182882726192474\n",
      "Step 1401 | Loss: 0.11970888823270798\n",
      "Step 1501 | Loss: 0.16463619470596313\n",
      "Step 1601 | Loss: 0.035499487072229385\n",
      "Step 1701 | Loss: 0.0566994771361351\n",
      "Step 1801 | Loss: 0.05131581798195839\n",
      "Step 1901 | Loss: 0.020261194556951523\n",
      "Step 2001 | Loss: 0.2989901006221771\n",
      "Step 2101 | Loss: 0.16448020935058594\n",
      "Step 2201 | Loss: 0.09984352439641953\n",
      "Step 2301 | Loss: 0.08077152073383331\n",
      "Step 2401 | Loss: 0.03324633091688156\n",
      "0.10329949087766367 0.9725\n",
      "Step 1 | Loss: 0.7666528820991516\n",
      "Step 101 | Loss: 0.14648820459842682\n",
      "Step 201 | Loss: 0.2166203260421753\n",
      "Step 301 | Loss: 0.05322857201099396\n",
      "Step 401 | Loss: 0.1400587409734726\n",
      "Step 501 | Loss: 0.1891908049583435\n",
      "Step 601 | Loss: 0.17400583624839783\n",
      "Step 701 | Loss: 0.16293227672576904\n",
      "Step 801 | Loss: 0.2410772442817688\n",
      "Step 901 | Loss: 0.04907958582043648\n",
      "Step 1001 | Loss: 0.16427908837795258\n",
      "Step 1101 | Loss: 0.31693035364151\n",
      "Step 1201 | Loss: 0.10985826700925827\n",
      "Step 1301 | Loss: 0.08948910236358643\n",
      "Step 1401 | Loss: 0.16967302560806274\n",
      "Step 1501 | Loss: 0.04468753933906555\n",
      "Step 1601 | Loss: 0.12522724270820618\n",
      "Step 1701 | Loss: 0.07700756937265396\n",
      "Step 1801 | Loss: 0.1930604875087738\n",
      "Step 1901 | Loss: 0.16502857208251953\n",
      "Step 2001 | Loss: 0.11846402287483215\n",
      "Step 2101 | Loss: 0.10391957312822342\n",
      "Step 2201 | Loss: 0.11323883384466171\n",
      "Step 2301 | Loss: 0.095884308218956\n",
      "Step 2401 | Loss: 0.21245770156383514\n",
      "0.12202038710655562 0.965\n",
      "Step 1 | Loss: 2.0404574871063232\n",
      "Step 101 | Loss: 0.18937528133392334\n",
      "Step 201 | Loss: 0.0531340092420578\n",
      "Step 301 | Loss: 0.09185989946126938\n",
      "Step 401 | Loss: 0.06821980327367783\n",
      "Step 501 | Loss: 0.0794748067855835\n",
      "Step 601 | Loss: 0.15749727189540863\n",
      "Step 701 | Loss: 0.07658099383115768\n",
      "Step 801 | Loss: 0.066877581179142\n",
      "Step 901 | Loss: 0.038418129086494446\n",
      "Step 1001 | Loss: 0.09111600369215012\n",
      "Step 1101 | Loss: 0.051223259419202805\n",
      "Step 1201 | Loss: 0.06643349677324295\n",
      "Step 1301 | Loss: 0.0591471903026104\n",
      "Step 1401 | Loss: 0.15853187441825867\n",
      "Step 1501 | Loss: 0.0865991860628128\n",
      "Step 1601 | Loss: 0.0293659009039402\n",
      "Step 1701 | Loss: 0.11323226243257523\n",
      "Step 1801 | Loss: 0.049133189022541046\n",
      "Step 1901 | Loss: 0.17892338335514069\n",
      "Step 2001 | Loss: 0.01875990629196167\n",
      "Step 2101 | Loss: 0.11129927635192871\n",
      "Step 2201 | Loss: 0.0851208046078682\n",
      "Step 2301 | Loss: 0.043329402804374695\n",
      "Step 2401 | Loss: 0.1432270109653473\n",
      "0.10165986753330013 0.9725\n",
      "Step 1 | Loss: 0.9757659435272217\n",
      "Step 101 | Loss: 0.16434462368488312\n",
      "Step 201 | Loss: 0.15973223745822906\n",
      "Step 301 | Loss: 0.08603759109973907\n",
      "Step 401 | Loss: 0.21272064745426178\n",
      "Step 501 | Loss: 0.1464398056268692\n",
      "Step 601 | Loss: 0.1747083216905594\n",
      "Step 701 | Loss: 0.1511923372745514\n",
      "Step 801 | Loss: 0.07536677271127701\n",
      "Step 901 | Loss: 0.09995025396347046\n",
      "Step 1001 | Loss: 0.20329506695270538\n",
      "Step 1101 | Loss: 0.11278335005044937\n",
      "Step 1201 | Loss: 0.15596196055412292\n",
      "Step 1301 | Loss: 0.14943751692771912\n",
      "Step 1401 | Loss: 0.09695860743522644\n",
      "Step 1501 | Loss: 0.13818001747131348\n",
      "Step 1601 | Loss: 0.07923219352960587\n",
      "Step 1701 | Loss: 0.18512563407421112\n",
      "Step 1801 | Loss: 0.1353582739830017\n",
      "Step 1901 | Loss: 0.11117956042289734\n",
      "Step 2001 | Loss: 0.12064643204212189\n",
      "Step 2101 | Loss: 0.1669745147228241\n",
      "Step 2201 | Loss: 0.05468906834721565\n",
      "Step 2301 | Loss: 0.3297902047634125\n",
      "Step 2401 | Loss: 0.03307494893670082\n",
      "0.09524432197935756 0.9775\n",
      "4\n",
      "[1091 1989  206 2097  494 1332 1849 2096  468  436 2455 2081 1015 1663\n",
      "   52 1713  571 2484  567 1490 2196 2080  901 1265 2042 2098  721 1979\n",
      " 1405 1589  845 2016 1124  639 1845 2026 1756 1134 1118 1566 1734 1254\n",
      "  725  592 1241 1202 2242 2205 1604 1737 1431 1502  477 2343  582 2154\n",
      " 1443 2337  130 1003 1692   54  937  984 2223 2301 2498 1462  935 1359\n",
      "  790  497  340  106 2384  334  442 2390 1121  957 2297  865   96   49\n",
      " 1305 1430   56 1723  254 1582  397 1615 2191 2070 1567 2359 1699  710\n",
      " 2227  495] 1091\n",
      "0.8823089599609375\n",
      "Step 1 | Loss: 1.0446785688400269\n",
      "Step 101 | Loss: 0.27703869342803955\n",
      "Step 201 | Loss: 0.2771068215370178\n",
      "Step 301 | Loss: 0.4410373568534851\n",
      "Step 401 | Loss: 0.270341694355011\n",
      "Step 501 | Loss: 0.18402035534381866\n",
      "Step 601 | Loss: 0.1697782576084137\n",
      "Step 701 | Loss: 0.2918858528137207\n",
      "Step 801 | Loss: 0.19150781631469727\n",
      "Step 901 | Loss: 0.17275992035865784\n",
      "Step 1001 | Loss: 0.25852513313293457\n",
      "Step 1101 | Loss: 0.25670120120048523\n",
      "Step 1201 | Loss: 0.18567126989364624\n",
      "Step 1301 | Loss: 0.16972938179969788\n",
      "Step 1401 | Loss: 0.13393069803714752\n",
      "Step 1501 | Loss: 0.14279374480247498\n",
      "Step 1601 | Loss: 0.2943163216114044\n",
      "Step 1701 | Loss: 0.20796261727809906\n",
      "Step 1801 | Loss: 0.1594589501619339\n",
      "Step 1901 | Loss: 0.10627931356430054\n",
      "Step 2001 | Loss: 0.16578485071659088\n",
      "Step 2101 | Loss: 0.2577173113822937\n",
      "Step 2201 | Loss: 0.28208404779434204\n",
      "Step 2301 | Loss: 0.201472207903862\n",
      "Step 2401 | Loss: 0.2539878487586975\n",
      "0.20346740805362898 0.9625\n",
      "Step 1 | Loss: 1.1163265705108643\n",
      "Step 101 | Loss: 0.22901999950408936\n",
      "Step 201 | Loss: 0.12269103527069092\n",
      "Step 301 | Loss: 0.15372799336910248\n",
      "Step 401 | Loss: 0.25614646077156067\n",
      "Step 501 | Loss: 0.16490858793258667\n",
      "Step 601 | Loss: 0.10878177732229233\n",
      "Step 701 | Loss: 0.196790412068367\n",
      "Step 801 | Loss: 0.25580042600631714\n",
      "Step 901 | Loss: 0.05923350527882576\n",
      "Step 1001 | Loss: 0.19719815254211426\n",
      "Step 1101 | Loss: 0.15681882202625275\n",
      "Step 1201 | Loss: 0.15058912336826324\n",
      "Step 1301 | Loss: 0.242267444729805\n",
      "Step 1401 | Loss: 0.14051519334316254\n",
      "Step 1501 | Loss: 0.26843276619911194\n",
      "Step 1601 | Loss: 0.2920529842376709\n",
      "Step 1701 | Loss: 0.22622530162334442\n",
      "Step 1801 | Loss: 0.23832710087299347\n",
      "Step 1901 | Loss: 0.15840573608875275\n",
      "Step 2001 | Loss: 0.13281182944774628\n",
      "Step 2101 | Loss: 0.1424534171819687\n",
      "Step 2201 | Loss: 0.20712614059448242\n",
      "Step 2301 | Loss: 0.1429412066936493\n",
      "Step 2401 | Loss: 0.17447727918624878\n",
      "0.20204335763035675 0.9625\n",
      "Step 1 | Loss: 0.652306079864502\n",
      "Step 101 | Loss: 0.3590613901615143\n",
      "Step 201 | Loss: 0.14744937419891357\n",
      "Step 301 | Loss: 0.2812023162841797\n",
      "Step 401 | Loss: 0.15076999366283417\n",
      "Step 501 | Loss: 0.11686474829912186\n",
      "Step 601 | Loss: 0.10668179392814636\n",
      "Step 701 | Loss: 0.2462780624628067\n",
      "Step 801 | Loss: 0.29418328404426575\n",
      "Step 901 | Loss: 0.23034873604774475\n",
      "Step 1001 | Loss: 0.16487522423267365\n",
      "Step 1101 | Loss: 0.22142857313156128\n",
      "Step 1201 | Loss: 0.2768647074699402\n",
      "Step 1301 | Loss: 0.14193759858608246\n",
      "Step 1401 | Loss: 0.31333819031715393\n",
      "Step 1501 | Loss: 0.19416087865829468\n",
      "Step 1601 | Loss: 0.13168205320835114\n",
      "Step 1701 | Loss: 0.22576549649238586\n",
      "Step 1801 | Loss: 0.17893002927303314\n",
      "Step 1901 | Loss: 0.10243682563304901\n",
      "Step 2001 | Loss: 0.12098488211631775\n",
      "Step 2101 | Loss: 0.08753420412540436\n",
      "Step 2201 | Loss: 0.14035512506961823\n",
      "Step 2301 | Loss: 0.21559512615203857\n",
      "Step 2401 | Loss: 0.17186260223388672\n",
      "0.20348023899677412 0.965\n",
      "Step 1 | Loss: 1.9599716663360596\n",
      "Step 101 | Loss: 0.15548300743103027\n",
      "Step 201 | Loss: 0.29192882776260376\n",
      "Step 301 | Loss: 0.09686283767223358\n",
      "Step 401 | Loss: 0.25755491852760315\n",
      "Step 501 | Loss: 0.2040194422006607\n",
      "Step 601 | Loss: 0.24175035953521729\n",
      "Step 701 | Loss: 0.18982192873954773\n",
      "Step 801 | Loss: 0.14482134580612183\n",
      "Step 901 | Loss: 0.17837782204151154\n",
      "Step 1001 | Loss: 0.13034451007843018\n",
      "Step 1101 | Loss: 0.168683260679245\n",
      "Step 1201 | Loss: 0.30478259921073914\n",
      "Step 1301 | Loss: 0.1959768831729889\n",
      "Step 1401 | Loss: 0.169732928276062\n",
      "Step 1501 | Loss: 0.3535829484462738\n",
      "Step 1601 | Loss: 0.2918683886528015\n",
      "Step 1701 | Loss: 0.26163148880004883\n",
      "Step 1801 | Loss: 0.19354787468910217\n",
      "Step 1901 | Loss: 0.17774756252765656\n",
      "Step 2001 | Loss: 0.18306884169578552\n",
      "Step 2101 | Loss: 0.20298950374126434\n",
      "Step 2201 | Loss: 0.13154947757720947\n",
      "Step 2301 | Loss: 0.2078041285276413\n",
      "Step 2401 | Loss: 0.19802750647068024\n",
      "0.20461142908903468 0.97\n",
      "Step 1 | Loss: 0.996667206287384\n",
      "Step 101 | Loss: 0.37734466791152954\n",
      "Step 201 | Loss: 0.3915082812309265\n",
      "Step 301 | Loss: 0.4837864935398102\n",
      "Step 401 | Loss: 0.3639376759529114\n",
      "Step 501 | Loss: 0.1995810717344284\n",
      "Step 601 | Loss: 0.3345848321914673\n",
      "Step 701 | Loss: 0.2646631896495819\n",
      "Step 801 | Loss: 0.28325915336608887\n",
      "Step 901 | Loss: 0.3696216344833374\n",
      "Step 1001 | Loss: 0.24784162640571594\n",
      "Step 1101 | Loss: 0.356727659702301\n",
      "Step 1201 | Loss: 0.5551247596740723\n",
      "Step 1301 | Loss: 0.3378969728946686\n",
      "Step 1401 | Loss: 0.20229803025722504\n",
      "Step 1501 | Loss: 0.348147451877594\n",
      "Step 1601 | Loss: 0.380393385887146\n",
      "Step 1701 | Loss: 0.3747274875640869\n",
      "Step 1801 | Loss: 0.5194588303565979\n",
      "Step 1901 | Loss: 0.42290568351745605\n",
      "Step 2001 | Loss: 0.4235299527645111\n",
      "Step 2101 | Loss: 0.2784543037414551\n",
      "Step 2201 | Loss: 0.25521281361579895\n",
      "Step 2301 | Loss: 0.45473042130470276\n",
      "Step 2401 | Loss: 0.3152425289154053\n",
      "0.3287012642672058 0.935\n",
      "5\n",
      "[ 835 1201  481  463 1229  516  993  392 1709  141 2136  741  469  634\n",
      " 2325 1167 1453 2068 2112   23 1179  859 1850 2259  393 2485 1947  712\n",
      "  959 1641 1899 1886  336 1950  351  755  345 1006  915 1322  911   39\n",
      " 1255 2440 1126  237 1026  720 2221  524 1539  700 1762  181 1806  942\n",
      " 1385 1426  543 2287 1307 2476  782 2148   25  434  977 1775 1750  538\n",
      " 1758 1766 1242 1335 1800  780 1142 2167 1258  596 2346 1534 1907 2110\n",
      " 2290   19 1368 2048 1288 1159  606 1971   41 1143 1114   83  826 1860\n",
      "  302  629] 1950\n",
      "0.9495487213134766\n",
      "Step 1 | Loss: 1.057870864868164\n",
      "Step 101 | Loss: 0.9665570259094238\n",
      "Step 201 | Loss: 0.6629628539085388\n",
      "Step 301 | Loss: 0.7085723876953125\n",
      "Step 401 | Loss: 0.7328003644943237\n",
      "Step 501 | Loss: 0.5761079788208008\n",
      "Step 601 | Loss: 0.5478734970092773\n",
      "Step 701 | Loss: 0.6398047804832458\n",
      "Step 801 | Loss: 0.6218579411506653\n",
      "Step 901 | Loss: 0.6611413359642029\n",
      "Step 1001 | Loss: 0.6386121511459351\n",
      "Step 1101 | Loss: 0.6167440414428711\n",
      "Step 1201 | Loss: 0.6368886232376099\n",
      "Step 1301 | Loss: 0.5131543874740601\n",
      "Step 1401 | Loss: 0.7328318953514099\n",
      "Step 1501 | Loss: 0.5480079650878906\n",
      "Step 1601 | Loss: 0.5641596913337708\n",
      "Step 1701 | Loss: 0.7588312029838562\n",
      "Step 1801 | Loss: 0.5398154258728027\n",
      "Step 1901 | Loss: 0.5628016591072083\n",
      "Step 2001 | Loss: 0.5484824776649475\n",
      "Step 2101 | Loss: 0.5329799652099609\n",
      "Step 2201 | Loss: 0.6353923082351685\n",
      "Step 2301 | Loss: 0.6470169425010681\n",
      "Step 2401 | Loss: 0.5768790245056152\n",
      "0.5507594305277437 0.8275\n",
      "Step 1 | Loss: 1.1205813884735107\n",
      "Step 101 | Loss: 0.6267310976982117\n",
      "Step 201 | Loss: 0.5813539624214172\n",
      "Step 301 | Loss: 0.5548942685127258\n",
      "Step 401 | Loss: 0.5528055429458618\n",
      "Step 501 | Loss: 0.8135616183280945\n",
      "Step 601 | Loss: 0.651644229888916\n",
      "Step 701 | Loss: 0.7112162113189697\n",
      "Step 801 | Loss: 0.8531849384307861\n",
      "Step 901 | Loss: 0.6180837750434875\n",
      "Step 1001 | Loss: 0.5832717418670654\n",
      "Step 1101 | Loss: 0.7965363264083862\n",
      "Step 1201 | Loss: 0.675817608833313\n",
      "Step 1301 | Loss: 0.5116026401519775\n",
      "Step 1401 | Loss: 0.5602166056632996\n",
      "Step 1501 | Loss: 0.7303708791732788\n",
      "Step 1601 | Loss: 0.5463848114013672\n",
      "Step 1701 | Loss: 0.6089610457420349\n",
      "Step 1801 | Loss: 0.5436195731163025\n",
      "Step 1901 | Loss: 0.6355007886886597\n",
      "Step 2001 | Loss: 0.5688381195068359\n",
      "Step 2101 | Loss: 0.623356282711029\n",
      "Step 2201 | Loss: 0.8129200339317322\n",
      "Step 2301 | Loss: 0.6127879619598389\n",
      "Step 2401 | Loss: 0.7023477554321289\n",
      "0.5522415690377026 0.84\n",
      "Step 1 | Loss: 0.9712324142456055\n",
      "Step 101 | Loss: 0.6723839640617371\n",
      "Step 201 | Loss: 0.5403469800949097\n",
      "Step 301 | Loss: 0.6046910285949707\n",
      "Step 401 | Loss: 0.668224573135376\n",
      "Step 501 | Loss: 0.6255563497543335\n",
      "Step 601 | Loss: 0.5127967596054077\n",
      "Step 701 | Loss: 0.7624915838241577\n",
      "Step 801 | Loss: 0.46691229939460754\n",
      "Step 901 | Loss: 0.7048770189285278\n",
      "Step 1001 | Loss: 0.5653012990951538\n",
      "Step 1101 | Loss: 0.6692191362380981\n",
      "Step 1201 | Loss: 0.6060186624526978\n",
      "Step 1301 | Loss: 0.7512300610542297\n",
      "Step 1401 | Loss: 0.7097293138504028\n",
      "Step 1501 | Loss: 0.6142551898956299\n",
      "Step 1601 | Loss: 0.5286904573440552\n",
      "Step 1701 | Loss: 0.5279605388641357\n",
      "Step 1801 | Loss: 0.5576483011245728\n",
      "Step 1901 | Loss: 0.4992556571960449\n",
      "Step 2001 | Loss: 0.5681549310684204\n",
      "Step 2101 | Loss: 0.6823056936264038\n",
      "Step 2201 | Loss: 0.6775593757629395\n",
      "Step 2301 | Loss: 0.4287610352039337\n",
      "Step 2401 | Loss: 0.6290925145149231\n",
      "0.5517180697761188 0.8475\n",
      "Step 1 | Loss: 1.8638495206832886\n",
      "Step 101 | Loss: 0.7119296193122864\n",
      "Step 201 | Loss: 0.7257106304168701\n",
      "Step 301 | Loss: 0.80120849609375\n",
      "Step 401 | Loss: 0.5631292462348938\n",
      "Step 501 | Loss: 0.6059356927871704\n",
      "Step 601 | Loss: 0.6501734256744385\n",
      "Step 701 | Loss: 0.49780452251434326\n",
      "Step 801 | Loss: 0.5572055578231812\n",
      "Step 901 | Loss: 0.6377130150794983\n",
      "Step 1001 | Loss: 0.3873368501663208\n",
      "Step 1101 | Loss: 0.6592728495597839\n",
      "Step 1201 | Loss: 0.7171944975852966\n",
      "Step 1301 | Loss: 0.6155261993408203\n",
      "Step 1401 | Loss: 0.3930127024650574\n",
      "Step 1501 | Loss: 0.4770515561103821\n",
      "Step 1601 | Loss: 0.5444728136062622\n",
      "Step 1701 | Loss: 0.8356121182441711\n",
      "Step 1801 | Loss: 0.5922648906707764\n",
      "Step 1901 | Loss: 0.5258111953735352\n",
      "Step 2001 | Loss: 0.5028494000434875\n",
      "Step 2101 | Loss: 0.4784632921218872\n",
      "Step 2201 | Loss: 0.6396924257278442\n",
      "Step 2301 | Loss: 0.5914454460144043\n",
      "Step 2401 | Loss: 0.5513739585876465\n",
      "0.5358342999607214 0.8525\n",
      "Step 1 | Loss: 1.9079833030700684\n",
      "Step 101 | Loss: 1.0290756225585938\n",
      "Step 201 | Loss: 0.8148642778396606\n",
      "Step 301 | Loss: 0.6616281867027283\n",
      "Step 401 | Loss: 0.8648228645324707\n",
      "Step 501 | Loss: 0.5979698300361633\n",
      "Step 601 | Loss: 0.500336766242981\n",
      "Step 701 | Loss: 0.37731266021728516\n",
      "Step 801 | Loss: 0.5991619825363159\n",
      "Step 901 | Loss: 0.607213020324707\n",
      "Step 1001 | Loss: 0.6448169946670532\n",
      "Step 1101 | Loss: 0.6511735320091248\n",
      "Step 1201 | Loss: 0.7090818285942078\n",
      "Step 1301 | Loss: 0.5595176219940186\n",
      "Step 1401 | Loss: 0.5784188508987427\n",
      "Step 1501 | Loss: 0.7454742193222046\n",
      "Step 1601 | Loss: 0.5816141963005066\n",
      "Step 1701 | Loss: 0.6156877875328064\n",
      "Step 1801 | Loss: 0.8048480749130249\n",
      "Step 1901 | Loss: 0.6062657237052917\n",
      "Step 2001 | Loss: 0.8211331367492676\n",
      "Step 2101 | Loss: 0.5223218202590942\n",
      "Step 2201 | Loss: 0.5078395009040833\n",
      "Step 2301 | Loss: 0.782028079032898\n",
      "Step 2401 | Loss: 0.7312642931938171\n",
      "0.5363108990387253 0.8525\n",
      "6\n",
      "[ 438  455 1876  403  744 2224 1475  768 1119 1906  572 1553 1286 2146\n",
      " 1135  733  537 1464 1187 1922  367 2170 1793  765 2456 2422  118 1459\n",
      " 1697 1607 1081  927  999  283 2408   68 2117 2352  117  216 1689 2279\n",
      "  698 1523 1839  671 1002  519  944  722 1636 1575 2388 1153  333 1759\n",
      "   34  651 2483  528  962 1386 1030 1648  737  778 1999  205 1885 1082\n",
      " 1820  837 1374  948 1511  134  994 2462 2204  974 1116  162 1863  770\n",
      " 1815 2355 1822 1501 1320 2399 1694  313  473   85  387  475  781 2057\n",
      " 1108 1357] 1511\n",
      "0.9269657135009766\n",
      "Step 1 | Loss: 0.8786702752113342\n",
      "Step 101 | Loss: 0.15254655480384827\n",
      "Step 201 | Loss: 0.17216260731220245\n",
      "Step 301 | Loss: 0.17494525015354156\n",
      "Step 401 | Loss: 0.19359657168388367\n",
      "Step 501 | Loss: 0.09971640259027481\n",
      "Step 601 | Loss: 0.19099323451519012\n",
      "Step 701 | Loss: 0.12717874348163605\n",
      "Step 801 | Loss: 0.2091996669769287\n",
      "Step 901 | Loss: 0.1416950821876526\n",
      "Step 1001 | Loss: 0.14984683692455292\n",
      "Step 1101 | Loss: 0.2084444761276245\n",
      "Step 1201 | Loss: 0.2918663024902344\n",
      "Step 1301 | Loss: 0.11591600626707077\n",
      "Step 1401 | Loss: 0.1470019817352295\n",
      "Step 1501 | Loss: 0.1999395191669464\n",
      "Step 1601 | Loss: 0.07405249774456024\n",
      "Step 1701 | Loss: 0.2544234097003937\n",
      "Step 1801 | Loss: 0.3687775731086731\n",
      "Step 1901 | Loss: 0.09655245393514633\n",
      "Step 2001 | Loss: 0.1922251582145691\n",
      "Step 2101 | Loss: 0.1664777547121048\n",
      "Step 2201 | Loss: 0.27534762024879456\n",
      "Step 2301 | Loss: 0.17844025790691376\n",
      "Step 2401 | Loss: 0.11297547817230225\n",
      "0.16733663414133312 0.96\n",
      "Step 1 | Loss: 0.9230377674102783\n",
      "Step 101 | Loss: 0.5624292492866516\n",
      "Step 201 | Loss: 0.4565298557281494\n",
      "Step 301 | Loss: 0.23296424746513367\n",
      "Step 401 | Loss: 0.2978336215019226\n",
      "Step 501 | Loss: 0.19274158775806427\n",
      "Step 601 | Loss: 0.415168821811676\n",
      "Step 701 | Loss: 0.18820491433143616\n",
      "Step 801 | Loss: 0.3234216272830963\n",
      "Step 901 | Loss: 0.139893040060997\n",
      "Step 1001 | Loss: 0.15796752274036407\n",
      "Step 1101 | Loss: 0.17438462376594543\n",
      "Step 1201 | Loss: 0.21514663100242615\n",
      "Step 1301 | Loss: 0.3095412254333496\n",
      "Step 1401 | Loss: 0.2989850342273712\n",
      "Step 1501 | Loss: 0.35417333245277405\n",
      "Step 1601 | Loss: 0.2891848087310791\n",
      "Step 1701 | Loss: 0.2004327028989792\n",
      "Step 1801 | Loss: 0.19604212045669556\n",
      "Step 1901 | Loss: 0.35448816418647766\n",
      "Step 2001 | Loss: 0.18389785289764404\n",
      "Step 2101 | Loss: 0.5275030136108398\n",
      "Step 2201 | Loss: 0.3281555473804474\n",
      "Step 2301 | Loss: 0.5035111308097839\n",
      "Step 2401 | Loss: 0.413300484418869\n",
      "0.2584963232168744 0.9325\n",
      "Step 1 | Loss: 1.0900698900222778\n",
      "Step 101 | Loss: 0.29955190420150757\n",
      "Step 201 | Loss: 0.14018359780311584\n",
      "Step 301 | Loss: 0.14422911405563354\n",
      "Step 401 | Loss: 0.3207545280456543\n",
      "Step 501 | Loss: 0.1657930165529251\n",
      "Step 601 | Loss: 0.06420933455228806\n",
      "Step 701 | Loss: 0.17298297584056854\n",
      "Step 801 | Loss: 0.2752213180065155\n",
      "Step 901 | Loss: 0.21750926971435547\n",
      "Step 1001 | Loss: 0.10326060652732849\n",
      "Step 1101 | Loss: 0.11484416574239731\n",
      "Step 1201 | Loss: 0.22675061225891113\n",
      "Step 1301 | Loss: 0.22018955647945404\n",
      "Step 1401 | Loss: 0.20028620958328247\n",
      "Step 1501 | Loss: 0.13302607834339142\n",
      "Step 1601 | Loss: 0.2500531077384949\n",
      "Step 1701 | Loss: 0.1891440600156784\n",
      "Step 1801 | Loss: 0.07216309010982513\n",
      "Step 1901 | Loss: 0.21737365424633026\n",
      "Step 2001 | Loss: 0.23942553997039795\n",
      "Step 2101 | Loss: 0.14170093834400177\n",
      "Step 2201 | Loss: 0.20417147874832153\n",
      "Step 2301 | Loss: 0.24104014039039612\n",
      "Step 2401 | Loss: 0.18230414390563965\n",
      "0.1761033298549246 0.9625\n",
      "Step 1 | Loss: 0.9353053569793701\n",
      "Step 101 | Loss: 0.47034692764282227\n",
      "Step 201 | Loss: 0.3938788175582886\n",
      "Step 301 | Loss: 0.40018147230148315\n",
      "Step 401 | Loss: 0.4461802542209625\n",
      "Step 501 | Loss: 0.38085103034973145\n",
      "Step 601 | Loss: 0.3263309895992279\n",
      "Step 701 | Loss: 0.45482486486434937\n",
      "Step 801 | Loss: 0.5461875796318054\n",
      "Step 901 | Loss: 0.2641858160495758\n",
      "Step 1001 | Loss: 0.3495291471481323\n",
      "Step 1101 | Loss: 0.357390820980072\n",
      "Step 1201 | Loss: 0.44221702218055725\n",
      "Step 1301 | Loss: 0.3066873848438263\n",
      "Step 1401 | Loss: 0.3602601885795593\n",
      "Step 1501 | Loss: 0.4410746991634369\n",
      "Step 1601 | Loss: 0.39934733510017395\n",
      "Step 1701 | Loss: 0.4796205759048462\n",
      "Step 1801 | Loss: 0.38593944907188416\n",
      "Step 1901 | Loss: 0.4159107804298401\n",
      "Step 2001 | Loss: 0.35327744483947754\n",
      "Step 2101 | Loss: 0.33876022696495056\n",
      "Step 2201 | Loss: 0.34369105100631714\n",
      "Step 2301 | Loss: 0.49114876985549927\n",
      "Step 2401 | Loss: 0.34296274185180664\n",
      "0.3840471568466616 0.905\n",
      "Step 1 | Loss: 0.4792850613594055\n",
      "Step 101 | Loss: 0.2313845157623291\n",
      "Step 201 | Loss: 0.1866355687379837\n",
      "Step 301 | Loss: 0.21971118450164795\n",
      "Step 401 | Loss: 0.22543096542358398\n",
      "Step 501 | Loss: 0.10019785165786743\n",
      "Step 601 | Loss: 0.11014439910650253\n",
      "Step 701 | Loss: 0.29292771220207214\n",
      "Step 801 | Loss: 0.31604066491127014\n",
      "Step 901 | Loss: 0.13489550352096558\n",
      "Step 1001 | Loss: 0.1582096964120865\n",
      "Step 1101 | Loss: 0.09459014236927032\n",
      "Step 1201 | Loss: 0.1394433081150055\n",
      "Step 1301 | Loss: 0.11132204532623291\n",
      "Step 1401 | Loss: 0.10374049842357635\n",
      "Step 1501 | Loss: 0.25871312618255615\n",
      "Step 1601 | Loss: 0.2648891806602478\n",
      "Step 1701 | Loss: 0.1815873682498932\n",
      "Step 1801 | Loss: 0.11840774118900299\n",
      "Step 1901 | Loss: 0.1347338855266571\n",
      "Step 2001 | Loss: 0.22463852167129517\n",
      "Step 2101 | Loss: 0.12431145459413528\n",
      "Step 2201 | Loss: 0.24780477583408356\n",
      "Step 2301 | Loss: 0.09633045643568039\n",
      "Step 2401 | Loss: 0.16081541776657104\n",
      "0.1674347929329177 0.9575\n",
      "7\n",
      "[1710  811  165  938 1778 1482 1182  353 1004  352  155 1088  501 1587\n",
      "  894  196 1261  617  208  407 2145  623  864 2003  306  276 1783  650\n",
      " 1514 1851 1945 2392 1902  819 1419  427 2039  452 1725 2219 2122  116\n",
      " 1791 1661 1769 1274  773   38 1856 1001 1893 2478  899  100  759 1070\n",
      " 1743   59  496 1666 1422  987  644 2273 2233 1076  236 1716  786 1897\n",
      " 1676 1554  850  487 1518 2206  878 2082  707  223 2217 1561 1439  440\n",
      "   26 1328 1570 1597 1735 2340 2079 2410 2332 1165  349 2089 1395  584\n",
      " 1046 1507] 349\n",
      "0.9057235717773438\n",
      "Step 1 | Loss: 1.1277652978897095\n",
      "Step 101 | Loss: 0.10287000983953476\n",
      "Step 201 | Loss: 0.09138384461402893\n",
      "Step 301 | Loss: 0.12274423241615295\n",
      "Step 401 | Loss: 0.19682428240776062\n",
      "Step 501 | Loss: 0.2548135817050934\n",
      "Step 601 | Loss: 0.2775484323501587\n",
      "Step 701 | Loss: 0.080807626247406\n",
      "Step 801 | Loss: 0.1597161740064621\n",
      "Step 901 | Loss: 0.07371742278337479\n",
      "Step 1001 | Loss: 0.163142129778862\n",
      "Step 1101 | Loss: 0.1441912204027176\n",
      "Step 1201 | Loss: 0.21422934532165527\n",
      "Step 1301 | Loss: 0.1618485450744629\n",
      "Step 1401 | Loss: 0.111727774143219\n",
      "Step 1501 | Loss: 0.13433629274368286\n",
      "Step 1601 | Loss: 0.16836519539356232\n",
      "Step 1701 | Loss: 0.1605312079191208\n",
      "Step 1801 | Loss: 0.32694071531295776\n",
      "Step 1901 | Loss: 0.10476027429103851\n",
      "Step 2001 | Loss: 0.20462805032730103\n",
      "Step 2101 | Loss: 0.21024836599826813\n",
      "Step 2201 | Loss: 0.10164416581392288\n",
      "Step 2301 | Loss: 0.06959453970193863\n",
      "Step 2401 | Loss: 0.18344533443450928\n",
      "0.10711629588928316 0.97\n",
      "Step 1 | Loss: 1.1439563035964966\n",
      "Step 101 | Loss: 0.16894078254699707\n",
      "Step 201 | Loss: 0.12164152413606644\n",
      "Step 301 | Loss: 0.08275053650140762\n",
      "Step 401 | Loss: 0.10368359088897705\n",
      "Step 501 | Loss: 0.21840128302574158\n",
      "Step 601 | Loss: 0.21067927777767181\n",
      "Step 701 | Loss: 0.31292736530303955\n",
      "Step 801 | Loss: 0.06782393157482147\n",
      "Step 901 | Loss: 0.46175718307495117\n",
      "Step 1001 | Loss: 0.21769094467163086\n",
      "Step 1101 | Loss: 0.11216309666633606\n",
      "Step 1201 | Loss: 0.07048626989126205\n",
      "Step 1301 | Loss: 0.09612954407930374\n",
      "Step 1401 | Loss: 0.04337311536073685\n",
      "Step 1501 | Loss: 0.11189786344766617\n",
      "Step 1601 | Loss: 0.11435092240571976\n",
      "Step 1701 | Loss: 0.15452702343463898\n",
      "Step 1801 | Loss: 0.17718327045440674\n",
      "Step 1901 | Loss: 0.1598106026649475\n",
      "Step 2001 | Loss: 0.06753582507371902\n",
      "Step 2101 | Loss: 0.1815868765115738\n",
      "Step 2201 | Loss: 0.19508200883865356\n",
      "Step 2301 | Loss: 0.11938363313674927\n",
      "Step 2401 | Loss: 0.21067054569721222\n",
      "0.14353558341212025 0.96\n",
      "Step 1 | Loss: 1.3585500717163086\n",
      "Step 101 | Loss: 0.3138732314109802\n",
      "Step 201 | Loss: 0.1587890088558197\n",
      "Step 301 | Loss: 0.24291664361953735\n",
      "Step 401 | Loss: 0.1880568116903305\n",
      "Step 501 | Loss: 0.10053589940071106\n",
      "Step 601 | Loss: 0.06725640594959259\n",
      "Step 701 | Loss: 0.033945485949516296\n",
      "Step 801 | Loss: 0.21199540793895721\n",
      "Step 901 | Loss: 0.08717844635248184\n",
      "Step 1001 | Loss: 0.2114669382572174\n",
      "Step 1101 | Loss: 0.27345433831214905\n",
      "Step 1201 | Loss: 0.07935303449630737\n",
      "Step 1301 | Loss: 0.2114284634590149\n",
      "Step 1401 | Loss: 0.2849481999874115\n",
      "Step 1501 | Loss: 0.04487571492791176\n",
      "Step 1601 | Loss: 0.12205341458320618\n",
      "Step 1701 | Loss: 0.24373860657215118\n",
      "Step 1801 | Loss: 0.2158031314611435\n",
      "Step 1901 | Loss: 0.11456194519996643\n",
      "Step 2001 | Loss: 0.1718536764383316\n",
      "Step 2101 | Loss: 0.0967501550912857\n",
      "Step 2201 | Loss: 0.16677933931350708\n",
      "Step 2301 | Loss: 0.14835773408412933\n",
      "Step 2401 | Loss: 0.0885491594672203\n",
      "0.1217640945444706 0.9675\n",
      "Step 1 | Loss: 0.9520987272262573\n",
      "Step 101 | Loss: 0.09443679451942444\n",
      "Step 201 | Loss: 0.10576197504997253\n",
      "Step 301 | Loss: 0.046125736087560654\n",
      "Step 401 | Loss: 0.06327501684427261\n",
      "Step 501 | Loss: 0.1214483231306076\n",
      "Step 601 | Loss: 0.1652897447347641\n",
      "Step 701 | Loss: 0.1541907638311386\n",
      "Step 801 | Loss: 0.29865995049476624\n",
      "Step 901 | Loss: 0.10840126872062683\n",
      "Step 1001 | Loss: 0.09385254979133606\n",
      "Step 1101 | Loss: 0.06300756335258484\n",
      "Step 1201 | Loss: 0.16271749138832092\n",
      "Step 1301 | Loss: 0.1610623151063919\n",
      "Step 1401 | Loss: 0.25550776720046997\n",
      "Step 1501 | Loss: 0.11950711905956268\n",
      "Step 1601 | Loss: 0.22340744733810425\n",
      "Step 1701 | Loss: 0.24446311593055725\n",
      "Step 1801 | Loss: 0.08113733679056168\n",
      "Step 1901 | Loss: 0.09127353876829147\n",
      "Step 2001 | Loss: 0.14851920306682587\n",
      "Step 2101 | Loss: 0.07688193023204803\n",
      "Step 2201 | Loss: 0.161028653383255\n",
      "Step 2301 | Loss: 0.09014055877923965\n",
      "Step 2401 | Loss: 0.17827624082565308\n",
      "0.10518946179931883 0.975\n",
      "Step 1 | Loss: 0.713474452495575\n",
      "Step 101 | Loss: 0.1280362457036972\n",
      "Step 201 | Loss: 0.24493175745010376\n",
      "Step 301 | Loss: 0.24636825919151306\n",
      "Step 401 | Loss: 0.2081010490655899\n",
      "Step 501 | Loss: 0.08082228899002075\n",
      "Step 601 | Loss: 0.23160842061042786\n",
      "Step 701 | Loss: 0.14933565258979797\n",
      "Step 801 | Loss: 0.07706975191831589\n",
      "Step 901 | Loss: 0.08935409039258957\n",
      "Step 1001 | Loss: 0.13889360427856445\n",
      "Step 1101 | Loss: 0.15547829866409302\n",
      "Step 1201 | Loss: 0.16398555040359497\n",
      "Step 1301 | Loss: 0.22358499467372894\n",
      "Step 1401 | Loss: 0.14570827782154083\n",
      "Step 1501 | Loss: 0.08858262002468109\n",
      "Step 1601 | Loss: 0.09338212013244629\n",
      "Step 1701 | Loss: 0.07534585148096085\n",
      "Step 1801 | Loss: 0.20184707641601562\n",
      "Step 1901 | Loss: 0.14891698956489563\n",
      "Step 2001 | Loss: 0.11066275835037231\n",
      "Step 2101 | Loss: 0.09608782082796097\n",
      "Step 2201 | Loss: 0.26795604825019836\n",
      "Step 2301 | Loss: 0.14715364575386047\n",
      "Step 2401 | Loss: 0.10214205831289291\n",
      "0.10369507039648938 0.9725\n",
      "8\n",
      "[ 841  249 1460 1746 1994 1195 1065 2101 1640 1369 2321 2398  579 2268\n",
      "  638 1148 1865 1781 1194 1730 1196 1854  895 2492 2066 1888  214 1634\n",
      " 2460  127  562   62 1483 2315 1543 1169  426   88 2229 1224 2299 1080\n",
      "  806  456  180 1753 1394 1755  704 1247 2412  627  154 1668 1373  435\n",
      "  685  129  664  122  892  796  799  238 2312 1537 1345  107 2272 1021\n",
      " 2427 1918 1240 1074 1652  958  395 2017  281  454  144 1708 1823  575\n",
      " 2031 2379  717 1606 2249  441  509   98 1140 2126 1318  869 2285 2032\n",
      " 1150 2014] 144\n",
      "0.8810482025146484\n",
      "Step 1 | Loss: 1.2015312910079956\n",
      "Step 101 | Loss: 0.39887502789497375\n",
      "Step 201 | Loss: 0.4251011312007904\n",
      "Step 301 | Loss: 0.2212773561477661\n",
      "Step 401 | Loss: 0.1670444756746292\n",
      "Step 501 | Loss: 0.2336333692073822\n",
      "Step 601 | Loss: 0.10335041582584381\n",
      "Step 701 | Loss: 0.20262418687343597\n",
      "Step 801 | Loss: 0.08862781524658203\n",
      "Step 901 | Loss: 0.0798298716545105\n",
      "Step 1001 | Loss: 0.13932496309280396\n",
      "Step 1101 | Loss: 0.0476546548306942\n",
      "Step 1201 | Loss: 0.060702018439769745\n",
      "Step 1301 | Loss: 0.08512300997972488\n",
      "Step 1401 | Loss: 0.16091956198215485\n",
      "Step 1501 | Loss: 0.05834478512406349\n",
      "Step 1601 | Loss: 0.0804005116224289\n",
      "Step 1701 | Loss: 0.07203613221645355\n",
      "Step 1801 | Loss: 0.0919782966375351\n",
      "Step 1901 | Loss: 0.04129340872168541\n",
      "Step 2001 | Loss: 0.10979082435369492\n",
      "Step 2101 | Loss: 0.06387026607990265\n",
      "Step 2201 | Loss: 0.08446019142866135\n",
      "Step 2301 | Loss: 0.06522325426340103\n",
      "Step 2401 | Loss: 0.06018461287021637\n",
      "0.06929810812512019 0.9925\n",
      "Step 1 | Loss: 1.0028438568115234\n",
      "Step 101 | Loss: 0.4054662585258484\n",
      "Step 201 | Loss: 0.24462822079658508\n",
      "Step 301 | Loss: 0.16774600744247437\n",
      "Step 401 | Loss: 0.2698138654232025\n",
      "Step 501 | Loss: 0.2869991958141327\n",
      "Step 601 | Loss: 0.17578542232513428\n",
      "Step 701 | Loss: 0.29696884751319885\n",
      "Step 801 | Loss: 0.16721253097057343\n",
      "Step 901 | Loss: 0.17507347464561462\n",
      "Step 1001 | Loss: 0.2337733805179596\n",
      "Step 1101 | Loss: 0.1932116597890854\n",
      "Step 1201 | Loss: 0.15613366663455963\n",
      "Step 1301 | Loss: 0.20773278176784515\n",
      "Step 1401 | Loss: 0.19116641581058502\n",
      "Step 1501 | Loss: 0.23946374654769897\n",
      "Step 1601 | Loss: 0.191288560628891\n",
      "Step 1701 | Loss: 0.12561196088790894\n",
      "Step 1801 | Loss: 0.16328652203083038\n",
      "Step 1901 | Loss: 0.1564818024635315\n",
      "Step 2001 | Loss: 0.18238040804862976\n",
      "Step 2101 | Loss: 0.208510160446167\n",
      "Step 2201 | Loss: 0.1734224408864975\n",
      "Step 2301 | Loss: 0.14360040426254272\n",
      "Step 2401 | Loss: 0.2351895123720169\n",
      "0.16296253412901535 0.9925\n",
      "Step 1 | Loss: 0.7099491953849792\n",
      "Step 101 | Loss: 0.2402488887310028\n",
      "Step 201 | Loss: 0.26664212346076965\n",
      "Step 301 | Loss: 0.19965316355228424\n",
      "Step 401 | Loss: 0.22131338715553284\n",
      "Step 501 | Loss: 0.15109515190124512\n",
      "Step 601 | Loss: 0.2565000355243683\n",
      "Step 701 | Loss: 0.13705454766750336\n",
      "Step 801 | Loss: 0.2584100365638733\n",
      "Step 901 | Loss: 0.10303764790296555\n",
      "Step 1001 | Loss: 0.10473319888114929\n",
      "Step 1101 | Loss: 0.17215566337108612\n"
     ]
    }
   ],
   "source": [
    "from elivagar.training.train_circuits_from_predictors import train_elivagar_circuits\n",
    "\n",
    "dataset = 'fmnist_2'\n",
    "device_name = None\n",
    "embed_type = 'angle'\n",
    "num_data_reps = 1\n",
    "num_train_steps = 2500\n",
    "num_params = 32\n",
    "num_test_data = 400\n",
    "num_qubits = 4\n",
    "num_meas_qubits = 1\n",
    "num_data_in_matrix = 32\n",
    "\n",
    "circ_dir = f'./ours/{dataset}/{num_params}_params'\n",
    "\n",
    "train_elivagar_circuits(circ_dir, dataset, embed_type, num_data_reps, device_name, num_train_steps, num_test_data, num_qubits, num_meas_qubits,\n",
    "                            num_data_in_matrix, num_candidates_per_circuit=100, num_circuits=2500, num_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'create_gate_circs_np' from '/root/create_gate_circs_np.py'>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import elivagar.training.train_circuits_from_predictors\n",
    "import elivagar.metric_computation.compute_composite_scores\n",
    "import elivagar.training.train_tq_circ_and_save\n",
    "import create_gate_circs_np\n",
    "\n",
    "reload(elivagar.training.train_circuits_from_predictors)\n",
    "reload(elivagar.training.train_tq_circ_and_save)\n",
    "reload(elivagar.metric_computation.compute_composite_scores)\n",
    "reload(create_gate_circs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'elivagar.inference.tq_circ_noisy_sim' from '/root/elivagar/inference/tq_circ_noisy_sim.py'>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import create_gate_circs\n",
    "import train_circ\n",
    "import elivagar.inference.tq_circ_noisy_sim\n",
    "\n",
    "reload(datasets)\n",
    "reload(create_gate_circs)\n",
    "reload(train_circ)\n",
    "reload(elivagar.inference.tq_circ_noisy_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-11-20 18:45:36,949: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43807296752929686 0.865\n"
     ]
    }
   ],
   "source": [
    "from elivagar.inference.tq_circ_noisy_sim import run_noisy_inference_for_tq_circuits\n",
    "\n",
    "dataset = 'mnist_2'\n",
    "embed_type = 'angle'\n",
    "num_data_reps = 1\n",
    "num_test_samples = 400\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "meas_qubits = [0]\n",
    "num_circs = 1000\n",
    "num_runs = 5\n",
    "\n",
    "device_name = 'ibmq_manila'\n",
    "\n",
    "circ_dir = f'./experiment_data/{dataset}/trained_circuits'\n",
    "circ_prefix = 'circ'\n",
    "\n",
    "run_noisy_inference_for_tq_circuits(circ_dir, circ_prefix, num_circs, num_runs, num_qubits, meas_qubits, device_name, dataset,\n",
    "                                        embed_type, num_data_reps, num_test_samples=None, human_design=False, compute_noiseless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## perf metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'create_gate_circs' from '/root/create_gate_circs.py'>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import metrics\n",
    "import create_gate_circs\n",
    "\n",
    "reload(metrics)\n",
    "reload(create_gate_circs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    }
   ],
   "source": [
    "from create_gate_circs import generate_random_gate_circ, generate_true_random_gate_circ, create_gate_circ, create_batched_gate_circ, get_circ_params\n",
    "from create_human_design_circs import generate_human_design_circ\n",
    "from metrics import compute_reduced_similarity\n",
    "from datasets import load_dataset\n",
    "\n",
    "# ideal = np.concatenate((np.ones((16, 16)), np.zeros((16, 16)), np.zeros((16, 16)), np.zeros((16, 16))))\n",
    "# ideal_2 = np.concatenate((np.zeros((16, 16)), np.ones((16, 16)), np.zeros((16, 16)), np.zeros((16, 16))))\n",
    "# ideal = np.concatenate((ideal, ideal_2, ideal_2[::-1, :], ideal[::-1, :]), 1)\n",
    "\n",
    "ideal = np.concatenate((np.ones((16, 16)), np.zeros((16, 16))))\n",
    "ideal = np.concatenate((ideal, ideal[::-1, :]), 1)\n",
    "\n",
    "curr_dir = './experiment_data/vowel_2/trained_circuits'\n",
    "num_qubits = 4\n",
    "meas_qubits = [0]\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset('vowel_2', 'angle', 1)\n",
    "\n",
    "# class_0_sel = np.random.choice(2000, 16, False)\n",
    "# class_1_sel = np.random.choice(2000, 16, False) + 2000\n",
    "# class_2_sel = np.random.choice(2000, 16, False) + 4000\n",
    "# class_3_sel = np.random.choice(2000, 16, False) + 6000\n",
    "# sel_inds = np.concatenate((class_0_sel, class_1_sel, class_2_sel, class_3_sel))\n",
    "\n",
    "class_0_sel = np.random.choice(300, 16, False)\n",
    "class_1_sel = np.random.choice(300, 16, False) + 300\n",
    "sel_inds = np.concatenate((class_0_sel, class_1_sel))\n",
    "\n",
    "sel_data = x_train[sel_inds]\n",
    "\n",
    "num_params = 32\n",
    "num_data = 32\n",
    "\n",
    "d2_min_scores = []\n",
    "d2_mean_scores = []\n",
    "d2_var_scores = []\n",
    "    \n",
    "d2_t_min_scores = []\n",
    "d2_t_mean_scores = []\n",
    "d2_t_var_scores = []\n",
    "\n",
    "mean_mat_scores = []\n",
    "mean_t_mat_scores = []\n",
    "\n",
    "# np.savetxt(curr_dir + '/fid_mats_sel_data.txt', sel_data)\n",
    "\n",
    "for i in range(1000):\n",
    "    circ_dir = curr_dir + '/circ_{}'.format(i + 1)\n",
    "    \n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(circ_dir)\n",
    "    \n",
    "    batched_circ = create_batched_gate_circ(qml.device('lightning.qubit', wires=num_qubits), circ_gates, gate_params, inputs_bounds,\n",
    "                                        weights_bounds, meas_qubits, 'matrix') \n",
    "    \n",
    "    params = 2 * np.pi * np.random.sample((num_params, weights_bounds[-1]))\n",
    "    \n",
    "    if not os.path.exists(circ_dir + '/fid_mats'):\n",
    "        os.mkdir(circ_dir + '/fid_mats')\n",
    "    \n",
    "    np.savetxt(circ_dir + '/fid_mats/metric_params.txt', params) \n",
    "    \n",
    "    circ_d2_scores = []\n",
    "    circ_d2_t_scores = []\n",
    "    circ_mean_mat = np.zeros((num_data, num_data))\n",
    "    circ_t_mean_mat = np.zeros((num_data, num_data))\n",
    "    \n",
    "    for j in range(num_params):\n",
    "        curr_params = np.concatenate([params[j] for k in range(num_data)]).reshape((num_data, weights_bounds[-1]))\n",
    "        mat = compute_reduced_similarity(batched_circ, curr_params, sel_data)\n",
    "        \n",
    "        t_mat = mat > ((np.sum(mat) - num_data) / (num_data * (num_data - 1)))\n",
    "        \n",
    "        diff_mat = mat - ideal\n",
    "        diff_2d = np.sum(np.multiply(diff_mat, diff_mat))\n",
    "            \n",
    "        diff_t_mat = t_mat - ideal\n",
    "        diff_2d_t = np.sum(np.multiply(diff_t_mat, diff_t_mat))\n",
    "            \n",
    "        circ_d2_scores.append(diff_2d)\n",
    "        circ_d2_t_scores.append(diff_2d_t)\n",
    "            \n",
    "        circ_mean_mat += mat / num_params\n",
    "        circ_t_mean_mat += t_mat / num_params\n",
    "\n",
    "    np.savetxt(circ_dir + '/fid_mats/d2_scores.txt', circ_d2_scores)\n",
    "    np.savetxt(circ_dir + '/fid_mats/d2_t_scores.txt', circ_d2_t_scores)\n",
    "        \n",
    "    d2_min_scores.append(np.min(circ_d2_scores))\n",
    "    d2_mean_scores.append(np.mean(circ_d2_scores))\n",
    "    d2_var_scores.append(np.var(circ_d2_scores))\n",
    "        \n",
    "    d2_t_min_scores.append(np.min(circ_d2_t_scores))\n",
    "    d2_t_mean_scores.append(np.mean(circ_d2_t_scores))\n",
    "    d2_t_var_scores.append(np.var(circ_d2_t_scores))\n",
    "        \n",
    "    diff_mean_mat = ideal - circ_mean_mat\n",
    "    diff_t_mean_mat = ideal - circ_t_mean_mat\n",
    "        \n",
    "    mean_mat_scores.append(np.sum(np.multiply(diff_mean_mat, diff_mean_mat)))\n",
    "    mean_t_mat_scores.append(np.sum(np.multiply(diff_t_mean_mat, diff_t_mean_mat)))     \n",
    "    \n",
    "    print(i)\n",
    "        \n",
    "np.savetxt(curr_dir + '/d2_mean_scores.txt', d2_mean_scores)\n",
    "np.savetxt(curr_dir + '/d2_min_scores.txt', d2_min_scores)\n",
    "np.savetxt(curr_dir + '/d2_var_scores.txt', d2_var_scores)\n",
    "    \n",
    "np.savetxt(curr_dir + '/d2_t_mean_scores.txt', d2_t_mean_scores)\n",
    "np.savetxt(curr_dir + '/d2_t_min_scores.txt', d2_t_min_scores)\n",
    "np.savetxt(curr_dir + '/d2_t_var_scores.txt', d2_t_var_scores)\n",
    "    \n",
    "np.savetxt(curr_dir + '/d2_mean_mat_scores.txt', mean_mat_scores)\n",
    "np.savetxt(curr_dir + '/d2_mean_t_mat_scores.txt', mean_t_mat_scores)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
