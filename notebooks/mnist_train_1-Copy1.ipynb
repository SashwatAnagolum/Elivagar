{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-72b8e8db4019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_gate_circs_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_circ_np\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets_nt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import create_gate_circs_np\n",
    "import train_circ_np\n",
    "\n",
    "reload(datasets)\n",
    "reload(create_gate_circs)\n",
    "reload(train_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Accuracy: 0.0\n",
      "Step 1001 | Accuracy: 0.0\n",
      "Step 2001 | Accuracy: 0.0\n",
      "Step 3001 | Accuracy: 1.0\n",
      "Step 4001 | Accuracy: 1.0\n",
      "Step 5001 | Accuracy: 0.0\n",
      "Step 6001 | Accuracy: 0.0\n",
      "Step 7001 | Accuracy: 1.0\n",
      "Step 8001 | Accuracy: 1.0\n",
      "Step 9001 | Accuracy: 0.0\n",
      "Step 10001 | Accuracy: 1.0\n",
      "Step 11001 | Accuracy: 1.0\n",
      "Step 12001 | Accuracy: 0.0\n",
      "Step 13001 | Accuracy: 0.0\n",
      "Step 14001 | Accuracy: 0.0\n",
      "Step 15001 | Accuracy: 1.0\n",
      "Step 16001 | Accuracy: 1.0\n",
      "Step 17001 | Accuracy: 1.0\n",
      "Step 18001 | Accuracy: 0.0\n",
      "Step 19001 | Accuracy: 1.0\n",
      "Step 20001 | Accuracy: 0.0\n",
      "Step 21001 | Accuracy: 0.0\n",
      "Step 22001 | Accuracy: 1.0\n",
      "Step 23001 | Accuracy: 0.0\n",
      "Step 24001 | Accuracy: 1.0\n",
      "Step 25001 | Accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset\n",
    "from create_gate_circs import get_circ_params, TQCirc, generate_random_gate_circ\n",
    "from train_circ import train_tq_model\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "num_qubits = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "dataset = 'mnist_10'\n",
    "\n",
    "train_data = TorchDataset(dataset, 'angle', 1, True)\n",
    "test_data = TorchDataset(dataset, 'angle', 1, False)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, sampler=torch.utils.data.RandomSampler(train_data))\n",
    "test_data_loader = torch.utils.data.DataLoader(train_data, batch_size=1, sampler=torch.utils.data.RandomSampler(train_data))\n",
    "\n",
    "val_test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=100, sampler=torch.utils.data.SequentialSampler(test_data))\n",
    "\n",
    "loss = torch.nn.functional.nll_loss\n",
    "val_loss = torch.nn.NLLLoss(reduction='none')\n",
    "\n",
    "for i in range(396, 400):\n",
    "    curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, True).to(device)\n",
    "    opt = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "    \n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    for j in range(1):\n",
    "        curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "        if os.path.exists(curr_train_dir):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(curr_train_dir)\n",
    "    \n",
    "        accs_history = train_tq_model(model, opt, loss, train_data_loader, test_data_loader, 120000, 1000, 1000)\n",
    "        \n",
    "    torch.save(model.state_dict(), curr_dir + '/model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  inference for MNISt-10 circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 9. 9. 9.]\n"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset, load_dataset\n",
    "from create_gate_circs_np import get_circ_params, TQCirc, generate_random_gate_circ\n",
    "from train_circ_np import train_tq_model\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "num_qubits = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "dataset = 'mnist_10'\n",
    "\n",
    "train_x, train_y, test_x, test_y = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "print(test_y)\n",
    "\n",
    "# loss = torch.nn.functional.nll_loss\n",
    "# val_loss = torch.nn.NLLLoss(reduction='none')\n",
    "\n",
    "# torch.inference_mode(True)\n",
    "\n",
    "# for i in range(1):\n",
    "#     curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "    \n",
    "#     circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "#     model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, True).to(device)\n",
    "    \n",
    "#     model_data = torch.load(curr_dir + '/model.pt')\n",
    "#     shutil.copyfile(curr_dir + '/model.pt', curr_dir + '/run_1/model.pt')\n",
    "    \n",
    "#     for j in range(1):\n",
    "#         curr_run_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "        \n",
    "#         model_data = torch.load(curr_run_dir + '/model.pt')\n",
    "#         model_params = list(model.parameters())\n",
    "\n",
    "#         for l in range(160):\n",
    "#             model_params[l].data.fill_(model_data['var_gates.{}.params'.format(l)].flatten().item())\n",
    "\n",
    "#         curr_val_losses = []\n",
    "#         curr_acc = 0\n",
    "\n",
    "#         for k in range(400):\n",
    "#             val_x, val_y = test_x[25 * k:(25 * k + 25)], test_y[25 * k:(25 * k + 25)]\n",
    "\n",
    "#             val_preds = model(val_x)\n",
    "#             curr_val_losses.append(val_loss(val_preds, val_y.to(torch.long)).detach().numpy().flatten())\n",
    "\n",
    "#             val_class = torch.argmax(val_preds.detach(), 1).detach().numpy().flatten()\n",
    "\n",
    "#             curr_acc += np.sum(val_class == val_y.detach().numpy().flatten().astype('int32'))\n",
    "\n",
    "#         print(i + 1, curr_acc / 10000)\n",
    "\n",
    "#         np.save(curr_run_dir + '/val_losses.npy', curr_val_losses)\n",
    "#         np.savetxt(curr_run_dir + '/accs.txt', [curr_acc / 10000])\n",
    "#         np.savetxt(curr_run_dir + '/accs_computed.txt', [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist-10 metric computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f4bd6c5639cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTorchDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcreate_gate_circs_np\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_circ_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTQCirc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerate_random_gate_circ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_circ_np\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_tq_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpkl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/datasets.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTorchDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshape_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset, load_dataset\n",
    "from create_gate_circs_np import get_circ_params, TQCirc, generate_random_gate_circ\n",
    "from train_circ_np import train_tq_model\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "\n",
    "num_qubits = 10\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "dataset = 'mnist_10'\n",
    "\n",
    "train_x, train_y, test_x, test_y = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "sel_data = train_x[np.random.choice(np.argwhere(train_y == 0).flatten(), 16, False)]\n",
    "\n",
    "for i in range(1, 10):\n",
    "    sel_data = np.concatenate((sel_data, train_x[np.random.choice(np.argwhere(train_y == i).flatten(), 16, False)]))\n",
    "\n",
    "torch.inference_mode(True)\n",
    "\n",
    "all_mats = []\n",
    "\n",
    "for i in range(1, 2):\n",
    "    curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "    \n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, False).to(device)\n",
    "    model_params = list(model.parameters())\n",
    "    random_params = np.random.sample((32, 160)) * 2 * np.pi\n",
    "    \n",
    "    overall_mat = torch.zeros((160, 160))\n",
    "    \n",
    "    for j in range(32):\n",
    "        out_states = torch.zeros((160, 1024), dtype=torch.cfloat)\n",
    "        \n",
    "        for l in range(160):\n",
    "            model_params[l].data.fill_(random_params[j, l])    \n",
    "    \n",
    "        for l in range(40):\n",
    "            model(sel_data[4 * l:4 * l + 4])\n",
    "            out_states[4 * l:4 * l + 4] = torch.reshape(model.device.states, (4, 1024))\n",
    "            \n",
    "        print(j)\n",
    "            \n",
    "        fid_mat = torch.pow(torch.abs(torch.matmul(out_states, torch.conj(torch.transpose(out_states, 0, 1)))), 2)\n",
    "        \n",
    "        overall_mat = torch.add(overall_mat, fid_mat)\n",
    "        \n",
    "    overall_mat = torch.divide(overall_mat, 32)\n",
    "    all_mats.append(overall_mat)\n",
    "    \n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdb9c297190>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "\n",
    "ax.imshow(overall_mat.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## supernet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_supernet_circ_into_gate_circ(subnet, num_embeds, layer_rots, layer_cnots, num_qubits, angle_embed=False):\n",
    "    circ_gates = []\n",
    "    gate_params = []\n",
    "    weights_bounds = [0]\n",
    "    inputs_bounds = [0]\n",
    "\n",
    "    curr_layers = subnet\n",
    "    \n",
    "    for i in range(1):\n",
    "        num_qubits = len(layer_rots[0])\n",
    "\n",
    "        if angle_embed:\n",
    "            rots = ['ry', 'rx', 'rz']\n",
    "            \n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += [rots[j % 3] for i in range(num_qubits)]\n",
    "                gate_params += [[i] for i in range(num_qubits)]\n",
    "                weights_bounds += [0 for i in range(num_qubits)]\n",
    "                inputs_bounds += [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]            \n",
    "        else:\n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += ['h' for i in range(num_qubits)] + ['ry' for i in range(num_qubits)] + ['cry' for i in range(num_qubits - 1)]\n",
    "                gate_params += [[i] for i in range(num_qubits)] * 2 + [[i, i + 1] for i in range(num_qubits - 1)]\n",
    "                weights_bounds += [0 for i in range(3 * num_qubits - 1)]\n",
    "                inputs_bounds += [inputs_bounds[-1] for i in range(num_qubits)] + [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]\n",
    "\n",
    "        for j in range(len(curr_layers)):\n",
    "            circ_gates += layer_rots[curr_layers[j]]\n",
    "            circ_gates += ['cx' for k in layer_cnots[curr_layers[j]]]\n",
    "\n",
    "            gate_params += [[k] for k in range(len(layer_rots[curr_layers[j]]))]\n",
    "            gate_params += layer_cnots[curr_layers[j]]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] + k + 1 for k in range(num_qubits)]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(num_qubits)]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] for k in range(len(layer_cnots[curr_layers[j]]))]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(len(layer_cnots[curr_layers[j]]))]\n",
    "            \n",
    "    return circ_gates, gate_params, inputs_bounds, weights_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_batched_gate_circ\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "\n",
    "dataset = 'bank'\n",
    "main_dir = './supernet/bank/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'supernet', 2)\n",
    "\n",
    "num_qubits = 4\n",
    "num_cnot_configs = 4096\n",
    "num_circs = 2500\n",
    "num_embed_layers = 2\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "param_nums = [12]\n",
    "\n",
    "for p in param_nums:\n",
    "    supernet_dir = main_dir + 'search-{}_params_mb'.format(p)\n",
    "\n",
    "    num_layers = p // num_qubits\n",
    "\n",
    "    layer_gates = [i[1:].split(')') for i in open(supernet_dir + '/search_space.txt').read().split('\\n')][:-1]\n",
    "\n",
    "    layer_rots = [''.join([j for j in i[0] if j.isupper()]) for i in layer_gates]\n",
    "    layer_rots = [[i[j * 2:j * 2 + 2].lower() for j in range(len(i) // 2)] for i in layer_rots]\n",
    "\n",
    "    layer_cnots = [''.join([j for j in i[1][1:] if j not in ['[', ']', ',', ' ']]) for i in layer_gates]\n",
    "    layer_cnots = [[[int(i[2 * j]), int(i[2 * j + 1])] for j in range(len(i) // 2)] for i in layer_cnots]\n",
    "        \n",
    "    search_space = len(layer_rots)\n",
    "        \n",
    "    accs = []\n",
    "    losses = []\n",
    "    circ_layers = []\n",
    "\n",
    "    params = np.genfromtxt(supernet_dir + '/training_params.txt')[-1].reshape((num_layers, search_space // num_cnot_configs, num_qubits))\n",
    "        \n",
    "    supernet_device_dir = supernet_dir + '/' + device_name\n",
    "    \n",
    "    if not os.path.exists(supernet_device_dir):\n",
    "        os.mkdir(supernet_device_dir)\n",
    "        \n",
    "    for i in range(num_circs):\n",
    "        curr_circ_desc = np.random.randint(0, search_space, num_layers)\n",
    "\n",
    "        curr_params = np.concatenate([params[k, curr_circ_desc[k] // num_cnot_configs] for k in range(num_layers)]).flatten()\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = convert_supernet_circ_into_gate_circ(curr_circ_desc, num_embed_layers, layer_rots, layer_cnots,\n",
    "                                                                                                          num_qubits, False) \n",
    "\n",
    "        \n",
    "        circ = create_batched_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, [0], 'exp')\n",
    "\n",
    "        val_exp_list = []\n",
    "        \n",
    "        for j in range(np.ceil(len(x_test) / 32).astype('int32')):\n",
    "            curr_batch_params = np.concatenate([[curr_params] for k in range(min(32, len(x_test) - 32 * j))], 0)\n",
    "            \n",
    "            val_exp_list.append(circ(x_test[32 * j:min(len(x_test), 32* j + 32)], curr_batch_params))\n",
    "            \n",
    "        val_exps = np.concatenate(val_exp_list)\n",
    "        val_losses = np.power(val_exps - y_test, 2)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        acc = np.mean(val_losses < 1)\n",
    "            \n",
    "        print(curr_circ_desc, acc, val_loss)\n",
    "\n",
    "        accs.append(acc)\n",
    "        losses.append(val_loss)\n",
    "        circ_layers.append(curr_circ_desc)\n",
    "        \n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_layers.txt', np.array(circ_layers))\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_accs.txt', accs)\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_losses.txt', losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane-qiskit\n",
      "  Using cached PennyLane_qiskit-0.24.0-py3-none-any.whl (34 kB)\n",
      "Collecting qiskit\n",
      "  Using cached qiskit-0.38.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pennylane-qiskit) (1.18.5)\n",
      "Collecting mthree>=0.17\n",
      "  Using cached mthree-1.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.7/site-packages (from pennylane-qiskit) (2.6.3)\n",
      "Requirement already satisfied: pennylane>=0.23 in /usr/local/lib/python3.7/site-packages (from pennylane-qiskit) (0.25.1)\n",
      "Collecting qiskit-aer==0.11.0\n",
      "  Using cached qiskit_aer-0.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "Collecting qiskit-terra==0.21.2\n",
      "  Using cached qiskit_terra-0.21.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Collecting qiskit-ibmq-provider==0.19.2\n",
      "  Using cached qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aer==0.11.0->qiskit) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.25.11)\n",
      "Collecting websockets>=10.0\n",
      "  Using cached websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
      "Collecting requests-ntlm>=1.1.0\n",
      "  Using cached requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.24.0)\n",
      "Collecting websocket-client>=1.0.1\n",
      "  Using cached websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (5.7.2)\n",
      "Collecting symengine>=0.9\n",
      "  Using cached symengine-0.9.2-cp37-cp37m-manylinux2010_x86_64.whl (37.5 MB)\n",
      "Collecting shared-memory38\n",
      "  Using cached shared_memory38-0.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (25 kB)\n",
      "Collecting stevedore>=3.0.0\n",
      "  Using cached stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "Collecting ply>=3.10\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Collecting sympy>=1.3\n",
      "  Using cached sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (0.3.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (3.10.0.0)\n",
      "Collecting tweedledum<2.0,>=1.1\n",
      "  Using cached tweedledum-1.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (943 kB)\n",
      "Requirement already satisfied: retworkx>=0.11.0 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (0.11.0)\n",
      "Collecting orjson>=3.0.0\n",
      "  Using cached orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
      "Collecting cython>=0.29\n",
      "  Using cached Cython-0.29.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Requirement already satisfied: semantic-version>=2.7 in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (2.10.0)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (4.2.2)\n",
      "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (1.4.4)\n",
      "Requirement already satisfied: pennylane-lightning>=0.25 in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (0.25.1)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (0.10.2)\n",
      "Requirement already satisfied: autograd in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (1.4)\n",
      "Requirement already satisfied: autoray>=0.3.1 in /usr/local/lib/python3.7/site-packages (from pennylane>=0.23->pennylane-qiskit) (0.3.2)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.7/site-packages (from pennylane-lightning>=0.25->pennylane>=0.23->pennylane-qiskit) (1.10.2.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (3.0.4)\n",
      "Collecting ntlm-auth>=1.0.2\n",
      "  Using cached ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (2.20)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Using cached pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/site-packages (from stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (3.4.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/site-packages (from autograd->pennylane>=0.23->pennylane-qiskit) (0.18.2)\n",
      "Installing collected packages: pbr, mpmath, tweedledum, sympy, symengine, stevedore, shared-memory38, ply, ntlm-auth, websockets, websocket-client, requests-ntlm, qiskit-terra, qiskit-ibmq-provider, qiskit-aer, orjson, cython, qiskit, mthree, pennylane-qiskit\n",
      "Successfully installed cython-0.29.32 mpmath-1.2.1 mthree-1.1.0 ntlm-auth-1.5.0 orjson-3.8.0 pbr-5.10.0 pennylane-qiskit-0.24.0 ply-3.11 qiskit-0.38.0 qiskit-aer-0.11.0 qiskit-ibmq-provider-0.19.2 qiskit-terra-0.21.2 requests-ntlm-1.1.0 shared-memory38-0.1.2 stevedore-3.5.0 symengine-0.9.2 sympy-1.10.1 tweedledum-1.1.1 websocket-client-1.4.1 websockets-10.3\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane-qiskit qiskit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-23 05:05:06.356279: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2022-09-23 05:05:06.356401: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2022-09-23 05:05:06.388106: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "(1600, 4, 4) (400, 4, 4)\n",
      "subnet: [266545, 192677, 286962, 125027, 24644], expert_idx: 0\n",
      "2022-09-23 05:05:19.928932: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 05:05:19.935736: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2499995000 Hz\n",
      "2022-09-23 05:05:19.935934: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55abc46a2530 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-09-23 05:05:19.935964: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-09-23 05:05:19.936087: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "[2022-09-23 05:05:20.073 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:936 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-23 05:05:20.132 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:936 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "subnet: [266545, 192677, 286962, 125027, 24644], loss: [1.13421025], acc: 0.53125, samples: 32\n",
      "subnet: [252909, 225185, 210978, 322110, 272179], expert_idx: 0\n",
      "subnet: [252909, 225185, 210978, 322110, 272179], loss: [1.03888637], acc: 0.46875, samples: 32\n",
      "subnet: [302894, 72968, 141787, 89688, 3832], expert_idx: 0\n",
      "subnet: [302894, 72968, 141787, 89688, 3832], loss: [1.01881714], acc: 0.53125, samples: 32\n",
      "subnet: [27462, 119754, 182584, 283339, 135273], expert_idx: 0\n",
      "subnet: [27462, 119754, 182584, 283339, 135273], loss: [1.02449503], acc: 0.46875, samples: 32\n",
      "subnet: [140120, 160340, 13732, 160079, 91926], expert_idx: 0\n",
      "subnet: [140120, 160340, 13732, 160079, 91926], loss: [1.10738939], acc: 0.46875, samples: 32\n",
      "subnet: [123135, 134142, 290706, 177220, 213730], expert_idx: 0\n",
      "subnet: [123135, 134142, 290706, 177220, 213730], loss: [1.01180447], acc: 0.59375, samples: 32\n",
      "subnet: [85155, 61669, 167705, 219353, 140988], expert_idx: 0\n",
      "subnet: [85155, 61669, 167705, 219353, 140988], loss: [1.08481717], acc: 0.5625, samples: 32\n",
      "subnet: [6929, 173900, 150887, 192097, 122651], expert_idx: 0\n",
      "subnet: [6929, 173900, 150887, 192097, 122651], loss: [0.97416332], acc: 0.625, samples: 32\n",
      "subnet: [280060, 138261, 54950, 10165, 216039], expert_idx: 0\n",
      "subnet: [280060, 138261, 54950, 10165, 216039], loss: [1.14093269], acc: 0.46875, samples: 32\n",
      "subnet: [137777, 197601, 240536, 76905, 190575], expert_idx: 0\n",
      "subnet: [137777, 197601, 240536, 76905, 190575], loss: [1.11337698], acc: 0.5, samples: 32\n",
      "subnet: [86787, 34048, 26167, 82894, 11515], expert_idx: 0\n",
      "subnet: [86787, 34048, 26167, 82894, 11515], loss: [1.21594665], acc: 0.34375, samples: 32\n",
      "subnet: [176165, 288651, 258491, 222615, 159595], expert_idx: 0\n",
      "subnet: [176165, 288651, 258491, 222615, 159595], loss: [0.97676233], acc: 0.53125, samples: 32\n",
      "subnet: [322282, 249179, 180445, 29644, 109284], expert_idx: 0\n",
      "subnet: [322282, 249179, 180445, 29644, 109284], loss: [1.06016325], acc: 0.40625, samples: 32\n",
      "subnet: [24941, 139790, 331091, 87158, 282608], expert_idx: 0\n",
      "subnet: [24941, 139790, 331091, 87158, 282608], loss: [1.19277714], acc: 0.4375, samples: 32\n",
      "subnet: [264175, 15645, 229594, 107101, 233564], expert_idx: 0\n",
      "subnet: [264175, 15645, 229594, 107101, 233564], loss: [1.09514332], acc: 0.375, samples: 32\n",
      "subnet: [102538, 80068, 69789, 44696, 84671], expert_idx: 0\n",
      "subnet: [102538, 80068, 69789, 44696, 84671], loss: [1.11227988], acc: 0.375, samples: 32\n",
      "subnet: [75139, 326625, 314182, 193000, 285754], expert_idx: 0\n",
      "subnet: [75139, 326625, 314182, 193000, 285754], loss: [1.15603689], acc: 0.40625, samples: 32\n",
      "subnet: [196261, 250051, 75062, 253953, 85179], expert_idx: 0\n",
      "subnet: [196261, 250051, 75062, 253953, 85179], loss: [0.83194619], acc: 0.6875, samples: 32\n",
      "subnet: [86706, 281156, 116446, 96421, 257128], expert_idx: 0\n",
      "subnet: [86706, 281156, 116446, 96421, 257128], loss: [0.95134538], acc: 0.625, samples: 32\n",
      "subnet: [31081, 305179, 169572, 233377, 266660], expert_idx: 0\n",
      "subnet: [31081, 305179, 169572, 233377, 266660], loss: [0.95372613], acc: 0.59375, samples: 32\n",
      "subnet: [144633, 238535, 264611, 129929, 208468], expert_idx: 0\n",
      "subnet: [144633, 238535, 264611, 129929, 208468], loss: [1.18906997], acc: 0.4375, samples: 32\n",
      "subnet: [39487, 13581, 203789, 287209, 289768], expert_idx: 0\n",
      "subnet: [39487, 13581, 203789, 287209, 289768], loss: [0.96692549], acc: 0.46875, samples: 32\n",
      "subnet: [192223, 96045, 263005, 47819, 249496], expert_idx: 0\n",
      "subnet: [192223, 96045, 263005, 47819, 249496], loss: [0.96780199], acc: 0.625, samples: 32\n",
      "subnet: [330521, 177437, 242263, 62320, 241978], expert_idx: 0\n",
      "subnet: [330521, 177437, 242263, 62320, 241978], loss: [1.13614531], acc: 0.53125, samples: 32\n",
      "subnet: [102968, 319806, 12094, 220269, 330660], expert_idx: 0\n",
      "subnet: [102968, 319806, 12094, 220269, 330660], loss: [1.08225666], acc: 0.46875, samples: 32\n",
      "subnet: [33104, 156101, 149132, 89526, 207417], expert_idx: 0\n",
      "subnet: [33104, 156101, 149132, 89526, 207417], loss: [1.07702613], acc: 0.53125, samples: 32\n",
      "subnet: [21202, 111804, 253454, 999, 168317], expert_idx: 0\n",
      "subnet: [21202, 111804, 253454, 999, 168317], loss: [1.02811795], acc: 0.5, samples: 32\n",
      "subnet: [1065, 288227, 273384, 214354, 130888], expert_idx: 0\n",
      "subnet: [1065, 288227, 273384, 214354, 130888], loss: [1.04954064], acc: 0.5, samples: 32\n",
      "subnet: [305990, 236733, 173344, 69383, 20445], expert_idx: 0\n",
      "subnet: [305990, 236733, 173344, 69383, 20445], loss: [1.07611557], acc: 0.53125, samples: 32\n",
      "subnet: [34044, 185744, 64318, 190193, 186604], expert_idx: 0\n",
      "subnet: [34044, 185744, 64318, 190193, 186604], loss: [1.02727889], acc: 0.5625, samples: 32\n",
      "subnet: [189602, 5549, 95532, 327653, 230677], expert_idx: 0\n",
      "subnet: [189602, 5549, 95532, 327653, 230677], loss: [0.97243559], acc: 0.5625, samples: 32\n",
      "subnet: [17255, 247561, 84413, 76159, 111471], expert_idx: 0\n",
      "subnet: [17255, 247561, 84413, 76159, 111471], loss: [1.07400531], acc: 0.5, samples: 32\n",
      "subnet: [87334, 239877, 205345, 42192, 91750], expert_idx: 0\n",
      "subnet: [87334, 239877, 205345, 42192, 91750], loss: [1.11757976], acc: 0.4375, samples: 32\n",
      "subnet: [122830, 30872, 312685, 245159, 247556], expert_idx: 0\n",
      "subnet: [122830, 30872, 312685, 245159, 247556], loss: [0.9763166], acc: 0.59375, samples: 32\n",
      "subnet: [64850, 171758, 260183, 69035, 92824], expert_idx: 0\n",
      "subnet: [64850, 171758, 260183, 69035, 92824], loss: [1.18400894], acc: 0.40625, samples: 32\n",
      "subnet: [287231, 96902, 31462, 221101, 41401], expert_idx: 0\n",
      "subnet: [287231, 96902, 31462, 221101, 41401], loss: [1.09659839], acc: 0.53125, samples: 32\n",
      "subnet: [84836, 191210, 141526, 142250, 283908], expert_idx: 0\n",
      "subnet: [84836, 191210, 141526, 142250, 283908], loss: [1.11329151], acc: 0.5, samples: 32\n",
      "subnet: [107007, 84384, 235206, 210081, 230134], expert_idx: 0\n",
      "subnet: [107007, 84384, 235206, 210081, 230134], loss: [0.93453875], acc: 0.625, samples: 32\n",
      "subnet: [263638, 196432, 87943, 28998, 172932], expert_idx: 0\n",
      "subnet: [263638, 196432, 87943, 28998, 172932], loss: [1.02334963], acc: 0.5, samples: 32\n",
      "subnet: [269772, 91300, 208064, 79196, 125611], expert_idx: 0\n",
      "subnet: [269772, 91300, 208064, 79196, 125611], loss: [1.03936773], acc: 0.625, samples: 32\n",
      "subnet: [265223, 96535, 54847, 323411, 127719], expert_idx: 0\n",
      "subnet: [265223, 96535, 54847, 323411, 127719], loss: [1.16046533], acc: 0.46875, samples: 32\n",
      "subnet: [44270, 221716, 149378, 286128, 223543], expert_idx: 0\n",
      "subnet: [44270, 221716, 149378, 286128, 223543], loss: [1.02956601], acc: 0.5625, samples: 32\n",
      "subnet: [296137, 52802, 268975, 297416, 17765], expert_idx: 0\n",
      "subnet: [296137, 52802, 268975, 297416, 17765], loss: [0.9906085], acc: 0.53125, samples: 32\n",
      "subnet: [208752, 111104, 108695, 111676, 287297], expert_idx: 0\n",
      "subnet: [208752, 111104, 108695, 111676, 287297], loss: [0.87638461], acc: 0.6875, samples: 32\n",
      "subnet: [190030, 154667, 25724, 177874, 298528], expert_idx: 0\n",
      "subnet: [190030, 154667, 25724, 177874, 298528], loss: [1.15214052], acc: 0.46875, samples: 32\n",
      "subnet: [125288, 57532, 147443, 77355, 317638], expert_idx: 0\n",
      "subnet: [125288, 57532, 147443, 77355, 317638], loss: [1.15723786], acc: 0.375, samples: 32\n",
      "subnet: [288353, 273851, 310480, 81287, 75146], expert_idx: 0\n",
      "subnet: [288353, 273851, 310480, 81287, 75146], loss: [1.03477058], acc: 0.46875, samples: 32\n",
      "subnet: [29993, 218509, 167108, 2085, 52119], expert_idx: 0\n",
      "subnet: [29993, 218509, 167108, 2085, 52119], loss: [1.02574064], acc: 0.5625, samples: 32\n",
      "subnet: [276634, 83274, 283815, 54114, 269781], expert_idx: 0\n",
      "subnet: [276634, 83274, 283815, 54114, 269781], loss: [1.18536177], acc: 0.40625, samples: 32\n",
      "subnet: [284130, 44916, 38857, 292705, 153796], expert_idx: 0\n",
      "subnet: [284130, 44916, 38857, 292705, 153796], loss: [1.15754816], acc: 0.5, samples: 32\n",
      "subnet: [241353, 62958, 183259, 271083, 59819], expert_idx: 0\n",
      "subnet: [241353, 62958, 183259, 271083, 59819], loss: [1.28198711], acc: 0.3125, samples: 32\n",
      "subnet: [54730, 319997, 315729, 2939, 313729], expert_idx: 0\n",
      "subnet: [54730, 319997, 315729, 2939, 313729], loss: [1.24702312], acc: 0.25, samples: 32\n",
      "subnet: [30111, 250392, 257253, 124677, 291637], expert_idx: 0\n",
      "subnet: [30111, 250392, 257253, 124677, 291637], loss: [1.13150839], acc: 0.46875, samples: 32\n",
      "subnet: [197820, 309173, 18613, 142644, 22137], expert_idx: 0\n",
      "subnet: [197820, 309173, 18613, 142644, 22137], loss: [1.00209561], acc: 0.53125, samples: 32\n",
      "subnet: [239578, 195271, 149282, 214395, 330884], expert_idx: 0\n",
      "subnet: [239578, 195271, 149282, 214395, 330884], loss: [0.99123492], acc: 0.4375, samples: 32\n",
      "subnet: [312802, 107040, 101877, 251639, 313354], expert_idx: 0\n",
      "subnet: [312802, 107040, 101877, 251639, 313354], loss: [1.09615615], acc: 0.59375, samples: 32\n",
      "subnet: [288268, 130742, 40853, 89296, 60205], expert_idx: 0\n",
      "subnet: [288268, 130742, 40853, 89296, 60205], loss: [1.03222299], acc: 0.5, samples: 32\n",
      "subnet: [217728, 55478, 99518, 91341, 198550], expert_idx: 0\n",
      "subnet: [217728, 55478, 99518, 91341, 198550], loss: [1.00020214], acc: 0.4375, samples: 32\n",
      "subnet: [229199, 18603, 70332, 107769, 245019], expert_idx: 0\n",
      "subnet: [229199, 18603, 70332, 107769, 245019], loss: [0.91027051], acc: 0.625, samples: 32\n",
      "subnet: [244260, 45031, 175879, 88992, 307925], expert_idx: 0\n",
      "subnet: [244260, 45031, 175879, 88992, 307925], loss: [1.25883167], acc: 0.4375, samples: 32\n",
      "subnet: [143717, 151120, 21154, 73730, 265586], expert_idx: 0\n",
      "subnet: [143717, 151120, 21154, 73730, 265586], loss: [1.22529219], acc: 0.46875, samples: 32\n",
      "subnet: [131297, 2787, 146536, 104293, 157238], expert_idx: 0\n",
      "subnet: [131297, 2787, 146536, 104293, 157238], loss: [1.08037791], acc: 0.5, samples: 32\n",
      "subnet: [117088, 231159, 89210, 211568, 49806], expert_idx: 0\n",
      "subnet: [117088, 231159, 89210, 211568, 49806], loss: [1.13283771], acc: 0.375, samples: 32\n",
      "subnet: [323943, 284730, 163739, 203907, 219652], expert_idx: 0\n",
      "subnet: [323943, 284730, 163739, 203907, 219652], loss: [1.14774832], acc: 0.34375, samples: 32\n",
      "subnet: [266012, 68999, 19512, 43315, 85678], expert_idx: 0\n",
      "subnet: [266012, 68999, 19512, 43315, 85678], loss: [0.89893526], acc: 0.625, samples: 32\n",
      "subnet: [294479, 260646, 106250, 202509, 48581], expert_idx: 0\n",
      "subnet: [294479, 260646, 106250, 202509, 48581], loss: [0.96492938], acc: 0.5, samples: 32\n",
      "subnet: [315514, 17896, 231740, 4538, 123098], expert_idx: 0\n",
      "subnet: [315514, 17896, 231740, 4538, 123098], loss: [1.15328833], acc: 0.40625, samples: 32\n",
      "subnet: [120444, 294269, 258763, 228254, 75447], expert_idx: 0\n",
      "subnet: [120444, 294269, 258763, 228254, 75447], loss: [1.04759764], acc: 0.5625, samples: 32\n",
      "subnet: [270721, 196350, 17946, 105400, 246615], expert_idx: 0\n",
      "subnet: [270721, 196350, 17946, 105400, 246615], loss: [0.89797757], acc: 0.625, samples: 32\n",
      "subnet: [245148, 79773, 304601, 275569, 148793], expert_idx: 0\n",
      "subnet: [245148, 79773, 304601, 275569, 148793], loss: [1.07141289], acc: 0.53125, samples: 32\n",
      "subnet: [7611, 91597, 148050, 175176, 142565], expert_idx: 0\n",
      "subnet: [7611, 91597, 148050, 175176, 142565], loss: [0.99788859], acc: 0.5625, samples: 32\n",
      "subnet: [280279, 284814, 93992, 33077, 194761], expert_idx: 0\n",
      "subnet: [280279, 284814, 93992, 33077, 194761], loss: [1.03880496], acc: 0.5, samples: 32\n",
      "subnet: [20827, 35983, 266528, 123298, 231310], expert_idx: 0\n",
      "subnet: [20827, 35983, 266528, 123298, 231310], loss: [1.10475324], acc: 0.4375, samples: 32\n",
      "subnet: [76647, 174001, 52045, 231274, 286392], expert_idx: 0\n",
      "subnet: [76647, 174001, 52045, 231274, 286392], loss: [1.00587292], acc: 0.5625, samples: 32\n",
      "subnet: [234935, 72361, 110737, 199615, 49038], expert_idx: 0\n",
      "subnet: [234935, 72361, 110737, 199615, 49038], loss: [1.0259527], acc: 0.5625, samples: 32\n",
      "subnet: [153651, 192945, 91239, 289172, 608], expert_idx: 0\n",
      "subnet: [153651, 192945, 91239, 289172, 608], loss: [1.12747263], acc: 0.4375, samples: 32\n",
      "subnet: [9670, 66426, 103730, 160949, 329896], expert_idx: 0\n",
      "subnet: [9670, 66426, 103730, 160949, 329896], loss: [1.12372713], acc: 0.46875, samples: 32\n",
      "subnet: [176647, 318560, 179268, 247363, 72572], expert_idx: 0\n",
      "subnet: [176647, 318560, 179268, 247363, 72572], loss: [1.02600879], acc: 0.4375, samples: 32\n",
      "subnet: [216546, 180999, 209989, 138317, 122641], expert_idx: 0\n",
      "subnet: [216546, 180999, 209989, 138317, 122641], loss: [1.15152449], acc: 0.40625, samples: 32\n",
      "subnet: [39139, 198085, 111208, 200828, 261211], expert_idx: 0\n",
      "subnet: [39139, 198085, 111208, 200828, 261211], loss: [0.89340061], acc: 0.65625, samples: 32\n",
      "subnet: [117180, 109323, 162161, 18460, 75739], expert_idx: 0\n",
      "subnet: [117180, 109323, 162161, 18460, 75739], loss: [0.98885878], acc: 0.59375, samples: 32\n",
      "subnet: [273953, 35337, 6881, 278625, 17262], expert_idx: 0\n",
      "subnet: [273953, 35337, 6881, 278625, 17262], loss: [1.04884303], acc: 0.5, samples: 32\n",
      "subnet: [67649, 176103, 25992, 262884, 95317], expert_idx: 0\n",
      "subnet: [67649, 176103, 25992, 262884, 95317], loss: [1.16130108], acc: 0.53125, samples: 32\n",
      "subnet: [36713, 256674, 292436, 6411, 163412], expert_idx: 0\n",
      "subnet: [36713, 256674, 292436, 6411, 163412], loss: [1.2026204], acc: 0.40625, samples: 32\n",
      "subnet: [208974, 19463, 151219, 221431, 323328], expert_idx: 0\n",
      "subnet: [208974, 19463, 151219, 221431, 323328], loss: [1.0334034], acc: 0.53125, samples: 32\n",
      "subnet: [87579, 182890, 112654, 56807, 109127], expert_idx: 0\n",
      "subnet: [87579, 182890, 112654, 56807, 109127], loss: [0.98923224], acc: 0.5625, samples: 32\n",
      "subnet: [24438, 95961, 83905, 303093, 315727], expert_idx: 0\n",
      "subnet: [24438, 95961, 83905, 303093, 315727], loss: [0.99189775], acc: 0.625, samples: 32\n",
      "subnet: [56471, 288448, 32299, 21146, 269905], expert_idx: 0\n",
      "subnet: [56471, 288448, 32299, 21146, 269905], loss: [1.12182753], acc: 0.46875, samples: 32\n",
      "subnet: [22488, 101078, 90929, 163149, 37996], expert_idx: 0\n",
      "subnet: [22488, 101078, 90929, 163149, 37996], loss: [0.93516032], acc: 0.53125, samples: 32\n",
      "subnet: [196513, 310866, 102232, 320058, 157068], expert_idx: 0\n",
      "subnet: [196513, 310866, 102232, 320058, 157068], loss: [1.05489789], acc: 0.59375, samples: 32\n",
      "subnet: [149043, 127683, 49948, 31878, 328501], expert_idx: 0\n",
      "subnet: [149043, 127683, 49948, 31878, 328501], loss: [1.10128684], acc: 0.40625, samples: 32\n",
      "subnet: [53741, 163969, 201414, 294889, 190531], expert_idx: 0\n",
      "subnet: [53741, 163969, 201414, 294889, 190531], loss: [1.13292291], acc: 0.5, samples: 32\n",
      "subnet: [291169, 107122, 286421, 248665, 46663], expert_idx: 0\n",
      "subnet: [291169, 107122, 286421, 248665, 46663], loss: [0.89038971], acc: 0.625, samples: 32\n",
      "subnet: [237067, 233857, 225215, 3890, 204050], expert_idx: 0\n",
      "subnet: [237067, 233857, 225215, 3890, 204050], loss: [1.07681676], acc: 0.46875, samples: 32\n",
      "subnet: [87213, 78699, 148925, 123358, 186435], expert_idx: 0\n",
      "subnet: [87213, 78699, 148925, 123358, 186435], loss: [1.1410917], acc: 0.375, samples: 32\n",
      "subnet: [51025, 1579, 215664, 49100, 270616], expert_idx: 0\n",
      "subnet: [51025, 1579, 215664, 49100, 270616], loss: [1.00814901], acc: 0.625, samples: 32\n",
      "subnet: [244613, 64608, 276494, 100614, 88559], expert_idx: 0\n",
      "subnet: [244613, 64608, 276494, 100614, 88559], loss: [1.15994923], acc: 0.40625, samples: 32\n",
      "subnet: [73832, 133037, 104405, 91383, 144183], expert_idx: 0\n",
      "subnet: [73832, 133037, 104405, 91383, 144183], loss: [1.20922363], acc: 0.375, samples: 32\n",
      "subnet: [53311, 153580, 688, 237407, 21145], expert_idx: 0\n",
      "subnet: [53311, 153580, 688, 237407, 21145], loss: [1.07916832], acc: 0.40625, samples: 32\n",
      "subnet: [257931, 44416, 181565, 24730, 284045], expert_idx: 0\n",
      "subnet: [257931, 44416, 181565, 24730, 284045], loss: [0.98628124], acc: 0.5625, samples: 32\n",
      "subnet: [246086, 186192, 267962, 166801, 99644], expert_idx: 0\n",
      "subnet: [246086, 186192, 267962, 166801, 99644], loss: [0.98895809], acc: 0.5, samples: 32\n",
      "subnet: [324969, 226251, 151736, 279832, 211028], expert_idx: 0\n",
      "subnet: [324969, 226251, 151736, 279832, 211028], loss: [1.19955626], acc: 0.40625, samples: 32\n",
      "subnet: [227460, 191565, 8514, 21183, 76492], expert_idx: 0\n",
      "subnet: [227460, 191565, 8514, 21183, 76492], loss: [1.08550166], acc: 0.46875, samples: 32\n",
      "subnet: [324927, 262974, 88942, 249666, 128215], expert_idx: 0\n",
      "subnet: [324927, 262974, 88942, 249666, 128215], loss: [1.08388791], acc: 0.46875, samples: 32\n",
      "subnet: [13453, 261398, 27721, 73800, 163023], expert_idx: 0\n",
      "subnet: [13453, 261398, 27721, 73800, 163023], loss: [1.06384264], acc: 0.40625, samples: 32\n",
      "subnet: [6803, 75181, 184478, 137418, 37659], expert_idx: 0\n",
      "subnet: [6803, 75181, 184478, 137418, 37659], loss: [1.20036455], acc: 0.5, samples: 32\n",
      "subnet: [170785, 182675, 273791, 156648, 279801], expert_idx: 0\n",
      "subnet: [170785, 182675, 273791, 156648, 279801], loss: [1.04915962], acc: 0.53125, samples: 32\n",
      "subnet: [117510, 22066, 143610, 298402, 6780], expert_idx: 0\n",
      "subnet: [117510, 22066, 143610, 298402, 6780], loss: [1.23673027], acc: 0.4375, samples: 32\n",
      "subnet: [168758, 102036, 211292, 265994, 157379], expert_idx: 0\n",
      "subnet: [168758, 102036, 211292, 265994, 157379], loss: [1.12552014], acc: 0.40625, samples: 32\n",
      "subnet: [174793, 118309, 113647, 2931, 234986], expert_idx: 0\n",
      "subnet: [174793, 118309, 113647, 2931, 234986], loss: [1.03392356], acc: 0.5, samples: 32\n",
      "subnet: [1142, 249694, 883, 235137, 229153], expert_idx: 0\n",
      "subnet: [1142, 249694, 883, 235137, 229153], loss: [1.06639899], acc: 0.4375, samples: 32\n",
      "subnet: [310360, 329735, 257575, 227062, 187363], expert_idx: 0\n",
      "subnet: [310360, 329735, 257575, 227062, 187363], loss: [1.16646575], acc: 0.40625, samples: 32\n",
      "subnet: [215986, 28212, 313884, 294083, 138991], expert_idx: 0\n",
      "subnet: [215986, 28212, 313884, 294083, 138991], loss: [1.00109821], acc: 0.59375, samples: 32\n",
      "subnet: [32919, 325755, 129161, 187173, 213109], expert_idx: 0\n",
      "subnet: [32919, 325755, 129161, 187173, 213109], loss: [1.04449185], acc: 0.5625, samples: 32\n",
      "subnet: [283050, 289144, 74431, 21747, 85180], expert_idx: 0\n",
      "subnet: [283050, 289144, 74431, 21747, 85180], loss: [0.89961435], acc: 0.59375, samples: 32\n",
      "subnet: [148237, 13485, 36506, 283816, 141097], expert_idx: 0\n",
      "subnet: [148237, 13485, 36506, 283816, 141097], loss: [1.11100093], acc: 0.53125, samples: 32\n",
      "subnet: [166736, 71602, 180185, 116045, 115821], expert_idx: 0\n",
      "subnet: [166736, 71602, 180185, 116045, 115821], loss: [0.95577065], acc: 0.59375, samples: 32\n",
      "subnet: [216720, 300425, 27869, 280570, 275634], expert_idx: 0\n",
      "subnet: [216720, 300425, 27869, 280570, 275634], loss: [1.11046734], acc: 0.46875, samples: 32\n",
      "subnet: [113900, 284354, 10476, 216182, 71352], expert_idx: 0\n",
      "subnet: [113900, 284354, 10476, 216182, 71352], loss: [1.10859581], acc: 0.5625, samples: 32\n",
      "subnet: [173046, 272358, 54432, 224713, 298737], expert_idx: 0\n",
      "subnet: [173046, 272358, 54432, 224713, 298737], loss: [1.07472675], acc: 0.53125, samples: 32\n",
      "subnet: [87099, 45974, 144395, 57883, 173832], expert_idx: 0\n",
      "subnet: [87099, 45974, 144395, 57883, 173832], loss: [1.09340066], acc: 0.5625, samples: 32\n",
      "subnet: [57034, 183786, 73128, 292728, 249721], expert_idx: 0\n",
      "subnet: [57034, 183786, 73128, 292728, 249721], loss: [1.1979293], acc: 0.5, samples: 32\n",
      "subnet: [254079, 183113, 16024, 221786, 178548], expert_idx: 0\n",
      "subnet: [254079, 183113, 16024, 221786, 178548], loss: [0.94864894], acc: 0.625, samples: 32\n",
      "subnet: [35410, 67923, 322245, 214133, 206926], expert_idx: 0\n",
      "subnet: [35410, 67923, 322245, 214133, 206926], loss: [1.06621569], acc: 0.5, samples: 32\n",
      "subnet: [297393, 60889, 231129, 68734, 9665], expert_idx: 0\n",
      "subnet: [297393, 60889, 231129, 68734, 9665], loss: [0.93391091], acc: 0.59375, samples: 32\n",
      "subnet: [57522, 309807, 306193, 220029, 98176], expert_idx: 0\n",
      "subnet: [57522, 309807, 306193, 220029, 98176], loss: [1.18331791], acc: 0.375, samples: 32\n",
      "subnet: [302370, 210816, 272654, 300689, 65068], expert_idx: 0\n",
      "subnet: [302370, 210816, 272654, 300689, 65068], loss: [1.04883308], acc: 0.5, samples: 32\n",
      "subnet: [17365, 13293, 65598, 141304, 81224], expert_idx: 0\n",
      "subnet: [17365, 13293, 65598, 141304, 81224], loss: [1.03260401], acc: 0.5, samples: 32\n",
      "subnet: [73709, 173822, 31729, 89034, 253546], expert_idx: 0\n",
      "subnet: [73709, 173822, 31729, 89034, 253546], loss: [0.98627771], acc: 0.46875, samples: 32\n",
      "subnet: [58627, 90224, 309282, 282594, 236421], expert_idx: 0\n",
      "subnet: [58627, 90224, 309282, 282594, 236421], loss: [0.95783374], acc: 0.65625, samples: 32\n",
      "subnet: [18465, 170880, 310170, 81455, 162328], expert_idx: 0\n",
      "subnet: [18465, 170880, 310170, 81455, 162328], loss: [0.98038371], acc: 0.59375, samples: 32\n",
      "subnet: [259747, 250444, 139366, 179467, 230158], expert_idx: 0\n",
      "subnet: [259747, 250444, 139366, 179467, 230158], loss: [1.06860967], acc: 0.46875, samples: 32\n",
      "subnet: [45447, 82832, 326749, 307493, 311626], expert_idx: 0\n",
      "subnet: [45447, 82832, 326749, 307493, 311626], loss: [1.32907148], acc: 0.28125, samples: 32\n",
      "subnet: [126857, 62568, 138641, 90415, 305485], expert_idx: 0\n",
      "subnet: [126857, 62568, 138641, 90415, 305485], loss: [1.12473986], acc: 0.4375, samples: 32\n",
      "subnet: [229155, 272976, 124121, 255385, 165314], expert_idx: 0\n",
      "subnet: [229155, 272976, 124121, 255385, 165314], loss: [1.00473669], acc: 0.5625, samples: 32\n",
      "subnet: [293750, 171057, 246530, 140188, 131108], expert_idx: 0\n",
      "subnet: [293750, 171057, 246530, 140188, 131108], loss: [0.95271496], acc: 0.5625, samples: 32\n",
      "subnet: [219738, 239607, 125584, 207407, 142844], expert_idx: 0\n",
      "subnet: [219738, 239607, 125584, 207407, 142844], loss: [0.98014599], acc: 0.5625, samples: 32\n",
      "subnet: [103600, 62158, 132647, 141314, 134580], expert_idx: 0\n",
      "subnet: [103600, 62158, 132647, 141314, 134580], loss: [1.04648623], acc: 0.59375, samples: 32\n",
      "subnet: [99830, 156431, 116379, 263030, 173759], expert_idx: 0\n",
      "subnet: [99830, 156431, 116379, 263030, 173759], loss: [0.93898011], acc: 0.71875, samples: 32\n",
      "subnet: [247636, 59960, 16770, 54436, 123860], expert_idx: 0\n",
      "subnet: [247636, 59960, 16770, 54436, 123860], loss: [0.94764291], acc: 0.625, samples: 32\n",
      "subnet: [123310, 290589, 282144, 283019, 272503], expert_idx: 0\n",
      "subnet: [123310, 290589, 282144, 283019, 272503], loss: [1.01036372], acc: 0.5625, samples: 32\n",
      "subnet: [150235, 36528, 36680, 56845, 228517], expert_idx: 0\n",
      "subnet: [150235, 36528, 36680, 56845, 228517], loss: [1.18815323], acc: 0.40625, samples: 32\n",
      "subnet: [251097, 198244, 59112, 173406, 146492], expert_idx: 0\n",
      "subnet: [251097, 198244, 59112, 173406, 146492], loss: [1.09154546], acc: 0.5, samples: 32\n",
      "subnet: [63601, 123028, 99147, 221151, 206899], expert_idx: 0\n",
      "subnet: [63601, 123028, 99147, 221151, 206899], loss: [1.07540366], acc: 0.4375, samples: 32\n",
      "subnet: [318274, 294456, 163548, 195337, 321253], expert_idx: 0\n",
      "subnet: [318274, 294456, 163548, 195337, 321253], loss: [1.05820947], acc: 0.46875, samples: 32\n",
      "subnet: [4939, 279973, 320689, 41707, 66710], expert_idx: 0\n",
      "subnet: [4939, 279973, 320689, 41707, 66710], loss: [1.06546284], acc: 0.40625, samples: 32\n",
      "subnet: [210442, 292929, 201934, 232642, 32639], expert_idx: 0\n",
      "subnet: [210442, 292929, 201934, 232642, 32639], loss: [0.96808471], acc: 0.46875, samples: 32\n",
      "subnet: [310120, 219219, 55634, 265689, 88032], expert_idx: 0\n",
      "subnet: [310120, 219219, 55634, 265689, 88032], loss: [1.18482986], acc: 0.4375, samples: 32\n",
      "subnet: [216654, 200264, 73071, 21812, 232484], expert_idx: 0\n",
      "subnet: [216654, 200264, 73071, 21812, 232484], loss: [1.12034727], acc: 0.5625, samples: 32\n",
      "subnet: [161836, 165879, 275264, 232261, 215720], expert_idx: 0\n",
      "subnet: [161836, 165879, 275264, 232261, 215720], loss: [1.13008202], acc: 0.46875, samples: 32\n",
      "subnet: [326923, 68925, 270867, 256041, 283035], expert_idx: 0\n",
      "subnet: [326923, 68925, 270867, 256041, 283035], loss: [0.98932292], acc: 0.59375, samples: 32\n",
      "subnet: [84318, 142462, 197555, 12121, 87612], expert_idx: 0\n",
      "subnet: [84318, 142462, 197555, 12121, 87612], loss: [1.10560138], acc: 0.46875, samples: 32\n",
      "subnet: [331474, 45087, 268646, 167692, 6243], expert_idx: 0\n",
      "subnet: [331474, 45087, 268646, 167692, 6243], loss: [1.07367621], acc: 0.40625, samples: 32\n",
      "subnet: [289208, 294423, 257622, 198122, 255651], expert_idx: 0\n",
      "subnet: [289208, 294423, 257622, 198122, 255651], loss: [0.97204646], acc: 0.5625, samples: 32\n",
      "subnet: [22059, 100573, 2761, 114163, 272298], expert_idx: 0\n",
      "subnet: [22059, 100573, 2761, 114163, 272298], loss: [1.11200705], acc: 0.5, samples: 32\n",
      "subnet: [57383, 285311, 120149, 110200, 176721], expert_idx: 0\n",
      "subnet: [57383, 285311, 120149, 110200, 176721], loss: [1.11635236], acc: 0.53125, samples: 32\n",
      "subnet: [64327, 248183, 6716, 96861, 37683], expert_idx: 0\n",
      "subnet: [64327, 248183, 6716, 96861, 37683], loss: [1.17643267], acc: 0.4375, samples: 32\n",
      "subnet: [285582, 78458, 195148, 42942, 223415], expert_idx: 0\n",
      "subnet: [285582, 78458, 195148, 42942, 223415], loss: [1.06716556], acc: 0.5, samples: 32\n",
      "subnet: [264377, 33772, 147870, 124504, 170072], expert_idx: 0\n",
      "subnet: [264377, 33772, 147870, 124504, 170072], loss: [1.01114802], acc: 0.5, samples: 32\n",
      "subnet: [110147, 15646, 180354, 191416, 305990], expert_idx: 0\n",
      "subnet: [110147, 15646, 180354, 191416, 305990], loss: [1.23188782], acc: 0.3125, samples: 32\n",
      "subnet: [306591, 303201, 191474, 206661, 168352], expert_idx: 0\n",
      "subnet: [306591, 303201, 191474, 206661, 168352], loss: [1.13942401], acc: 0.40625, samples: 32\n",
      "subnet: [262169, 224047, 222060, 216548, 292622], expert_idx: 0\n",
      "subnet: [262169, 224047, 222060, 216548, 292622], loss: [1.04875007], acc: 0.5, samples: 32\n",
      "subnet: [245873, 129987, 22090, 114966, 62559], expert_idx: 0\n",
      "subnet: [245873, 129987, 22090, 114966, 62559], loss: [1.02169595], acc: 0.46875, samples: 32\n",
      "subnet: [176435, 301282, 120098, 155522, 238190], expert_idx: 0\n",
      "subnet: [176435, 301282, 120098, 155522, 238190], loss: [0.96892026], acc: 0.59375, samples: 32\n",
      "subnet: [271344, 260860, 211557, 278360, 276601], expert_idx: 0\n",
      "subnet: [271344, 260860, 211557, 278360, 276601], loss: [1.00679645], acc: 0.5, samples: 32\n",
      "subnet: [117357, 277076, 325041, 83608, 51282], expert_idx: 0\n",
      "subnet: [117357, 277076, 325041, 83608, 51282], loss: [1.07074952], acc: 0.5, samples: 32\n",
      "subnet: [287712, 175581, 215940, 196102, 272179], expert_idx: 0\n",
      "subnet: [287712, 175581, 215940, 196102, 272179], loss: [1.00267546], acc: 0.53125, samples: 32\n",
      "subnet: [323830, 296716, 316991, 5485, 44675], expert_idx: 0\n"
     ]
    }
   ],
   "source": [
    "!python supernet/train_search_mnist.py --warmup_epochs 0 --steps 25000 --n_search 1 --n_qubits 4 --n_experts 1 --n_layers 5 --n_encode_layers 4 --save \"20_params\" --data \"./experiment_data/mnist_2/\" --save_dir \"./supernet/mnist_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-09-28 08:08:08,732: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1091  483 2033 2383 1431 1940 1595 1409  932  744  826  856  289  501\n",
      "  655 2156  145 1487 1260  169  801  731 1208 1198  696 2368 2444  627\n",
      " 1887  492 1476  549 2167  889 1559 1365 1426    5   62 2082  720 1386\n",
      " 1393 1289 1836   85  535 1874  374 1728  328 1017 1661    7  972 2318\n",
      " 2118 1478 1517  593 1126 1743  787   37 1407 2446 2151  515  559 1933\n",
      "  838 1960   72  604 1502  229 2382 1612 2076  313  401 2212 1039   44\n",
      " 2178 1026 2255 1698 1818 1144 2032 1257  625 1303  337 1930 1442 2115\n",
      " 2192 1072] [0.73005717 0.69252977 0.75933413 0.69670997 0.64488682 0.72362488\n",
      " 0.67361456 0.73261076 0.74365658 0.64855466 0.69069077 0.77061042\n",
      " 0.72143045 0.68627325 0.43232428 0.76281446 0.67268712 0.77791441\n",
      " 0.68510105 0.69801382 0.77351881 0.71320305 0.61773136 0.72091305\n",
      " 0.67458034 0.69637016 0.75506948 0.73932606 0.70385377 0.62686416\n",
      " 0.73342242 0.73480707 0.61192711 0.62623635 0.69091555 0.7560822\n",
      " 0.66333645 0.72343101 0.63039414 0.62693306 0.66792519 0.72012884\n",
      " 0.69190784 0.70228061 0.70950215 0.75727062 0.75173716 0.75606896\n",
      " 0.7304798  0.77340435 0.74211613 0.67615667 0.72020216 0.73439425\n",
      " 0.72190225 0.27831115 0.71898872 0.59168164 0.7335764  0.45553569\n",
      " 0.68253219 0.77509044 0.72609508 0.71931579 0.73286903 0.74643349\n",
      " 0.61487223 0.65420429 0.70985333 0.6016518  0.45107288 0.69096925\n",
      " 0.75581455 0.60582275 0.76776613 0.76038524 0.71167064 0.65875766\n",
      " 0.74629922 0.71240313 0.70409058 0.71167458 0.72640008 0.61983232\n",
      " 0.66469333 0.71330495 0.71373147 0.67667654 0.75721993 0.71211937\n",
      " 0.70585478 0.60103806 0.74797836 0.73242293 0.71327215 0.74862149\n",
      " 0.74802452 0.74063764 0.70865869 0.723091  ]\n",
      "1487 0.7779144097148674\n",
      "0.9354095458984375 0.831629753112793\n",
      "[2022-09-28 08:08:55.212 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:1591 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-28 08:08:55.333 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:1591 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Step 1 | Sliding Loss Window : 0.8066631360255749\n",
      "Step 51 | Sliding Loss Window : 1.4160903833932266\n",
      "Step 101 | Sliding Loss Window : 1.4410226603789311\n",
      "Step 151 | Sliding Loss Window : 1.3229533534028073\n",
      "Step 201 | Sliding Loss Window : 1.255643920462854\n",
      "Step 251 | Sliding Loss Window : 1.2073519696153683\n",
      "Step 301 | Sliding Loss Window : 1.1639084230094965\n",
      "Step 351 | Sliding Loss Window : 1.2621130727913907\n",
      "Step 401 | Sliding Loss Window : 1.1507398179311572\n",
      "Step 451 | Sliding Loss Window : 1.2216611022646602\n",
      "Step 501 | Sliding Loss Window : 1.1943049553042084\n",
      "Step 551 | Sliding Loss Window : 1.2243874801211367\n",
      "Step 601 | Sliding Loss Window : 1.1830077886822865\n",
      "Step 651 | Sliding Loss Window : 1.0507137809993732\n",
      "Step 701 | Sliding Loss Window : 0.9824269514119435\n",
      "Step 751 | Sliding Loss Window : 1.0562425904034896\n",
      "Step 801 | Sliding Loss Window : 1.1619494818561902\n",
      "Step 851 | Sliding Loss Window : 0.9792052440382609\n",
      "Step 901 | Sliding Loss Window : 1.179722225090317\n",
      "Step 951 | Sliding Loss Window : 1.0787239393817714\n",
      "Step 1001 | Sliding Loss Window : 1.0714104293906794\n",
      "Step 1051 | Sliding Loss Window : 1.0051082485421718\n",
      "Step 1101 | Sliding Loss Window : 1.040867245379166\n",
      "Step 1151 | Sliding Loss Window : 1.1905972668218632\n",
      "Step 1201 | Sliding Loss Window : 1.0687054096271296\n",
      "Step 1251 | Sliding Loss Window : 1.0806346440418033\n",
      "Step 1301 | Sliding Loss Window : 1.17087945463173\n",
      "Step 1351 | Sliding Loss Window : 1.0137245114379312\n",
      "Step 1401 | Sliding Loss Window : 1.2436825779326939\n",
      "Step 1451 | Sliding Loss Window : 1.0007136179304097\n",
      "Step 1501 | Sliding Loss Window : 1.0133854876976733\n",
      "Step 1551 | Sliding Loss Window : 1.1685757783806912\n",
      "Step 1601 | Sliding Loss Window : 1.1198399577007674\n",
      "Step 1651 | Sliding Loss Window : 1.2284622124077684\n",
      "Step 1701 | Sliding Loss Window : 1.0907485583723076\n",
      "Step 1751 | Sliding Loss Window : 1.1686623332111035\n",
      "Step 1801 | Sliding Loss Window : 1.1261679597069563\n",
      "Step 1851 | Sliding Loss Window : 1.1481649234222477\n",
      "Step 1901 | Sliding Loss Window : 1.0346906707509698\n",
      "Step 1951 | Sliding Loss Window : 1.0989016475224556\n",
      "Step 2001 | Sliding Loss Window : 0.9497314017498978\n",
      "Step 2051 | Sliding Loss Window : 0.9437174923494653\n",
      "Step 2101 | Sliding Loss Window : 1.1925735392340597\n",
      "Step 2151 | Sliding Loss Window : 1.1402209858272516\n",
      "Step 2201 | Sliding Loss Window : 1.124309970204655\n",
      "Step 2251 | Sliding Loss Window : 1.2038707634963262\n",
      "Step 2301 | Sliding Loss Window : 1.1317267855157043\n",
      "Step 2351 | Sliding Loss Window : 1.128580692372467\n",
      "Step 2401 | Sliding Loss Window : 1.0270643111994078\n",
      "Step 2451 | Sliding Loss Window : 1.1813320266012888\n",
      "Step 2501 | Sliding Loss Window : 1.0887738936125986\n",
      "Step 2551 | Sliding Loss Window : 1.033322775010183\n",
      "Step 2601 | Sliding Loss Window : 0.9840752524871655\n",
      "Step 2651 | Sliding Loss Window : 1.1172775880427401\n",
      "Step 2701 | Sliding Loss Window : 1.0857912543154584\n",
      "Step 2751 | Sliding Loss Window : 1.0951839824476999\n",
      "Step 2801 | Sliding Loss Window : 1.0364764348336206\n",
      "Step 2851 | Sliding Loss Window : 0.9553733838603107\n",
      "Step 2901 | Sliding Loss Window : 1.0804951028233287\n",
      "Step 2951 | Sliding Loss Window : 1.2094902452656031\n",
      "Step 3001 | Sliding Loss Window : 0.9788898837457733\n",
      "Step 3051 | Sliding Loss Window : 1.207313472991943\n",
      "Step 3101 | Sliding Loss Window : 1.1939013682947726\n",
      "Step 3151 | Sliding Loss Window : 1.1887109860158949\n",
      "Step 3201 | Sliding Loss Window : 1.0763310316219206\n",
      "Step 3251 | Sliding Loss Window : 1.0922423487206052\n",
      "Step 3301 | Sliding Loss Window : 1.1559854432921812\n",
      "Step 3351 | Sliding Loss Window : 1.0953383243320196\n",
      "Step 3401 | Sliding Loss Window : 1.0512236875354954\n",
      "Step 3451 | Sliding Loss Window : 1.024853035122716\n",
      "Step 3501 | Sliding Loss Window : 1.128497604427909\n",
      "Step 3551 | Sliding Loss Window : 1.128670862774435\n",
      "Step 3601 | Sliding Loss Window : 1.1917820504997025\n",
      "Step 3651 | Sliding Loss Window : 1.0327538031627477\n",
      "Step 3701 | Sliding Loss Window : 1.0479176783318846\n",
      "Step 3751 | Sliding Loss Window : 1.1617907820986446\n",
      "Step 3801 | Sliding Loss Window : 1.0312811223498926\n",
      "Step 3851 | Sliding Loss Window : 1.0805019838260845\n",
      "Step 3901 | Sliding Loss Window : 0.9100285869103719\n",
      "Step 3951 | Sliding Loss Window : 1.1937383810792201\n",
      "Step 4001 | Sliding Loss Window : 1.0472734545027014\n",
      "Step 4051 | Sliding Loss Window : 1.130148612071631\n",
      "Step 4101 | Sliding Loss Window : 0.9310282676817375\n",
      "Step 4151 | Sliding Loss Window : 1.1647550240194398\n",
      "Step 4201 | Sliding Loss Window : 1.1233184182193803\n",
      "Step 4251 | Sliding Loss Window : 0.8726817878265404\n",
      "Step 4301 | Sliding Loss Window : 1.0752954590996722\n",
      "Step 4351 | Sliding Loss Window : 1.1205277859710352\n",
      "Step 4401 | Sliding Loss Window : 1.0547765128526132\n",
      "Step 4451 | Sliding Loss Window : 0.9605816780003622\n",
      "Step 4501 | Sliding Loss Window : 1.0746729450170065\n",
      "Step 4551 | Sliding Loss Window : 1.0689502998382436\n",
      "Step 4601 | Sliding Loss Window : 1.0361266963035936\n",
      "Step 4651 | Sliding Loss Window : 1.0995559808861386\n",
      "Step 4701 | Sliding Loss Window : 1.066314734119777\n",
      "Step 4751 | Sliding Loss Window : 1.1405991859572113\n",
      "Step 4801 | Sliding Loss Window : 1.152622864571856\n",
      "Step 4851 | Sliding Loss Window : 1.1360087714265474\n",
      "Step 4901 | Sliding Loss Window : 1.0475139798126913\n",
      "Step 4951 | Sliding Loss Window : 1.1380524323314127\n",
      "Step 5001 | Sliding Loss Window : 1.073831097122355\n",
      "Step 5051 | Sliding Loss Window : 1.0579650200875446\n",
      "Step 5101 | Sliding Loss Window : 1.0231062255353904\n",
      "Step 5151 | Sliding Loss Window : 1.0330190693579662\n",
      "Step 5201 | Sliding Loss Window : 0.9365050994263886\n",
      "Step 5251 | Sliding Loss Window : 1.0954966816422982\n",
      "Step 5301 | Sliding Loss Window : 1.0735274586514745\n",
      "Step 5351 | Sliding Loss Window : 1.095900672748082\n",
      "Step 5401 | Sliding Loss Window : 1.0774967095228272\n",
      "Step 5451 | Sliding Loss Window : 1.1778438520825742\n",
      "Step 5501 | Sliding Loss Window : 1.0950491901269832\n",
      "Step 5551 | Sliding Loss Window : 1.193409752936725\n",
      "Step 5601 | Sliding Loss Window : 1.091798248916322\n",
      "Step 5651 | Sliding Loss Window : 0.977789722549413\n",
      "Step 5701 | Sliding Loss Window : 1.088626699435332\n",
      "Step 5751 | Sliding Loss Window : 1.1074639013015333\n",
      "Step 5801 | Sliding Loss Window : 1.0492897843069742\n",
      "Step 5851 | Sliding Loss Window : 0.9820116988995597\n",
      "Step 5901 | Sliding Loss Window : 1.1275394427455763\n",
      "Step 5951 | Sliding Loss Window : 1.1042314804946614\n",
      "Step 6001 | Sliding Loss Window : 0.9686196497385535\n",
      "Step 6051 | Sliding Loss Window : 1.1116843740600415\n",
      "Step 6101 | Sliding Loss Window : 1.1050405876645106\n",
      "Step 6151 | Sliding Loss Window : 1.0649580303463944\n",
      "Step 6201 | Sliding Loss Window : 0.9905773809379632\n",
      "Step 6251 | Sliding Loss Window : 1.215378779038048\n",
      "Step 6301 | Sliding Loss Window : 1.1317429782194266\n",
      "Step 6351 | Sliding Loss Window : 1.074520645944027\n",
      "Step 6401 | Sliding Loss Window : 1.037317291516016\n",
      "Step 6451 | Sliding Loss Window : 1.0504776738438968\n",
      "Step 6501 | Sliding Loss Window : 1.1878761313342807\n",
      "Step 6551 | Sliding Loss Window : 0.9776281596024959\n",
      "Step 6601 | Sliding Loss Window : 1.0327629017489786\n",
      "Step 6651 | Sliding Loss Window : 0.9906937259467231\n",
      "Step 6701 | Sliding Loss Window : 1.0866715528328417\n",
      "Step 6751 | Sliding Loss Window : 1.2192712868008\n",
      "Step 6801 | Sliding Loss Window : 1.1954217136012784\n",
      "Step 6851 | Sliding Loss Window : 1.1752952180711564\n",
      "Step 6901 | Sliding Loss Window : 1.1021776194679525\n",
      "Step 6951 | Sliding Loss Window : 0.9441972344866891\n",
      "Step 7001 | Sliding Loss Window : 0.9541369923064401\n",
      "Step 7051 | Sliding Loss Window : 0.9675387348442952\n",
      "Step 7101 | Sliding Loss Window : 1.2394006396324215\n",
      "Step 7151 | Sliding Loss Window : 1.1374879011443388\n",
      "Step 7201 | Sliding Loss Window : 1.1485802053962622\n",
      "Step 7251 | Sliding Loss Window : 1.0601045470806938\n",
      "Step 7301 | Sliding Loss Window : 1.0748789886328116\n",
      "Step 7351 | Sliding Loss Window : 1.0451930594527483\n",
      "Step 7401 | Sliding Loss Window : 1.0414797766945185\n",
      "Step 7451 | Sliding Loss Window : 1.0909513030300166\n",
      "Step 7501 | Sliding Loss Window : 1.0394099384601\n",
      "Step 7551 | Sliding Loss Window : 1.0206631055500985\n",
      "Step 7601 | Sliding Loss Window : 1.1533572539034116\n",
      "Step 7651 | Sliding Loss Window : 0.9937052817876976\n",
      "Step 7701 | Sliding Loss Window : 1.1406316641457248\n",
      "Step 7751 | Sliding Loss Window : 1.1203189051741849\n",
      "Step 7801 | Sliding Loss Window : 1.230124882132701\n",
      "Step 7851 | Sliding Loss Window : 1.0031185450383726\n",
      "Step 7901 | Sliding Loss Window : 1.1689189110737113\n",
      "Step 7951 | Sliding Loss Window : 1.1486708322394978\n",
      "Step 8001 | Sliding Loss Window : 1.0429902919446332\n",
      "Step 8051 | Sliding Loss Window : 1.0209825534726011\n",
      "Step 8101 | Sliding Loss Window : 1.0259355471994478\n",
      "Step 8151 | Sliding Loss Window : 1.0671906939174982\n",
      "Step 8201 | Sliding Loss Window : 0.9430862563623935\n",
      "Step 8251 | Sliding Loss Window : 1.0548488810974166\n",
      "Step 8301 | Sliding Loss Window : 1.0151161739436814\n",
      "Step 8351 | Sliding Loss Window : 1.1443080247350927\n",
      "Step 8401 | Sliding Loss Window : 1.0807025876807963\n",
      "Step 8451 | Sliding Loss Window : 1.2028274431006651\n",
      "Step 8501 | Sliding Loss Window : 1.0889857498579532\n",
      "Step 8551 | Sliding Loss Window : 1.1459313632066976\n",
      "Step 8601 | Sliding Loss Window : 1.097498733573262\n",
      "Step 8651 | Sliding Loss Window : 0.9751251286032825\n",
      "Step 8701 | Sliding Loss Window : 0.9408017569061445\n",
      "Step 8751 | Sliding Loss Window : 1.0466650584567012\n",
      "Step 8801 | Sliding Loss Window : 1.1415836366446295\n",
      "Step 8851 | Sliding Loss Window : 0.9574026238108745\n",
      "Step 8901 | Sliding Loss Window : 1.1817674919947194\n",
      "Step 8951 | Sliding Loss Window : 1.0406496184462517\n",
      "Step 9001 | Sliding Loss Window : 1.0413772023656467\n",
      "Step 9051 | Sliding Loss Window : 1.0223031082425773\n",
      "Step 9101 | Sliding Loss Window : 1.048588131934413\n",
      "Step 9151 | Sliding Loss Window : 1.1780455440881257\n",
      "Step 9201 | Sliding Loss Window : 1.0672513493764435\n",
      "Step 9251 | Sliding Loss Window : 1.0836621494075669\n",
      "Step 9301 | Sliding Loss Window : 1.15147122209158\n",
      "Step 9351 | Sliding Loss Window : 1.0204392411682934\n",
      "Step 9401 | Sliding Loss Window : 1.1749239899459383\n",
      "Step 9451 | Sliding Loss Window : 1.0114866860088312\n",
      "Step 9501 | Sliding Loss Window : 0.9806012973783028\n",
      "Step 9551 | Sliding Loss Window : 1.154417911749759\n",
      "Step 9601 | Sliding Loss Window : 1.0974123987277702\n",
      "Step 9651 | Sliding Loss Window : 1.224810817908467\n",
      "Step 9701 | Sliding Loss Window : 1.080890398254332\n",
      "Step 9751 | Sliding Loss Window : 1.1542517208612688\n",
      "Step 9801 | Sliding Loss Window : 1.1287795371013747\n",
      "Step 9851 | Sliding Loss Window : 1.135263177931642\n",
      "Step 9901 | Sliding Loss Window : 1.0440875165467611\n",
      "Step 9951 | Sliding Loss Window : 1.0661532247588208\n",
      "Step 10001 | Sliding Loss Window : 0.9392311530063445\n",
      "Step 10051 | Sliding Loss Window : 0.9225719752322725\n",
      "Step 10101 | Sliding Loss Window : 1.194159065352341\n",
      "Step 10151 | Sliding Loss Window : 1.1307324328946695\n",
      "Step 10201 | Sliding Loss Window : 1.068051920845445\n",
      "Step 10251 | Sliding Loss Window : 1.1978769166674068\n",
      "Step 10301 | Sliding Loss Window : 1.1262136600932975\n",
      "Step 10351 | Sliding Loss Window : 1.119834742513767\n",
      "Step 10401 | Sliding Loss Window : 1.0260149136640118\n",
      "Step 10451 | Sliding Loss Window : 1.1345147259419415\n",
      "Step 10501 | Sliding Loss Window : 1.0822924003277778\n",
      "Step 10551 | Sliding Loss Window : 1.0156308155310034\n",
      "Step 10601 | Sliding Loss Window : 0.9632724785380427\n",
      "Step 10651 | Sliding Loss Window : 1.115039517507351\n",
      "Step 10701 | Sliding Loss Window : 1.082765933386408\n",
      "Step 10751 | Sliding Loss Window : 1.0750617869073233\n",
      "Step 10801 | Sliding Loss Window : 1.024788825444479\n",
      "Step 10851 | Sliding Loss Window : 0.9230270850017446\n",
      "Step 10901 | Sliding Loss Window : 1.0858421225547947\n",
      "Step 10951 | Sliding Loss Window : 1.2338553686755636\n",
      "Step 11001 | Sliding Loss Window : 0.9677409458014441\n",
      "Step 11051 | Sliding Loss Window : 1.1673682260226819\n",
      "Step 11101 | Sliding Loss Window : 1.18937949885486\n",
      "Step 11151 | Sliding Loss Window : 1.1448665824040443\n",
      "Step 11201 | Sliding Loss Window : 1.057144961342364\n",
      "Step 11251 | Sliding Loss Window : 1.0560269896456063\n",
      "Step 11301 | Sliding Loss Window : 1.1439699652813675\n",
      "Step 11351 | Sliding Loss Window : 1.0881486612165263\n",
      "Step 11401 | Sliding Loss Window : 1.0612900061305788\n",
      "Step 11451 | Sliding Loss Window : 0.9970143884479192\n",
      "Step 11501 | Sliding Loss Window : 1.0920034777181749\n",
      "Step 11551 | Sliding Loss Window : 1.0958847104149696\n",
      "Step 11601 | Sliding Loss Window : 1.2176813721683517\n",
      "Step 11651 | Sliding Loss Window : 1.0235448169909782\n",
      "Step 11701 | Sliding Loss Window : 1.0349478721798469\n",
      "Step 11751 | Sliding Loss Window : 1.1503505094842836\n",
      "Step 11801 | Sliding Loss Window : 1.0175419797848266\n",
      "Step 11851 | Sliding Loss Window : 1.045554613189688\n",
      "Step 11901 | Sliding Loss Window : 0.890820887052541\n",
      "Step 11951 | Sliding Loss Window : 1.178883630654098\n",
      "Step 12001 | Sliding Loss Window : 1.0662325131580586\n",
      "Step 12051 | Sliding Loss Window : 1.1207239617520404\n",
      "Step 12101 | Sliding Loss Window : 0.9418323682382055\n",
      "Step 12151 | Sliding Loss Window : 1.0998911440071897\n",
      "Step 12201 | Sliding Loss Window : 1.1126832704286085\n",
      "Step 12251 | Sliding Loss Window : 0.8749166618084853\n",
      "Step 12301 | Sliding Loss Window : 1.0658873935881552\n",
      "Step 12351 | Sliding Loss Window : 1.1157695305007804\n",
      "Step 12401 | Sliding Loss Window : 1.051828018246632\n",
      "Step 12451 | Sliding Loss Window : 0.9509495948380193\n",
      "Step 12501 | Sliding Loss Window : 1.0481447042419136\n",
      "Step 12551 | Sliding Loss Window : 1.0798609846419005\n",
      "Step 12601 | Sliding Loss Window : 1.0302247328830643\n",
      "Step 12651 | Sliding Loss Window : 1.1060963344631154\n",
      "Step 12701 | Sliding Loss Window : 1.0175464628812354\n",
      "Step 12751 | Sliding Loss Window : 1.1142753844614004\n",
      "Step 12801 | Sliding Loss Window : 1.1054497506564211\n",
      "Step 12851 | Sliding Loss Window : 1.1504226708455354\n",
      "Step 12901 | Sliding Loss Window : 1.0469653117764508\n",
      "Step 12951 | Sliding Loss Window : 1.1345519071833767\n",
      "Step 13001 | Sliding Loss Window : 1.0473687528436462\n",
      "Step 13051 | Sliding Loss Window : 1.048570044249701\n",
      "Step 13101 | Sliding Loss Window : 1.0302882566707956\n",
      "Step 13151 | Sliding Loss Window : 1.021814702176342\n",
      "Step 13201 | Sliding Loss Window : 0.9291628167272463\n",
      "Step 13251 | Sliding Loss Window : 1.0967230175765372\n",
      "Step 13301 | Sliding Loss Window : 1.061800719788108\n",
      "Step 13351 | Sliding Loss Window : 1.0897737561054757\n",
      "Step 13401 | Sliding Loss Window : 1.1001590963529717\n",
      "Step 13451 | Sliding Loss Window : 1.1210745991724063\n",
      "Step 13501 | Sliding Loss Window : 1.0815571710534682\n",
      "Step 13551 | Sliding Loss Window : 1.1707131790868446\n",
      "Step 13601 | Sliding Loss Window : 1.1178033760695705\n",
      "Step 13651 | Sliding Loss Window : 0.9956609962342751\n",
      "Step 13701 | Sliding Loss Window : 1.0865562861958251\n",
      "Step 13751 | Sliding Loss Window : 1.0824650157394655\n",
      "Step 13801 | Sliding Loss Window : 1.0523342955861927\n",
      "Step 13851 | Sliding Loss Window : 0.9574993189815119\n",
      "Step 13901 | Sliding Loss Window : 1.1050993889712073\n",
      "Step 13951 | Sliding Loss Window : 1.0909572287223381\n",
      "Step 14001 | Sliding Loss Window : 0.9561822951869687\n",
      "Step 14051 | Sliding Loss Window : 1.110303174162404\n",
      "Step 14101 | Sliding Loss Window : 1.0808255231863972\n",
      "Step 14151 | Sliding Loss Window : 1.0589983569908354\n",
      "Step 14201 | Sliding Loss Window : 0.9864276159685442\n",
      "Step 14251 | Sliding Loss Window : 1.2142953360256385\n",
      "Step 14301 | Sliding Loss Window : 1.1282778232586113\n",
      "Step 14351 | Sliding Loss Window : 1.0507645237181769\n",
      "Step 14401 | Sliding Loss Window : 1.004774282425619\n",
      "Step 14451 | Sliding Loss Window : 1.0592192746453957\n",
      "Step 14501 | Sliding Loss Window : 1.1618513722072275\n",
      "Step 14551 | Sliding Loss Window : 0.9769485114887041\n",
      "Step 14601 | Sliding Loss Window : 1.040414967289684\n",
      "Step 14651 | Sliding Loss Window : 0.9818845267992434\n",
      "Step 14701 | Sliding Loss Window : 1.0639625523912963\n",
      "Step 14751 | Sliding Loss Window : 1.202248308220339\n",
      "Step 14801 | Sliding Loss Window : 1.2138983417064357\n",
      "Step 14851 | Sliding Loss Window : 1.1925434290206751\n",
      "Step 14901 | Sliding Loss Window : 1.0968711488823046\n",
      "Step 14951 | Sliding Loss Window : 0.9473634921964256\n",
      "Step 15001 | Sliding Loss Window : 0.9526619423855199\n",
      "Step 15051 | Sliding Loss Window : 0.9617165015841934\n",
      "Step 15101 | Sliding Loss Window : 1.240840486263204\n",
      "Step 15151 | Sliding Loss Window : 1.1379071522123279\n",
      "Step 15201 | Sliding Loss Window : 1.132063899902925\n",
      "Step 15251 | Sliding Loss Window : 1.0416548449019603\n",
      "Step 15301 | Sliding Loss Window : 1.0806436354389153\n",
      "Step 15351 | Sliding Loss Window : 1.022508030851709\n",
      "Step 15401 | Sliding Loss Window : 1.0491876188463773\n",
      "Step 15451 | Sliding Loss Window : 1.07051088876129\n",
      "Step 15501 | Sliding Loss Window : 1.034139212371066\n",
      "Step 15551 | Sliding Loss Window : 1.021059023490842\n",
      "Step 15601 | Sliding Loss Window : 1.1519386147181294\n",
      "Step 15651 | Sliding Loss Window : 0.9893534105260146\n",
      "Step 15701 | Sliding Loss Window : 1.1396368100936698\n",
      "Step 15751 | Sliding Loss Window : 1.1203163453496452\n",
      "Step 15801 | Sliding Loss Window : 1.2127243664920415\n",
      "Step 15851 | Sliding Loss Window : 1.0009396406706634\n",
      "Step 15901 | Sliding Loss Window : 1.1632364955175176\n",
      "Step 15951 | Sliding Loss Window : 1.1523759631587702\n",
      "Step 1 | Sliding Loss Window : 1.126940116847278\n",
      "Step 51 | Sliding Loss Window : 1.519102395156497\n",
      "Step 101 | Sliding Loss Window : 1.3600031407258604\n",
      "Step 151 | Sliding Loss Window : 1.3910895936414114\n",
      "Step 201 | Sliding Loss Window : 1.4625699852201368\n",
      "Step 251 | Sliding Loss Window : 1.3793528043075098\n",
      "Step 301 | Sliding Loss Window : 1.3293429353381134\n",
      "Step 351 | Sliding Loss Window : 1.3442377195163422\n",
      "Step 401 | Sliding Loss Window : 1.2972318078499856\n",
      "Step 451 | Sliding Loss Window : 1.2612189083584584\n",
      "Step 501 | Sliding Loss Window : 1.1522228744957947\n",
      "Step 551 | Sliding Loss Window : 1.1447887783378965\n",
      "Step 601 | Sliding Loss Window : 1.1467294934143812\n",
      "Step 651 | Sliding Loss Window : 1.2486729236300425\n",
      "Step 701 | Sliding Loss Window : 1.1874369031784067\n",
      "Step 751 | Sliding Loss Window : 1.138658955825119\n",
      "Step 801 | Sliding Loss Window : 1.037264629983813\n",
      "Step 851 | Sliding Loss Window : 1.1051024980442512\n",
      "Step 901 | Sliding Loss Window : 1.064789808606734\n",
      "Step 951 | Sliding Loss Window : 1.1554175154908672\n",
      "Step 1001 | Sliding Loss Window : 1.0994568467420118\n",
      "Step 1051 | Sliding Loss Window : 1.1258280881981908\n",
      "Step 1101 | Sliding Loss Window : 1.0144286663529596\n",
      "Step 1151 | Sliding Loss Window : 1.136348328959658\n",
      "Step 1201 | Sliding Loss Window : 1.1746981636042888\n",
      "Step 1251 | Sliding Loss Window : 1.1707517187781675\n",
      "Step 1301 | Sliding Loss Window : 1.0545825317087882\n",
      "Step 1351 | Sliding Loss Window : 1.055740121992126\n",
      "Step 1401 | Sliding Loss Window : 1.10923899087487\n",
      "Step 1451 | Sliding Loss Window : 1.081769463542859\n",
      "Step 1501 | Sliding Loss Window : 1.0510711873390322\n",
      "Step 1551 | Sliding Loss Window : 1.0230226440899772\n",
      "Step 1601 | Sliding Loss Window : 1.1739656203242677\n",
      "Step 1651 | Sliding Loss Window : 1.1117369651580242\n",
      "Step 1701 | Sliding Loss Window : 1.1893144700003884\n",
      "Step 1751 | Sliding Loss Window : 1.0414139526722221\n",
      "Step 1801 | Sliding Loss Window : 1.0466708260038333\n",
      "Step 1851 | Sliding Loss Window : 0.9728283910684558\n",
      "Step 1901 | Sliding Loss Window : 1.0117366055036907\n",
      "Step 1951 | Sliding Loss Window : 1.0205977944149638\n",
      "Step 2001 | Sliding Loss Window : 1.1850654443956623\n",
      "Step 2051 | Sliding Loss Window : 1.1751469015214264\n",
      "Step 2101 | Sliding Loss Window : 1.0808066867710924\n",
      "Step 2151 | Sliding Loss Window : 1.0838437270280263\n",
      "Step 2201 | Sliding Loss Window : 1.04726923219719\n",
      "Step 2251 | Sliding Loss Window : 1.0919125105406355\n",
      "Step 2301 | Sliding Loss Window : 1.154533955434431\n",
      "Step 2351 | Sliding Loss Window : 1.223939143279385\n",
      "Step 2401 | Sliding Loss Window : 1.1550144733663403\n",
      "Step 2451 | Sliding Loss Window : 1.0909910249103405\n",
      "Step 2501 | Sliding Loss Window : 1.1674787607195305\n",
      "Step 2551 | Sliding Loss Window : 1.174451373575745\n",
      "Step 2601 | Sliding Loss Window : 1.1617845014054486\n",
      "Step 2651 | Sliding Loss Window : 1.0470463525358709\n",
      "Step 2701 | Sliding Loss Window : 1.0066958009952884\n",
      "Step 2751 | Sliding Loss Window : 0.9118900432849409\n",
      "Step 2801 | Sliding Loss Window : 1.073015030577149\n",
      "Step 2851 | Sliding Loss Window : 1.2151599892757623\n",
      "Step 2901 | Sliding Loss Window : 1.0277435980436465\n",
      "Step 2951 | Sliding Loss Window : 1.0448651083529668\n",
      "Step 3001 | Sliding Loss Window : 1.0353515397124013\n",
      "Step 3051 | Sliding Loss Window : 1.1825750110772542\n",
      "Step 3101 | Sliding Loss Window : 1.0493264174374135\n",
      "Step 3151 | Sliding Loss Window : 1.222075116996853\n",
      "Step 3201 | Sliding Loss Window : 0.8010429110224639\n",
      "Step 3251 | Sliding Loss Window : 1.0333223556647373\n",
      "Step 3301 | Sliding Loss Window : 1.2780286335633149\n",
      "Step 3351 | Sliding Loss Window : 1.0887841141946906\n",
      "Step 3401 | Sliding Loss Window : 1.0177359030865565\n",
      "Step 3451 | Sliding Loss Window : 1.0610086859353915\n",
      "Step 3501 | Sliding Loss Window : 1.0483397736739197\n",
      "Step 3551 | Sliding Loss Window : 1.0012753416741296\n",
      "Step 3601 | Sliding Loss Window : 1.0339775371277562\n",
      "Step 3651 | Sliding Loss Window : 1.0749327080235107\n",
      "Step 3701 | Sliding Loss Window : 0.9485580556012496\n",
      "Step 3751 | Sliding Loss Window : 1.0072085344704764\n",
      "Step 3801 | Sliding Loss Window : 0.981529563532821\n",
      "Step 3851 | Sliding Loss Window : 0.9875312597981764\n",
      "Step 3901 | Sliding Loss Window : 1.0369380969047928\n",
      "Step 3951 | Sliding Loss Window : 1.151732725647412\n",
      "Step 4001 | Sliding Loss Window : 1.0568958481663957\n",
      "Step 4051 | Sliding Loss Window : 0.9909552910155077\n",
      "Step 4101 | Sliding Loss Window : 1.1682889305321904\n",
      "Step 4151 | Sliding Loss Window : 1.0171462300429166\n",
      "Step 4201 | Sliding Loss Window : 1.257055478259889\n",
      "Step 4251 | Sliding Loss Window : 1.1267990578511276\n",
      "Step 4301 | Sliding Loss Window : 1.043138456333008\n",
      "Step 4351 | Sliding Loss Window : 1.0128423993883426\n",
      "Step 4401 | Sliding Loss Window : 1.0592716583162902\n",
      "Step 4451 | Sliding Loss Window : 1.0111869753416491\n",
      "Step 4501 | Sliding Loss Window : 1.2656831034672067\n",
      "Step 4551 | Sliding Loss Window : 0.8620274445834284\n",
      "Step 4601 | Sliding Loss Window : 1.0562702564345279\n",
      "Step 4651 | Sliding Loss Window : 1.1360443061709322\n",
      "Step 4701 | Sliding Loss Window : 1.1371115410397703\n",
      "Step 4751 | Sliding Loss Window : 1.0769087973714955\n",
      "Step 4801 | Sliding Loss Window : 1.0730180320036842\n",
      "Step 4851 | Sliding Loss Window : 1.1179753720100551\n",
      "Step 4901 | Sliding Loss Window : 1.1115965172563502\n",
      "Step 4951 | Sliding Loss Window : 1.0487226109053591\n",
      "Step 5001 | Sliding Loss Window : 1.122754258104578\n",
      "Step 5051 | Sliding Loss Window : 1.2346693801013515\n",
      "Step 5101 | Sliding Loss Window : 1.0576714621838186\n",
      "Step 5151 | Sliding Loss Window : 1.1668687181977215\n",
      "Step 5201 | Sliding Loss Window : 1.0023887403984697\n",
      "Step 5251 | Sliding Loss Window : 1.1552553659874591\n",
      "Step 5301 | Sliding Loss Window : 1.1528553143014562\n",
      "Step 5351 | Sliding Loss Window : 1.0452523096058193\n",
      "Step 5401 | Sliding Loss Window : 0.9436032480353204\n",
      "Step 5451 | Sliding Loss Window : 1.0981857641576176\n",
      "Step 5501 | Sliding Loss Window : 1.0087032795616315\n",
      "Step 5551 | Sliding Loss Window : 1.0518991033104403\n",
      "Step 5601 | Sliding Loss Window : 1.233292594180802\n",
      "Step 5651 | Sliding Loss Window : 1.1163392415287792\n",
      "Step 5701 | Sliding Loss Window : 1.0471551146280689\n",
      "Step 5751 | Sliding Loss Window : 1.1195910097374417\n",
      "Step 5801 | Sliding Loss Window : 1.0680971302784983\n",
      "Step 5851 | Sliding Loss Window : 1.052444300071302\n",
      "Step 5901 | Sliding Loss Window : 1.0117560683900575\n",
      "Step 5951 | Sliding Loss Window : 0.9815568887295\n",
      "Step 6001 | Sliding Loss Window : 1.0355528608626792\n",
      "Step 6051 | Sliding Loss Window : 1.1260579198928762\n",
      "Step 6101 | Sliding Loss Window : 1.258501995848408\n",
      "Step 6151 | Sliding Loss Window : 1.0621909771045666\n",
      "Step 6201 | Sliding Loss Window : 1.1070601157881854\n",
      "Step 6251 | Sliding Loss Window : 0.97505807595884\n",
      "Step 6301 | Sliding Loss Window : 1.0258240274404167\n",
      "Step 6351 | Sliding Loss Window : 1.0916013988225612\n",
      "Step 6401 | Sliding Loss Window : 0.9355015377459078\n",
      "Step 6451 | Sliding Loss Window : 1.0979302347338167\n",
      "Step 6501 | Sliding Loss Window : 1.0722364286671455\n",
      "Step 6551 | Sliding Loss Window : 1.0315130296996875\n",
      "Step 6601 | Sliding Loss Window : 1.1696483633428396\n",
      "Step 6651 | Sliding Loss Window : 1.0331324189487001\n",
      "Step 6701 | Sliding Loss Window : 1.2440242443381166\n",
      "Step 6751 | Sliding Loss Window : 1.126625984825774\n",
      "Step 6801 | Sliding Loss Window : 1.0710418459761726\n",
      "Step 6851 | Sliding Loss Window : 1.135421552475246\n",
      "Step 6901 | Sliding Loss Window : 0.9478040570784039\n",
      "Step 6951 | Sliding Loss Window : 1.001290402321753\n",
      "Step 7001 | Sliding Loss Window : 0.9947176918995778\n",
      "Step 7051 | Sliding Loss Window : 1.046362061341784\n",
      "Step 7101 | Sliding Loss Window : 1.112097962723892\n",
      "Step 7151 | Sliding Loss Window : 0.9118575838507235\n",
      "Step 7201 | Sliding Loss Window : 0.9630224608336246\n",
      "Step 7251 | Sliding Loss Window : 1.1410539922054288\n",
      "Step 7301 | Sliding Loss Window : 1.0840740433473517\n",
      "Step 7351 | Sliding Loss Window : 1.086100148533559\n",
      "Step 7401 | Sliding Loss Window : 1.009279179410245\n",
      "Step 7451 | Sliding Loss Window : 1.0995539726403936\n",
      "Step 7501 | Sliding Loss Window : 1.1035353607738265\n",
      "Step 7551 | Sliding Loss Window : 1.1159327517637736\n",
      "Step 7601 | Sliding Loss Window : 0.9623939918060955\n",
      "Step 7651 | Sliding Loss Window : 1.1752541029799086\n",
      "Step 7701 | Sliding Loss Window : 1.1531127485601582\n",
      "Step 7751 | Sliding Loss Window : 0.9640872328723539\n",
      "Step 7801 | Sliding Loss Window : 0.9560259068168164\n",
      "Step 7851 | Sliding Loss Window : 1.047635110981928\n",
      "Step 7901 | Sliding Loss Window : 1.1137365906541619\n",
      "Step 7951 | Sliding Loss Window : 1.0711569129008058\n",
      "Step 8001 | Sliding Loss Window : 1.1503097192339227\n",
      "Step 8051 | Sliding Loss Window : 1.0439365906590317\n",
      "Step 8101 | Sliding Loss Window : 1.0024615470912943\n",
      "Step 8151 | Sliding Loss Window : 0.9737614180334336\n",
      "Step 8201 | Sliding Loss Window : 1.060967046998883\n",
      "Step 8251 | Sliding Loss Window : 1.092447506318161\n",
      "Step 8301 | Sliding Loss Window : 1.14853294137873\n",
      "Step 8351 | Sliding Loss Window : 1.065981408102264\n",
      "Step 8401 | Sliding Loss Window : 1.0989452773649921\n",
      "Step 8451 | Sliding Loss Window : 1.05916959684128\n",
      "Step 8501 | Sliding Loss Window : 0.9433074397568959\n",
      "Step 8551 | Sliding Loss Window : 1.1062305029458894\n",
      "Step 8601 | Sliding Loss Window : 0.8882692124837698\n",
      "Step 8651 | Sliding Loss Window : 1.1295401050539957\n",
      "Step 8701 | Sliding Loss Window : 1.1275878244159898\n",
      "Step 8751 | Sliding Loss Window : 1.1322423082422275\n",
      "Step 8801 | Sliding Loss Window : 0.9615445166412264\n",
      "Step 8851 | Sliding Loss Window : 0.9998416084016205\n",
      "Step 8901 | Sliding Loss Window : 1.0168810799049637\n",
      "Step 8951 | Sliding Loss Window : 1.0865388926640862\n",
      "Step 9001 | Sliding Loss Window : 1.0226895724721075\n",
      "Step 9051 | Sliding Loss Window : 1.0968247315524968\n",
      "Step 9101 | Sliding Loss Window : 0.9718573554928439\n",
      "Step 9151 | Sliding Loss Window : 1.1176764214812596\n",
      "Step 9201 | Sliding Loss Window : 1.1794181294751613\n",
      "Step 9251 | Sliding Loss Window : 1.1369467606932717\n",
      "Step 9301 | Sliding Loss Window : 1.0519534124494552\n",
      "Step 9351 | Sliding Loss Window : 1.020819024727492\n",
      "Step 9401 | Sliding Loss Window : 1.0955978307983225\n",
      "Step 9451 | Sliding Loss Window : 1.0510030061055198\n",
      "Step 9501 | Sliding Loss Window : 1.0708697008590016\n",
      "Step 9551 | Sliding Loss Window : 1.0562186912353373\n",
      "Step 9601 | Sliding Loss Window : 1.0742005156842946\n",
      "Step 9651 | Sliding Loss Window : 1.0939316288580097\n",
      "Step 9701 | Sliding Loss Window : 1.1246894289250122\n",
      "Step 9751 | Sliding Loss Window : 0.949581270473832\n",
      "Step 9801 | Sliding Loss Window : 0.985180690926864\n",
      "Step 9851 | Sliding Loss Window : 0.9587507026053612\n",
      "Step 9901 | Sliding Loss Window : 0.9678966896563136\n",
      "Step 9951 | Sliding Loss Window : 0.9839825061495714\n",
      "Step 10001 | Sliding Loss Window : 1.1481558748532652\n",
      "Step 10051 | Sliding Loss Window : 1.1648622481474975\n",
      "Step 10101 | Sliding Loss Window : 1.0743917309918152\n",
      "Step 10151 | Sliding Loss Window : 1.0393352612676339\n",
      "Step 10201 | Sliding Loss Window : 1.0127379910270664\n",
      "Step 10251 | Sliding Loss Window : 1.0772146426937983\n",
      "Step 10301 | Sliding Loss Window : 1.1509398828132278\n",
      "Step 10351 | Sliding Loss Window : 1.2344138723723523\n",
      "Step 10401 | Sliding Loss Window : 1.1491513324772265\n",
      "Step 10451 | Sliding Loss Window : 1.0823487090625459\n",
      "Step 10501 | Sliding Loss Window : 1.1265275956466994\n",
      "Step 10551 | Sliding Loss Window : 1.1810962012429946\n",
      "Step 10601 | Sliding Loss Window : 1.1593949781425676\n",
      "Step 10651 | Sliding Loss Window : 1.0365193914495918\n",
      "Step 10701 | Sliding Loss Window : 1.0028993947407592\n",
      "Step 10751 | Sliding Loss Window : 0.9000723338479004\n",
      "Step 10801 | Sliding Loss Window : 1.0932069885916187\n",
      "Step 10851 | Sliding Loss Window : 1.1801207923859856\n",
      "Step 10901 | Sliding Loss Window : 1.0063061098330368\n",
      "Step 10951 | Sliding Loss Window : 1.0235131267037867\n",
      "Step 11001 | Sliding Loss Window : 1.0403132881351467\n",
      "Step 11051 | Sliding Loss Window : 1.1797154934907121\n",
      "Step 11101 | Sliding Loss Window : 1.0522604375027582\n",
      "Step 11151 | Sliding Loss Window : 1.2347242985802667\n",
      "Step 11201 | Sliding Loss Window : 0.7964727959212454\n",
      "Step 11251 | Sliding Loss Window : 1.0285161451159843\n",
      "Step 11301 | Sliding Loss Window : 1.2702817771084698\n",
      "Step 11351 | Sliding Loss Window : 1.0692451438238668\n",
      "Step 11401 | Sliding Loss Window : 1.0137361976238783\n",
      "Step 11451 | Sliding Loss Window : 1.0520161392453886\n",
      "Step 11501 | Sliding Loss Window : 1.02423150499222\n",
      "Step 11551 | Sliding Loss Window : 1.005205512905533\n",
      "Step 11601 | Sliding Loss Window : 1.0232644852640567\n",
      "Step 11651 | Sliding Loss Window : 1.0596506042275673\n",
      "Step 11701 | Sliding Loss Window : 0.9363545825265572\n",
      "Step 11751 | Sliding Loss Window : 0.9826448714212891\n",
      "Step 11801 | Sliding Loss Window : 0.9743622240842044\n",
      "Step 11851 | Sliding Loss Window : 0.983503116400367\n",
      "Step 11901 | Sliding Loss Window : 1.04246062642522\n",
      "Step 11951 | Sliding Loss Window : 1.174627239139772\n",
      "Step 12001 | Sliding Loss Window : 1.0492831806722094\n",
      "Step 12051 | Sliding Loss Window : 0.9934572811719259\n",
      "Step 12101 | Sliding Loss Window : 1.1579170935825802\n",
      "Step 12151 | Sliding Loss Window : 1.0110570470616216\n",
      "Step 12201 | Sliding Loss Window : 1.246753240113016\n",
      "Step 12251 | Sliding Loss Window : 1.124172776335618\n",
      "Step 12301 | Sliding Loss Window : 1.0456209598805968\n",
      "Step 12351 | Sliding Loss Window : 1.0158854196315097\n",
      "Step 12401 | Sliding Loss Window : 1.059676305530514\n",
      "Step 12451 | Sliding Loss Window : 1.003046466503941\n",
      "Step 12501 | Sliding Loss Window : 1.2508719679604203\n",
      "Step 12551 | Sliding Loss Window : 0.8612729127253457\n",
      "Step 12601 | Sliding Loss Window : 1.0767584673146833\n",
      "Step 12651 | Sliding Loss Window : 1.1377632080568436\n",
      "Step 12701 | Sliding Loss Window : 1.1336184280229331\n",
      "Step 12751 | Sliding Loss Window : 1.0679561887215496\n",
      "Step 12801 | Sliding Loss Window : 1.0478544440461697\n",
      "Step 12851 | Sliding Loss Window : 1.1212037202836276\n",
      "Step 12901 | Sliding Loss Window : 1.0896781468494352\n",
      "Step 12951 | Sliding Loss Window : 1.0545424620436021\n",
      "Step 13001 | Sliding Loss Window : 1.1125178825593756\n",
      "Step 13051 | Sliding Loss Window : 1.239243515382364\n",
      "Step 13101 | Sliding Loss Window : 1.0527538833554735\n",
      "Step 13151 | Sliding Loss Window : 1.1610776053565504\n",
      "Step 13201 | Sliding Loss Window : 0.9965555099524093\n",
      "Step 13251 | Sliding Loss Window : 1.1553644179290499\n",
      "Step 13301 | Sliding Loss Window : 1.148829879550855\n",
      "Step 13351 | Sliding Loss Window : 1.0377194356525394\n",
      "Step 13401 | Sliding Loss Window : 0.9451502011907206\n",
      "Step 13451 | Sliding Loss Window : 1.0901353720821292\n",
      "Step 13501 | Sliding Loss Window : 1.007795490375624\n",
      "Step 13551 | Sliding Loss Window : 1.0505049481980147\n",
      "Step 13601 | Sliding Loss Window : 1.2302573514043584\n",
      "Step 13651 | Sliding Loss Window : 1.118929167550861\n",
      "Step 13701 | Sliding Loss Window : 1.0410636428657845\n",
      "Step 13751 | Sliding Loss Window : 1.1228326426475703\n",
      "Step 13801 | Sliding Loss Window : 1.0723020745837788\n",
      "Step 13851 | Sliding Loss Window : 1.049093406306345\n",
      "Step 13901 | Sliding Loss Window : 1.0039231815074596\n",
      "Step 13951 | Sliding Loss Window : 0.9762505763985518\n",
      "Step 14001 | Sliding Loss Window : 1.0333896667449196\n",
      "Step 14051 | Sliding Loss Window : 1.1208855812243101\n",
      "Step 14101 | Sliding Loss Window : 1.2548373499580359\n",
      "Step 14151 | Sliding Loss Window : 1.0568197071222147\n",
      "Step 14201 | Sliding Loss Window : 1.100579598544908\n",
      "Step 14251 | Sliding Loss Window : 0.9737353017339475\n",
      "Step 14301 | Sliding Loss Window : 1.0231293850423118\n",
      "Step 14351 | Sliding Loss Window : 1.0910717851411476\n",
      "Step 14401 | Sliding Loss Window : 0.9394017743423347\n",
      "Step 14451 | Sliding Loss Window : 1.0961071642688223\n",
      "Step 14501 | Sliding Loss Window : 1.0751709986726201\n",
      "Step 14551 | Sliding Loss Window : 1.0365517481507742\n",
      "Step 14601 | Sliding Loss Window : 1.1689326603706491\n",
      "Step 14651 | Sliding Loss Window : 1.0364499938308567\n",
      "Step 14701 | Sliding Loss Window : 1.243186057392596\n",
      "Step 14751 | Sliding Loss Window : 1.1246651992687797\n",
      "Step 14801 | Sliding Loss Window : 1.0708925410726728\n",
      "Step 14851 | Sliding Loss Window : 1.1376371253361734\n",
      "Step 14901 | Sliding Loss Window : 0.948512697690217\n",
      "Step 14951 | Sliding Loss Window : 1.0003127692244438\n",
      "Step 15001 | Sliding Loss Window : 0.9958309170924226\n",
      "Step 15051 | Sliding Loss Window : 1.0438111413340307\n",
      "Step 15101 | Sliding Loss Window : 1.1118022057239558\n",
      "Step 15151 | Sliding Loss Window : 0.9120969620433379\n",
      "Step 15201 | Sliding Loss Window : 0.9609588893290733\n",
      "Step 15251 | Sliding Loss Window : 1.1386731304908837\n",
      "Step 15301 | Sliding Loss Window : 1.082876897035847\n",
      "Step 15351 | Sliding Loss Window : 1.0807454819340356\n",
      "Step 15401 | Sliding Loss Window : 1.0119501567809184\n",
      "Step 15451 | Sliding Loss Window : 1.1017830489407734\n",
      "Step 15501 | Sliding Loss Window : 1.1036865842263268\n",
      "Step 15551 | Sliding Loss Window : 1.1149675197104205\n",
      "Step 15601 | Sliding Loss Window : 0.9627251869069633\n",
      "Step 15651 | Sliding Loss Window : 1.1755768821301142\n",
      "Step 15701 | Sliding Loss Window : 1.1535400256870245\n",
      "Step 15751 | Sliding Loss Window : 0.9640912284980842\n",
      "Step 15801 | Sliding Loss Window : 0.9559074078232915\n",
      "Step 15851 | Sliding Loss Window : 1.0471957847682463\n",
      "Step 15901 | Sliding Loss Window : 1.1152563721792879\n",
      "Step 15951 | Sliding Loss Window : 1.0736357343557013\n",
      "Step 1 | Sliding Loss Window : 1.9654648067507234\n",
      "Step 51 | Sliding Loss Window : 1.3704475889660919\n",
      "Step 101 | Sliding Loss Window : 1.418704364982116\n",
      "Step 151 | Sliding Loss Window : 1.3054224210476384\n",
      "Step 201 | Sliding Loss Window : 1.2725746415402102\n",
      "Step 251 | Sliding Loss Window : 1.2309440087932009\n",
      "Step 301 | Sliding Loss Window : 1.156943516319923\n",
      "Step 351 | Sliding Loss Window : 1.229590297750864\n",
      "Step 401 | Sliding Loss Window : 1.1276654880779569\n",
      "Step 451 | Sliding Loss Window : 1.1709269167704934\n",
      "Step 501 | Sliding Loss Window : 1.2090308470939475\n",
      "Step 551 | Sliding Loss Window : 1.1815711872909516\n",
      "Step 601 | Sliding Loss Window : 1.0984955442707252\n",
      "Step 651 | Sliding Loss Window : 1.1272362534931766\n",
      "Step 701 | Sliding Loss Window : 1.060029477697978\n",
      "Step 751 | Sliding Loss Window : 1.2201108208825837\n",
      "Step 801 | Sliding Loss Window : 1.0692698780444674\n",
      "Step 851 | Sliding Loss Window : 1.0903589750988463\n",
      "Step 901 | Sliding Loss Window : 1.0191375551251758\n",
      "Step 951 | Sliding Loss Window : 1.007728510866686\n",
      "Step 1001 | Sliding Loss Window : 1.037297483122081\n",
      "Step 1051 | Sliding Loss Window : 1.0672255746384796\n",
      "Step 1101 | Sliding Loss Window : 1.0201869838983926\n",
      "Step 1151 | Sliding Loss Window : 1.0969737820950618\n",
      "Step 1201 | Sliding Loss Window : 1.0520147117912866\n",
      "Step 1251 | Sliding Loss Window : 1.0583298052909145\n",
      "Step 1301 | Sliding Loss Window : 1.051702644797426\n",
      "Step 1351 | Sliding Loss Window : 1.1644805944759644\n",
      "Step 1401 | Sliding Loss Window : 1.0775157113640255\n",
      "Step 1451 | Sliding Loss Window : 0.9768629957096044\n",
      "Step 1501 | Sliding Loss Window : 1.0839436892304424\n",
      "Step 1551 | Sliding Loss Window : 0.9495757424835609\n",
      "Step 1601 | Sliding Loss Window : 0.9990801699484827\n",
      "Step 1651 | Sliding Loss Window : 1.140425604903107\n",
      "Step 1701 | Sliding Loss Window : 0.9783488326618872\n",
      "Step 1751 | Sliding Loss Window : 1.143482230603813\n",
      "Step 1801 | Sliding Loss Window : 1.0134892191117584\n",
      "Step 1851 | Sliding Loss Window : 1.1594567409199612\n",
      "Step 1901 | Sliding Loss Window : 1.0815323055701258\n",
      "Step 1951 | Sliding Loss Window : 1.049666898916216\n",
      "Step 2001 | Sliding Loss Window : 1.215610534067404\n",
      "Step 2051 | Sliding Loss Window : 1.1567643077287881\n",
      "Step 2101 | Sliding Loss Window : 0.9846303405987124\n",
      "Step 2151 | Sliding Loss Window : 1.0504112169342164\n",
      "Step 2201 | Sliding Loss Window : 1.0253062068690983\n",
      "Step 2251 | Sliding Loss Window : 1.075280624342784\n",
      "Step 2301 | Sliding Loss Window : 1.1160257414054084\n",
      "Step 2351 | Sliding Loss Window : 1.1183336055369852\n",
      "Step 2401 | Sliding Loss Window : 1.1332280670986246\n",
      "Step 2451 | Sliding Loss Window : 1.0260954182967492\n",
      "Step 2501 | Sliding Loss Window : 1.0443337616394461\n",
      "Step 2551 | Sliding Loss Window : 1.0773045961326304\n",
      "Step 2601 | Sliding Loss Window : 1.032102994366791\n",
      "Step 2651 | Sliding Loss Window : 1.1414862622929804\n",
      "Step 2701 | Sliding Loss Window : 1.2155769121693363\n",
      "Step 2751 | Sliding Loss Window : 1.100125557288071\n",
      "Step 2801 | Sliding Loss Window : 1.1219304600632798\n",
      "Step 2851 | Sliding Loss Window : 1.1797706925937874\n",
      "Step 2901 | Sliding Loss Window : 1.1764711301381154\n",
      "Step 2951 | Sliding Loss Window : 0.9867167938053136\n",
      "Step 3001 | Sliding Loss Window : 1.0206318537147732\n",
      "Step 3051 | Sliding Loss Window : 1.0775792560534463\n",
      "Step 3101 | Sliding Loss Window : 1.0967879036768429\n",
      "Step 3151 | Sliding Loss Window : 1.1116546629935875\n",
      "Step 3201 | Sliding Loss Window : 0.9832378651363405\n",
      "Step 3251 | Sliding Loss Window : 1.207230476582124\n",
      "Step 3301 | Sliding Loss Window : 1.1301475380258579\n",
      "Step 3351 | Sliding Loss Window : 1.1340790594935377\n",
      "Step 3401 | Sliding Loss Window : 1.044955622581407\n",
      "Step 3451 | Sliding Loss Window : 1.061286382669377\n",
      "Step 3501 | Sliding Loss Window : 1.0292496686344492\n",
      "Step 3551 | Sliding Loss Window : 1.088516696154251\n",
      "Step 3601 | Sliding Loss Window : 0.9549618668090294\n",
      "Step 3651 | Sliding Loss Window : 0.9735787116392441\n",
      "Step 3701 | Sliding Loss Window : 1.0860429400282035\n",
      "Step 3751 | Sliding Loss Window : 0.9651202040575736\n",
      "Step 3801 | Sliding Loss Window : 1.0793875025117454\n",
      "Step 3851 | Sliding Loss Window : 0.8706971879402527\n",
      "Step 3901 | Sliding Loss Window : 1.1525039386442204\n",
      "Step 3951 | Sliding Loss Window : 1.0925045437466891\n",
      "Step 4001 | Sliding Loss Window : 0.9893000896128814\n",
      "Step 4051 | Sliding Loss Window : 1.1112487507843993\n",
      "Step 4101 | Sliding Loss Window : 1.1356416380263001\n",
      "Step 4151 | Sliding Loss Window : 1.185325925758879\n",
      "Step 4201 | Sliding Loss Window : 0.9634230201964867\n",
      "Step 4251 | Sliding Loss Window : 0.9402429514278642\n",
      "Step 4301 | Sliding Loss Window : 0.9499930705299416\n",
      "Step 4351 | Sliding Loss Window : 1.1882288332524562\n",
      "Step 4401 | Sliding Loss Window : 1.0592353321643992\n",
      "Step 4451 | Sliding Loss Window : 1.0090598522440144\n",
      "Step 4501 | Sliding Loss Window : 1.0725233024689436\n",
      "Step 4551 | Sliding Loss Window : 1.083933412972027\n",
      "Step 4601 | Sliding Loss Window : 1.202508308609923\n",
      "Step 4651 | Sliding Loss Window : 1.1534244251637944\n",
      "Step 4701 | Sliding Loss Window : 1.2150030415580542\n",
      "Step 4751 | Sliding Loss Window : 1.0673322024344842\n",
      "Step 4801 | Sliding Loss Window : 1.0766129618542666\n",
      "Step 4851 | Sliding Loss Window : 1.0276053959945868\n",
      "Step 4901 | Sliding Loss Window : 0.9899962845261127\n",
      "Step 4951 | Sliding Loss Window : 1.0374188416430572\n",
      "Step 5001 | Sliding Loss Window : 1.0926707217209772\n",
      "Step 5051 | Sliding Loss Window : 1.0463398390315484\n",
      "Step 5101 | Sliding Loss Window : 0.9775812575074814\n",
      "Step 5151 | Sliding Loss Window : 1.0539577855634215\n",
      "Step 5201 | Sliding Loss Window : 1.0389448888155406\n",
      "Step 5251 | Sliding Loss Window : 1.0124242843594309\n",
      "Step 5301 | Sliding Loss Window : 1.0481891334571833\n",
      "Step 5351 | Sliding Loss Window : 1.050228788633991\n",
      "Step 5401 | Sliding Loss Window : 0.9186906257700859\n",
      "Step 5451 | Sliding Loss Window : 0.9964718558109137\n",
      "Step 5501 | Sliding Loss Window : 1.2134752203921872\n",
      "Step 5551 | Sliding Loss Window : 1.2713860012410574\n",
      "Step 5601 | Sliding Loss Window : 1.0918471054873118\n",
      "Step 5651 | Sliding Loss Window : 1.1522580235007627\n",
      "Step 5701 | Sliding Loss Window : 1.0491851209610321\n",
      "Step 5751 | Sliding Loss Window : 1.0559970102484242\n",
      "Step 5801 | Sliding Loss Window : 1.1187503764683437\n",
      "Step 5851 | Sliding Loss Window : 1.0386974642237496\n",
      "Step 5901 | Sliding Loss Window : 1.0891582246685028\n",
      "Step 5951 | Sliding Loss Window : 1.037407579190304\n",
      "Step 6001 | Sliding Loss Window : 1.2090791252735282\n",
      "Step 6051 | Sliding Loss Window : 1.1705474382655363\n",
      "Step 6101 | Sliding Loss Window : 0.9934517546843834\n",
      "Step 6151 | Sliding Loss Window : 0.9621905908067778\n",
      "Step 6201 | Sliding Loss Window : 1.0535109098001554\n",
      "Step 6251 | Sliding Loss Window : 0.8770947878737266\n",
      "Step 6301 | Sliding Loss Window : 1.0696157848420693\n",
      "Step 6351 | Sliding Loss Window : 0.9138743364434976\n",
      "Step 6401 | Sliding Loss Window : 0.9471933779589361\n",
      "Step 6451 | Sliding Loss Window : 1.0213491033173192\n",
      "Step 6501 | Sliding Loss Window : 0.947642825184396\n",
      "Step 6551 | Sliding Loss Window : 1.1596116809962438\n",
      "Step 6601 | Sliding Loss Window : 1.0766382146509699\n",
      "Step 6651 | Sliding Loss Window : 1.0237986146082005\n",
      "Step 6701 | Sliding Loss Window : 1.112262578197938\n",
      "Step 6751 | Sliding Loss Window : 1.131079390524604\n",
      "Step 6801 | Sliding Loss Window : 1.0855767861219052\n",
      "Step 6851 | Sliding Loss Window : 1.116419830664545\n",
      "Step 6901 | Sliding Loss Window : 0.9072817674615196\n",
      "Step 6951 | Sliding Loss Window : 1.1657996010286817\n",
      "Step 7001 | Sliding Loss Window : 0.9406280342235144\n",
      "Step 7051 | Sliding Loss Window : 1.1708797533946407\n",
      "Step 7101 | Sliding Loss Window : 1.2304393324187983\n",
      "Step 7151 | Sliding Loss Window : 0.9923783317842948\n",
      "Step 7201 | Sliding Loss Window : 1.1065811809337804\n",
      "Step 7251 | Sliding Loss Window : 1.0743807151987173\n",
      "Step 7301 | Sliding Loss Window : 1.0313029816938302\n",
      "Step 7351 | Sliding Loss Window : 1.065822642699736\n",
      "Step 7401 | Sliding Loss Window : 1.1219651826823462\n",
      "Step 7451 | Sliding Loss Window : 1.0399420797658097\n",
      "Step 7501 | Sliding Loss Window : 1.1933206608017928\n",
      "Step 7551 | Sliding Loss Window : 1.1695799099914992\n",
      "Step 7601 | Sliding Loss Window : 0.885473813728157\n",
      "Step 7651 | Sliding Loss Window : 1.0286156259504136\n",
      "Step 7701 | Sliding Loss Window : 1.0633067414964825\n",
      "Step 7751 | Sliding Loss Window : 0.9329942314381996\n",
      "Step 7801 | Sliding Loss Window : 1.0800759990961286\n",
      "Step 7851 | Sliding Loss Window : 1.14883237191155\n",
      "Step 7901 | Sliding Loss Window : 1.2150426373245324\n",
      "Step 7951 | Sliding Loss Window : 0.994310523337764\n",
      "Step 8001 | Sliding Loss Window : 1.0490848150656031\n",
      "Step 8051 | Sliding Loss Window : 1.1050864156105633\n",
      "Step 8101 | Sliding Loss Window : 1.0555236637539887\n",
      "Step 8151 | Sliding Loss Window : 0.9456925471549185\n",
      "Step 8201 | Sliding Loss Window : 1.1395614637936777\n",
      "Step 8251 | Sliding Loss Window : 1.0028612410629734\n",
      "Step 8301 | Sliding Loss Window : 1.078711523769504\n",
      "Step 8351 | Sliding Loss Window : 0.9782817498549785\n",
      "Step 8401 | Sliding Loss Window : 0.9576329175684417\n",
      "Step 8451 | Sliding Loss Window : 1.1037003857210317\n",
      "Step 8501 | Sliding Loss Window : 1.1669973284859418\n",
      "Step 8551 | Sliding Loss Window : 1.1744986141766207\n",
      "Step 8601 | Sliding Loss Window : 1.0523002556056205\n",
      "Step 8651 | Sliding Loss Window : 1.1168054903743518\n",
      "Step 8701 | Sliding Loss Window : 1.0317880572941385\n",
      "Step 8751 | Sliding Loss Window : 1.2161459811118467\n",
      "Step 8801 | Sliding Loss Window : 1.0676679146790062\n",
      "Step 8851 | Sliding Loss Window : 1.0745688594372182\n",
      "Step 8901 | Sliding Loss Window : 0.9897090113486341\n",
      "Step 8951 | Sliding Loss Window : 0.9802891509303897\n",
      "Step 9001 | Sliding Loss Window : 1.0152644549088807\n",
      "Step 9051 | Sliding Loss Window : 1.0739855853639078\n",
      "Step 9101 | Sliding Loss Window : 1.0017588337011103\n",
      "Step 9151 | Sliding Loss Window : 1.1154106787409825\n",
      "Step 9201 | Sliding Loss Window : 1.0606230143827982\n",
      "Step 9251 | Sliding Loss Window : 1.0444930780645592\n",
      "Step 9301 | Sliding Loss Window : 1.0525166282643572\n",
      "Step 9351 | Sliding Loss Window : 1.1660366675492293\n",
      "Step 9401 | Sliding Loss Window : 1.0772143723979575\n",
      "Step 9451 | Sliding Loss Window : 0.9726252126066988\n",
      "Step 9501 | Sliding Loss Window : 1.0852883110481228\n",
      "Step 9551 | Sliding Loss Window : 0.9547329538820345\n",
      "Step 9601 | Sliding Loss Window : 0.9969753337245127\n",
      "Step 9651 | Sliding Loss Window : 1.1428053850590587\n",
      "Step 9701 | Sliding Loss Window : 0.9762452064916409\n",
      "Step 9751 | Sliding Loss Window : 1.1471417885260808\n",
      "Step 9801 | Sliding Loss Window : 1.0184516467284235\n",
      "Step 9851 | Sliding Loss Window : 1.16217390082063\n",
      "Step 9901 | Sliding Loss Window : 1.082174007853106\n",
      "Step 9951 | Sliding Loss Window : 1.0467526308077033\n",
      "Step 10001 | Sliding Loss Window : 1.2183488343416504\n",
      "Step 10051 | Sliding Loss Window : 1.1527928185872658\n",
      "Step 10101 | Sliding Loss Window : 0.9854331153115937\n",
      "Step 10151 | Sliding Loss Window : 1.0491904869091198\n",
      "Step 10201 | Sliding Loss Window : 1.0263798679657579\n",
      "Step 10251 | Sliding Loss Window : 1.0761915333821408\n",
      "Step 10301 | Sliding Loss Window : 1.1145651747868115\n",
      "Step 10351 | Sliding Loss Window : 1.1201496837827059\n",
      "Step 10401 | Sliding Loss Window : 1.1332832784904934\n",
      "Step 10451 | Sliding Loss Window : 1.0242816538883066\n",
      "Step 10501 | Sliding Loss Window : 1.0434393921784288\n",
      "Step 10551 | Sliding Loss Window : 1.0751403461906934\n",
      "Step 10601 | Sliding Loss Window : 1.0299809789711\n",
      "Step 10651 | Sliding Loss Window : 1.1402164183437922\n",
      "Step 10701 | Sliding Loss Window : 1.2158406866565081\n",
      "Step 10751 | Sliding Loss Window : 1.1009806312462513\n",
      "Step 10801 | Sliding Loss Window : 1.1237306752055654\n",
      "Step 10851 | Sliding Loss Window : 1.1804925326603688\n",
      "Step 10901 | Sliding Loss Window : 1.176734759436248\n",
      "Step 10951 | Sliding Loss Window : 0.9871557392519692\n",
      "Step 11001 | Sliding Loss Window : 1.0201099085698444\n",
      "Step 11051 | Sliding Loss Window : 1.0767076299167078\n",
      "Step 11101 | Sliding Loss Window : 1.096022317828165\n",
      "Step 11151 | Sliding Loss Window : 1.1111936047993327\n",
      "Step 11201 | Sliding Loss Window : 0.9845211279424951\n",
      "Step 11251 | Sliding Loss Window : 1.2066573778262144\n",
      "Step 11301 | Sliding Loss Window : 1.1297593386902665\n",
      "Step 11351 | Sliding Loss Window : 1.1338454050073146\n",
      "Step 11401 | Sliding Loss Window : 1.0456196509850693\n",
      "Step 11451 | Sliding Loss Window : 1.0586771310052525\n",
      "Step 11501 | Sliding Loss Window : 1.0284374327818298\n",
      "Step 11551 | Sliding Loss Window : 1.0890494680706642\n",
      "Step 11601 | Sliding Loss Window : 0.9545568619133646\n",
      "Step 11651 | Sliding Loss Window : 0.9737296841867211\n",
      "Step 11701 | Sliding Loss Window : 1.0860189532747737\n",
      "Step 11751 | Sliding Loss Window : 0.9653311674480176\n",
      "Step 11801 | Sliding Loss Window : 1.0788940540087575\n",
      "Step 11851 | Sliding Loss Window : 0.8701034127086282\n",
      "Step 11901 | Sliding Loss Window : 1.1531211251735594\n",
      "Step 11951 | Sliding Loss Window : 1.0927157789269424\n",
      "Step 12001 | Sliding Loss Window : 0.9886627758942361\n",
      "Step 12051 | Sliding Loss Window : 1.1113524495915124\n",
      "Step 12101 | Sliding Loss Window : 1.1358026640311656\n",
      "Step 12151 | Sliding Loss Window : 1.1845674185880586\n",
      "Step 12201 | Sliding Loss Window : 0.9626154860016198\n",
      "Step 12251 | Sliding Loss Window : 0.939170178474219\n",
      "Step 12301 | Sliding Loss Window : 0.9481236514841536\n",
      "Step 12351 | Sliding Loss Window : 1.1871228346854579\n",
      "Step 12401 | Sliding Loss Window : 1.058157613386994\n",
      "Step 12451 | Sliding Loss Window : 1.0115088129624226\n",
      "Step 12501 | Sliding Loss Window : 1.0716813797007914\n",
      "Step 12551 | Sliding Loss Window : 1.0829580421101848\n",
      "Step 12601 | Sliding Loss Window : 1.2031473189996418\n",
      "Step 12651 | Sliding Loss Window : 1.1516081143183556\n",
      "Step 12701 | Sliding Loss Window : 1.2149976244605931\n",
      "Step 12751 | Sliding Loss Window : 1.0699529021581737\n",
      "Step 12801 | Sliding Loss Window : 1.0764174892632072\n",
      "Step 12851 | Sliding Loss Window : 1.0297080831652061\n",
      "Step 12901 | Sliding Loss Window : 0.9905647652741844\n",
      "Step 12951 | Sliding Loss Window : 1.0360402719115562\n",
      "Step 13001 | Sliding Loss Window : 1.0912812958923184\n",
      "Step 13051 | Sliding Loss Window : 1.0443179890698828\n",
      "Step 13101 | Sliding Loss Window : 0.977197782052138\n",
      "Step 13151 | Sliding Loss Window : 1.054673660973308\n",
      "Step 13201 | Sliding Loss Window : 1.040090888658497\n",
      "Step 13251 | Sliding Loss Window : 1.013315336363075\n",
      "Step 13301 | Sliding Loss Window : 1.0485084777419633\n",
      "Step 13351 | Sliding Loss Window : 1.0483745674653333\n",
      "Step 13401 | Sliding Loss Window : 0.9179581801794514\n",
      "Step 13451 | Sliding Loss Window : 0.9976327126738344\n",
      "Step 13501 | Sliding Loss Window : 1.2130777239537858\n",
      "Step 13551 | Sliding Loss Window : 1.27139269638403\n",
      "Step 13601 | Sliding Loss Window : 1.091390750349087\n",
      "Step 13651 | Sliding Loss Window : 1.152328408119212\n",
      "Step 13701 | Sliding Loss Window : 1.0494146801627318\n",
      "Step 13751 | Sliding Loss Window : 1.0566485600080346\n",
      "Step 13801 | Sliding Loss Window : 1.1197713715849045\n",
      "Step 13851 | Sliding Loss Window : 1.0378763133085571\n",
      "Step 13901 | Sliding Loss Window : 1.0878111992018134\n",
      "Step 13951 | Sliding Loss Window : 1.038574893930454\n",
      "Step 14001 | Sliding Loss Window : 1.2096411986009037\n",
      "Step 14051 | Sliding Loss Window : 1.1691690276226863\n",
      "Step 14101 | Sliding Loss Window : 0.993459990409221\n",
      "Step 14151 | Sliding Loss Window : 0.9631955913162759\n",
      "Step 14201 | Sliding Loss Window : 1.0538754494453364\n",
      "Step 14251 | Sliding Loss Window : 0.8771058865038128\n",
      "Step 14301 | Sliding Loss Window : 1.0688255404940727\n",
      "Step 14351 | Sliding Loss Window : 0.9129968415520218\n",
      "Step 14401 | Sliding Loss Window : 0.9471011735564444\n",
      "Step 14451 | Sliding Loss Window : 1.019232558584414\n",
      "Step 14501 | Sliding Loss Window : 0.9474539223972761\n",
      "Step 14551 | Sliding Loss Window : 1.1575745645320104\n",
      "Step 14601 | Sliding Loss Window : 1.0766792394898181\n",
      "Step 14651 | Sliding Loss Window : 1.0235630142434935\n",
      "Step 14701 | Sliding Loss Window : 1.1127845474786116\n",
      "Step 14751 | Sliding Loss Window : 1.131163393006645\n",
      "Step 14801 | Sliding Loss Window : 1.085297709382265\n",
      "Step 14851 | Sliding Loss Window : 1.1185324071474876\n",
      "Step 14901 | Sliding Loss Window : 0.9070101857619954\n",
      "Step 14951 | Sliding Loss Window : 1.1644572021012527\n",
      "Step 15001 | Sliding Loss Window : 0.9418260194183274\n",
      "Step 15051 | Sliding Loss Window : 1.1676849177444724\n",
      "Step 15101 | Sliding Loss Window : 1.2303566478425458\n",
      "Step 15151 | Sliding Loss Window : 0.9914355500358698\n",
      "Step 15201 | Sliding Loss Window : 1.1057150572190382\n",
      "Step 15251 | Sliding Loss Window : 1.0763176308654545\n",
      "Step 15301 | Sliding Loss Window : 1.0317428030355034\n",
      "Step 15351 | Sliding Loss Window : 1.0670163234522956\n",
      "Step 15401 | Sliding Loss Window : 1.1204746333569942\n",
      "Step 15451 | Sliding Loss Window : 1.0399445121576767\n",
      "Step 15501 | Sliding Loss Window : 1.1950775311907191\n",
      "Step 15551 | Sliding Loss Window : 1.168902688559742\n",
      "Step 15601 | Sliding Loss Window : 0.8854263808689453\n",
      "Step 15651 | Sliding Loss Window : 1.0272686682390806\n",
      "Step 15701 | Sliding Loss Window : 1.061428836009722\n",
      "Step 15751 | Sliding Loss Window : 0.9332602877691571\n",
      "Step 15801 | Sliding Loss Window : 1.0797270156434056\n",
      "Step 15851 | Sliding Loss Window : 1.1483073786350018\n",
      "Step 15901 | Sliding Loss Window : 1.2188480557234183\n",
      "Step 15951 | Sliding Loss Window : 0.9936115168382501\n",
      "Step 1 | Sliding Loss Window : 1.389124037308346\n",
      "Step 51 | Sliding Loss Window : 1.4139333769207656\n",
      "Step 101 | Sliding Loss Window : 1.0937124901131399\n",
      "Step 151 | Sliding Loss Window : 1.2665692413651786\n",
      "Step 201 | Sliding Loss Window : 1.2856617270214228\n",
      "Step 251 | Sliding Loss Window : 1.324126175867976\n",
      "Step 301 | Sliding Loss Window : 1.171769090249549\n",
      "Step 351 | Sliding Loss Window : 1.192720035918153\n",
      "Step 401 | Sliding Loss Window : 1.1248842406040351\n",
      "Step 451 | Sliding Loss Window : 1.0526789158648224\n",
      "Step 501 | Sliding Loss Window : 1.0964139704230547\n",
      "Step 551 | Sliding Loss Window : 0.9035513255555778\n",
      "Step 601 | Sliding Loss Window : 1.1691164223100625\n",
      "Step 651 | Sliding Loss Window : 1.02340832626681\n",
      "Step 701 | Sliding Loss Window : 0.9175277452855098\n",
      "Step 751 | Sliding Loss Window : 1.0806936810363155\n",
      "Step 801 | Sliding Loss Window : 1.111527376082555\n",
      "Step 851 | Sliding Loss Window : 1.2364159740989933\n",
      "Step 901 | Sliding Loss Window : 1.0665490274115792\n",
      "Step 951 | Sliding Loss Window : 1.0693331627188\n",
      "Step 1001 | Sliding Loss Window : 1.1679208903251956\n",
      "Step 1051 | Sliding Loss Window : 1.190537823507939\n",
      "Step 1101 | Sliding Loss Window : 1.015488928041506\n",
      "Step 1151 | Sliding Loss Window : 0.9523960056921728\n",
      "Step 1201 | Sliding Loss Window : 1.139178432636291\n",
      "Step 1251 | Sliding Loss Window : 1.1146763899906187\n",
      "Step 1301 | Sliding Loss Window : 1.1915985979715786\n",
      "Step 1351 | Sliding Loss Window : 1.0842159055175196\n",
      "Step 1401 | Sliding Loss Window : 0.9996099366376225\n",
      "Step 1451 | Sliding Loss Window : 1.0358076925102249\n",
      "Step 1501 | Sliding Loss Window : 1.1098777046365271\n",
      "Step 1551 | Sliding Loss Window : 1.171512358472493\n",
      "Step 1601 | Sliding Loss Window : 1.0666659724319565\n",
      "Step 1651 | Sliding Loss Window : 1.0321198341098035\n",
      "Step 1701 | Sliding Loss Window : 1.0426411619634006\n",
      "Step 1751 | Sliding Loss Window : 1.091143051887617\n",
      "Step 1801 | Sliding Loss Window : 1.2113927351452873\n",
      "Step 1851 | Sliding Loss Window : 1.2262864832907472\n",
      "Step 1901 | Sliding Loss Window : 0.9393552232033328\n",
      "Step 1951 | Sliding Loss Window : 1.0511941244414063\n",
      "Step 2001 | Sliding Loss Window : 0.996979660445924\n",
      "Step 2051 | Sliding Loss Window : 1.1330090456816613\n",
      "Step 2101 | Sliding Loss Window : 0.994952821246653\n",
      "Step 2151 | Sliding Loss Window : 1.2498213346484908\n",
      "Step 2201 | Sliding Loss Window : 1.134498190110574\n",
      "Step 2251 | Sliding Loss Window : 1.0289336277284757\n",
      "Step 2301 | Sliding Loss Window : 1.0471982536082993\n",
      "Step 2351 | Sliding Loss Window : 1.1352567086477157\n",
      "Step 2401 | Sliding Loss Window : 1.0669211093677162\n",
      "Step 2451 | Sliding Loss Window : 1.2044820196421326\n",
      "Step 2501 | Sliding Loss Window : 1.0373388016022789\n",
      "Step 2551 | Sliding Loss Window : 1.0617108237448678\n",
      "Step 2601 | Sliding Loss Window : 1.0433372240461949\n",
      "Step 2651 | Sliding Loss Window : 1.2398397177619074\n",
      "Step 2701 | Sliding Loss Window : 1.0646543616918618\n",
      "Step 2751 | Sliding Loss Window : 1.1173242146014652\n",
      "Step 2801 | Sliding Loss Window : 1.051505785821106\n",
      "Step 2851 | Sliding Loss Window : 1.0791020955847552\n",
      "Step 2901 | Sliding Loss Window : 1.0717620107927643\n",
      "Step 2951 | Sliding Loss Window : 1.1864374610208102\n",
      "Step 3001 | Sliding Loss Window : 1.095197459745029\n",
      "Step 3051 | Sliding Loss Window : 1.0147331800344934\n",
      "Step 3101 | Sliding Loss Window : 1.0542906616190992\n",
      "Step 3151 | Sliding Loss Window : 1.0087299792803506\n",
      "Step 3201 | Sliding Loss Window : 1.1759531448260538\n",
      "Step 3251 | Sliding Loss Window : 1.0728943126827217\n",
      "Step 3301 | Sliding Loss Window : 1.26938114608583\n",
      "Step 3351 | Sliding Loss Window : 1.0990217585878528\n",
      "Step 3401 | Sliding Loss Window : 1.1061560646921917\n",
      "Step 3451 | Sliding Loss Window : 1.013514812528632\n",
      "Step 3501 | Sliding Loss Window : 0.9711234299399376\n",
      "Step 3551 | Sliding Loss Window : 1.2852209857644552\n",
      "Step 3601 | Sliding Loss Window : 1.1009011559357902\n",
      "Step 3651 | Sliding Loss Window : 1.144206335851407\n",
      "Step 3701 | Sliding Loss Window : 1.0291045661561673\n",
      "Step 3751 | Sliding Loss Window : 1.1104243158881486\n",
      "Step 3801 | Sliding Loss Window : 0.9493253728584233\n",
      "Step 3851 | Sliding Loss Window : 1.0641475595977403\n",
      "Step 3901 | Sliding Loss Window : 1.077717306524137\n",
      "Step 3951 | Sliding Loss Window : 1.2207654124680227\n",
      "Step 4001 | Sliding Loss Window : 0.99382124998073\n",
      "Step 4051 | Sliding Loss Window : 1.061920343668516\n",
      "Step 4101 | Sliding Loss Window : 1.1232940915729523\n",
      "Step 4151 | Sliding Loss Window : 0.8904175732433481\n",
      "Step 4201 | Sliding Loss Window : 1.1741749834594444\n",
      "Step 4251 | Sliding Loss Window : 1.0261378874686673\n",
      "Step 4301 | Sliding Loss Window : 1.1814045686515093\n",
      "Step 4351 | Sliding Loss Window : 1.0073601728063812\n",
      "Step 4401 | Sliding Loss Window : 1.0461596885581437\n",
      "Step 4451 | Sliding Loss Window : 1.1670600030447098\n",
      "Step 4501 | Sliding Loss Window : 1.0057654094153827\n",
      "Step 4551 | Sliding Loss Window : 1.135084811740497\n",
      "Step 4601 | Sliding Loss Window : 1.0968587714229778\n",
      "Step 4651 | Sliding Loss Window : 0.9381499573866344\n",
      "Step 4701 | Sliding Loss Window : 1.021840236511968\n",
      "Step 4751 | Sliding Loss Window : 1.0346366914068101\n",
      "Step 4801 | Sliding Loss Window : 1.184288432859673\n",
      "Step 4851 | Sliding Loss Window : 1.1149268599879671\n",
      "Step 4901 | Sliding Loss Window : 1.102096709519974\n",
      "Step 4951 | Sliding Loss Window : 1.1568449475895453\n",
      "Step 5001 | Sliding Loss Window : 1.1297654343292312\n",
      "Step 5051 | Sliding Loss Window : 1.003134963446256\n",
      "Step 5101 | Sliding Loss Window : 0.9519506036278692\n",
      "Step 5151 | Sliding Loss Window : 1.2157354463190593\n",
      "Step 5201 | Sliding Loss Window : 1.1310212776714719\n",
      "Step 5251 | Sliding Loss Window : 1.1030392098652768\n",
      "Step 5301 | Sliding Loss Window : 1.1752676587592221\n",
      "Step 5351 | Sliding Loss Window : 1.1906649880447573\n",
      "Step 5401 | Sliding Loss Window : 0.9458104971979263\n",
      "Step 5451 | Sliding Loss Window : 1.1573621388952453\n",
      "Step 5501 | Sliding Loss Window : 1.1713455964245265\n",
      "Step 5551 | Sliding Loss Window : 1.1166751287992898\n",
      "Step 5601 | Sliding Loss Window : 1.0106138220722212\n",
      "Step 5651 | Sliding Loss Window : 1.1120614170374423\n",
      "Step 5701 | Sliding Loss Window : 1.171846967222689\n",
      "Step 5751 | Sliding Loss Window : 1.2609798568161126\n",
      "Step 5801 | Sliding Loss Window : 1.0524017990859083\n",
      "Step 5851 | Sliding Loss Window : 1.1341968401987592\n",
      "Step 5901 | Sliding Loss Window : 1.0285448674963202\n",
      "Step 5951 | Sliding Loss Window : 1.092452844099417\n",
      "Step 6001 | Sliding Loss Window : 1.2812755863489607\n",
      "Step 6051 | Sliding Loss Window : 1.082365070057301\n",
      "Step 6101 | Sliding Loss Window : 1.1123437510760603\n",
      "Step 6151 | Sliding Loss Window : 1.0577027969898705\n",
      "Step 6201 | Sliding Loss Window : 1.0983736540790205\n",
      "Step 6251 | Sliding Loss Window : 1.2184311307570486\n",
      "Step 6301 | Sliding Loss Window : 1.2305214863370453\n",
      "Step 6351 | Sliding Loss Window : 1.0999467797333686\n",
      "Step 6401 | Sliding Loss Window : 1.038601807077035\n",
      "Step 6451 | Sliding Loss Window : 0.9193701359798097\n",
      "Step 6501 | Sliding Loss Window : 1.1818976018845564\n",
      "Step 6551 | Sliding Loss Window : 1.1003807204214366\n",
      "Step 6601 | Sliding Loss Window : 0.9360525793757398\n",
      "Step 6651 | Sliding Loss Window : 1.2937566113995198\n",
      "Step 6701 | Sliding Loss Window : 0.9941083903163438\n",
      "Step 6751 | Sliding Loss Window : 1.109915871425275\n",
      "Step 6801 | Sliding Loss Window : 0.9767449942901629\n",
      "Step 6851 | Sliding Loss Window : 1.043982176097101\n",
      "Step 6901 | Sliding Loss Window : 1.069767961879426\n",
      "Step 6951 | Sliding Loss Window : 1.033750049008557\n",
      "Step 7001 | Sliding Loss Window : 1.156406217103533\n",
      "Step 7051 | Sliding Loss Window : 1.0665590680282984\n",
      "Step 7101 | Sliding Loss Window : 0.9913989394485493\n",
      "Step 7151 | Sliding Loss Window : 1.0754055732273056\n",
      "Step 7201 | Sliding Loss Window : 1.099943301318991\n",
      "Step 7251 | Sliding Loss Window : 1.1412941335478624\n",
      "Step 7301 | Sliding Loss Window : 0.920061257192723\n",
      "Step 7351 | Sliding Loss Window : 0.9788208330916659\n",
      "Step 7401 | Sliding Loss Window : 1.19998939435374\n",
      "Step 7451 | Sliding Loss Window : 1.0912023479859085\n",
      "Step 7501 | Sliding Loss Window : 1.2400262125155062\n",
      "Step 7551 | Sliding Loss Window : 1.1093666896822656\n",
      "Step 7601 | Sliding Loss Window : 1.1252656453229155\n",
      "Step 7651 | Sliding Loss Window : 1.07678941370135\n",
      "Step 7701 | Sliding Loss Window : 1.1933100811615773\n",
      "Step 7751 | Sliding Loss Window : 0.9477522907807807\n",
      "Step 7801 | Sliding Loss Window : 1.093430821264258\n",
      "Step 7851 | Sliding Loss Window : 1.1049136698671858\n",
      "Step 7901 | Sliding Loss Window : 1.0834788938993605\n",
      "Step 7951 | Sliding Loss Window : 1.071946504823188\n",
      "Step 8001 | Sliding Loss Window : 0.9292668875010639\n",
      "Step 8051 | Sliding Loss Window : 1.1273838017766296\n",
      "Step 8101 | Sliding Loss Window : 0.922517355259896\n",
      "Step 8151 | Sliding Loss Window : 1.16182801836626\n",
      "Step 8201 | Sliding Loss Window : 1.1016595605543404\n",
      "Step 8251 | Sliding Loss Window : 1.2886360300620185\n",
      "Step 8301 | Sliding Loss Window : 1.1360125405771746\n",
      "Step 8351 | Sliding Loss Window : 1.1558021756503327\n",
      "Step 8401 | Sliding Loss Window : 1.1065282048934935\n",
      "Step 8451 | Sliding Loss Window : 0.9867865799356892\n",
      "Step 8501 | Sliding Loss Window : 1.0788508466997977\n",
      "Step 8551 | Sliding Loss Window : 0.9046737438676449\n",
      "Step 8601 | Sliding Loss Window : 1.1629193744339181\n",
      "Step 8651 | Sliding Loss Window : 1.0175774157177795\n",
      "Step 8701 | Sliding Loss Window : 0.8955205996126476\n",
      "Step 8751 | Sliding Loss Window : 1.0638817519606958\n",
      "Step 8801 | Sliding Loss Window : 1.1198760002093742\n",
      "Step 8851 | Sliding Loss Window : 1.2366340782404412\n",
      "Step 8901 | Sliding Loss Window : 1.0559232609505937\n",
      "Step 8951 | Sliding Loss Window : 1.0641134359023523\n",
      "Step 9001 | Sliding Loss Window : 1.1639091034473419\n",
      "Step 9051 | Sliding Loss Window : 1.1893646228302632\n",
      "Step 9101 | Sliding Loss Window : 1.0152060434185435\n",
      "Step 9151 | Sliding Loss Window : 0.9456241656325295\n",
      "Step 9201 | Sliding Loss Window : 1.1360953831101028\n",
      "Step 9251 | Sliding Loss Window : 1.1090543930494532\n",
      "Step 9301 | Sliding Loss Window : 1.1966377853539958\n",
      "Step 9351 | Sliding Loss Window : 1.0840147036451067\n",
      "Step 9401 | Sliding Loss Window : 0.9963659116433554\n",
      "Step 9451 | Sliding Loss Window : 1.0424642597404898\n",
      "Step 9501 | Sliding Loss Window : 1.102180373886513\n",
      "Step 9551 | Sliding Loss Window : 1.1617660007170747\n",
      "Step 9601 | Sliding Loss Window : 1.0521815186342225\n",
      "Step 9651 | Sliding Loss Window : 1.03793469311247\n",
      "Step 9701 | Sliding Loss Window : 1.03954797550593\n",
      "Step 9751 | Sliding Loss Window : 1.0835609326561673\n",
      "Step 9801 | Sliding Loss Window : 1.2102582487820375\n",
      "Step 9851 | Sliding Loss Window : 1.222757337317152\n",
      "Step 9901 | Sliding Loss Window : 0.9253575464751974\n",
      "Step 9951 | Sliding Loss Window : 1.0502852946349988\n",
      "Step 10001 | Sliding Loss Window : 0.9964589488913146\n",
      "Step 10051 | Sliding Loss Window : 1.1345848998284624\n",
      "Step 10101 | Sliding Loss Window : 1.000784415040185\n",
      "Step 10151 | Sliding Loss Window : 1.2413611185811835\n",
      "Step 10201 | Sliding Loss Window : 1.1337895102710032\n",
      "Step 10251 | Sliding Loss Window : 1.0270886064896292\n",
      "Step 10301 | Sliding Loss Window : 1.0390509504416832\n",
      "Step 10351 | Sliding Loss Window : 1.1387292053285865\n",
      "Step 10401 | Sliding Loss Window : 1.065300473600518\n",
      "Step 10451 | Sliding Loss Window : 1.202005454226088\n",
      "Step 10501 | Sliding Loss Window : 1.0403687974940699\n",
      "Step 10551 | Sliding Loss Window : 1.0529417485132873\n",
      "Step 10601 | Sliding Loss Window : 1.0418329400457933\n",
      "Step 10651 | Sliding Loss Window : 1.241897544592507\n",
      "Step 10701 | Sliding Loss Window : 1.0583575388959339\n",
      "Step 10751 | Sliding Loss Window : 1.1107602603311142\n",
      "Step 10801 | Sliding Loss Window : 1.0465401635640958\n",
      "Step 10851 | Sliding Loss Window : 1.0867647489730303\n",
      "Step 10901 | Sliding Loss Window : 1.0651712430621252\n",
      "Step 10951 | Sliding Loss Window : 1.1746290401064021\n",
      "Step 11001 | Sliding Loss Window : 1.0937824467524546\n",
      "Step 11051 | Sliding Loss Window : 1.0101760818848722\n",
      "Step 11101 | Sliding Loss Window : 1.059192914200107\n",
      "Step 11151 | Sliding Loss Window : 1.0117173047428978\n",
      "Step 11201 | Sliding Loss Window : 1.1666937434017937\n",
      "Step 11251 | Sliding Loss Window : 1.0611270126537293\n",
      "Step 11301 | Sliding Loss Window : 1.2643209744865522\n",
      "Step 11351 | Sliding Loss Window : 1.0970986072226179\n",
      "Step 11401 | Sliding Loss Window : 1.1016496541198786\n",
      "Step 11451 | Sliding Loss Window : 1.0016965998089553\n",
      "Step 11501 | Sliding Loss Window : 0.9750574448815001\n",
      "Step 11551 | Sliding Loss Window : 1.2934544527403136\n",
      "Step 11601 | Sliding Loss Window : 1.1049433766443897\n",
      "Step 11651 | Sliding Loss Window : 1.1423897314468494\n",
      "Step 11701 | Sliding Loss Window : 1.024633314207535\n",
      "Step 11751 | Sliding Loss Window : 1.1079835675316319\n",
      "Step 11801 | Sliding Loss Window : 0.9519495721063739\n",
      "Step 11851 | Sliding Loss Window : 1.06147186168867\n",
      "Step 11901 | Sliding Loss Window : 1.0686878390779104\n",
      "Step 11951 | Sliding Loss Window : 1.2248145047060317\n",
      "Step 12001 | Sliding Loss Window : 0.9958578753792078\n",
      "Step 12051 | Sliding Loss Window : 1.0593175862192319\n",
      "Step 12101 | Sliding Loss Window : 1.118841812010068\n",
      "Step 12151 | Sliding Loss Window : 0.8949414896139032\n",
      "Step 12201 | Sliding Loss Window : 1.1716673065345429\n",
      "Step 12251 | Sliding Loss Window : 1.0303679195927358\n",
      "Step 12301 | Sliding Loss Window : 1.169372584423352\n",
      "Step 12351 | Sliding Loss Window : 0.9975280905369242\n",
      "Step 12401 | Sliding Loss Window : 1.037979578518619\n",
      "Step 12451 | Sliding Loss Window : 1.173108306775883\n",
      "Step 12501 | Sliding Loss Window : 1.001755616538568\n",
      "Step 12551 | Sliding Loss Window : 1.1366467104791336\n",
      "Step 12601 | Sliding Loss Window : 1.0995725921082757\n",
      "Step 12651 | Sliding Loss Window : 0.9333294038719202\n",
      "Step 12701 | Sliding Loss Window : 1.0208558539729562\n",
      "Step 12751 | Sliding Loss Window : 1.0336516038289096\n",
      "Step 12801 | Sliding Loss Window : 1.1830631739492736\n",
      "Step 12851 | Sliding Loss Window : 1.1139060958648936\n",
      "Step 12901 | Sliding Loss Window : 1.0992262235573231\n",
      "Step 12951 | Sliding Loss Window : 1.1489774810040156\n",
      "Step 13001 | Sliding Loss Window : 1.1322283982016481\n",
      "Step 13051 | Sliding Loss Window : 0.9958701621181195\n",
      "Step 13101 | Sliding Loss Window : 0.9442952344699966\n",
      "Step 13151 | Sliding Loss Window : 1.214143206681929\n",
      "Step 13201 | Sliding Loss Window : 1.1290681334315003\n",
      "Step 13251 | Sliding Loss Window : 1.1038053607603338\n",
      "Step 13301 | Sliding Loss Window : 1.1718107457546432\n",
      "Step 13351 | Sliding Loss Window : 1.1919263327225404\n",
      "Step 13401 | Sliding Loss Window : 0.9429630767304756\n",
      "Step 13451 | Sliding Loss Window : 1.161207772305767\n",
      "Step 13501 | Sliding Loss Window : 1.1713186549391943\n",
      "Step 13551 | Sliding Loss Window : 1.1061913373855479\n",
      "Step 13601 | Sliding Loss Window : 1.0135676766625736\n",
      "Step 13651 | Sliding Loss Window : 1.1118262464655797\n",
      "Step 13701 | Sliding Loss Window : 1.1729551723176872\n",
      "Step 13751 | Sliding Loss Window : 1.2630938292725826\n",
      "Step 13801 | Sliding Loss Window : 1.0518163741971431\n",
      "Step 13851 | Sliding Loss Window : 1.1329529300122847\n",
      "Step 13901 | Sliding Loss Window : 1.0217653560895772\n",
      "Step 13951 | Sliding Loss Window : 1.0908600642930282\n",
      "Step 14001 | Sliding Loss Window : 1.2790697951130903\n",
      "Step 14051 | Sliding Loss Window : 1.0828345543084452\n",
      "Step 14101 | Sliding Loss Window : 1.1149493080791288\n",
      "Step 14151 | Sliding Loss Window : 1.0613237396428454\n",
      "Step 14201 | Sliding Loss Window : 1.092744550016916\n",
      "Step 14251 | Sliding Loss Window : 1.2218440271550919\n",
      "Step 14301 | Sliding Loss Window : 1.2333537255255635\n",
      "Step 14351 | Sliding Loss Window : 1.0956728956673736\n",
      "Step 14401 | Sliding Loss Window : 1.0324085581640496\n",
      "Step 14451 | Sliding Loss Window : 0.9150153476159569\n",
      "Step 14501 | Sliding Loss Window : 1.1853976051795891\n",
      "Step 14551 | Sliding Loss Window : 1.0980898735729077\n",
      "Step 14601 | Sliding Loss Window : 0.9292454057047972\n",
      "Step 14651 | Sliding Loss Window : 1.2823588690644436\n",
      "Step 14701 | Sliding Loss Window : 0.9934883460875028\n",
      "Step 14751 | Sliding Loss Window : 1.0989266297821134\n",
      "Step 14801 | Sliding Loss Window : 0.9765961825401849\n",
      "Step 14851 | Sliding Loss Window : 1.0378779675033918\n",
      "Step 14901 | Sliding Loss Window : 1.0676075274889352\n",
      "Step 14951 | Sliding Loss Window : 1.0394983345346842\n",
      "Step 15001 | Sliding Loss Window : 1.1541298964393396\n",
      "Step 15051 | Sliding Loss Window : 1.068658493612665\n",
      "Step 15101 | Sliding Loss Window : 0.9968320404876588\n",
      "Step 15151 | Sliding Loss Window : 1.0556750949239302\n",
      "Step 15201 | Sliding Loss Window : 1.0924179569568755\n",
      "Step 15251 | Sliding Loss Window : 1.1379471953037783\n",
      "Step 15301 | Sliding Loss Window : 0.9165694075468916\n",
      "Step 15351 | Sliding Loss Window : 0.980151264542982\n",
      "Step 15401 | Sliding Loss Window : 1.1998820045621605\n",
      "Step 15451 | Sliding Loss Window : 1.0949969033007223\n",
      "Step 15501 | Sliding Loss Window : 1.2357186038383576\n",
      "Step 15551 | Sliding Loss Window : 1.1145585153533697\n",
      "Step 1 | Sliding Loss Window : 1.1224355417684562\n",
      "Step 51 | Sliding Loss Window : 1.3829489207961578\n",
      "Step 101 | Sliding Loss Window : 1.2053068925721646\n",
      "Step 151 | Sliding Loss Window : 1.1529878488981309\n",
      "Step 201 | Sliding Loss Window : 1.0018255154169804\n",
      "Step 251 | Sliding Loss Window : 1.1172226355540793\n",
      "Step 301 | Sliding Loss Window : 1.0752164188719855\n",
      "Step 351 | Sliding Loss Window : 1.1818295058073884\n",
      "Step 401 | Sliding Loss Window : 1.1392238493876579\n",
      "Step 451 | Sliding Loss Window : 1.0894660658096542\n",
      "Step 501 | Sliding Loss Window : 1.0718783541936903\n",
      "Step 551 | Sliding Loss Window : 1.1543171899119617\n",
      "Step 601 | Sliding Loss Window : 1.1614576521174664\n",
      "Step 651 | Sliding Loss Window : 1.0711523932901674\n",
      "Step 701 | Sliding Loss Window : 1.1042573652294465\n",
      "Step 751 | Sliding Loss Window : 1.0896476773142325\n",
      "Step 801 | Sliding Loss Window : 0.988631215698094\n",
      "Step 851 | Sliding Loss Window : 1.1644963624209275\n",
      "Step 901 | Sliding Loss Window : 1.171148727349094\n",
      "Step 951 | Sliding Loss Window : 1.0918681540972892\n",
      "Step 1001 | Sliding Loss Window : 1.0833271579614077\n",
      "Step 1051 | Sliding Loss Window : 1.1164990965814214\n",
      "Step 1101 | Sliding Loss Window : 0.9896105938748911\n",
      "Step 1151 | Sliding Loss Window : 1.137344356541511\n",
      "Step 1201 | Sliding Loss Window : 1.2430473627302807\n",
      "Step 1251 | Sliding Loss Window : 1.1288584351938853\n",
      "Step 1301 | Sliding Loss Window : 1.15936937718847\n",
      "Step 1351 | Sliding Loss Window : 1.054664845346551\n",
      "Step 1401 | Sliding Loss Window : 0.9685830832690973\n",
      "Step 1451 | Sliding Loss Window : 1.1118273866756017\n",
      "Step 1501 | Sliding Loss Window : 1.0595595283848975\n",
      "Step 1551 | Sliding Loss Window : 1.2999597336812783\n",
      "Step 1601 | Sliding Loss Window : 1.1037089235679511\n",
      "Step 1651 | Sliding Loss Window : 1.0136548807707755\n",
      "Step 1701 | Sliding Loss Window : 1.0801122944846697\n",
      "Step 1751 | Sliding Loss Window : 1.1659239868147935\n",
      "Step 1801 | Sliding Loss Window : 1.0657856672531785\n",
      "Step 1851 | Sliding Loss Window : 1.0582833192412868\n",
      "Step 1901 | Sliding Loss Window : 1.2030031021422978\n",
      "Step 1951 | Sliding Loss Window : 1.1548542830447617\n",
      "Step 2001 | Sliding Loss Window : 1.1969344879692256\n",
      "Step 2051 | Sliding Loss Window : 1.1776376035830753\n",
      "Step 2101 | Sliding Loss Window : 1.1619254276228035\n",
      "Step 2151 | Sliding Loss Window : 1.134555640819996\n",
      "Step 2201 | Sliding Loss Window : 1.1153649294155386\n",
      "Step 2251 | Sliding Loss Window : 1.128020967629191\n",
      "Step 2301 | Sliding Loss Window : 1.044330417332238\n",
      "Step 2351 | Sliding Loss Window : 1.2305769841054723\n",
      "Step 2401 | Sliding Loss Window : 1.0954220962813386\n",
      "Step 2451 | Sliding Loss Window : 1.2040639582454318\n",
      "Step 2501 | Sliding Loss Window : 1.1293775303077975\n",
      "Step 2551 | Sliding Loss Window : 1.121487081871492\n",
      "Step 2601 | Sliding Loss Window : 1.0344363215118622\n",
      "Step 2651 | Sliding Loss Window : 1.0820167774444398\n",
      "Step 2701 | Sliding Loss Window : 1.0292222956055266\n",
      "Step 2751 | Sliding Loss Window : 1.055222405411662\n",
      "Step 2801 | Sliding Loss Window : 1.1166814984305042\n",
      "Step 2851 | Sliding Loss Window : 1.1077588712485036\n",
      "Step 2901 | Sliding Loss Window : 1.1279070626164682\n",
      "Step 2951 | Sliding Loss Window : 0.8911359741952309\n",
      "Step 3001 | Sliding Loss Window : 1.0325984343767454\n",
      "Step 3051 | Sliding Loss Window : 1.1898722033859155\n",
      "Step 3101 | Sliding Loss Window : 1.0827969319003383\n",
      "Step 3151 | Sliding Loss Window : 1.0492886169416735\n",
      "Step 3201 | Sliding Loss Window : 1.2055654646871095\n",
      "Step 3251 | Sliding Loss Window : 1.2569683655355077\n",
      "Step 3301 | Sliding Loss Window : 1.0760185813940593\n",
      "Step 3351 | Sliding Loss Window : 1.1183990839067155\n",
      "Step 3401 | Sliding Loss Window : 1.207611033053957\n",
      "Step 3451 | Sliding Loss Window : 1.2792707075284921\n",
      "Step 3501 | Sliding Loss Window : 1.3304953465561302\n",
      "Step 3551 | Sliding Loss Window : 1.1639982649545189\n",
      "Step 3601 | Sliding Loss Window : 1.1387128460646847\n",
      "Step 3651 | Sliding Loss Window : 1.263831944660505\n",
      "Step 3701 | Sliding Loss Window : 1.0689377846670416\n",
      "Step 3751 | Sliding Loss Window : 1.2793756473013027\n",
      "Step 3801 | Sliding Loss Window : 0.918732641420462\n",
      "Step 3851 | Sliding Loss Window : 1.078356057444091\n",
      "Step 3901 | Sliding Loss Window : 1.00631372389637\n",
      "Step 3951 | Sliding Loss Window : 1.0947940883649907\n",
      "Step 4001 | Sliding Loss Window : 1.062724457378502\n",
      "Step 4051 | Sliding Loss Window : 1.1311763072887786\n",
      "Step 4101 | Sliding Loss Window : 0.9845757274046504\n",
      "Step 4151 | Sliding Loss Window : 1.147174555310838\n",
      "Step 4201 | Sliding Loss Window : 1.0458643303213955\n",
      "Step 4251 | Sliding Loss Window : 1.0611874074281515\n",
      "Step 4301 | Sliding Loss Window : 1.0598562502044644\n",
      "Step 4351 | Sliding Loss Window : 0.968773677769361\n",
      "Step 4401 | Sliding Loss Window : 1.1049281352760476\n",
      "Step 4451 | Sliding Loss Window : 1.0403662217241036\n",
      "Step 4501 | Sliding Loss Window : 1.0987931704252258\n",
      "Step 4551 | Sliding Loss Window : 1.0677886030398827\n",
      "Step 4601 | Sliding Loss Window : 1.1724624933816574\n",
      "Step 4651 | Sliding Loss Window : 1.0127983271353984\n",
      "Step 4701 | Sliding Loss Window : 1.238653679230839\n",
      "Step 4751 | Sliding Loss Window : 1.1715516332797822\n",
      "Step 4801 | Sliding Loss Window : 1.239005713082614\n",
      "Step 4851 | Sliding Loss Window : 1.0236898935377459\n",
      "Step 4901 | Sliding Loss Window : 1.037015226040126\n",
      "Step 4951 | Sliding Loss Window : 1.1600408459451608\n",
      "Step 5001 | Sliding Loss Window : 1.079439073186975\n",
      "Step 5051 | Sliding Loss Window : 1.1275731903522193\n",
      "Step 5101 | Sliding Loss Window : 1.0526170465929885\n",
      "Step 5151 | Sliding Loss Window : 1.0175620905443221\n",
      "Step 5201 | Sliding Loss Window : 1.1212040024141414\n",
      "Step 5251 | Sliding Loss Window : 0.9765072380225537\n",
      "Step 5301 | Sliding Loss Window : 0.9880563506667592\n",
      "Step 5351 | Sliding Loss Window : 0.9414456838535785\n",
      "Step 5401 | Sliding Loss Window : 1.044226405648856\n",
      "Step 5451 | Sliding Loss Window : 0.9122704100582429\n",
      "Step 5501 | Sliding Loss Window : 1.1594984241499477\n",
      "Step 5551 | Sliding Loss Window : 1.0773482622134225\n",
      "Step 5601 | Sliding Loss Window : 1.174395345595225\n",
      "Step 5651 | Sliding Loss Window : 1.0273956459649016\n",
      "Step 5701 | Sliding Loss Window : 1.1848415587120527\n",
      "Step 5751 | Sliding Loss Window : 1.0013551958136182\n",
      "Step 5801 | Sliding Loss Window : 1.0143641401478967\n",
      "Step 5851 | Sliding Loss Window : 1.1134633534638794\n",
      "Step 5901 | Sliding Loss Window : 1.08323396596236\n",
      "Step 5951 | Sliding Loss Window : 1.0017014753845852\n",
      "Step 6001 | Sliding Loss Window : 1.0216735753303392\n",
      "Step 6051 | Sliding Loss Window : 1.0092494504619365\n",
      "Step 6101 | Sliding Loss Window : 1.010845032221022\n",
      "Step 6151 | Sliding Loss Window : 0.9422576482340865\n",
      "Step 6201 | Sliding Loss Window : 1.0442604393192534\n",
      "Step 6251 | Sliding Loss Window : 1.11512676962762\n",
      "Step 6301 | Sliding Loss Window : 1.085577103559329\n",
      "Step 6351 | Sliding Loss Window : 1.0932631398671686\n",
      "Step 6401 | Sliding Loss Window : 1.1792789634720648\n",
      "Step 6451 | Sliding Loss Window : 0.984920218036861\n",
      "Step 6501 | Sliding Loss Window : 1.101066744603736\n",
      "Step 6551 | Sliding Loss Window : 1.0949279655696227\n",
      "Step 6601 | Sliding Loss Window : 1.13016951859592\n",
      "Step 6651 | Sliding Loss Window : 0.929406562132027\n",
      "Step 6701 | Sliding Loss Window : 0.9079879355347473\n",
      "Step 6751 | Sliding Loss Window : 1.0487212133004804\n",
      "Step 6801 | Sliding Loss Window : 1.0617953183349638\n",
      "Step 6851 | Sliding Loss Window : 1.041773945775366\n",
      "Step 6901 | Sliding Loss Window : 1.040013583357233\n",
      "Step 6951 | Sliding Loss Window : 1.1983598892865046\n",
      "Step 7001 | Sliding Loss Window : 0.9725587581421375\n",
      "Step 7051 | Sliding Loss Window : 1.0742923061512284\n",
      "Step 7101 | Sliding Loss Window : 1.0999211153026685\n",
      "Step 7151 | Sliding Loss Window : 1.0569249586592024\n",
      "Step 7201 | Sliding Loss Window : 1.0355584707261951\n",
      "Step 7251 | Sliding Loss Window : 0.9789444917693575\n",
      "Step 7301 | Sliding Loss Window : 1.022088412028312\n",
      "Step 7351 | Sliding Loss Window : 0.957758295027401\n",
      "Step 7401 | Sliding Loss Window : 0.8830964219753825\n",
      "Step 7451 | Sliding Loss Window : 0.9075471941337833\n",
      "Step 7501 | Sliding Loss Window : 1.1777409861143038\n",
      "Step 7551 | Sliding Loss Window : 1.1166249117698666\n",
      "Step 7601 | Sliding Loss Window : 1.1552901931915813\n",
      "Step 7651 | Sliding Loss Window : 1.0520992815269776\n",
      "Step 7701 | Sliding Loss Window : 1.0316297719708019\n",
      "Step 7751 | Sliding Loss Window : 1.2412372514289904\n",
      "Step 7801 | Sliding Loss Window : 1.117909963187593\n",
      "Step 7851 | Sliding Loss Window : 1.1830727798313165\n",
      "Step 7901 | Sliding Loss Window : 0.9604775508409957\n",
      "Step 7951 | Sliding Loss Window : 1.0013524934927454\n",
      "Step 8001 | Sliding Loss Window : 1.0701040730760543\n",
      "Step 8051 | Sliding Loss Window : 1.0262969959062611\n",
      "Step 8101 | Sliding Loss Window : 1.081748939506464\n",
      "Step 8151 | Sliding Loss Window : 1.0501888551989413\n",
      "Step 8201 | Sliding Loss Window : 0.9337115401110548\n",
      "Step 8251 | Sliding Loss Window : 1.0390886092771574\n",
      "Step 8301 | Sliding Loss Window : 1.0330794598687465\n",
      "Step 8351 | Sliding Loss Window : 1.1598811922929864\n",
      "Step 8401 | Sliding Loss Window : 1.1268176276361763\n",
      "Step 8451 | Sliding Loss Window : 1.0248916986610808\n",
      "Step 8501 | Sliding Loss Window : 1.065840182811397\n",
      "Step 8551 | Sliding Loss Window : 1.1475761367653197\n",
      "Step 8601 | Sliding Loss Window : 1.0708734387973395\n",
      "Step 8651 | Sliding Loss Window : 1.0637317429815567\n",
      "Step 8701 | Sliding Loss Window : 1.1144437632535387\n",
      "Step 8751 | Sliding Loss Window : 1.0735758248729723\n",
      "Step 8801 | Sliding Loss Window : 0.9529423452138869\n",
      "Step 8851 | Sliding Loss Window : 1.170216175582936\n",
      "Step 8901 | Sliding Loss Window : 1.1488398462987628\n",
      "Step 8951 | Sliding Loss Window : 1.0492437707732412\n",
      "Step 9001 | Sliding Loss Window : 1.0625405009793227\n",
      "Step 9051 | Sliding Loss Window : 1.134852698550301\n",
      "Step 9101 | Sliding Loss Window : 0.9551160039174801\n",
      "Step 9151 | Sliding Loss Window : 1.1124447450081183\n",
      "Step 9201 | Sliding Loss Window : 1.2112076084673489\n",
      "Step 9251 | Sliding Loss Window : 1.1096929366389343\n",
      "Step 9301 | Sliding Loss Window : 1.0921823058382392\n",
      "Step 9351 | Sliding Loss Window : 1.0423060496232113\n",
      "Step 9401 | Sliding Loss Window : 0.9302391819501225\n",
      "Step 9451 | Sliding Loss Window : 1.0553006459456011\n",
      "Step 9501 | Sliding Loss Window : 0.9933705905400054\n",
      "Step 9551 | Sliding Loss Window : 1.2935917337806573\n",
      "Step 9601 | Sliding Loss Window : 1.046635950159467\n",
      "Step 9651 | Sliding Loss Window : 0.9923009335050691\n",
      "Step 9701 | Sliding Loss Window : 1.0713354677941553\n",
      "Step 9751 | Sliding Loss Window : 1.1020563321864996\n",
      "Step 9801 | Sliding Loss Window : 1.0549368547362992\n",
      "Step 9851 | Sliding Loss Window : 1.0163252133727463\n",
      "Step 9901 | Sliding Loss Window : 1.1814625995450165\n",
      "Step 9951 | Sliding Loss Window : 1.1487392646286438\n",
      "Step 10001 | Sliding Loss Window : 1.139947886435619\n",
      "Step 10051 | Sliding Loss Window : 1.1279887035945768\n",
      "Step 10101 | Sliding Loss Window : 1.163424232388222\n",
      "Step 10151 | Sliding Loss Window : 1.0669427156582603\n",
      "Step 10201 | Sliding Loss Window : 1.072351620220868\n",
      "Step 10251 | Sliding Loss Window : 1.0706697283592779\n",
      "Step 10301 | Sliding Loss Window : 1.0060085082009034\n",
      "Step 10351 | Sliding Loss Window : 1.1896473324510812\n",
      "Step 10401 | Sliding Loss Window : 1.048396873117623\n",
      "Step 10451 | Sliding Loss Window : 1.194606318875933\n",
      "Step 10501 | Sliding Loss Window : 1.0679978134547907\n",
      "Step 10551 | Sliding Loss Window : 1.091778394035181\n",
      "Step 10601 | Sliding Loss Window : 0.9957680061480941\n",
      "Step 10651 | Sliding Loss Window : 1.0888687854684138\n",
      "Step 10701 | Sliding Loss Window : 1.0171383912356544\n",
      "Step 10751 | Sliding Loss Window : 1.0108542656335286\n",
      "Step 10801 | Sliding Loss Window : 1.0524254570617493\n",
      "Step 10851 | Sliding Loss Window : 1.0657150658874324\n",
      "Step 10901 | Sliding Loss Window : 1.1033668181209095\n",
      "Step 10951 | Sliding Loss Window : 0.900816423267471\n",
      "Step 11001 | Sliding Loss Window : 0.9726504799760767\n",
      "Step 11051 | Sliding Loss Window : 1.172294929132733\n",
      "Step 11101 | Sliding Loss Window : 1.0451359367204818\n",
      "Step 11151 | Sliding Loss Window : 0.9859274619026024\n",
      "Step 11201 | Sliding Loss Window : 1.19989838943865\n",
      "Step 11251 | Sliding Loss Window : 1.2451237725809592\n",
      "Step 11301 | Sliding Loss Window : 1.064453554742876\n",
      "Step 11351 | Sliding Loss Window : 1.085692134933297\n",
      "Step 11401 | Sliding Loss Window : 1.1896019168135785\n",
      "Step 11451 | Sliding Loss Window : 1.2165453341160923\n",
      "Step 11501 | Sliding Loss Window : 1.2604731547378092\n",
      "Step 11551 | Sliding Loss Window : 1.1495456853464174\n",
      "Step 11601 | Sliding Loss Window : 1.0563933494416642\n",
      "Step 11651 | Sliding Loss Window : 1.2098266937553843\n",
      "Step 11701 | Sliding Loss Window : 1.047790799660419\n",
      "Step 11751 | Sliding Loss Window : 1.2579306971237338\n",
      "Step 11801 | Sliding Loss Window : 0.9030973013893567\n",
      "Step 11851 | Sliding Loss Window : 1.0527880887597723\n",
      "Step 11901 | Sliding Loss Window : 1.0308237646361038\n",
      "Step 11951 | Sliding Loss Window : 1.0594076525079192\n",
      "Step 12001 | Sliding Loss Window : 1.0058398121903265\n",
      "Step 12051 | Sliding Loss Window : 1.0849053877004855\n",
      "Step 12101 | Sliding Loss Window : 0.9718274305429446\n",
      "Step 12151 | Sliding Loss Window : 1.1343654679215989\n",
      "Step 12201 | Sliding Loss Window : 1.0331170913610266\n",
      "Step 12251 | Sliding Loss Window : 1.007805603643853\n",
      "Step 12301 | Sliding Loss Window : 1.0510605159195192\n",
      "Step 12351 | Sliding Loss Window : 0.9106254644492686\n",
      "Step 12401 | Sliding Loss Window : 1.0531499459725806\n",
      "Step 12451 | Sliding Loss Window : 0.9856788039251961\n",
      "Step 12501 | Sliding Loss Window : 1.0849972473136602\n",
      "Step 12551 | Sliding Loss Window : 1.0623677421657376\n",
      "Step 12601 | Sliding Loss Window : 1.1698178471233904\n",
      "Step 12651 | Sliding Loss Window : 0.9865510099762408\n",
      "Step 12701 | Sliding Loss Window : 1.2261145936046742\n",
      "Step 12751 | Sliding Loss Window : 1.1248896683335878\n",
      "Step 12801 | Sliding Loss Window : 1.1916564874725344\n",
      "Step 12851 | Sliding Loss Window : 1.015651732992495\n",
      "Step 12901 | Sliding Loss Window : 1.015139938863541\n",
      "Step 12951 | Sliding Loss Window : 1.1599675642322937\n",
      "Step 13001 | Sliding Loss Window : 1.0587614225219617\n",
      "Step 13051 | Sliding Loss Window : 1.08517662714658\n",
      "Step 13101 | Sliding Loss Window : 1.0131979413324215\n",
      "Step 13151 | Sliding Loss Window : 1.0446749403850313\n",
      "Step 13201 | Sliding Loss Window : 1.104146245507139\n",
      "Step 13251 | Sliding Loss Window : 0.9883563319101687\n",
      "Step 13301 | Sliding Loss Window : 0.980741325924782\n",
      "Step 13351 | Sliding Loss Window : 0.9449428830448363\n",
      "Step 13401 | Sliding Loss Window : 1.0535746392693326\n",
      "Step 13451 | Sliding Loss Window : 0.9028262439323553\n",
      "Step 13501 | Sliding Loss Window : 1.155882307348599\n",
      "Step 13551 | Sliding Loss Window : 1.0776418869929334\n",
      "Step 13601 | Sliding Loss Window : 1.1735496241199894\n",
      "Step 13651 | Sliding Loss Window : 1.0263182312453987\n",
      "Step 13701 | Sliding Loss Window : 1.1822668517649348\n",
      "Step 13751 | Sliding Loss Window : 0.99824904490718\n",
      "Step 13801 | Sliding Loss Window : 1.0117802792940966\n",
      "Step 13851 | Sliding Loss Window : 1.1082447994577753\n",
      "Step 13901 | Sliding Loss Window : 1.080788509503033\n",
      "Step 13951 | Sliding Loss Window : 1.0022507515685182\n",
      "Step 14001 | Sliding Loss Window : 1.0201616619691811\n",
      "Step 14051 | Sliding Loss Window : 1.0085347128114537\n",
      "Step 14101 | Sliding Loss Window : 1.0123128114751145\n",
      "Step 14151 | Sliding Loss Window : 0.9407076832986553\n",
      "Step 14201 | Sliding Loss Window : 1.0436565412165137\n",
      "Step 14251 | Sliding Loss Window : 1.1140367420272457\n",
      "Step 14301 | Sliding Loss Window : 1.086739390817808\n",
      "Step 14351 | Sliding Loss Window : 1.092681565847346\n",
      "Step 14401 | Sliding Loss Window : 1.1789501859132892\n",
      "Step 14451 | Sliding Loss Window : 0.9839802100971882\n",
      "Step 14501 | Sliding Loss Window : 1.1013520829515353\n",
      "Step 14551 | Sliding Loss Window : 1.0958579082789661\n",
      "Step 14601 | Sliding Loss Window : 1.1289989311196207\n",
      "Step 14651 | Sliding Loss Window : 0.9300782164742497\n",
      "Step 14701 | Sliding Loss Window : 0.9087866489445343\n",
      "Step 14751 | Sliding Loss Window : 1.0492626811709842\n",
      "Step 14801 | Sliding Loss Window : 1.0612972273111245\n",
      "Step 14851 | Sliding Loss Window : 1.0422021215055786\n",
      "Step 14901 | Sliding Loss Window : 1.0396252975034295\n",
      "Step 14951 | Sliding Loss Window : 1.1987616028896557\n",
      "Step 15001 | Sliding Loss Window : 0.9728061266560764\n",
      "Step 15051 | Sliding Loss Window : 1.074713589779915\n",
      "Step 15101 | Sliding Loss Window : 1.0998192417328978\n",
      "Step 15151 | Sliding Loss Window : 1.0570071132033483\n",
      "Step 15201 | Sliding Loss Window : 1.0356202842363702\n",
      "Step 15251 | Sliding Loss Window : 0.9788021671226533\n",
      "Step 15301 | Sliding Loss Window : 1.0227419485579452\n",
      "Step 15351 | Sliding Loss Window : 0.957445214740086\n",
      "Step 15401 | Sliding Loss Window : 0.8831646985173508\n",
      "Step 15451 | Sliding Loss Window : 0.9069584729035469\n",
      "Step 15501 | Sliding Loss Window : 1.1772912608238608\n",
      "Step 15551 | Sliding Loss Window : 1.1154970301420213\n",
      "Step 15601 | Sliding Loss Window : 1.1565632615941994\n",
      "Step 15651 | Sliding Loss Window : 1.0511179066679244\n",
      "Step 15701 | Sliding Loss Window : 1.031416610311499\n",
      "Step 15751 | Sliding Loss Window : 1.2410928692063872\n",
      "Step 15801 | Sliding Loss Window : 1.117181088084451\n",
      "Step 15851 | Sliding Loss Window : 1.1831907764505274\n",
      "Step 15901 | Sliding Loss Window : 0.960781667787024\n",
      "Step 15951 | Sliding Loss Window : 1.0014071469777033\n",
      "[2055 2289   61  952 1166 1732  255  935 2114  768 1795 1342 1699  568\n",
      " 1369 1681  103  722   39 2393  869 1917 1667 1216 1555  645 1358 1134\n",
      "  301  598 1223 1391 1569  742 1731 2426 2219 1355    6  925   75 2410\n",
      " 1450 1262 2490 1184 1129  635 1904  157 1751  807 2236 2179 1175  868\n",
      " 2172 1804 1885 2477 1321 1808  605  811 1581  637 2125 2283  406  138\n",
      " 1684 2119 1338 1666  495 1511 2196  306  327 2314 1962 2027  443 1744\n",
      " 1564 2186  643 2288 1682  574   67 1685 1379  127  459 1137  806  800\n",
      "  907  902] [0.73286292 0.64055978 0.47032171 0.46014863 0.62548372 0.74695244\n",
      " 0.71162559 0.6574104  0.75890385 0.65318245 0.76571253 0.72321884\n",
      " 0.7427699  0.74401751 0.69104483 0.67767554 0.74656156 0.74747096\n",
      " 0.63297972 0.59015347 0.73066523 0.61070434 0.66929732 0.66943057\n",
      " 0.57122744 0.73877335 0.72734561 0.67502152 0.7622658  0.73870426\n",
      " 0.69031483 0.69711211 0.7147305  0.7367756  0.63523865 0.72429231\n",
      " 0.67442113 0.74554335 0.6526094  0.73231591 0.73582359 0.7605805\n",
      " 0.67336388 0.6343441  0.35321932 0.71948653 0.74760404 0.67625156\n",
      " 0.72663139 0.68405336 0.65786134 0.73262708 0.50763256 0.76071812\n",
      " 0.69562295 0.74804808 0.53361171 0.71786941 0.65459263 0.70228653\n",
      " 0.73507337 0.6989388  0.72101005 0.66310447 0.7406919  0.68840729\n",
      " 0.66751732 0.62353703 0.74574493 0.69404794 0.639704   0.70057376\n",
      " 0.668106   0.65538597 0.72323594 0.56678564 0.73391438 0.72992789\n",
      " 0.71973838 0.7394077  0.68428015 0.66053134 0.75582449 0.72732453\n",
      " 0.74740427 0.64178191 0.63409743 0.58374251 0.75496249 0.72583299\n",
      " 0.72884615 0.68029295 0.68871782 0.52500891 0.59006218 0.70836263\n",
      " 0.69401449 0.72881308 0.68292281 0.68788227]\n",
      "1795 0.7657125272598932\n",
      "0.9214706420898438 0.830967903137207\n",
      "Step 1 | Sliding Loss Window : 2.0265994933739804\n",
      "Step 51 | Sliding Loss Window : 1.462779928294785\n",
      "Step 101 | Sliding Loss Window : 1.3508409028826867\n",
      "Step 151 | Sliding Loss Window : 1.2297503984710452\n",
      "Step 201 | Sliding Loss Window : 1.1694331697856126\n",
      "Step 251 | Sliding Loss Window : 1.1301658648290434\n",
      "Step 301 | Sliding Loss Window : 1.1714213581206048\n",
      "Step 351 | Sliding Loss Window : 0.9994489029609126\n",
      "Step 401 | Sliding Loss Window : 0.9377863967138856\n",
      "Step 451 | Sliding Loss Window : 1.0668897070879424\n",
      "Step 501 | Sliding Loss Window : 0.9347497918989225\n",
      "Step 551 | Sliding Loss Window : 0.7864759437711144\n",
      "Step 601 | Sliding Loss Window : 0.9086529452428721\n",
      "Step 651 | Sliding Loss Window : 0.9563985615736778\n",
      "Step 701 | Sliding Loss Window : 0.9421127373133072\n",
      "Step 751 | Sliding Loss Window : 0.7850527237729185\n",
      "Step 801 | Sliding Loss Window : 0.9199114241057674\n",
      "Step 851 | Sliding Loss Window : 0.9851233539011242\n",
      "Step 901 | Sliding Loss Window : 0.8348124079599376\n",
      "Step 951 | Sliding Loss Window : 0.8377534618446457\n",
      "Step 1001 | Sliding Loss Window : 1.0191949933806765\n",
      "Step 1051 | Sliding Loss Window : 0.7999436689699917\n",
      "Step 1101 | Sliding Loss Window : 0.7600378206653408\n",
      "Step 1151 | Sliding Loss Window : 0.8447886623072217\n",
      "Step 1201 | Sliding Loss Window : 0.8154241048979384\n",
      "Step 1251 | Sliding Loss Window : 0.8199968206332309\n",
      "Step 1301 | Sliding Loss Window : 0.8579850003863134\n",
      "Step 1351 | Sliding Loss Window : 0.707602968805497\n",
      "Step 1401 | Sliding Loss Window : 0.7752419995735306\n",
      "Step 1451 | Sliding Loss Window : 0.9859814916945379\n",
      "Step 1501 | Sliding Loss Window : 0.8519249542018644\n",
      "Step 1551 | Sliding Loss Window : 0.7152296452788781\n",
      "Step 1601 | Sliding Loss Window : 0.9389454632649462\n",
      "Step 1651 | Sliding Loss Window : 0.9848400605419834\n",
      "Step 1701 | Sliding Loss Window : 0.8628399891652795\n",
      "Step 1751 | Sliding Loss Window : 0.9577158655244676\n",
      "Step 1801 | Sliding Loss Window : 0.8480574281487355\n",
      "Step 1851 | Sliding Loss Window : 0.8324935516602259\n",
      "Step 1901 | Sliding Loss Window : 0.9197911886999871\n",
      "Step 1951 | Sliding Loss Window : 0.8082958529179133\n",
      "Step 2001 | Sliding Loss Window : 0.7297618644569016\n",
      "Step 2051 | Sliding Loss Window : 0.7820917057685322\n",
      "Step 2101 | Sliding Loss Window : 0.8749909623164772\n",
      "Step 2151 | Sliding Loss Window : 0.9047348816553666\n",
      "Step 2201 | Sliding Loss Window : 0.841713689970953\n",
      "Step 2251 | Sliding Loss Window : 0.8254826581684064\n",
      "Step 2301 | Sliding Loss Window : 0.697997926984508\n",
      "Step 2351 | Sliding Loss Window : 0.8755275014355708\n",
      "Step 2401 | Sliding Loss Window : 0.8391687251305102\n",
      "Step 2451 | Sliding Loss Window : 0.8549116400893443\n",
      "Step 2501 | Sliding Loss Window : 0.9450707691992024\n",
      "Step 2551 | Sliding Loss Window : 0.8136088359660033\n",
      "Step 2601 | Sliding Loss Window : 0.7699023057283241\n",
      "Step 2651 | Sliding Loss Window : 0.7181270704935336\n",
      "Step 2701 | Sliding Loss Window : 0.891313358096681\n",
      "Step 2751 | Sliding Loss Window : 0.684377697078614\n",
      "Step 2801 | Sliding Loss Window : 0.7912831316786851\n",
      "Step 2851 | Sliding Loss Window : 0.8353910137855184\n",
      "Step 2901 | Sliding Loss Window : 0.7833080031239753\n",
      "Step 2951 | Sliding Loss Window : 0.8412694807631206\n",
      "Step 3001 | Sliding Loss Window : 0.883988183399293\n",
      "Step 3051 | Sliding Loss Window : 0.8445458426464921\n",
      "Step 3101 | Sliding Loss Window : 0.797276195101261\n",
      "Step 3151 | Sliding Loss Window : 0.8343762979828204\n",
      "Step 3201 | Sliding Loss Window : 0.9207698488725895\n",
      "Step 3251 | Sliding Loss Window : 0.9147102793381191\n",
      "Step 3301 | Sliding Loss Window : 0.8789036344337888\n",
      "Step 3351 | Sliding Loss Window : 0.9027255579838689\n",
      "Step 3401 | Sliding Loss Window : 0.9466754579212917\n",
      "Step 3451 | Sliding Loss Window : 0.8869475228084442\n",
      "Step 3501 | Sliding Loss Window : 0.7621392991607907\n",
      "Step 3551 | Sliding Loss Window : 0.8372301621005518\n",
      "Step 3601 | Sliding Loss Window : 0.9238954094959463\n",
      "Step 3651 | Sliding Loss Window : 0.8765854532957789\n",
      "Step 3701 | Sliding Loss Window : 0.9100102256517243\n",
      "Step 3751 | Sliding Loss Window : 0.9571616533057834\n",
      "Step 3801 | Sliding Loss Window : 0.8392197967507513\n",
      "Step 3851 | Sliding Loss Window : 0.7904399094089324\n",
      "Step 3901 | Sliding Loss Window : 0.8143287067291198\n",
      "Step 3951 | Sliding Loss Window : 0.7947688737728135\n",
      "Step 4001 | Sliding Loss Window : 0.8311897516748483\n",
      "Step 4051 | Sliding Loss Window : 0.8068946723326219\n",
      "Step 4101 | Sliding Loss Window : 0.9082302733656016\n",
      "Step 4151 | Sliding Loss Window : 0.8139140434086707\n",
      "Step 4201 | Sliding Loss Window : 0.7692602500672814\n",
      "Step 4251 | Sliding Loss Window : 0.9476822136095006\n",
      "Step 4301 | Sliding Loss Window : 0.701368598467313\n",
      "Step 4351 | Sliding Loss Window : 0.7516423756752753\n",
      "Step 4401 | Sliding Loss Window : 0.8155365141211477\n",
      "Step 4451 | Sliding Loss Window : 0.8176850501216865\n",
      "Step 4501 | Sliding Loss Window : 0.8392762721973668\n",
      "Step 4551 | Sliding Loss Window : 0.9951324541857768\n",
      "Step 4601 | Sliding Loss Window : 0.8818548260728477\n",
      "Step 4651 | Sliding Loss Window : 0.9004719736310858\n",
      "Step 4701 | Sliding Loss Window : 0.938004861886827\n",
      "Step 4751 | Sliding Loss Window : 0.9287190148154559\n",
      "Step 4801 | Sliding Loss Window : 0.866254868520794\n",
      "Step 4851 | Sliding Loss Window : 0.8793126027781019\n",
      "Step 4901 | Sliding Loss Window : 0.8611777316183158\n",
      "Step 4951 | Sliding Loss Window : 0.9023568254916665\n",
      "Step 5001 | Sliding Loss Window : 0.7998522677700112\n",
      "Step 5051 | Sliding Loss Window : 0.8660710799144451\n",
      "Step 5101 | Sliding Loss Window : 0.8339054809830891\n",
      "Step 5151 | Sliding Loss Window : 0.9228707596001129\n",
      "Step 5201 | Sliding Loss Window : 0.9498566780021126\n",
      "Step 5251 | Sliding Loss Window : 0.8121228513211446\n",
      "Step 5301 | Sliding Loss Window : 0.8350568040485213\n",
      "Step 5351 | Sliding Loss Window : 0.7351432462922755\n",
      "Step 5401 | Sliding Loss Window : 0.86507070097163\n",
      "Step 5451 | Sliding Loss Window : 0.8842573475114205\n",
      "Step 5501 | Sliding Loss Window : 0.7371531401517413\n",
      "Step 5551 | Sliding Loss Window : 0.9173878869395192\n",
      "Step 5601 | Sliding Loss Window : 0.8153692686421288\n",
      "Step 5651 | Sliding Loss Window : 0.8409118017174699\n",
      "Step 5701 | Sliding Loss Window : 0.8854259352777757\n",
      "Step 5751 | Sliding Loss Window : 0.8533732417085893\n",
      "Step 5801 | Sliding Loss Window : 0.9472026698111372\n",
      "Step 5851 | Sliding Loss Window : 0.9071301634233077\n",
      "Step 5901 | Sliding Loss Window : 0.7872780599954239\n",
      "Step 5951 | Sliding Loss Window : 0.7359592021426387\n",
      "Step 6001 | Sliding Loss Window : 0.9227213985111499\n",
      "Step 6051 | Sliding Loss Window : 0.9024813826694134\n",
      "Step 6101 | Sliding Loss Window : 0.7919794722661304\n",
      "Step 6151 | Sliding Loss Window : 0.7978788525939958\n",
      "Step 6201 | Sliding Loss Window : 0.8168960092558525\n",
      "Step 6251 | Sliding Loss Window : 0.8605834089204732\n",
      "Step 6301 | Sliding Loss Window : 0.8820676746004176\n",
      "Step 6351 | Sliding Loss Window : 0.7433660606292923\n",
      "Step 6401 | Sliding Loss Window : 0.902838658455246\n",
      "Step 6451 | Sliding Loss Window : 0.7252893245495227\n",
      "Step 6501 | Sliding Loss Window : 0.849889615706633\n",
      "Step 6551 | Sliding Loss Window : 0.8631662410558905\n",
      "Step 6601 | Sliding Loss Window : 0.7433819163520811\n",
      "Step 6651 | Sliding Loss Window : 0.8014731430612295\n",
      "Step 6701 | Sliding Loss Window : 0.7578521327465134\n",
      "Step 6751 | Sliding Loss Window : 0.9259521970729963\n",
      "Step 6801 | Sliding Loss Window : 0.7430143331151413\n",
      "Step 6851 | Sliding Loss Window : 0.8101146079085327\n",
      "Step 6901 | Sliding Loss Window : 1.0045479153037338\n",
      "Step 6951 | Sliding Loss Window : 0.9023842297183979\n",
      "Step 7001 | Sliding Loss Window : 0.9577993771619033\n",
      "Step 7051 | Sliding Loss Window : 0.8625654372678444\n",
      "Step 7101 | Sliding Loss Window : 0.7977846628282532\n",
      "Step 7151 | Sliding Loss Window : 0.6795138532425977\n",
      "Step 7201 | Sliding Loss Window : 0.875421342165815\n",
      "Step 7251 | Sliding Loss Window : 0.6559504271428971\n",
      "Step 7301 | Sliding Loss Window : 0.8398142642255109\n",
      "Step 7351 | Sliding Loss Window : 0.8477205515646733\n",
      "Step 7401 | Sliding Loss Window : 0.907690771120722\n",
      "Step 7451 | Sliding Loss Window : 0.8009205132999293\n",
      "Step 7501 | Sliding Loss Window : 0.8667008572433328\n",
      "Step 7551 | Sliding Loss Window : 0.8304396004077232\n",
      "Step 7601 | Sliding Loss Window : 0.7480706027318184\n",
      "Step 7651 | Sliding Loss Window : 0.8172805371556814\n",
      "Step 7701 | Sliding Loss Window : 0.8126117654856455\n",
      "Step 7751 | Sliding Loss Window : 0.8864649846023801\n",
      "Step 7801 | Sliding Loss Window : 0.8082119482579178\n",
      "Step 7851 | Sliding Loss Window : 0.9274997810970226\n",
      "Step 7901 | Sliding Loss Window : 0.8466513170744204\n",
      "Step 7951 | Sliding Loss Window : 0.8814113778729297\n",
      "Step 8001 | Sliding Loss Window : 0.8504389052433315\n",
      "Step 8051 | Sliding Loss Window : 0.9316179678608643\n",
      "Step 8101 | Sliding Loss Window : 0.8251324762858633\n",
      "Step 8151 | Sliding Loss Window : 0.8995003431415353\n",
      "Step 8201 | Sliding Loss Window : 0.8464977423362094\n",
      "Step 8251 | Sliding Loss Window : 0.9025842045335614\n",
      "Step 8301 | Sliding Loss Window : 0.8871147952848955\n",
      "Step 8351 | Sliding Loss Window : 0.7933403264317334\n",
      "Step 8401 | Sliding Loss Window : 0.8598690531466553\n",
      "Step 8451 | Sliding Loss Window : 0.9271456224065266\n",
      "Step 8501 | Sliding Loss Window : 0.7901809426259903\n",
      "Step 8551 | Sliding Loss Window : 0.6967128565384507\n",
      "Step 8601 | Sliding Loss Window : 0.8826605511375812\n",
      "Step 8651 | Sliding Loss Window : 0.8398681999923514\n",
      "Step 8701 | Sliding Loss Window : 0.839569193104483\n",
      "Step 8751 | Sliding Loss Window : 0.757325064485495\n",
      "Step 8801 | Sliding Loss Window : 0.7881584546554365\n",
      "Step 8851 | Sliding Loss Window : 0.8286094703232381\n",
      "Step 8901 | Sliding Loss Window : 0.7181582570967124\n",
      "Step 8951 | Sliding Loss Window : 0.813666140695371\n",
      "Step 9001 | Sliding Loss Window : 0.9506050483585164\n",
      "Step 9051 | Sliding Loss Window : 0.757716290773823\n",
      "Step 9101 | Sliding Loss Window : 0.7495487816682486\n",
      "Step 9151 | Sliding Loss Window : 0.7620046358419527\n",
      "Step 9201 | Sliding Loss Window : 0.8354937736860631\n",
      "Step 9251 | Sliding Loss Window : 0.7686745953204854\n",
      "Step 9301 | Sliding Loss Window : 0.8109753500147027\n",
      "Step 9351 | Sliding Loss Window : 0.7181882856861123\n",
      "Step 9401 | Sliding Loss Window : 0.7457699823048226\n",
      "Step 9451 | Sliding Loss Window : 0.9937164446297903\n",
      "Step 9501 | Sliding Loss Window : 0.8485953398795831\n",
      "Step 9551 | Sliding Loss Window : 0.7076737228950221\n",
      "Step 9601 | Sliding Loss Window : 0.9363208548670466\n",
      "Step 9651 | Sliding Loss Window : 0.9389674018243643\n",
      "Step 9701 | Sliding Loss Window : 0.84293753092327\n",
      "Step 9751 | Sliding Loss Window : 0.9517768717205964\n",
      "Step 9801 | Sliding Loss Window : 0.8505698533132992\n",
      "Step 9851 | Sliding Loss Window : 0.8370054635156581\n",
      "Step 9901 | Sliding Loss Window : 0.950283887556322\n",
      "Step 9951 | Sliding Loss Window : 0.7785801657634879\n",
      "Step 10001 | Sliding Loss Window : 0.7014133133345882\n",
      "Step 10051 | Sliding Loss Window : 0.7801685215770817\n",
      "Step 10101 | Sliding Loss Window : 0.8625572589777138\n",
      "Step 10151 | Sliding Loss Window : 0.9089533058370495\n",
      "Step 10201 | Sliding Loss Window : 0.8504334608810997\n",
      "Step 10251 | Sliding Loss Window : 0.8157062988885778\n",
      "Step 10301 | Sliding Loss Window : 0.6986469537945639\n",
      "Step 10351 | Sliding Loss Window : 0.8620107953959054\n",
      "Step 10401 | Sliding Loss Window : 0.8469022779874984\n",
      "Step 10451 | Sliding Loss Window : 0.8537697229326079\n",
      "Step 10501 | Sliding Loss Window : 0.9418741615417499\n",
      "Step 10551 | Sliding Loss Window : 0.8157641604897599\n",
      "Step 10601 | Sliding Loss Window : 0.7690998035451233\n",
      "Step 10651 | Sliding Loss Window : 0.7177746556829173\n",
      "Step 10701 | Sliding Loss Window : 0.8915366496991893\n",
      "Step 10751 | Sliding Loss Window : 0.6858490315234375\n",
      "Step 10801 | Sliding Loss Window : 0.7942943902379942\n",
      "Step 10851 | Sliding Loss Window : 0.8353784688738234\n",
      "Step 10901 | Sliding Loss Window : 0.781724798013604\n",
      "Step 10951 | Sliding Loss Window : 0.837521454935441\n",
      "Step 11001 | Sliding Loss Window : 0.884485261701546\n",
      "Step 11051 | Sliding Loss Window : 0.8448314823242907\n",
      "Step 11101 | Sliding Loss Window : 0.7984551499933744\n",
      "Step 11151 | Sliding Loss Window : 0.8344793754151689\n",
      "Step 11201 | Sliding Loss Window : 0.9171186358266309\n",
      "Step 11251 | Sliding Loss Window : 0.9157493691298142\n",
      "Step 11301 | Sliding Loss Window : 0.8762761917238738\n",
      "Step 11351 | Sliding Loss Window : 0.8997895585997965\n",
      "Step 11401 | Sliding Loss Window : 0.9468832718172472\n",
      "Step 11451 | Sliding Loss Window : 0.8859319778607884\n",
      "Step 11501 | Sliding Loss Window : 0.7624064966596845\n",
      "Step 11551 | Sliding Loss Window : 0.8361140548353816\n",
      "Step 11601 | Sliding Loss Window : 0.9242378728333126\n",
      "Step 11651 | Sliding Loss Window : 0.878647981540391\n",
      "Step 11701 | Sliding Loss Window : 0.9117876566698503\n",
      "Step 11751 | Sliding Loss Window : 0.9561402019185858\n",
      "Step 11801 | Sliding Loss Window : 0.8369982181425207\n",
      "Step 11851 | Sliding Loss Window : 0.7905347952362465\n",
      "Step 11901 | Sliding Loss Window : 0.8142055250460948\n",
      "Step 11951 | Sliding Loss Window : 0.7935108294634182\n",
      "Step 12001 | Sliding Loss Window : 0.8318071632051783\n",
      "Step 12051 | Sliding Loss Window : 0.8054302992821876\n",
      "Step 12101 | Sliding Loss Window : 0.9072113261613111\n",
      "Step 12151 | Sliding Loss Window : 0.8140303885460141\n",
      "Step 12201 | Sliding Loss Window : 0.7699407221488805\n",
      "Step 12251 | Sliding Loss Window : 0.9477506021874134\n",
      "Step 12301 | Sliding Loss Window : 0.7022210529004334\n",
      "Step 12351 | Sliding Loss Window : 0.7526000216423371\n",
      "Step 12401 | Sliding Loss Window : 0.8153144890200619\n",
      "Step 12451 | Sliding Loss Window : 0.8159784825493547\n",
      "Step 12501 | Sliding Loss Window : 0.8383672241357643\n",
      "Step 12551 | Sliding Loss Window : 0.99530202619159\n",
      "Step 12601 | Sliding Loss Window : 0.8820315866613768\n",
      "Step 12651 | Sliding Loss Window : 0.8994862742471887\n",
      "Step 12701 | Sliding Loss Window : 0.9373254934436419\n",
      "Step 12751 | Sliding Loss Window : 0.9287239427852368\n",
      "Step 12801 | Sliding Loss Window : 0.865493318310306\n",
      "Step 12851 | Sliding Loss Window : 0.8789290042722587\n",
      "Step 12901 | Sliding Loss Window : 0.8612559916789688\n",
      "Step 12951 | Sliding Loss Window : 0.9032593517439941\n",
      "Step 13001 | Sliding Loss Window : 0.8000523801815814\n",
      "Step 13051 | Sliding Loss Window : 0.8662490676271628\n",
      "Step 13101 | Sliding Loss Window : 0.8346336498886485\n",
      "Step 13151 | Sliding Loss Window : 0.9228979692673613\n",
      "Step 13201 | Sliding Loss Window : 0.9481073063054535\n",
      "Step 13251 | Sliding Loss Window : 0.8120433891162238\n",
      "Step 13301 | Sliding Loss Window : 0.8330381731243365\n",
      "Step 13351 | Sliding Loss Window : 0.735432762395521\n",
      "Step 13401 | Sliding Loss Window : 0.865005872955041\n",
      "Step 13451 | Sliding Loss Window : 0.8834540348469937\n",
      "Step 13501 | Sliding Loss Window : 0.7370464272304121\n",
      "Step 13551 | Sliding Loss Window : 0.9164721008651325\n",
      "Step 13601 | Sliding Loss Window : 0.8164598647657004\n",
      "Step 13651 | Sliding Loss Window : 0.8410582802304005\n",
      "Step 13701 | Sliding Loss Window : 0.8853611406837462\n",
      "Step 13751 | Sliding Loss Window : 0.8546596859697556\n",
      "Step 13801 | Sliding Loss Window : 0.9460689212408591\n",
      "Step 13851 | Sliding Loss Window : 0.9074757475417027\n",
      "Step 13901 | Sliding Loss Window : 0.7881237176027399\n",
      "Step 13951 | Sliding Loss Window : 0.7364479123421295\n",
      "Step 14001 | Sliding Loss Window : 0.9213461575718275\n",
      "Step 14051 | Sliding Loss Window : 0.9032502946147994\n",
      "Step 14101 | Sliding Loss Window : 0.7915199312750769\n",
      "Step 14151 | Sliding Loss Window : 0.7971521832376627\n",
      "Step 14201 | Sliding Loss Window : 0.8179850674367466\n",
      "Step 14251 | Sliding Loss Window : 0.8609898978072822\n",
      "Step 14301 | Sliding Loss Window : 0.8819498811902885\n",
      "Step 14351 | Sliding Loss Window : 0.7440859901311461\n",
      "Step 14401 | Sliding Loss Window : 0.9030001110676877\n",
      "Step 14451 | Sliding Loss Window : 0.7257122967876708\n",
      "Step 14501 | Sliding Loss Window : 0.8501749060326282\n",
      "Step 14551 | Sliding Loss Window : 0.8625486601779109\n",
      "Step 14601 | Sliding Loss Window : 0.7439913752186658\n",
      "Step 14651 | Sliding Loss Window : 0.8011983537547991\n",
      "Step 14701 | Sliding Loss Window : 0.7573058689596972\n",
      "Step 14751 | Sliding Loss Window : 0.9258651266120381\n",
      "Step 14801 | Sliding Loss Window : 0.7428605701287542\n",
      "Step 14851 | Sliding Loss Window : 0.8101493290831283\n",
      "Step 14901 | Sliding Loss Window : 1.0042960304719113\n",
      "Step 14951 | Sliding Loss Window : 0.9019496338812076\n",
      "Step 15001 | Sliding Loss Window : 0.9578043939319448\n",
      "Step 15051 | Sliding Loss Window : 0.8627706283936107\n",
      "Step 15101 | Sliding Loss Window : 0.7982509023715623\n",
      "Step 15151 | Sliding Loss Window : 0.6796164455761604\n",
      "Step 15201 | Sliding Loss Window : 0.8749125409245617\n",
      "Step 15251 | Sliding Loss Window : 0.6560665827676623\n",
      "Step 15301 | Sliding Loss Window : 0.8395435721160949\n",
      "Step 15351 | Sliding Loss Window : 0.847824476093745\n",
      "Step 15401 | Sliding Loss Window : 0.9080549651366215\n",
      "Step 15451 | Sliding Loss Window : 0.8019348433288053\n"
     ]
    }
   ],
   "source": [
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import get_circ_params, create_gate_circ\n",
    "from train_circ import train_qnn, mse_vec_loss\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "search_nums = [100]\n",
    "param_nums = [20, 24]\n",
    "\n",
    "dataset = 'fmnist_4'\n",
    "num_reps = 1\n",
    "\n",
    "num_qubits = 4\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "meas_qubits = [0, 1]\n",
    "mat_size = 4096\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "noisy_dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "ours_dir = './ours/fmnist_4/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', num_reps)\n",
    "\n",
    "for search_num in search_nums:       \n",
    "    for param_num in param_nums:\n",
    "        noise_scores = []\n",
    "        \n",
    "        for i in range(2500):\n",
    "            noise_scores.append(np.genfromtxt(ours_dir + '/{}_params/circ_{}/noise_metric/{}/metric_tvd_score.txt'.format(param_num, i + 1, device_name)))\n",
    "            \n",
    "        noise_scores = np.array(noise_scores)\n",
    "        \n",
    "        mean_t_mat_scores = np.genfromtxt(ours_dir + '{}_params/d2_mean_t_mat_scores.txt'.format(param_num))\n",
    "        perf_scores = (mat_size - mean_t_mat_scores) / mat_size\n",
    "        combined_scores = np.multiply(perf_scores, noise_scores)\n",
    "        \n",
    "        circ_inds = np.random.permutation(2500)\n",
    "        \n",
    "        curr_dir = ours_dir + '/{}_params/search_{}_{}'.format(param_num, search_num, device_name)\n",
    "        \n",
    "        if not os.path.exists(curr_dir):\n",
    "            os.mkdir(curr_dir)\n",
    "\n",
    "        for j in range(25):\n",
    "            curr_trial_dir = curr_dir + '/trial_{}'.format(j + 1)\n",
    "\n",
    "            if not os.path.exists(curr_trial_dir):\n",
    "                os.mkdir(curr_trial_dir)\n",
    "        \n",
    "            sel_inds = circ_inds[range(j * 100, j * 100 + 100)]\n",
    "            sel_ind = sel_inds[np.argmax(combined_scores[sel_inds])]\n",
    "\n",
    "            np.savetxt(curr_trial_dir + '/searched_circuit_inds.txt', sel_inds)\n",
    "            np.savetxt(curr_trial_dir + '/searched_circuit_scores.txt', combined_scores[sel_inds])\n",
    "            np.savetxt(curr_trial_dir + '/sel_circuit_ind.txt', [sel_ind])\n",
    "            np.savetxt(curr_trial_dir + '/sel_circuit_score.txt', [combined_scores[sel_ind]])\n",
    "            \n",
    "            print(sel_inds, combined_scores[sel_inds])\n",
    "            print(sel_ind, combined_scores[sel_ind])\n",
    "            print(noise_scores[sel_ind], perf_scores[sel_ind])\n",
    "\n",
    "            circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(ours_dir + '{}_params/circ_{}'.format(param_num, sel_ind + 1))\n",
    "\n",
    "            circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'exp', 'adjoint')\n",
    "            \n",
    "            noisy_circ = create_gate_circ(noisy_dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'exp', None)\n",
    "        \n",
    "            losses_list = []\n",
    "            accs_list = []\n",
    "            noisy_losses_list = []\n",
    "            noisy_accs_list = []\n",
    "\n",
    "            for j in range(5):\n",
    "                curr_train_dir = curr_trial_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "                if os.path.exists(curr_train_dir):\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(curr_train_dir)\n",
    "\n",
    "\n",
    "                info = train_qnn(circ, x_train, y_train, x_test, y_test, [weights_bounds[-1]], 16000, 0.05, 1, mse_vec_loss, verbosity=17300, \n",
    "                                                                                                loss_window=50, init_params=None, \n",
    "                                                                                                acc_thres=1.1, shuffle=True, print_loss=50)\n",
    "\n",
    "                val_exps = [circ(x_test[i], info[-1][-1]) for i in range(len(x_test))]\n",
    "                val_loss = np.array([mse_vec_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "                \n",
    "                noisy_val_exps = [noisy_circ(x_test[i], info[-1][-1]) for i in range(len(x_test))]\n",
    "                noisy_val_loss = np.array([mse_vec_loss(y_test[k], noisy_val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "#                 acc = np.mean(val_loss < 1)\n",
    "#                 noisy_acc = np.mean(noisy_val_loss < 1)\n",
    "                acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "                noisy_acc = np.mean(np.sum(np.multiply(noisy_val_exps, y_test) > 0, 1) == 2)\n",
    "\n",
    "                np.savetxt(curr_train_dir + '/params_{}.txt'.format(j + 1), info[-1])\n",
    "                np.savetxt(curr_train_dir + '/losses_{}.txt'.format(j + 1), info[0])             \n",
    "\n",
    "                losses_list.append(val_loss)\n",
    "                accs_list.append(acc)\n",
    "                \n",
    "                noisy_losses_list.append(noisy_val_loss)\n",
    "                noisy_accs_list.append(noisy_acc)\n",
    "\n",
    "            np.savetxt(curr_trial_dir + '/accs.txt', accs_list)\n",
    "            np.savetxt(curr_trial_dir + '/val_losses.txt', losses_list)          \n",
    "            np.savetxt(curr_trial_dir + '/noisy_accs.txt', noisy_accs_list)\n",
    "            np.savetxt(curr_trial_dir + '/noisy_val_losses.txt', noisy_losses_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train correlation circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'datasets_nt' from '/root/datasets_nt.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import datasets_nt\n",
    "\n",
    "reload(datasets_nt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 16)\n",
      "[2022-10-19 08:16:42.295 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:28 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-10-19 08:16:42.529 tensorflow-2-3-cpu-py-ml-t3-medium-dbca98283d57d615662c4efa28c8:28 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Step 1 | Sliding Loss Window : 2.0585027670945735\n",
      "Step 51 | Sliding Loss Window : 1.622913617420368\n",
      "Step 101 | Sliding Loss Window : 1.643977672324379\n",
      "Step 151 | Sliding Loss Window : 1.3790320180656328\n",
      "Step 201 | Sliding Loss Window : 1.209902928555085\n",
      "Step 251 | Sliding Loss Window : 1.127645131514105\n",
      "Step 301 | Sliding Loss Window : 1.152621094899257\n",
      "Step 351 | Sliding Loss Window : 1.2084384994405584\n",
      "Step 401 | Sliding Loss Window : 0.9669426851925617\n",
      "Step 451 | Sliding Loss Window : 1.1709646634413087\n",
      "Step 501 | Sliding Loss Window : 1.123326784538358\n",
      "Step 551 | Sliding Loss Window : 1.0220899558357732\n",
      "Step 601 | Sliding Loss Window : 1.0508961647062827\n",
      "Step 651 | Sliding Loss Window : 1.0743157054449175\n",
      "Step 701 | Sliding Loss Window : 1.0220622496926806\n",
      "Step 751 | Sliding Loss Window : 0.9554763393528565\n",
      "Step 801 | Sliding Loss Window : 1.0316119930001326\n",
      "Step 851 | Sliding Loss Window : 1.1051662880261035\n",
      "Step 901 | Sliding Loss Window : 1.0811060156194665\n",
      "Step 951 | Sliding Loss Window : 1.2098317286870195\n",
      "Step 1001 | Sliding Loss Window : 0.9815542427456592\n",
      "Step 1051 | Sliding Loss Window : 1.131837968329562\n",
      "Step 1101 | Sliding Loss Window : 1.0677897666006324\n",
      "Step 1151 | Sliding Loss Window : 1.0659556863675703\n",
      "Step 1201 | Sliding Loss Window : 1.1124961579716672\n",
      "Step 1251 | Sliding Loss Window : 1.1717266159900117\n",
      "Step 1301 | Sliding Loss Window : 1.0138314429718907\n",
      "Step 1351 | Sliding Loss Window : 1.1367831435142375\n",
      "Step 1401 | Sliding Loss Window : 1.0647878624728049\n",
      "Step 1451 | Sliding Loss Window : 0.9923160633265604\n",
      "Step 1501 | Sliding Loss Window : 1.044105611288733\n",
      "Step 1551 | Sliding Loss Window : 1.0395653104649747\n",
      "Step 1601 | Sliding Loss Window : 1.030027297782107\n",
      "Step 1651 | Sliding Loss Window : 1.1194379740313907\n",
      "Step 1701 | Sliding Loss Window : 1.0652169136821221\n",
      "Step 1751 | Sliding Loss Window : 1.0984032803297257\n",
      "Step 1801 | Sliding Loss Window : 1.089795786044612\n",
      "Step 1851 | Sliding Loss Window : 1.115324884035856\n",
      "Step 1901 | Sliding Loss Window : 1.1115422924341622\n",
      "Step 1951 | Sliding Loss Window : 1.0780450990139245\n",
      "Step 2001 | Sliding Loss Window : 1.2050536857348677\n",
      "Step 2051 | Sliding Loss Window : 1.0809359912539187\n",
      "Step 2101 | Sliding Loss Window : 1.0966336125207479\n",
      "Step 2151 | Sliding Loss Window : 1.0641872843985407\n",
      "Step 2201 | Sliding Loss Window : 1.0823186812901668\n",
      "Step 2251 | Sliding Loss Window : 1.0659861632913665\n",
      "Step 2301 | Sliding Loss Window : 1.1227013564300732\n",
      "Step 2351 | Sliding Loss Window : 1.1567361153389972\n",
      "Step 2401 | Sliding Loss Window : 1.0545196314696192\n",
      "Step 2451 | Sliding Loss Window : 1.1209102507139597\n",
      "Step 2501 | Sliding Loss Window : 1.0969555950996701\n",
      "Step 2551 | Sliding Loss Window : 0.9946122077501544\n",
      "Step 2601 | Sliding Loss Window : 1.0476688754043657\n",
      "Step 2651 | Sliding Loss Window : 1.0129217604872083\n",
      "Step 2701 | Sliding Loss Window : 0.9568371071492939\n",
      "Step 2751 | Sliding Loss Window : 1.0628244764780792\n",
      "Step 2801 | Sliding Loss Window : 1.0175359518718896\n",
      "Step 2851 | Sliding Loss Window : 1.0386537973616372\n",
      "Step 2901 | Sliding Loss Window : 1.0196816582555057\n",
      "Step 2951 | Sliding Loss Window : 1.0654007791369924\n",
      "Step 3001 | Sliding Loss Window : 0.9639289622626339\n",
      "Step 3051 | Sliding Loss Window : 1.0907318489384144\n",
      "Step 3101 | Sliding Loss Window : 0.9928963792855265\n",
      "Step 3151 | Sliding Loss Window : 0.9583828039314954\n",
      "Step 3201 | Sliding Loss Window : 1.0855367964921028\n",
      "Step 3251 | Sliding Loss Window : 1.0986361387170314\n",
      "Step 3301 | Sliding Loss Window : 1.2024122936546602\n",
      "Step 3351 | Sliding Loss Window : 1.09449940554224\n",
      "Step 3401 | Sliding Loss Window : 1.0802091359066894\n",
      "Step 3451 | Sliding Loss Window : 1.0377844771589162\n",
      "Step 3501 | Sliding Loss Window : 1.0934206663902541\n",
      "Step 3551 | Sliding Loss Window : 1.169183319954539\n",
      "Step 3601 | Sliding Loss Window : 1.066624796809042\n",
      "Step 3651 | Sliding Loss Window : 1.140080279797995\n",
      "Step 3701 | Sliding Loss Window : 0.981023754121062\n",
      "Step 3751 | Sliding Loss Window : 1.087028476067403\n",
      "Step 3801 | Sliding Loss Window : 1.05555074053337\n",
      "Step 3851 | Sliding Loss Window : 1.1885476526188512\n",
      "Step 3901 | Sliding Loss Window : 1.089488368907534\n",
      "Step 3951 | Sliding Loss Window : 1.0762084029140164\n",
      "Step 4001 | Sliding Loss Window : 1.0282857710704494\n",
      "Step 4051 | Sliding Loss Window : 0.9927444590412793\n",
      "Step 4101 | Sliding Loss Window : 0.9178813691869854\n",
      "Step 4151 | Sliding Loss Window : 1.176569800048747\n",
      "Step 4201 | Sliding Loss Window : 1.0287565613993583\n",
      "Step 4251 | Sliding Loss Window : 1.0916450652013627\n",
      "Step 4301 | Sliding Loss Window : 1.0161409026127572\n",
      "Step 4351 | Sliding Loss Window : 1.0782087509457385\n",
      "Step 4401 | Sliding Loss Window : 1.1005081276537603\n",
      "Step 4451 | Sliding Loss Window : 1.0888010319550658\n",
      "Step 4501 | Sliding Loss Window : 1.058738551391039\n",
      "Step 4551 | Sliding Loss Window : 1.044460573313102\n",
      "Step 4601 | Sliding Loss Window : 1.042659421386516\n",
      "Step 4651 | Sliding Loss Window : 1.144011844859053\n",
      "Step 4701 | Sliding Loss Window : 1.108286619691094\n",
      "Step 4751 | Sliding Loss Window : 1.0807214789719701\n"
     ]
    }
   ],
   "source": [
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ, get_circ_params\n",
    "from train_circ import train_qnn, mse_vec_loss\n",
    "\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset = 'mnist_4'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "num_params = 32\n",
    "\n",
    "for i in range(50, 65):\n",
    "    curr_dir = './experiment_data/mnist_4/trained_circuits/circ_{}'.format(i + 1)\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    circ = create_gate_circ(qml.device('lightning.qubit', wires=num_qubits), circ_gates, gate_params, inputs_bounds,\n",
    "                                                    weights_bounds, [0, 1], 'exp')    \n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    for j in range(1):\n",
    "        curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "        if os.path.exists(curr_train_dir):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(curr_train_dir)\n",
    "    \n",
    "        \n",
    "        info = train_qnn(circ, x_train, y_train, x_test, y_test, [num_params], 16000, 0.05, 1, mse_vec_loss, verbosity=127300, \n",
    "                                                                                        loss_window=50, init_params=None, \n",
    "                                                                                        acc_thres=1.1, shuffle=True, print_loss=50)\n",
    "        \n",
    "        val_exps = [circ(x_test[i], info[-1][-1]) for i in range(len(x_test))]\n",
    "        val_loss = np.array([mse_vec_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "        acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "        \n",
    "        np.savetxt(curr_train_dir + '/params_{}.txt'.format(j + 1), info[-1])\n",
    "        np.savetxt(curr_train_dir + '/losses_{}.txt'.format(j + 1), info[0])\n",
    "        \n",
    "        losses_list.append(val_loss)\n",
    "        accs_list.append(acc)\n",
    "        \n",
    "    np.savetxt(curr_dir + '/val_losses.txt', losses_list)\n",
    "    np.savetxt(curr_dir + '/accs.txt', accs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-10-24 04:06:24,690: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2173890654772193 1.4748181733456003 201\n",
      "1.0869884485243808 1.3934497278049196 202\n"
     ]
    }
   ],
   "source": [
    "from create_noise_models import noisy_dev_from_backend\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ, get_circ_params\n",
    "from train_circ import train_qnn, mse_loss\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset = 'fmnist_4'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "num_params = 18\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "for i in range(200, 400):\n",
    "    curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                    weights_bounds, [0], 'exp')\n",
    "    \n",
    "    \n",
    "    noiseless_losses = np.genfromtxt(curr_dir + '/val_losses.txt')\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    curr_dev_dir = curr_dir + '/' + device_name\n",
    "\n",
    "#     if not os.path.exists(curr_dev_dir + '/accs_inference_only.txt'):\n",
    "    if True:\n",
    "        if not os.path.exists(curr_dev_dir):\n",
    "            os.mkdir(curr_dev_dir)\n",
    "\n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "            curr_params = np.genfromtxt(curr_train_dir + '/params_{}.txt'.format(j + 1))[-1]\n",
    "\n",
    "            val_exps = [circ(x_test[i], curr_params) for i in range(len(x_test))]\n",
    "            val_loss = np.array([mse_vec_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "            acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "#             acc = np.mean(val_loss < 1)\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        print(np.mean(noiseless_losses), np.mean(losses_list), i + 1)\n",
    "\n",
    "        np.save(curr_dev_dir + '/val_losses_inference_only.npy', losses_list)\n",
    "        np.savetxt(curr_dev_dir + '/accs_inference_only.txt', accs_list)\n",
    "    else:\n",
    "        print(i)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
