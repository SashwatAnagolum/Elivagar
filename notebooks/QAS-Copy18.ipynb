{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric-based Quantum Architecture Search\n",
    "\n",
    "## Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "from itertools import combinations, product\n",
    "from functools import reduce\n",
    "import copy\n",
    "\n",
    "import pennylane as qml\n",
    "from pennylane.templates.layers import RandomLayers\n",
    "# from pennylane import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.13.0-cp37-cp37m-manylinux1_x86_64.whl (890.2 MB)\n",
      "Collecting torchquantum\n",
      "  Using cached torchquantum-0.1.4-py3-none-any.whl (119 kB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (3.10.0.0)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.36.2)\n",
      "Requirement already satisfied: pathos>=0.2.7 in /usr/local/lib/python3.7/site-packages (from torchquantum) (0.2.7)\n",
      "Collecting tqdm>=4.56.0\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.7/site-packages (from torchquantum) (3.5.3)\n",
      "Collecting torchvision>=0.9.0.dev20210130\n",
      "  Using cached torchvision-0.14.0-cp37-cp37m-manylinux1_x86_64.whl (24.3 MB)\n",
      "Collecting torchpack>=0.3.0\n",
      "  Using cached torchpack-0.3.1-py3-none-any.whl (34 kB)\n",
      "Collecting numpy>=1.19.2\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Collecting qiskit==0.32.1\n",
      "  Using cached qiskit-0.32.1-py3-none-any.whl\n",
      "Collecting pylatexenc>=2.10\n",
      "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
      "Collecting qiskit-ignis==0.6.0\n",
      "  Using cached qiskit_ignis-0.6.0-py3-none-any.whl (207 kB)\n",
      "Collecting qiskit-ibmq-provider==0.18.1\n",
      "  Using cached qiskit_ibmq_provider-0.18.1-py3-none-any.whl (237 kB)\n",
      "Collecting qiskit-aer==0.9.1\n",
      "  Using cached qiskit_aer-0.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (17.9 MB)\n",
      "Collecting qiskit-terra==0.18.3\n",
      "  Using cached qiskit_terra-0.18.3-cp37-cp37m-manylinux2010_x86_64.whl (6.1 MB)\n",
      "Collecting qiskit-aqua==0.9.5\n",
      "  Using cached qiskit_aqua-0.9.5-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aer==0.9.1->qiskit==0.32.1->torchquantum) (1.5.2)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (5.7.2)\n",
      "Requirement already satisfied: h5py<3.3.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2.10.0)\n",
      "Requirement already satisfied: retworkx>=0.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (0.11.0)\n",
      "Collecting yfinance>=0.1.62\n",
      "  Using cached yfinance-0.1.84-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.10.1)\n",
      "Collecting dlx<=1.0.4\n",
      "  Using cached dlx-1.0.4-py3-none-any.whl\n",
      "Collecting fastdtw<=0.3.4\n",
      "  Using cached fastdtw-0.3.4-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting quandl\n",
      "  Using cached Quandl-3.7.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.1.0)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (0.23.0)\n",
      "Collecting docplex>=2.21.207\n",
      "  Using cached docplex-2.23.222-py3-none-any.whl\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.25.11)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.24.0)\n",
      "Requirement already satisfied: websocket-client>=1.0.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.4.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.8.2)\n",
      "Collecting python-constraint>=1.4\n",
      "  Using cached python_constraint-1.4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: symengine>0.7 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (0.9.2)\n",
      "Requirement already satisfied: tweedledum<2.0,>=1.1 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (1.1.1)\n",
      "Collecting fastjsonschema>=2.10\n",
      "  Using cached fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n",
      "Collecting jsonschema>=2.6\n",
      "  Using cached jsonschema-4.17.0-py3-none-any.whl (83 kB)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (3.11)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (0.3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from docplex>=2.21.207->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.16.0)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.19.2-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (4.5.0)\n",
      "Collecting importlib-resources>=1.4.0\n",
      "  Using cached importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/site-packages (from jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (21.2.0)\n",
      "Collecting pkgutil-resolve-name>=1.3.10\n",
      "  Using cached pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->qiskit-terra==0.18.3->qiskit==0.32.1->torchquantum) (3.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (20.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (8.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (4.37.2)\n",
      "Requirement already satisfied: pox>=0.2.9 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (0.2.9)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (1.6.6.3)\n",
      "Requirement already satisfied: multiprocess>=0.70.11 in /usr/local/lib/python3.7/site-packages (from pathos>=0.2.7->torchquantum) (0.70.11.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.10)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (3.4.7)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.5.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.18.1->qiskit==0.32.1->torchquantum) (2.20)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=0.20.0->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/site-packages (from sympy>=1.3->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.2.1)\n",
      "Collecting multimethod\n",
      "  Using cached multimethod-1.9-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (0.10.2)\n",
      "Collecting loguru\n",
      "  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Collecting tensorpack\n",
      "  Using cached tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (5.4.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (2.5.0)\n",
      "Collecting lxml>=4.5.1\n",
      "  Using cached lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
      "Collecting multitasking>=0.0.7\n",
      "  Using cached multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
      "Collecting requests>=2.19\n",
      "  Using cached requests-2.28.1-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.7/site-packages (from yfinance>=0.1.62->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (1.4.4)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas->qiskit-aqua==0.9.5->qiskit==0.32.1->torchquantum) (2021.1)\n",
      "Collecting charset-normalizer<3,>=2\n",
      "  Using cached charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
      "Collecting more-itertools\n",
      "  Using cached more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "Collecting inflection>=0.3.1\n",
      "  Using cached inflection-0.5.1-py2.py3-none-any.whl (9.5 kB)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.38.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.17.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.30.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.4.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.0.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.10.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (3.1.1)\n",
      "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (23.2.1)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (0.8.9)\n",
      "Collecting msgpack>=0.5.2\n",
      "  Using cached msgpack-1.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (1.1.0)\n",
      "Collecting msgpack-numpy>=0.4.4.2\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pyrsistent, pkgutil-resolve-name, numpy, importlib-resources, charset-normalizer, requests, python-constraint, nvidia-cublas-cu11, jsonschema, fastjsonschema, qiskit-terra, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, multitasking, msgpack, more-itertools, lxml, inflection, yfinance, tqdm, torch, quandl, qiskit-ignis, msgpack-numpy, fastdtw, docplex, dlx, torchvision, tensorpack, qiskit-ibmq-provider, qiskit-aqua, qiskit-aer, multimethod, loguru, torchpack, qiskit, pylatexenc, torchquantum\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "  Attempting uninstall: qiskit-terra\n",
      "    Found existing installation: qiskit-terra 0.21.2\n",
      "    Uninstalling qiskit-terra-0.21.2:\n",
      "      Successfully uninstalled qiskit-terra-0.21.2\n",
      "  Attempting uninstall: qiskit-ibmq-provider\n",
      "    Found existing installation: qiskit-ibmq-provider 0.19.2\n",
      "    Uninstalling qiskit-ibmq-provider-0.19.2:\n",
      "      Successfully uninstalled qiskit-ibmq-provider-0.19.2\n",
      "  Attempting uninstall: qiskit-aer\n",
      "    Found existing installation: qiskit-aer 0.11.0\n",
      "    Uninstalling qiskit-aer-0.11.0:\n",
      "      Successfully uninstalled qiskit-aer-0.11.0\n",
      "  Attempting uninstall: qiskit\n",
      "    Found existing installation: qiskit 0.38.0\n",
      "    Uninstalling qiskit-0.38.0:\n",
      "      Successfully uninstalled qiskit-0.38.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu 2.3.2 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\n",
      "mthree 1.0.0 requires qiskit-ibmq-provider>=0.19.2, but you have qiskit-ibmq-provider 0.18.1 which is incompatible.\n",
      "mthree 1.0.0 requires qiskit-terra>=0.21, but you have qiskit-terra 0.18.3 which is incompatible.\u001b[0m\n",
      "Successfully installed charset-normalizer-2.1.1 dlx-1.0.4 docplex-2.23.222 fastdtw-0.3.4 fastjsonschema-2.16.2 importlib-resources-5.10.0 inflection-0.5.1 jsonschema-4.17.0 loguru-0.6.0 lxml-4.9.1 more-itertools-9.0.0 msgpack-1.0.4 msgpack-numpy-0.4.8 multimethod-1.9 multitasking-0.0.11 numpy-1.21.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pkgutil-resolve-name-1.3.10 pylatexenc-2.10 pyrsistent-0.19.2 python-constraint-1.4.0 qiskit-0.32.1 qiskit-aer-0.9.1 qiskit-aqua-0.9.5 qiskit-ibmq-provider-0.18.1 qiskit-ignis-0.6.0 qiskit-terra-0.18.3 quandl-3.7.0 requests-2.28.1 tensorpack-0.11 torch-1.13.0 torchpack-0.3.1 torchquantum-0.1.4 torchvision-0.14.0 tqdm-4.64.1 yfinance-0.1.84\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchquantum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Using cached PennyLane-0.25.1-py3-none-any.whl (1.0 MB)\n",
      "Collecting pennylane-qiskit\n",
      "  Using cached PennyLane_qiskit-0.24.0-py3-none-any.whl (34 kB)\n",
      "Collecting qiskit\n",
      "  Using cached qiskit-0.38.0-py3-none-any.whl\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from pennylane) (1.5.2)\n",
      "Collecting retworkx\n",
      "  Using cached retworkx-0.11.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "Requirement already satisfied: cachetools in /usr/local/lib/python3.7/site-packages (from pennylane) (4.2.2)\n",
      "Collecting networkx\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting autoray>=0.3.1\n",
      "  Using cached autoray-0.3.2-py3-none-any.whl (36 kB)\n",
      "Collecting semantic-version>=2.7\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Collecting autograd\n",
      "  Using cached autograd-1.4-py3-none-any.whl (48 kB)\n",
      "Collecting appdirs\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from pennylane) (1.18.5)\n",
      "Collecting pennylane-lightning>=0.25\n",
      "  Using cached PennyLane_Lightning-0.25.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.6 MB)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
      "Collecting mthree>=0.17\n",
      "  Using cached mthree-1.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Collecting qiskit-ibmq-provider==0.19.2\n",
      "  Using cached qiskit_ibmq_provider-0.19.2-py3-none-any.whl (240 kB)\n",
      "Collecting qiskit-terra==0.21.2\n",
      "  Using cached qiskit_terra-0.21.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.7 MB)\n",
      "Collecting qiskit-aer==0.11.0\n",
      "  Using cached qiskit_aer-0.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.2 MB)\n",
      "Collecting websockets>=10.0\n",
      "  Using cached websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.8.2)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (2.24.0)\n",
      "Collecting requests-ntlm>=1.1.0\n",
      "  Using cached requests_ntlm-1.1.0-py2.py3-none-any.whl (5.7 kB)\n",
      "Collecting websocket-client>=1.0.1\n",
      "  Using cached websocket_client-1.4.1-py3-none-any.whl (55 kB)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit) (1.25.11)\n",
      "Collecting stevedore>=3.0.0\n",
      "  Using cached stevedore-3.5.0-py3-none-any.whl (49 kB)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (5.7.2)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (0.3.3)\n",
      "Collecting symengine>=0.9\n",
      "  Using cached symengine-0.9.2-cp37-cp37m-manylinux2010_x86_64.whl (37.5 MB)\n",
      "Collecting sympy>=1.3\n",
      "  Using cached sympy-1.10.1-py3-none-any.whl (6.4 MB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.2->qiskit) (3.10.0.0)\n",
      "Collecting shared-memory38\n",
      "  Using cached shared_memory38-0.1.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (25 kB)\n",
      "Collecting ply>=3.10\n",
      "  Using cached ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "Collecting tweedledum<2.0,>=1.1\n",
      "  Using cached tweedledum-1.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (943 kB)\n",
      "Collecting cython>=0.29\n",
      "  Using cached Cython-0.29.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
      "Collecting orjson>=3.0.0\n",
      "  Using cached orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.8.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit) (2.10)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (3.4.7)\n",
      "Collecting ntlm-auth>=1.0.2\n",
      "  Using cached ntlm_auth-1.5.0-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit) (2.20)\n",
      "Collecting pbr!=2.1.0,>=2.0.0\n",
      "  Using cached pbr-5.10.0-py2.py3-none-any.whl (112 kB)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/site-packages (from stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (4.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.21.2->qiskit) (3.4.1)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.2.1-py3-none-any.whl (532 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib) (8.2.0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Using cached fonttools-4.37.2-py3-none-any.whl (959 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Collecting future>=0.15.2\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Installing collected packages: pbr, mpmath, tweedledum, sympy, symengine, stevedore, shared-memory38, retworkx, ply, ntlm-auth, websockets, websocket-client, requests-ntlm, qiskit-terra, ninja, future, toml, semantic-version, qiskit-ibmq-provider, qiskit-aer, pennylane-lightning, orjson, networkx, cython, autoray, autograd, appdirs, qiskit, pennylane, mthree, kiwisolver, fonttools, cycler, pennylane-qiskit, matplotlib\n",
      "Successfully installed appdirs-1.4.4 autograd-1.4 autoray-0.3.2 cycler-0.11.0 cython-0.29.32 fonttools-4.37.2 future-0.18.2 kiwisolver-1.4.4 matplotlib-3.5.3 mpmath-1.2.1 mthree-1.0.0 networkx-2.6.3 ninja-1.10.2.3 ntlm-auth-1.5.0 orjson-3.8.0 pbr-5.10.0 pennylane-0.25.1 pennylane-lightning-0.25.1 pennylane-qiskit-0.24.0 ply-3.11 qiskit-0.38.0 qiskit-aer-0.11.0 qiskit-ibmq-provider-0.19.2 qiskit-terra-0.21.2 requests-ntlm-1.1.0 retworkx-0.11.0 semantic-version-2.10.0 shared-memory38-0.1.2 stevedore-3.5.0 symengine-0.9.2 sympy-1.10.1 toml-0.10.2 tweedledum-1.1.1 websocket-client-1.4.1 websockets-10.3\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pennylane pennylane-qiskit qiskit matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n",
      "Collecting torchquantum\n",
      "  Using cached torchquantum-0.1.0-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch) (3.10.0.0)\n",
      "Collecting tqdm>=4.56.0\n",
      "  Using cached tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Collecting torchpack>=0.3.0\n",
      "  Using cached torchpack-0.3.1-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: qiskit>=0.24.0 in /usr/local/lib/python3.7/site-packages (from torchquantum) (0.37.1)\n",
      "Collecting numpy>=1.19.2\n",
      "  Using cached numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.7/site-packages (from torchquantum) (3.5.0)\n",
      "Collecting torchvision>=0.9.0.dev20210130\n",
      "  Using cached torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl (19.1 MB)\n",
      "Requirement already satisfied: setuptools>=52.0.0 in /usr/local/lib/python3.7/site-packages (from torchquantum) (57.0.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (4.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (20.9)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (8.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (1.3.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (6.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib>=3.3.2->torchquantum) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.2->torchquantum) (1.16.0)\n",
      "Requirement already satisfied: qiskit-aer==0.10.4 in /usr/local/lib/python3.7/site-packages (from qiskit>=0.24.0->torchquantum) (0.10.4)\n",
      "Requirement already satisfied: qiskit-terra==0.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit>=0.24.0->torchquantum) (0.21.1)\n",
      "Requirement already satisfied: qiskit-ibmq-provider==0.19.2 in /usr/local/lib/python3.7/site-packages (from qiskit>=0.24.0->torchquantum) (0.19.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-aer==0.10.4->qiskit>=0.24.0->torchquantum) (1.5.2)\n",
      "Requirement already satisfied: requests-ntlm>=1.1.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (1.1.0)\n",
      "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (2.24.0)\n",
      "Requirement already satisfied: websocket-client>=1.0.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (1.3.3)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (1.25.11)\n",
      "Requirement already satisfied: websockets>=10.0 in /usr/local/lib/python3.7/site-packages (from qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (10.3)\n",
      "Requirement already satisfied: ply>=3.10 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (3.11)\n",
      "Requirement already satisfied: retworkx>=0.11.0 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (0.11.0)\n",
      "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (0.3.3)\n",
      "Requirement already satisfied: tweedledum<2.0,>=1.1 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (1.1.1)\n",
      "Requirement already satisfied: symengine>=0.9 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (0.9.2)\n",
      "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (3.5.0)\n",
      "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (5.7.2)\n",
      "Requirement already satisfied: sympy>=1.3 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (1.10.1)\n",
      "Requirement already satisfied: shared-memory38 in /usr/local/lib/python3.7/site-packages (from qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (0.1.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (2021.5.30)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests>=2.19->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (3.0.4)\n",
      "Requirement already satisfied: ntlm-auth>=1.0.2 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (1.5.0)\n",
      "Requirement already satisfied: cryptography>=1.3 in /usr/local/lib/python3.7/site-packages (from requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (3.4.7)\n",
      "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/site-packages (from cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=1.3->requests-ntlm>=1.1.0->qiskit-ibmq-provider==0.19.2->qiskit>=0.24.0->torchquantum) (2.20)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib>=3.3.2->torchquantum) (1.2.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.7.0 in /usr/local/lib/python3.7/site-packages (from stevedore>=3.0.0->qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (4.11.1)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from stevedore>=3.0.0->qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (5.10.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=1.7.0->stevedore>=3.0.0->qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (3.4.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/site-packages (from sympy>=1.3->qiskit-terra==0.21.1->qiskit>=0.24.0->torchquantum) (1.2.1)\n",
      "Collecting multimethod\n",
      "  Using cached multimethod-1.8-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (0.10.2)\n",
      "Collecting loguru\n",
      "  Using cached loguru-0.6.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (5.4.1)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (2.5.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from torchpack>=0.3.0->torchquantum) (2.10.0)\n",
      "Collecting tensorpack\n",
      "  Using cached tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.4.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.3.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (3.17.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.38.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.30.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/site-packages (from tensorboard->torchpack>=0.3.0->torchquantum) (1.8.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->torchpack>=0.3.0->torchquantum) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->torchpack>=0.3.0->torchquantum) (3.1.1)\n",
      "Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (22.3.0)\n",
      "Collecting msgpack-numpy>=0.4.4.2\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (1.1.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/site-packages (from tensorpack->torchpack>=0.3.0->torchquantum) (0.8.9)\n",
      "Collecting msgpack>=0.5.2\n",
      "  Using cached msgpack-1.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "Installing collected packages: numpy, msgpack, tqdm, torch, msgpack-numpy, torchvision, tensorpack, multimethod, loguru, torchpack, torchquantum\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.5\n",
      "    Uninstalling numpy-1.18.5:\n",
      "      Successfully uninstalled numpy-1.18.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-cpu 2.3.2 requires numpy<1.19.0,>=1.16.0, but you have numpy 1.21.6 which is incompatible.\u001b[0m\n",
      "Successfully installed loguru-0.6.0 msgpack-1.0.4 msgpack-numpy-0.4.8 multimethod-1.8 numpy-1.21.6 tensorpack-0.11 torch-1.12.1 torchpack-0.3.1 torchquantum-0.1.0 torchvision-0.13.1 tqdm-4.64.0\n",
      "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.2.2 is available.\n",
      "You should consider upgrading via the '/usr/local/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchquantum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-RDM separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reduced_similarity(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, params, data, meas_qubits=[0]):\n",
    "    circ = create_batched_gate_circ(qml.device('lightning.qubit', wires=num_qubits), circ_gates, gate_params, inputs_bounds,\n",
    "                                        weights_bounds, meas_qubits, 'matrix') \n",
    "\n",
    "    num_data = len(data)\n",
    "    traces = []\n",
    "    circ_fids = np.zeros((num_data, num_data))\n",
    "\n",
    "    circ_dms = circ(data, params)\n",
    "    \n",
    "    for i in range(num_data):\n",
    "        traces.append(np.trace(circ_dms[i]))\n",
    "\n",
    "    for s1 in range(num_data):\n",
    "        for s2 in range(s1 + 1, num_data):\n",
    "            trace_1 = traces[s1]\n",
    "            trace_2 = traces[s2]\n",
    "            fid_trace = np.trace(np.matmul(circ_dms[s1], circ_dms[s2]))\n",
    "\n",
    "            curr_score = ((fid_trace) ** 2 / (trace_1 * trace_2)).real\n",
    "\n",
    "            circ_fids[s1, s2] = curr_score\n",
    "            circ_fids[s2, s1] = curr_score  \n",
    "            \n",
    "    circ_fids += np.eye(num_data)\n",
    "            \n",
    "    return circ_fids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-23 05:01:10.989900: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "2022-09-23 05:01:10.990005: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\n",
      "2022-09-23 05:01:11.016255: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\n",
      "(1600, 4, 4) (400, 4, 4)\n",
      "subnet: [151462], expert_idx: 0\n",
      "2022-09-23 05:01:22.051044: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX512F\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 05:01:22.076130: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2999995000 Hz\n",
      "2022-09-23 05:01:22.076520: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563899a0be60 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-09-23 05:01:22.076549: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-09-23 05:01:22.076663: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "[2022-09-23 05:01:22.180 tensorflow-2-3-cpu-p-ml-c5-2xlarge-75506731229c09a72ceb54dc8a58:11290 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-23 05:01:22.226 tensorflow-2-3-cpu-p-ml-c5-2xlarge-75506731229c09a72ceb54dc8a58:11290 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "subnet: [151462], loss: [1.17410142], acc: 0.5, samples: 32\n",
      "subnet: [276697], expert_idx: 0\n",
      "subnet: [276697], loss: [1.34225341], acc: 0.40625, samples: 32\n",
      "subnet: [112480], expert_idx: 0\n",
      "subnet: [112480], loss: [0.88976809], acc: 0.65625, samples: 32\n",
      "subnet: [67121], expert_idx: 0\n",
      "subnet: [67121], loss: [1.33392681], acc: 0.28125, samples: 32\n",
      "subnet: [93660], expert_idx: 0\n",
      "subnet: [93660], loss: [1.08150835], acc: 0.5, samples: 32\n",
      "subnet: [219789], expert_idx: 0\n",
      "subnet: [219789], loss: [1.11055845], acc: 0.5, samples: 32\n",
      "subnet: [318627], expert_idx: 0\n",
      "subnet: [318627], loss: [1.25951708], acc: 0.28125, samples: 32\n",
      "subnet: [218620], expert_idx: 0\n",
      "subnet: [218620], loss: [1.4194441], acc: 0.34375, samples: 32\n",
      "subnet: [49716], expert_idx: 0\n",
      "subnet: [49716], loss: [0.97759007], acc: 0.59375, samples: 32\n",
      "subnet: [102074], expert_idx: 0\n",
      "subnet: [102074], loss: [1.36068014], acc: 0.3125, samples: 32\n",
      "subnet: [281895], expert_idx: 0\n",
      "subnet: [281895], loss: [1.26155673], acc: 0.4375, samples: 32\n",
      "subnet: [208153], expert_idx: 0\n",
      "subnet: [208153], loss: [1.14065936], acc: 0.5, samples: 32\n",
      "subnet: [61044], expert_idx: 0\n",
      "subnet: [61044], loss: [1.12896248], acc: 0.46875, samples: 32\n",
      "subnet: [126681], expert_idx: 0\n",
      "subnet: [126681], loss: [1.02424025], acc: 0.5625, samples: 32\n",
      "subnet: [165327], expert_idx: 0\n",
      "subnet: [165327], loss: [1.13815839], acc: 0.40625, samples: 32\n",
      "subnet: [146160], expert_idx: 0\n",
      "subnet: [146160], loss: [1.00420656], acc: 0.5625, samples: 32\n",
      "subnet: [276550], expert_idx: 0\n",
      "subnet: [276550], loss: [1.12279705], acc: 0.53125, samples: 32\n",
      "subnet: [137920], expert_idx: 0\n",
      "subnet: [137920], loss: [1.00081875], acc: 0.53125, samples: 32\n",
      "subnet: [77512], expert_idx: 0\n",
      "subnet: [77512], loss: [1.29545464], acc: 0.375, samples: 32\n",
      "subnet: [174659], expert_idx: 0\n",
      "subnet: [174659], loss: [1.21311197], acc: 0.40625, samples: 32\n",
      "subnet: [149283], expert_idx: 0\n",
      "subnet: [149283], loss: [1.07007035], acc: 0.53125, samples: 32\n",
      "subnet: [128744], expert_idx: 0\n",
      "subnet: [128744], loss: [1.27899443], acc: 0.40625, samples: 32\n",
      "subnet: [125412], expert_idx: 0\n",
      "subnet: [125412], loss: [1.31659492], acc: 0.4375, samples: 32\n",
      "subnet: [237795], expert_idx: 0\n",
      "subnet: [237795], loss: [1.14393943], acc: 0.5, samples: 32\n",
      "subnet: [225680], expert_idx: 0\n",
      "subnet: [225680], loss: [1.00062643], acc: 0.625, samples: 32\n",
      "subnet: [213313], expert_idx: 0\n",
      "subnet: [213313], loss: [1.27089448], acc: 0.40625, samples: 32\n",
      "subnet: [69206], expert_idx: 0\n",
      "subnet: [69206], loss: [1.10459986], acc: 0.5, samples: 32\n",
      "subnet: [329038], expert_idx: 0\n",
      "subnet: [329038], loss: [0.92304804], acc: 0.65625, samples: 32\n",
      "subnet: [148286], expert_idx: 0\n",
      "subnet: [148286], loss: [1.10009834], acc: 0.4375, samples: 32\n",
      "subnet: [1657], expert_idx: 0\n",
      "subnet: [1657], loss: [1.11247005], acc: 0.4375, samples: 32\n",
      "subnet: [217616], expert_idx: 0\n",
      "subnet: [217616], loss: [0.88434517], acc: 0.625, samples: 32\n",
      "subnet: [132839], expert_idx: 0\n",
      "subnet: [132839], loss: [1.09405096], acc: 0.5625, samples: 32\n",
      "subnet: [44688], expert_idx: 0\n",
      "subnet: [44688], loss: [0.91506923], acc: 0.65625, samples: 32\n",
      "subnet: [316559], expert_idx: 0\n",
      "subnet: [316559], loss: [1.16258669], acc: 0.59375, samples: 32\n",
      "subnet: [17702], expert_idx: 0\n",
      "subnet: [17702], loss: [0.95401192], acc: 0.65625, samples: 32\n",
      "subnet: [278077], expert_idx: 0\n",
      "subnet: [278077], loss: [1.14113372], acc: 0.5625, samples: 32\n",
      "subnet: [42218], expert_idx: 0\n",
      "subnet: [42218], loss: [1.02764517], acc: 0.53125, samples: 32\n",
      "subnet: [165903], expert_idx: 0\n",
      "subnet: [165903], loss: [1.04198115], acc: 0.5625, samples: 32\n",
      "subnet: [268291], expert_idx: 0\n",
      "subnet: [268291], loss: [1.39672997], acc: 0.46875, samples: 32\n",
      "subnet: [262854], expert_idx: 0\n",
      "subnet: [262854], loss: [1.03880471], acc: 0.4375, samples: 32\n",
      "subnet: [137251], expert_idx: 0\n",
      "subnet: [137251], loss: [1.1972707], acc: 0.34375, samples: 32\n",
      "subnet: [291550], expert_idx: 0\n",
      "subnet: [291550], loss: [1.16931224], acc: 0.5, samples: 32\n",
      "subnet: [65398], expert_idx: 0\n",
      "subnet: [65398], loss: [1.29955388], acc: 0.40625, samples: 32\n",
      "subnet: [235082], expert_idx: 0\n",
      "subnet: [235082], loss: [1.00652942], acc: 0.59375, samples: 32\n",
      "subnet: [49589], expert_idx: 0\n",
      "subnet: [49589], loss: [0.99024475], acc: 0.5625, samples: 32\n",
      "subnet: [242143], expert_idx: 0\n",
      "subnet: [242143], loss: [0.96984625], acc: 0.65625, samples: 32\n",
      "subnet: [191223], expert_idx: 0\n",
      "subnet: [191223], loss: [1.0400572], acc: 0.46875, samples: 32\n",
      "subnet: [108560], expert_idx: 0\n",
      "subnet: [108560], loss: [0.94793743], acc: 0.5625, samples: 32\n",
      "subnet: [25626], expert_idx: 0\n",
      "subnet: [25626], loss: [1.00085493], acc: 0.4375, samples: 32\n",
      "subnet: [73299], expert_idx: 0\n",
      "subnet: [73299], loss: [0.92573939], acc: 0.65625, samples: 32\n",
      "subnet: [47421], expert_idx: 0\n",
      "subnet: [47421], loss: [0.91356361], acc: 0.59375, samples: 32\n",
      "subnet: [302903], expert_idx: 0\n",
      "subnet: [302903], loss: [0.8716726], acc: 0.6875, samples: 32\n",
      "subnet: [50626], expert_idx: 0\n",
      "subnet: [50626], loss: [1.00130902], acc: 0.59375, samples: 32\n",
      "subnet: [240463], expert_idx: 0\n",
      "subnet: [240463], loss: [1.00745634], acc: 0.65625, samples: 32\n",
      "subnet: [203184], expert_idx: 0\n",
      "subnet: [203184], loss: [1.14553395], acc: 0.40625, samples: 32\n",
      "subnet: [87387], expert_idx: 0\n",
      "subnet: [87387], loss: [1.00079011], acc: 0.46875, samples: 32\n",
      "subnet: [82215], expert_idx: 0\n",
      "subnet: [82215], loss: [1.42543714], acc: 0.375, samples: 32\n",
      "subnet: [114239], expert_idx: 0\n",
      "subnet: [114239], loss: [1.25210506], acc: 0.46875, samples: 32\n",
      "subnet: [90165], expert_idx: 0\n",
      "subnet: [90165], loss: [1.04107392], acc: 0.5, samples: 32\n",
      "subnet: [323091], expert_idx: 0\n",
      "subnet: [323091], loss: [0.95190522], acc: 0.625, samples: 32\n",
      "subnet: [109102], expert_idx: 0\n",
      "subnet: [109102], loss: [1.26155673], acc: 0.4375, samples: 32\n",
      "subnet: [46457], expert_idx: 0\n",
      "subnet: [46457], loss: [1.12132463], acc: 0.5625, samples: 32\n",
      "subnet: [49624], expert_idx: 0\n",
      "subnet: [49624], loss: [1.06605842], acc: 0.53125, samples: 32\n",
      "subnet: [120250], expert_idx: 0\n",
      "subnet: [120250], loss: [0.99434612], acc: 0.5, samples: 32\n",
      "subnet: [248031], expert_idx: 0\n",
      "subnet: [248031], loss: [1.21117134], acc: 0.34375, samples: 32\n",
      "subnet: [64155], expert_idx: 0\n",
      "subnet: [64155], loss: [1.15077153], acc: 0.46875, samples: 32\n",
      "subnet: [247473], expert_idx: 0\n",
      "subnet: [247473], loss: [0.9935029], acc: 0.59375, samples: 32\n",
      "subnet: [316501], expert_idx: 0\n",
      "subnet: [316501], loss: [1.24464068], acc: 0.46875, samples: 32\n",
      "subnet: [100725], expert_idx: 0\n",
      "subnet: [100725], loss: [0.84844424], acc: 0.6875, samples: 32\n",
      "subnet: [158259], expert_idx: 0\n",
      "subnet: [158259], loss: [1.08060583], acc: 0.53125, samples: 32\n",
      "subnet: [14273], expert_idx: 0\n",
      "subnet: [14273], loss: [1.10032654], acc: 0.5625, samples: 32\n",
      "subnet: [275608], expert_idx: 0\n",
      "subnet: [275608], loss: [1.12441657], acc: 0.53125, samples: 32\n",
      "subnet: [228894], expert_idx: 0\n",
      "subnet: [228894], loss: [1.27441209], acc: 0.4375, samples: 32\n",
      "subnet: [166518], expert_idx: 0\n",
      "subnet: [166518], loss: [1.04374239], acc: 0.5625, samples: 32\n",
      "subnet: [52101], expert_idx: 0\n",
      "subnet: [52101], loss: [0.92955009], acc: 0.65625, samples: 32\n",
      "subnet: [156728], expert_idx: 0\n",
      "subnet: [156728], loss: [0.9218904], acc: 0.6875, samples: 32\n",
      "subnet: [294872], expert_idx: 0\n",
      "subnet: [294872], loss: [1.13380416], acc: 0.59375, samples: 32\n",
      "subnet: [58437], expert_idx: 0\n",
      "subnet: [58437], loss: [1.44132242], acc: 0.3125, samples: 32\n",
      "subnet: [169068], expert_idx: 0\n",
      "subnet: [169068], loss: [1.46482989], acc: 0.34375, samples: 32\n",
      "subnet: [55597], expert_idx: 0\n",
      "subnet: [55597], loss: [0.84532326], acc: 0.65625, samples: 32\n",
      "subnet: [32003], expert_idx: 0\n",
      "subnet: [32003], loss: [1.15520289], acc: 0.40625, samples: 32\n",
      "subnet: [244442], expert_idx: 0\n",
      "subnet: [244442], loss: [1.03801151], acc: 0.5, samples: 32\n",
      "subnet: [162219], expert_idx: 0\n",
      "subnet: [162219], loss: [1.04644831], acc: 0.53125, samples: 32\n",
      "subnet: [309945], expert_idx: 0\n",
      "subnet: [309945], loss: [0.87235427], acc: 0.5625, samples: 32\n",
      "subnet: [199676], expert_idx: 0\n",
      "subnet: [199676], loss: [1.03454141], acc: 0.53125, samples: 32\n",
      "subnet: [244641], expert_idx: 0\n",
      "subnet: [244641], loss: [0.97213506], acc: 0.59375, samples: 32\n",
      "subnet: [259442], expert_idx: 0\n",
      "subnet: [259442], loss: [1.12282254], acc: 0.59375, samples: 32\n",
      "subnet: [322841], expert_idx: 0\n",
      "subnet: [322841], loss: [1.01264683], acc: 0.5, samples: 32\n",
      "subnet: [157402], expert_idx: 0\n",
      "subnet: [157402], loss: [1.00363649], acc: 0.53125, samples: 32\n",
      "subnet: [230121], expert_idx: 0\n",
      "subnet: [230121], loss: [0.89267795], acc: 0.71875, samples: 32\n",
      "subnet: [183641], expert_idx: 0\n",
      "subnet: [183641], loss: [1.317869], acc: 0.4375, samples: 32\n",
      "subnet: [77447], expert_idx: 0\n",
      "subnet: [77447], loss: [0.99174462], acc: 0.65625, samples: 32\n",
      "subnet: [157887], expert_idx: 0\n",
      "subnet: [157887], loss: [1.02743061], acc: 0.53125, samples: 32\n",
      "subnet: [52872], expert_idx: 0\n",
      "subnet: [52872], loss: [0.8846685], acc: 0.71875, samples: 32\n",
      "subnet: [39283], expert_idx: 0\n",
      "subnet: [39283], loss: [1.004432], acc: 0.5625, samples: 32\n",
      "subnet: [57165], expert_idx: 0\n",
      "subnet: [57165], loss: [1.10176219], acc: 0.53125, samples: 32\n",
      "subnet: [106927], expert_idx: 0\n",
      "subnet: [106927], loss: [1.13251392], acc: 0.5625, samples: 32\n",
      "subnet: [179290], expert_idx: 0\n",
      "subnet: [179290], loss: [1.24628726], acc: 0.4375, samples: 32\n",
      "subnet: [141665], expert_idx: 0\n",
      "subnet: [141665], loss: [1.10305906], acc: 0.5, samples: 32\n",
      "subnet: [285325], expert_idx: 0\n",
      "subnet: [285325], loss: [1.1019878], acc: 0.34375, samples: 32\n",
      "subnet: [205472], expert_idx: 0\n",
      "subnet: [205472], loss: [0.97640463], acc: 0.53125, samples: 32\n",
      "subnet: [163275], expert_idx: 0\n",
      "subnet: [163275], loss: [1.00999046], acc: 0.5, samples: 32\n",
      "subnet: [175602], expert_idx: 0\n",
      "subnet: [175602], loss: [1.53974333], acc: 0.40625, samples: 32\n",
      "subnet: [66891], expert_idx: 0\n",
      "subnet: [66891], loss: [1.28461077], acc: 0.4375, samples: 32\n",
      "subnet: [60421], expert_idx: 0\n",
      "subnet: [60421], loss: [1.25500052], acc: 0.40625, samples: 32\n",
      "subnet: [147407], expert_idx: 0\n",
      "subnet: [147407], loss: [1.11997921], acc: 0.625, samples: 32\n",
      "subnet: [75357], expert_idx: 0\n",
      "subnet: [75357], loss: [0.9481228], acc: 0.625, samples: 32\n",
      "subnet: [126393], expert_idx: 0\n",
      "subnet: [126393], loss: [1.12198875], acc: 0.59375, samples: 32\n",
      "subnet: [180966], expert_idx: 0\n",
      "subnet: [180966], loss: [1.47473613], acc: 0.25, samples: 32\n",
      "subnet: [305694], expert_idx: 0\n",
      "subnet: [305694], loss: [1.38381169], acc: 0.375, samples: 32\n",
      "subnet: [307755], expert_idx: 0\n",
      "subnet: [307755], loss: [1.01199636], acc: 0.5625, samples: 32\n",
      "subnet: [275975], expert_idx: 0\n",
      "subnet: [275975], loss: [1.09163161], acc: 0.59375, samples: 32\n",
      "subnet: [285387], expert_idx: 0\n",
      "subnet: [285387], loss: [1.13538895], acc: 0.46875, samples: 32\n",
      "subnet: [77999], expert_idx: 0\n",
      "subnet: [77999], loss: [1.00821858], acc: 0.53125, samples: 32\n",
      "subnet: [206657], expert_idx: 0\n",
      "subnet: [206657], loss: [1.09966853], acc: 0.5, samples: 32\n",
      "subnet: [70047], expert_idx: 0\n",
      "subnet: [70047], loss: [1.07098872], acc: 0.53125, samples: 32\n",
      "subnet: [129814], expert_idx: 0\n",
      "subnet: [129814], loss: [0.9285866], acc: 0.53125, samples: 32\n",
      "subnet: [211467], expert_idx: 0\n",
      "subnet: [211467], loss: [0.97925352], acc: 0.5625, samples: 32\n",
      "subnet: [250602], expert_idx: 0\n",
      "subnet: [250602], loss: [1.04568426], acc: 0.53125, samples: 32\n",
      "subnet: [165908], expert_idx: 0\n",
      "subnet: [165908], loss: [1.07249064], acc: 0.40625, samples: 32\n",
      "subnet: [66788], expert_idx: 0\n",
      "subnet: [66788], loss: [1.31840851], acc: 0.375, samples: 32\n",
      "subnet: [53516], expert_idx: 0\n",
      "subnet: [53516], loss: [1.25622749], acc: 0.40625, samples: 32\n",
      "subnet: [82171], expert_idx: 0\n",
      "subnet: [82171], loss: [0.99797172], acc: 0.46875, samples: 32\n",
      "subnet: [258888], expert_idx: 0\n",
      "subnet: [258888], loss: [1.35277847], acc: 0.40625, samples: 32\n",
      "subnet: [74008], expert_idx: 0\n",
      "subnet: [74008], loss: [1.03912277], acc: 0.4375, samples: 32\n",
      "subnet: [160096], expert_idx: 0\n",
      "subnet: [160096], loss: [1.11027065], acc: 0.5, samples: 32\n",
      "subnet: [37708], expert_idx: 0\n",
      "subnet: [37708], loss: [1.36471265], acc: 0.4375, samples: 32\n",
      "subnet: [37661], expert_idx: 0\n",
      "subnet: [37661], loss: [1.00260995], acc: 0.46875, samples: 32\n",
      "subnet: [95264], expert_idx: 0\n",
      "subnet: [95264], loss: [1.12937331], acc: 0.4375, samples: 32\n",
      "subnet: [202398], expert_idx: 0\n",
      "subnet: [202398], loss: [1.33865194], acc: 0.3125, samples: 32\n",
      "subnet: [303136], expert_idx: 0\n",
      "subnet: [303136], loss: [0.88434517], acc: 0.625, samples: 32\n",
      "subnet: [312752], expert_idx: 0\n",
      "subnet: [312752], loss: [1.22271947], acc: 0.5, samples: 32\n",
      "subnet: [254652], expert_idx: 0\n",
      "subnet: [254652], loss: [1.54849569], acc: 0.28125, samples: 32\n",
      "subnet: [153458], expert_idx: 0\n",
      "subnet: [153458], loss: [1.53932094], acc: 0.46875, samples: 32\n",
      "subnet: [74010], expert_idx: 0\n",
      "subnet: [74010], loss: [0.98904308], acc: 0.5625, samples: 32\n",
      "subnet: [63648], expert_idx: 0\n",
      "subnet: [63648], loss: [1.08738937], acc: 0.46875, samples: 32\n",
      "subnet: [328849], expert_idx: 0\n",
      "subnet: [328849], loss: [1.12876462], acc: 0.5625, samples: 32\n",
      "subnet: [275661], expert_idx: 0\n",
      "subnet: [275661], loss: [1.34965394], acc: 0.40625, samples: 32\n",
      "subnet: [319221], expert_idx: 0\n",
      "subnet: [319221], loss: [0.92054059], acc: 0.59375, samples: 32\n",
      "subnet: [233824], expert_idx: 0\n",
      "subnet: [233824], loss: [1.16517639], acc: 0.46875, samples: 32\n",
      "subnet: [233244], expert_idx: 0\n",
      "subnet: [233244], loss: [0.99231149], acc: 0.65625, samples: 32\n",
      "subnet: [14408], expert_idx: 0\n",
      "subnet: [14408], loss: [1.28870376], acc: 0.46875, samples: 32\n",
      "subnet: [309358], expert_idx: 0\n",
      "subnet: [309358], loss: [1.40754526], acc: 0.375, samples: 32\n",
      "subnet: [74956], expert_idx: 0\n",
      "subnet: [74956], loss: [1.2415762], acc: 0.4375, samples: 32\n",
      "subnet: [294543], expert_idx: 0\n",
      "subnet: [294543], loss: [1.08798779], acc: 0.46875, samples: 32\n",
      "subnet: [48684], expert_idx: 0\n",
      "subnet: [48684], loss: [1.05521516], acc: 0.5625, samples: 32\n",
      "subnet: [291178], expert_idx: 0\n",
      "subnet: [291178], loss: [1.27405069], acc: 0.40625, samples: 32\n",
      "subnet: [150908], expert_idx: 0\n",
      "subnet: [150908], loss: [1.06682271], acc: 0.53125, samples: 32\n",
      "subnet: [116113], expert_idx: 0\n",
      "subnet: [116113], loss: [1.21211789], acc: 0.46875, samples: 32\n",
      "subnet: [303762], expert_idx: 0\n",
      "subnet: [303762], loss: [0.99823724], acc: 0.46875, samples: 32\n",
      "subnet: [206559], expert_idx: 0\n",
      "subnet: [206559], loss: [1.23160996], acc: 0.46875, samples: 32\n",
      "subnet: [153619], expert_idx: 0\n",
      "subnet: [153619], loss: [1.17650655], acc: 0.59375, samples: 32\n",
      "subnet: [139462], expert_idx: 0\n",
      "subnet: [139462], loss: [1.19512534], acc: 0.4375, samples: 32\n",
      "subnet: [43637], expert_idx: 0\n",
      "subnet: [43637], loss: [1.00826516], acc: 0.5625, samples: 32\n",
      "subnet: [327860], expert_idx: 0\n",
      "subnet: [327860], loss: [1.22991566], acc: 0.34375, samples: 32\n",
      "subnet: [152814], expert_idx: 0\n",
      "subnet: [152814], loss: [1.11414425], acc: 0.53125, samples: 32\n",
      "subnet: [62264], expert_idx: 0\n",
      "subnet: [62264], loss: [1.40041936], acc: 0.375, samples: 32\n",
      "subnet: [71285], expert_idx: 0\n",
      "subnet: [71285], loss: [1.02187222], acc: 0.59375, samples: 32\n",
      "subnet: [241497], expert_idx: 0\n",
      "subnet: [241497], loss: [1.3427646], acc: 0.5, samples: 32\n",
      "subnet: [58495], expert_idx: 0\n",
      "subnet: [58495], loss: [1.17256797], acc: 0.4375, samples: 32\n",
      "subnet: [281866], expert_idx: 0\n",
      "subnet: [281866], loss: [1.26155673], acc: 0.4375, samples: 32\n",
      "subnet: [36025], expert_idx: 0\n",
      "subnet: [36025], loss: [1.03590606], acc: 0.5625, samples: 32\n",
      "subnet: [204056], expert_idx: 0\n",
      "subnet: [204056], loss: [1.04565445], acc: 0.4375, samples: 32\n",
      "subnet: [237872], expert_idx: 0\n",
      "subnet: [237872], loss: [1.20732081], acc: 0.4375, samples: 32\n",
      "subnet: [14227], expert_idx: 0\n",
      "subnet: [14227], loss: [1.14984101], acc: 0.40625, samples: 32\n",
      "subnet: [151628], expert_idx: 0\n",
      "subnet: [151628], loss: [1.36935121], acc: 0.46875, samples: 32\n",
      "subnet: [322471], expert_idx: 0\n",
      "subnet: [322471], loss: [0.96664709], acc: 0.5625, samples: 32\n",
      "subnet: [158679], expert_idx: 0\n",
      "subnet: [158679], loss: [1.37477148], acc: 0.4375, samples: 32\n",
      "subnet: [248846], expert_idx: 0\n",
      "subnet: [248846], loss: [1.01330838], acc: 0.4375, samples: 32\n",
      "subnet: [43089], expert_idx: 0\n",
      "subnet: [43089], loss: [0.91759228], acc: 0.6875, samples: 32\n",
      "subnet: [297729], expert_idx: 0\n",
      "subnet: [297729], loss: [1.08894795], acc: 0.53125, samples: 32\n",
      "subnet: [326276], expert_idx: 0\n",
      "subnet: [326276], loss: [1.31358378], acc: 0.3125, samples: 32\n",
      "subnet: [125548], expert_idx: 0\n",
      "subnet: [125548], loss: [1.14194952], acc: 0.46875, samples: 32\n",
      "subnet: [201446], expert_idx: 0\n",
      "subnet: [201446], loss: [1.06823789], acc: 0.65625, samples: 32\n",
      "subnet: [128528], expert_idx: 0\n",
      "subnet: [128528], loss: [1.20555479], acc: 0.4375, samples: 32\n",
      "subnet: [326706], expert_idx: 0\n",
      "subnet: [326706], loss: [1.27366835], acc: 0.34375, samples: 32\n",
      "subnet: [53342], expert_idx: 0\n",
      "subnet: [53342], loss: [1.18705416], acc: 0.5625, samples: 32\n",
      "subnet: [317303], expert_idx: 0\n",
      "subnet: [317303], loss: [0.92304804], acc: 0.65625, samples: 32\n",
      "subnet: [90483], expert_idx: 0\n",
      "subnet: [90483], loss: [0.98099266], acc: 0.5625, samples: 32\n",
      "subnet: [12390], expert_idx: 0\n",
      "subnet: [12390], loss: [1.323058], acc: 0.4375, samples: 32\n",
      "subnet: [145569], expert_idx: 0\n",
      "subnet: [145569], loss: [1.18681134], acc: 0.5, samples: 32\n",
      "subnet: [218092], expert_idx: 0\n",
      "subnet: [218092], loss: [1.31790051], acc: 0.5, samples: 32\n",
      "subnet: [44706], expert_idx: 0\n",
      "subnet: [44706], loss: [0.91557697], acc: 0.65625, samples: 32\n",
      "subnet: [300522], expert_idx: 0\n",
      "subnet: [300522], loss: [1.13283501], acc: 0.5625, samples: 32\n",
      "subnet: [92804], expert_idx: 0\n",
      "subnet: [92804], loss: [0.92750516], acc: 0.625, samples: 32\n",
      "subnet: [217287], expert_idx: 0\n",
      "subnet: [217287], loss: [1.30170991], acc: 0.375, samples: 32\n",
      "subnet: [5833], expert_idx: 0\n",
      "subnet: [5833], loss: [1.10616421], acc: 0.375, samples: 32\n",
      "subnet: [114415], expert_idx: 0\n",
      "subnet: [114415], loss: [1.09385352], acc: 0.5625, samples: 32\n",
      "subnet: [23923], expert_idx: 0\n",
      "subnet: [23923], loss: [1.17962934], acc: 0.4375, samples: 32\n",
      "subnet: [47497], expert_idx: 0\n",
      "subnet: [47497], loss: [1.14371263], acc: 0.53125, samples: 32\n",
      "subnet: [83314], expert_idx: 0\n",
      "subnet: [83314], loss: [1.12861261], acc: 0.5, samples: 32\n",
      "subnet: [144923], expert_idx: 0\n",
      "subnet: [144923], loss: [1.22528335], acc: 0.46875, samples: 32\n",
      "subnet: [148830], expert_idx: 0\n",
      "subnet: [148830], loss: [1.01147598], acc: 0.65625, samples: 32\n",
      "subnet: [125670], expert_idx: 0\n",
      "subnet: [125670], loss: [1.28273207], acc: 0.375, samples: 32\n",
      "subnet: [141241], expert_idx: 0\n",
      "subnet: [141241], loss: [1.14350132], acc: 0.4375, samples: 32\n",
      "subnet: [149885], expert_idx: 0\n",
      "subnet: [149885], loss: [0.94992818], acc: 0.65625, samples: 32\n",
      "subnet: [98792], expert_idx: 0\n",
      "subnet: [98792], loss: [1.27385028], acc: 0.40625, samples: 32\n",
      "subnet: [291027], expert_idx: 0\n",
      "subnet: [291027], loss: [1.24628726], acc: 0.4375, samples: 32\n",
      "subnet: [243102], expert_idx: 0\n",
      "subnet: [243102], loss: [1.02297142], acc: 0.53125, samples: 32\n",
      "subnet: [256039], expert_idx: 0\n",
      "subnet: [256039], loss: [1.27297598], acc: 0.3125, samples: 32\n",
      "subnet: [191223], expert_idx: 0\n",
      "subnet: [191223], loss: [0.99055161], acc: 0.5625, samples: 32\n",
      "subnet: [320668], expert_idx: 0\n",
      "subnet: [320668], loss: [0.99717322], acc: 0.5625, samples: 32\n",
      "subnet: [302162], expert_idx: 0\n",
      "subnet: [302162], loss: [0.96164835], acc: 0.5625, samples: 32\n",
      "subnet: [174367], expert_idx: 0\n",
      "subnet: [174367], loss: [1.00850097], acc: 0.53125, samples: 32\n",
      "subnet: [187109], expert_idx: 0\n",
      "subnet: [187109], loss: [1.02866303], acc: 0.5, samples: 32\n",
      "subnet: [114038], expert_idx: 0\n",
      "subnet: [114038], loss: [1.05128715], acc: 0.53125, samples: 32\n",
      "subnet: [25230], expert_idx: 0\n",
      "subnet: [25230], loss: [1.04263656], acc: 0.46875, samples: 32\n",
      "subnet: [289511], expert_idx: 0\n",
      "subnet: [289511], loss: [1.14359559], acc: 0.5625, samples: 32\n",
      "subnet: [261163], expert_idx: 0\n",
      "subnet: [261163], loss: [0.98098272], acc: 0.59375, samples: 32\n",
      "subnet: [312018], expert_idx: 0\n",
      "subnet: [312018], loss: [1.11633365], acc: 0.46875, samples: 32\n",
      "subnet: [308560], expert_idx: 0\n",
      "subnet: [308560], loss: [1.14060429], acc: 0.59375, samples: 32\n",
      "subnet: [254612], expert_idx: 0\n",
      "subnet: [254612], loss: [1.09458381], acc: 0.5625, samples: 32\n",
      "subnet: [258283], expert_idx: 0\n",
      "subnet: [258283], loss: [1.00578235], acc: 0.5, samples: 32\n",
      "subnet: [232328], expert_idx: 0\n",
      "subnet: [232328], loss: [1.0693176], acc: 0.4375, samples: 32\n",
      "subnet: [116866], expert_idx: 0\n",
      "subnet: [116866], loss: [0.82546875], acc: 0.71875, samples: 32\n",
      "subnet: [92247], expert_idx: 0\n",
      "subnet: [92247], loss: [1.13672988], acc: 0.46875, samples: 32\n",
      "subnet: [182451], expert_idx: 0\n",
      "subnet: [182451], loss: [1.21167516], acc: 0.5, samples: 32\n",
      "subnet: [289369], expert_idx: 0\n",
      "subnet: [289369], loss: [1.16806462], acc: 0.46875, samples: 32\n",
      "subnet: [237963], expert_idx: 0\n",
      "subnet: [237963], loss: [1.07355353], acc: 0.46875, samples: 32\n",
      "subnet: [135806], expert_idx: 0\n",
      "subnet: [135806], loss: [1.02311195], acc: 0.46875, samples: 32\n",
      "subnet: [62675], expert_idx: 0\n",
      "subnet: [62675], loss: [1.31840851], acc: 0.375, samples: 32\n",
      "subnet: [208273], expert_idx: 0\n",
      "subnet: [208273], loss: [1.07158825], acc: 0.53125, samples: 32\n",
      "subnet: [273424], expert_idx: 0\n",
      "subnet: [273424], loss: [1.27166476], acc: 0.4375, samples: 32\n",
      "subnet: [163668], expert_idx: 0\n",
      "subnet: [163668], loss: [1.39618271], acc: 0.34375, samples: 32\n",
      "subnet: [217193], expert_idx: 0\n",
      "subnet: [217193], loss: [1.15941165], acc: 0.5, samples: 32\n",
      "subnet: [325618], expert_idx: 0\n",
      "subnet: [325618], loss: [1.03913513], acc: 0.5625, samples: 32\n",
      "subnet: [215119], expert_idx: 0\n",
      "subnet: [215119], loss: [1.16556692], acc: 0.4375, samples: 32\n",
      "subnet: [167204], expert_idx: 0\n",
      "subnet: [167204], loss: [0.99155466], acc: 0.5625, samples: 32\n",
      "subnet: [270641], expert_idx: 0\n",
      "subnet: [270641], loss: [1.12701604], acc: 0.4375, samples: 32\n",
      "subnet: [314120], expert_idx: 0\n",
      "subnet: [314120], loss: [1.45818967], acc: 0.34375, samples: 32\n",
      "subnet: [47343], expert_idx: 0\n",
      "subnet: [47343], loss: [1.1517882], acc: 0.53125, samples: 32\n",
      "subnet: [39655], expert_idx: 0\n",
      "subnet: [39655], loss: [1.06239177], acc: 0.46875, samples: 32\n",
      "subnet: [132903], expert_idx: 0\n",
      "subnet: [132903], loss: [1.08467046], acc: 0.625, samples: 32\n",
      "subnet: [117617], expert_idx: 0\n",
      "subnet: [117617], loss: [1.1978478], acc: 0.40625, samples: 32\n",
      "subnet: [136976], expert_idx: 0\n",
      "subnet: [136976], loss: [1.03485958], acc: 0.5625, samples: 32\n",
      "subnet: [88178], expert_idx: 0\n",
      "subnet: [88178], loss: [1.1777695], acc: 0.46875, samples: 32\n",
      "subnet: [251970], expert_idx: 0\n",
      "subnet: [251970], loss: [1.17416357], acc: 0.46875, samples: 32\n",
      "subnet: [92279], expert_idx: 0\n",
      "subnet: [92279], loss: [1.43229452], acc: 0.34375, samples: 32\n",
      "subnet: [199037], expert_idx: 0\n",
      "subnet: [199037], loss: [1.56033335], acc: 0.5, samples: 32\n",
      "subnet: [203909], expert_idx: 0\n",
      "subnet: [203909], loss: [1.23181057], acc: 0.5, samples: 32\n",
      "subnet: [154028], expert_idx: 0\n",
      "subnet: [154028], loss: [1.24754394], acc: 0.5, samples: 32\n",
      "subnet: [303282], expert_idx: 0\n",
      "subnet: [303282], loss: [1.03391097], acc: 0.53125, samples: 32\n",
      "subnet: [323968], expert_idx: 0\n",
      "subnet: [323968], loss: [1.43513824], acc: 0.3125, samples: 32\n",
      "subnet: [64038], expert_idx: 0\n",
      "subnet: [64038], loss: [1.09987464], acc: 0.5625, samples: 32\n",
      "subnet: [55416], expert_idx: 0\n",
      "subnet: [55416], loss: [0.9457157], acc: 0.625, samples: 32\n",
      "subnet: [182932], expert_idx: 0\n",
      "subnet: [182932], loss: [0.96698359], acc: 0.625, samples: 32\n",
      "subnet: [107916], expert_idx: 0\n",
      "subnet: [107916], loss: [1.10564391], acc: 0.4375, samples: 32\n",
      "subnet: [183892], expert_idx: 0\n",
      "subnet: [183892], loss: [1.30738642], acc: 0.4375, samples: 32\n",
      "subnet: [135513], expert_idx: 0\n",
      "subnet: [135513], loss: [1.02797026], acc: 0.53125, samples: 32\n",
      "subnet: [221097], expert_idx: 0\n",
      "subnet: [221097], loss: [0.94966773], acc: 0.46875, samples: 32\n",
      "subnet: [282585], expert_idx: 0\n",
      "subnet: [282585], loss: [1.1455441], acc: 0.59375, samples: 32\n",
      "subnet: [23521], expert_idx: 0\n",
      "subnet: [23521], loss: [0.91929843], acc: 0.625, samples: 32\n",
      "subnet: [138199], expert_idx: 0\n",
      "subnet: [138199], loss: [1.01207481], acc: 0.46875, samples: 32\n",
      "subnet: [114908], expert_idx: 0\n",
      "subnet: [114908], loss: [0.92599576], acc: 0.625, samples: 32\n",
      "subnet: [285193], expert_idx: 0\n",
      "subnet: [285193], loss: [0.98442806], acc: 0.59375, samples: 32\n",
      "subnet: [227296], expert_idx: 0\n",
      "subnet: [227296], loss: [1.17663291], acc: 0.53125, samples: 32\n",
      "subnet: [122165], expert_idx: 0\n",
      "subnet: [122165], loss: [1.42543714], acc: 0.375, samples: 32\n",
      "subnet: [7604], expert_idx: 0\n",
      "subnet: [7604], loss: [1.09786068], acc: 0.5, samples: 32\n",
      "subnet: [178293], expert_idx: 0\n",
      "subnet: [178293], loss: [1.19542615], acc: 0.4375, samples: 32\n",
      "subnet: [152701], expert_idx: 0\n",
      "subnet: [152701], loss: [1.08978065], acc: 0.53125, samples: 32\n",
      "subnet: [139175], expert_idx: 0\n",
      "subnet: [139175], loss: [1.22527588], acc: 0.40625, samples: 32\n",
      "subnet: [48494], expert_idx: 0\n",
      "subnet: [48494], loss: [1.11731837], acc: 0.5625, samples: 32\n",
      "subnet: [207501], expert_idx: 0\n",
      "subnet: [207501], loss: [1.22466774], acc: 0.4375, samples: 32\n",
      "subnet: [39250], expert_idx: 0\n",
      "subnet: [39250], loss: [0.99981244], acc: 0.46875, samples: 32\n",
      "subnet: [235745], expert_idx: 0\n",
      "subnet: [235745], loss: [1.19770735], acc: 0.34375, samples: 32\n",
      "subnet: [211893], expert_idx: 0\n",
      "subnet: [211893], loss: [0.96416025], acc: 0.59375, samples: 32\n",
      "subnet: [287817], expert_idx: 0\n",
      "subnet: [287817], loss: [1.24963887], acc: 0.375, samples: 32\n",
      "subnet: [106556], expert_idx: 0\n",
      "subnet: [106556], loss: [1.30261298], acc: 0.4375, samples: 32\n",
      "subnet: [144793], expert_idx: 0\n",
      "subnet: [144793], loss: [1.06926445], acc: 0.5, samples: 32\n",
      "subnet: [133201], expert_idx: 0\n",
      "subnet: [133201], loss: [1.42869712], acc: 0.28125, samples: 32\n",
      "subnet: [23640], expert_idx: 0\n",
      "subnet: [23640], loss: [1.09721129], acc: 0.625, samples: 32\n",
      "subnet: [128508], expert_idx: 0\n",
      "subnet: [128508], loss: [1.08301572], acc: 0.5625, samples: 32\n",
      "subnet: [286066], expert_idx: 0\n",
      "subnet: [286066], loss: [1.38130234], acc: 0.46875, samples: 32\n",
      "subnet: [93831], expert_idx: 0\n",
      "subnet: [93831], loss: [1.14955253], acc: 0.5, samples: 32\n",
      "subnet: [64156], expert_idx: 0\n",
      "subnet: [64156], loss: [1.05061072], acc: 0.5, samples: 32\n",
      "subnet: [189117], expert_idx: 0\n",
      "subnet: [189117], loss: [1.21956013], acc: 0.40625, samples: 32\n",
      "subnet: [318939], expert_idx: 0\n",
      "subnet: [318939], loss: [1.13380416], acc: 0.59375, samples: 32\n",
      "subnet: [53073], expert_idx: 0\n",
      "subnet: [53073], loss: [1.72366649], acc: 0.34375, samples: 32\n",
      "subnet: [258677], expert_idx: 0\n",
      "subnet: [258677], loss: [1.02275805], acc: 0.625, samples: 32\n",
      "subnet: [66799], expert_idx: 0\n",
      "subnet: [66799], loss: [1.33616822], acc: 0.5, samples: 32\n",
      "subnet: [166155], expert_idx: 0\n",
      "subnet: [166155], loss: [1.12637862], acc: 0.53125, samples: 32\n",
      "subnet: [95474], expert_idx: 0\n",
      "subnet: [95474], loss: [0.96749107], acc: 0.5625, samples: 32\n",
      "subnet: [101026], expert_idx: 0\n",
      "subnet: [101026], loss: [0.92468626], acc: 0.5625, samples: 32\n",
      "subnet: [86631], expert_idx: 0\n",
      "subnet: [86631], loss: [1.04119869], acc: 0.59375, samples: 32\n",
      "subnet: [63823], expert_idx: 0\n",
      "subnet: [63823], loss: [1.08551498], acc: 0.625, samples: 32\n",
      "subnet: [77286], expert_idx: 0\n",
      "subnet: [77286], loss: [1.09219499], acc: 0.59375, samples: 32\n",
      "subnet: [231718], expert_idx: 0\n",
      "subnet: [231718], loss: [1.1855856], acc: 0.5, samples: 32\n",
      "subnet: [193298], expert_idx: 0\n",
      "subnet: [193298], loss: [1.12292483], acc: 0.5, samples: 32\n",
      "subnet: [291341], expert_idx: 0\n",
      "subnet: [291341], loss: [1.39672997], acc: 0.46875, samples: 32\n",
      "subnet: [268187], expert_idx: 0\n",
      "subnet: [268187], loss: [1.09858941], acc: 0.46875, samples: 32\n",
      "subnet: [78956], expert_idx: 0\n",
      "subnet: [78956], loss: [1.08077371], acc: 0.5625, samples: 32\n",
      "subnet: [295826], expert_idx: 0\n",
      "subnet: [295826], loss: [1.17391839], acc: 0.46875, samples: 32\n",
      "subnet: [149264], expert_idx: 0\n",
      "subnet: [149264], loss: [1.19833238], acc: 0.5, samples: 32\n",
      "subnet: [225455], expert_idx: 0\n",
      "subnet: [225455], loss: [1.34665881], acc: 0.40625, samples: 32\n",
      "subnet: [17760], expert_idx: 0\n",
      "subnet: [17760], loss: [0.97608551], acc: 0.5, samples: 32\n",
      "subnet: [197979], expert_idx: 0\n",
      "subnet: [197979], loss: [1.17618303], acc: 0.46875, samples: 32\n",
      "subnet: [93719], expert_idx: 0\n",
      "subnet: [93719], loss: [0.96428287], acc: 0.59375, samples: 32\n",
      "subnet: [61174], expert_idx: 0\n",
      "subnet: [61174], loss: [1.07574836], acc: 0.5, samples: 32\n",
      "subnet: [176792], expert_idx: 0\n",
      "subnet: [176792], loss: [1.03344655], acc: 0.625, samples: 32\n",
      "subnet: [107240], expert_idx: 0\n",
      "subnet: [107240], loss: [1.58560367], acc: 0.375, samples: 32\n",
      "subnet: [327641], expert_idx: 0\n",
      "subnet: [327641], loss: [1.1455441], acc: 0.59375, samples: 32\n",
      "subnet: [25914], expert_idx: 0\n",
      "subnet: [25914], loss: [0.99548798], acc: 0.625, samples: 32\n",
      "subnet: [93063], expert_idx: 0\n",
      "subnet: [93063], loss: [1.10935549], acc: 0.5625, samples: 32\n",
      "subnet: [152917], expert_idx: 0\n",
      "subnet: [152917], loss: [1.14535139], acc: 0.5, samples: 32\n",
      "subnet: [225584], expert_idx: 0\n",
      "subnet: [225584], loss: [1.25551004], acc: 0.40625, samples: 32\n",
      "subnet: [195862], expert_idx: 0\n",
      "subnet: [195862], loss: [0.99785004], acc: 0.5625, samples: 32\n",
      "subnet: [211615], expert_idx: 0\n",
      "subnet: [211615], loss: [1.41630267], acc: 0.375, samples: 32\n",
      "subnet: [160652], expert_idx: 0\n",
      "subnet: [160652], loss: [1.30150245], acc: 0.4375, samples: 32\n",
      "subnet: [136815], expert_idx: 0\n",
      "subnet: [136815], loss: [1.04884447], acc: 0.625, samples: 32\n",
      "subnet: [260132], expert_idx: 0\n",
      "subnet: [260132], loss: [1.21083652], acc: 0.375, samples: 32\n",
      "subnet: [60328], expert_idx: 0\n",
      "subnet: [60328], loss: [0.98205826], acc: 0.59375, samples: 32\n",
      "subnet: [182911], expert_idx: 0\n",
      "subnet: [182911], loss: [1.08010262], acc: 0.53125, samples: 32\n",
      "subnet: [212579], expert_idx: 0\n",
      "subnet: [212579], loss: [1.13590506], acc: 0.4375, samples: 32\n",
      "subnet: [26795], expert_idx: 0\n",
      "subnet: [26795], loss: [1.00126207], acc: 0.5, samples: 32\n",
      "subnet: [173789], expert_idx: 0\n",
      "subnet: [173789], loss: [1.149159], acc: 0.4375, samples: 32\n",
      "subnet: [32557], expert_idx: 0\n",
      "subnet: [32557], loss: [0.86920805], acc: 0.59375, samples: 32\n",
      "subnet: [290676], expert_idx: 0\n",
      "subnet: [290676], loss: [0.91491213], acc: 0.65625, samples: 32\n",
      "subnet: [89153], expert_idx: 0\n",
      "subnet: [89153], loss: [1.31856072], acc: 0.34375, samples: 32\n",
      "subnet: [75723], expert_idx: 0\n",
      "subnet: [75723], loss: [1.34931563], acc: 0.46875, samples: 32\n",
      "subnet: [228529], expert_idx: 0\n",
      "subnet: [228529], loss: [1.05850573], acc: 0.53125, samples: 32\n",
      "subnet: [15283], expert_idx: 0\n",
      "subnet: [15283], loss: [1.0111778], acc: 0.59375, samples: 32\n",
      "subnet: [208393], expert_idx: 0\n",
      "subnet: [208393], loss: [1.24602852], acc: 0.375, samples: 32\n",
      "subnet: [221017], expert_idx: 0\n",
      "subnet: [221017], loss: [0.91684418], acc: 0.625, samples: 32\n",
      "subnet: [295596], expert_idx: 0\n",
      "subnet: [295596], loss: [1.13566693], acc: 0.53125, samples: 32\n",
      "subnet: [328691], expert_idx: 0\n",
      "subnet: [328691], loss: [1.09093285], acc: 0.625, samples: 32\n",
      "subnet: [50455], expert_idx: 0\n",
      "subnet: [50455], loss: [0.9386603], acc: 0.6875, samples: 32\n",
      "subnet: [86364], expert_idx: 0\n",
      "subnet: [86364], loss: [1.07517924], acc: 0.59375, samples: 32\n",
      "subnet: [1870], expert_idx: 0\n",
      "subnet: [1870], loss: [0.8785027], acc: 0.65625, samples: 32\n",
      "subnet: [293459], expert_idx: 0\n",
      "subnet: [293459], loss: [1.17252332], acc: 0.5, samples: 32\n",
      "subnet: [132861], expert_idx: 0\n",
      "subnet: [132861], loss: [1.26149644], acc: 0.46875, samples: 32\n",
      "subnet: [235421], expert_idx: 0\n",
      "subnet: [235421], loss: [1.04496705], acc: 0.4375, samples: 32\n",
      "subnet: [299955], expert_idx: 0\n",
      "subnet: [299955], loss: [0.86890482], acc: 0.65625, samples: 32\n",
      "subnet: [254295], expert_idx: 0\n",
      "subnet: [254295], loss: [1.40575575], acc: 0.46875, samples: 32\n",
      "subnet: [27435], expert_idx: 0\n",
      "subnet: [27435], loss: [0.99752103], acc: 0.46875, samples: 32\n",
      "subnet: [193914], expert_idx: 0\n",
      "subnet: [193914], loss: [1.08911008], acc: 0.5, samples: 32\n",
      "subnet: [39594], expert_idx: 0\n",
      "subnet: [39594], loss: [1.11611449], acc: 0.40625, samples: 32\n",
      "subnet: [321645], expert_idx: 0\n",
      "subnet: [321645], loss: [1.17416357], acc: 0.46875, samples: 32\n",
      "subnet: [309706], expert_idx: 0\n",
      "subnet: [309706], loss: [1.30859217], acc: 0.46875, samples: 32\n",
      "subnet: [60235], expert_idx: 0\n",
      "subnet: [60235], loss: [1.46095374], acc: 0.375, samples: 32\n",
      "subnet: [32771], expert_idx: 0\n",
      "subnet: [32771], loss: [1.08986822], acc: 0.53125, samples: 32\n",
      "subnet: [71094], expert_idx: 0\n",
      "subnet: [71094], loss: [1.34165227], acc: 0.34375, samples: 32\n",
      "subnet: [165421], expert_idx: 0\n",
      "subnet: [165421], loss: [1.04613676], acc: 0.53125, samples: 32\n",
      "subnet: [152325], expert_idx: 0\n",
      "subnet: [152325], loss: [1.09561528], acc: 0.5, samples: 32\n",
      "subnet: [232035], expert_idx: 0\n",
      "subnet: [232035], loss: [1.09449134], acc: 0.40625, samples: 32\n",
      "subnet: [210592], expert_idx: 0\n",
      "subnet: [210592], loss: [1.05886979], acc: 0.53125, samples: 32\n",
      "subnet: [26097], expert_idx: 0\n",
      "subnet: [26097], loss: [1.19726528], acc: 0.53125, samples: 32\n",
      "subnet: [124139], expert_idx: 0\n",
      "subnet: [124139], loss: [1.24584919], acc: 0.4375, samples: 32\n",
      "subnet: [124728], expert_idx: 0\n",
      "subnet: [124728], loss: [1.26971751], acc: 0.4375, samples: 32\n",
      "subnet: [128899], expert_idx: 0\n",
      "subnet: [128899], loss: [0.97025228], acc: 0.5625, samples: 32\n",
      "subnet: [212616], expert_idx: 0\n",
      "subnet: [212616], loss: [1.17638677], acc: 0.4375, samples: 32\n",
      "subnet: [25415], expert_idx: 0\n",
      "subnet: [25415], loss: [1.38671621], acc: 0.375, samples: 32\n",
      "subnet: [117665], expert_idx: 0\n",
      "subnet: [117665], loss: [1.03952598], acc: 0.5, samples: 32\n",
      "subnet: [244191], expert_idx: 0\n",
      "subnet: [244191], loss: [1.00468803], acc: 0.53125, samples: 32\n",
      "subnet: [323343], expert_idx: 0\n",
      "subnet: [323343], loss: [1.0083649], acc: 0.46875, samples: 32\n",
      "subnet: [38530], expert_idx: 0\n",
      "subnet: [38530], loss: [1.16053546], acc: 0.46875, samples: 32\n",
      "subnet: [62240], expert_idx: 0\n",
      "subnet: [62240], loss: [0.99272107], acc: 0.5625, samples: 32\n",
      "subnet: [223268], expert_idx: 0\n",
      "subnet: [223268], loss: [0.8855333], acc: 0.625, samples: 32\n",
      "subnet: [280468], expert_idx: 0\n",
      "subnet: [280468], loss: [1.19546927], acc: 0.46875, samples: 32\n",
      "subnet: [271870], expert_idx: 0\n",
      "subnet: [271870], loss: [1.3427646], acc: 0.5, samples: 32\n",
      "subnet: [19580], expert_idx: 0\n",
      "subnet: [19580], loss: [1.17423395], acc: 0.5, samples: 32\n",
      "subnet: [93991], expert_idx: 0\n",
      "subnet: [93991], loss: [1.03122749], acc: 0.59375, samples: 32\n",
      "subnet: [11972], expert_idx: 0\n",
      "subnet: [11972], loss: [0.9385012], acc: 0.625, samples: 32\n",
      "subnet: [226008], expert_idx: 0\n",
      "subnet: [226008], loss: [1.04345122], acc: 0.4375, samples: 32\n",
      "subnet: [101166], expert_idx: 0\n",
      "subnet: [101166], loss: [1.19089981], acc: 0.46875, samples: 32\n",
      "subnet: [179493], expert_idx: 0\n",
      "subnet: [179493], loss: [0.99293343], acc: 0.53125, samples: 32\n",
      "subnet: [135310], expert_idx: 0\n",
      "subnet: [135310], loss: [1.29239626], acc: 0.4375, samples: 32\n",
      "subnet: [41163], expert_idx: 0\n",
      "subnet: [41163], loss: [0.9248648], acc: 0.625, samples: 32\n",
      "subnet: [324544], expert_idx: 0\n",
      "subnet: [324544], loss: [1.17205071], acc: 0.59375, samples: 32\n",
      "subnet: [205328], expert_idx: 0\n",
      "subnet: [205328], loss: [1.02185082], acc: 0.5625, samples: 32\n",
      "subnet: [123880], expert_idx: 0\n",
      "subnet: [123880], loss: [1.47541227], acc: 0.34375, samples: 32\n",
      "subnet: [206306], expert_idx: 0\n",
      "subnet: [206306], loss: [1.42470971], acc: 0.4375, samples: 32\n",
      "subnet: [77702], expert_idx: 0\n",
      "subnet: [77702], loss: [1.03039229], acc: 0.5, samples: 32\n",
      "subnet: [54236], expert_idx: 0\n",
      "subnet: [54236], loss: [0.92738141], acc: 0.59375, samples: 32\n",
      "subnet: [66852], expert_idx: 0\n",
      "subnet: [66852], loss: [1.38207008], acc: 0.34375, samples: 32\n",
      "subnet: [225469], expert_idx: 0\n",
      "subnet: [225469], loss: [1.11347765], acc: 0.5, samples: 32\n",
      "subnet: [21887], expert_idx: 0\n",
      "subnet: [21887], loss: [1.16463013], acc: 0.4375, samples: 32\n",
      "subnet: [302283], expert_idx: 0\n",
      "subnet: [302283], loss: [1.00850203], acc: 0.5625, samples: 32\n",
      "subnet: [248395], expert_idx: 0\n",
      "subnet: [248395], loss: [1.10669204], acc: 0.4375, samples: 32\n",
      "subnet: [87882], expert_idx: 0\n",
      "subnet: [87882], loss: [1.00211936], acc: 0.4375, samples: 32\n",
      "subnet: [90125], expert_idx: 0\n",
      "subnet: [90125], loss: [1.12561334], acc: 0.625, samples: 32\n",
      "subnet: [162769], expert_idx: 0\n",
      "subnet: [162769], loss: [1.29796173], acc: 0.375, samples: 32\n",
      "subnet: [173984], expert_idx: 0\n",
      "subnet: [173984], loss: [0.98554026], acc: 0.53125, samples: 32\n",
      "subnet: [157538], expert_idx: 0\n",
      "subnet: [157538], loss: [1.20715969], acc: 0.46875, samples: 32\n",
      "subnet: [237512], expert_idx: 0\n",
      "subnet: [237512], loss: [1.13283501], acc: 0.5625, samples: 32\n",
      "subnet: [8439], expert_idx: 0\n",
      "subnet: [8439], loss: [0.98958775], acc: 0.5625, samples: 32\n",
      "subnet: [265782], expert_idx: 0\n",
      "subnet: [265782], loss: [1.11584091], acc: 0.53125, samples: 32\n",
      "subnet: [7396], expert_idx: 0\n",
      "subnet: [7396], loss: [1.02083437], acc: 0.53125, samples: 32\n",
      "subnet: [69338], expert_idx: 0\n",
      "subnet: [69338], loss: [1.226812], acc: 0.4375, samples: 32\n",
      "subnet: [168687], expert_idx: 0\n",
      "subnet: [168687], loss: [1.21954161], acc: 0.40625, samples: 32\n",
      "subnet: [68265], expert_idx: 0\n",
      "subnet: [68265], loss: [1.15131965], acc: 0.4375, samples: 32\n",
      "subnet: [186469], expert_idx: 0\n",
      "subnet: [186469], loss: [1.04068859], acc: 0.4375, samples: 32\n",
      "subnet: [89846], expert_idx: 0\n",
      "subnet: [89846], loss: [1.00029732], acc: 0.4375, samples: 32\n",
      "subnet: [268838], expert_idx: 0\n",
      "subnet: [268838], loss: [1.24608577], acc: 0.5, samples: 32\n",
      "subnet: [306074], expert_idx: 0\n",
      "subnet: [306074], loss: [1.02445184], acc: 0.5, samples: 32\n",
      "subnet: [89196], expert_idx: 0\n",
      "subnet: [89196], loss: [1.24326293], acc: 0.46875, samples: 32\n",
      "subnet: [259629], expert_idx: 0\n",
      "subnet: [259629], loss: [1.00716783], acc: 0.5625, samples: 32\n",
      "subnet: [207817], expert_idx: 0\n",
      "subnet: [207817], loss: [0.96105523], acc: 0.53125, samples: 32\n",
      "subnet: [221059], expert_idx: 0\n",
      "subnet: [221059], loss: [1.09494489], acc: 0.625, samples: 32\n",
      "subnet: [136310], expert_idx: 0\n",
      "subnet: [136310], loss: [1.2886422], acc: 0.40625, samples: 32\n",
      "subnet: [304792], expert_idx: 0\n",
      "subnet: [304792], loss: [1.0522713], acc: 0.59375, samples: 32\n",
      "subnet: [148141], expert_idx: 0\n",
      "subnet: [148141], loss: [1.12028686], acc: 0.40625, samples: 32\n",
      "subnet: [264649], expert_idx: 0\n",
      "subnet: [264649], loss: [1.29457907], acc: 0.375, samples: 32\n",
      "subnet: [259625], expert_idx: 0\n",
      "subnet: [259625], loss: [1.00183537], acc: 0.5625, samples: 32\n",
      "subnet: [251879], expert_idx: 0\n",
      "subnet: [251879], loss: [1.35741653], acc: 0.4375, samples: 32\n",
      "subnet: [70114], expert_idx: 0\n",
      "subnet: [70114], loss: [0.99138818], acc: 0.625, samples: 32\n",
      "subnet: [68173], expert_idx: 0\n",
      "subnet: [68173], loss: [1.26481517], acc: 0.40625, samples: 32\n",
      "subnet: [331210], expert_idx: 0\n",
      "subnet: [331210], loss: [0.82175758], acc: 0.65625, samples: 32\n",
      "subnet: [168686], expert_idx: 0\n",
      "subnet: [168686], loss: [1.24556271], acc: 0.4375, samples: 32\n",
      "subnet: [7351], expert_idx: 0\n",
      "subnet: [7351], loss: [0.96499282], acc: 0.53125, samples: 32\n",
      "subnet: [169906], expert_idx: 0\n",
      "subnet: [169906], loss: [1.17489451], acc: 0.5625, samples: 32\n",
      "subnet: [79718], expert_idx: 0\n",
      "subnet: [79718], loss: [1.07139943], acc: 0.59375, samples: 32\n",
      "subnet: [190743], expert_idx: 0\n",
      "subnet: [190743], loss: [1.23655762], acc: 0.46875, samples: 32\n",
      "subnet: [192181], expert_idx: 0\n",
      "subnet: [192181], loss: [0.98669821], acc: 0.625, samples: 32\n",
      "subnet: [39812], expert_idx: 0\n",
      "subnet: [39812], loss: [1.10973381], acc: 0.375, samples: 32\n",
      "subnet: [84046], expert_idx: 0\n",
      "subnet: [84046], loss: [1.0026757], acc: 0.34375, samples: 32\n",
      "subnet: [7675], expert_idx: 0\n",
      "subnet: [7675], loss: [1.41935782], acc: 0.46875, samples: 32\n",
      "subnet: [37721], expert_idx: 0\n",
      "subnet: [37721], loss: [1.30973678], acc: 0.5, samples: 32\n",
      "subnet: [276636], expert_idx: 0\n",
      "subnet: [276636], loss: [1.33999704], acc: 0.3125, samples: 32\n",
      "subnet: [84526], expert_idx: 0\n",
      "subnet: [84526], loss: [1.02185082], acc: 0.5625, samples: 32\n",
      "subnet: [236925], expert_idx: 0\n",
      "subnet: [236925], loss: [1.45448085], acc: 0.4375, samples: 32\n",
      "subnet: [317378], expert_idx: 0\n",
      "subnet: [317378], loss: [1.34762188], acc: 0.4375, samples: 32\n",
      "subnet: [187867], expert_idx: 0\n",
      "subnet: [187867], loss: [1.07671291], acc: 0.53125, samples: 32\n",
      "subnet: [167514], expert_idx: 0\n",
      "subnet: [167514], loss: [1.05291492], acc: 0.53125, samples: 32\n",
      "subnet: [212493], expert_idx: 0\n",
      "subnet: [212493], loss: [1.2485808], acc: 0.4375, samples: 32\n",
      "subnet: [250528], expert_idx: 0\n",
      "subnet: [250528], loss: [1.19360876], acc: 0.375, samples: 32\n",
      "subnet: [302422], expert_idx: 0\n",
      "subnet: [302422], loss: [1.03913513], acc: 0.5625, samples: 32\n",
      "subnet: [259832], expert_idx: 0\n",
      "subnet: [259832], loss: [1.09904708], acc: 0.5, samples: 32\n",
      "subnet: [143799], expert_idx: 0\n",
      "subnet: [143799], loss: [1.19222893], acc: 0.4375, samples: 32\n",
      "subnet: [282349], expert_idx: 0\n",
      "subnet: [282349], loss: [1.23595947], acc: 0.5, samples: 32\n",
      "subnet: [258056], expert_idx: 0\n",
      "subnet: [258056], loss: [0.94325696], acc: 0.625, samples: 32\n",
      "subnet: [19705], expert_idx: 0\n",
      "subnet: [19705], loss: [1.16113874], acc: 0.5625, samples: 32\n",
      "subnet: [209896], expert_idx: 0\n",
      "subnet: [209896], loss: [1.31790051], acc: 0.5, samples: 32\n",
      "subnet: [286083], expert_idx: 0\n",
      "subnet: [286083], loss: [1.00696868], acc: 0.5625, samples: 32\n",
      "subnet: [207991], expert_idx: 0\n",
      "subnet: [207991], loss: [0.92947291], acc: 0.53125, samples: 32\n",
      "subnet: [316827], expert_idx: 0\n",
      "subnet: [316827], loss: [1.28221902], acc: 0.34375, samples: 32\n",
      "subnet: [233805], expert_idx: 0\n",
      "subnet: [233805], loss: [1.12046968], acc: 0.4375, samples: 32\n",
      "subnet: [21399], expert_idx: 0\n",
      "subnet: [21399], loss: [1.0730362], acc: 0.5, samples: 32\n",
      "subnet: [255153], expert_idx: 0\n",
      "subnet: [255153], loss: [1.0644483], acc: 0.5625, samples: 32\n",
      "subnet: [85045], expert_idx: 0\n",
      "subnet: [85045], loss: [1.39672997], acc: 0.46875, samples: 32\n",
      "subnet: [327388], expert_idx: 0\n",
      "subnet: [327388], loss: [1.10816137], acc: 0.4375, samples: 32\n",
      "subnet: [189988], expert_idx: 0\n",
      "subnet: [189988], loss: [1.39716373], acc: 0.375, samples: 32\n",
      "subnet: [220604], expert_idx: 0\n",
      "subnet: [220604], loss: [1.04082503], acc: 0.5625, samples: 32\n",
      "subnet: [140540], expert_idx: 0\n",
      "subnet: [140540], loss: [1.44381571], acc: 0.34375, samples: 32\n",
      "subnet: [273013], expert_idx: 0\n",
      "subnet: [273013], loss: [1.03916421], acc: 0.46875, samples: 32\n",
      "subnet: [298224], expert_idx: 0\n",
      "subnet: [298224], loss: [1.09486454], acc: 0.4375, samples: 32\n",
      "subnet: [262647], expert_idx: 0\n",
      "subnet: [262647], loss: [1.09747648], acc: 0.4375, samples: 32\n",
      "subnet: [50177], expert_idx: 0\n",
      "subnet: [50177], loss: [1.01239225], acc: 0.53125, samples: 32\n",
      "subnet: [164333], expert_idx: 0\n",
      "subnet: [164333], loss: [1.18069271], acc: 0.4375, samples: 32\n",
      "subnet: [161024], expert_idx: 0\n",
      "subnet: [161024], loss: [1.13207927], acc: 0.4375, samples: 32\n",
      "subnet: [65705], expert_idx: 0\n",
      "subnet: [65705], loss: [1.27724593], acc: 0.46875, samples: 32\n",
      "subnet: [259197], expert_idx: 0\n",
      "subnet: [259197], loss: [1.18480955], acc: 0.46875, samples: 32\n",
      "subnet: [269481], expert_idx: 0\n",
      "subnet: [269481], loss: [1.03482762], acc: 0.4375, samples: 32\n",
      "subnet: [111517], expert_idx: 0\n",
      "subnet: [111517], loss: [1.05757269], acc: 0.46875, samples: 32\n",
      "subnet: [279274], expert_idx: 0\n",
      "subnet: [279274], loss: [1.04428724], acc: 0.59375, samples: 32\n",
      "subnet: [76453], expert_idx: 0\n",
      "subnet: [76453], loss: [0.96533525], acc: 0.5625, samples: 32\n",
      "subnet: [178299], expert_idx: 0\n",
      "subnet: [178299], loss: [1.25379728], acc: 0.46875, samples: 32\n",
      "subnet: [100475], expert_idx: 0\n",
      "subnet: [100475], loss: [1.471781], acc: 0.40625, samples: 32\n",
      "subnet: [26148], expert_idx: 0\n",
      "subnet: [26148], loss: [1.00356465], acc: 0.46875, samples: 32\n",
      "subnet: [274191], expert_idx: 0\n",
      "subnet: [274191], loss: [1.01050113], acc: 0.59375, samples: 32\n",
      "subnet: [95444], expert_idx: 0\n",
      "subnet: [95444], loss: [1.10056077], acc: 0.53125, samples: 32\n",
      "subnet: [263151], expert_idx: 0\n",
      "subnet: [263151], loss: [1.12707123], acc: 0.59375, samples: 32\n",
      "subnet: [252317], expert_idx: 0\n",
      "subnet: [252317], loss: [1.19550137], acc: 0.46875, samples: 32\n",
      "subnet: [26698], expert_idx: 0\n",
      "subnet: [26698], loss: [1.0257159], acc: 0.5, samples: 32\n",
      "subnet: [87315], expert_idx: 0\n",
      "subnet: [87315], loss: [1.15331221], acc: 0.46875, samples: 32\n",
      "subnet: [28941], expert_idx: 0\n",
      "subnet: [28941], loss: [1.0556471], acc: 0.59375, samples: 32\n",
      "subnet: [68550], expert_idx: 0\n",
      "subnet: [68550], loss: [0.97046057], acc: 0.5625, samples: 32\n",
      "subnet: [275623], expert_idx: 0\n",
      "subnet: [275623], loss: [0.97960649], acc: 0.71875, samples: 32\n",
      "subnet: [90948], expert_idx: 0\n",
      "subnet: [90948], loss: [1.17205071], acc: 0.59375, samples: 32\n",
      "subnet: [11648], expert_idx: 0\n",
      "subnet: [11648], loss: [1.08014938], acc: 0.53125, samples: 32\n",
      "subnet: [100875], expert_idx: 0\n",
      "subnet: [100875], loss: [1.06961882], acc: 0.53125, samples: 32\n",
      "subnet: [295061], expert_idx: 0\n",
      "subnet: [295061], loss: [1.12816253], acc: 0.53125, samples: 32\n",
      "subnet: [114064], expert_idx: 0\n",
      "subnet: [114064], loss: [0.97463962], acc: 0.5, samples: 32\n",
      "subnet: [178372], expert_idx: 0\n",
      "subnet: [178372], loss: [1.04325269], acc: 0.46875, samples: 32\n",
      "subnet: [176704], expert_idx: 0\n",
      "subnet: [176704], loss: [1.00479582], acc: 0.625, samples: 32\n",
      "subnet: [60121], expert_idx: 0\n",
      "subnet: [60121], loss: [1.18130218], acc: 0.5, samples: 32\n",
      "subnet: [196063], expert_idx: 0\n",
      "subnet: [196063], loss: [1.34648626], acc: 0.4375, samples: 32\n",
      "subnet: [234870], expert_idx: 0\n",
      "subnet: [234870], loss: [1.13380416], acc: 0.59375, samples: 32\n",
      "subnet: [32153], expert_idx: 0\n",
      "subnet: [32153], loss: [1.02114278], acc: 0.5625, samples: 32\n",
      "subnet: [109338], expert_idx: 0\n",
      "subnet: [109338], loss: [1.12937331], acc: 0.4375, samples: 32\n",
      "subnet: [72573], expert_idx: 0\n",
      "subnet: [72573], loss: [0.99085756], acc: 0.53125, samples: 32\n",
      "subnet: [222745], expert_idx: 0\n",
      "subnet: [222745], loss: [1.31567756], acc: 0.375, samples: 32\n",
      "subnet: [29575], expert_idx: 0\n",
      "subnet: [29575], loss: [0.91064333], acc: 0.625, samples: 32\n",
      "subnet: [314985], expert_idx: 0\n",
      "subnet: [314985], loss: [1.01512394], acc: 0.53125, samples: 32\n",
      "subnet: [119177], expert_idx: 0\n",
      "subnet: [119177], loss: [0.99270676], acc: 0.5625, samples: 32\n",
      "subnet: [253228], expert_idx: 0\n",
      "subnet: [253228], loss: [1.03827703], acc: 0.59375, samples: 32\n",
      "subnet: [219816], expert_idx: 0\n",
      "subnet: [219816], loss: [0.92779841], acc: 0.59375, samples: 32\n",
      "subnet: [24314], expert_idx: 0\n",
      "subnet: [24314], loss: [0.97103162], acc: 0.5625, samples: 32\n",
      "subnet: [125888], expert_idx: 0\n",
      "subnet: [125888], loss: [1.13348461], acc: 0.5, samples: 32\n",
      "subnet: [301441], expert_idx: 0\n",
      "subnet: [301441], loss: [1.2369318], acc: 0.28125, samples: 32\n",
      "subnet: [148367], expert_idx: 0\n",
      "subnet: [148367], loss: [0.98076399], acc: 0.625, samples: 32\n",
      "subnet: [199971], expert_idx: 0\n",
      "subnet: [199971], loss: [1.25415708], acc: 0.34375, samples: 32\n",
      "subnet: [99090], expert_idx: 0\n",
      "subnet: [99090], loss: [1.20929973], acc: 0.46875, samples: 32\n",
      "subnet: [196854], expert_idx: 0\n",
      "subnet: [196854], loss: [0.8799087], acc: 0.65625, samples: 32\n",
      "subnet: [105876], expert_idx: 0\n",
      "subnet: [105876], loss: [1.26392935], acc: 0.40625, samples: 32\n",
      "subnet: [44185], expert_idx: 0\n",
      "subnet: [44185], loss: [1.14579949], acc: 0.46875, samples: 32\n",
      "subnet: [77613], expert_idx: 0\n",
      "subnet: [77613], loss: [1.02640363], acc: 0.4375, samples: 32\n",
      "subnet: [20284], expert_idx: 0\n",
      "subnet: [20284], loss: [0.9223974], acc: 0.65625, samples: 32\n",
      "subnet: [40370], expert_idx: 0\n",
      "subnet: [40370], loss: [0.98605158], acc: 0.6875, samples: 32\n",
      "subnet: [220080], expert_idx: 0\n",
      "subnet: [220080], loss: [1.22324774], acc: 0.46875, samples: 32\n",
      "subnet: [88131], expert_idx: 0\n",
      "subnet: [88131], loss: [1.55637974], acc: 0.375, samples: 32\n",
      "subnet: [209521], expert_idx: 0\n",
      "subnet: [209521], loss: [1.08212742], acc: 0.53125, samples: 32\n",
      "subnet: [19819], expert_idx: 0\n",
      "subnet: [19819], loss: [0.99601523], acc: 0.59375, samples: 32\n",
      "subnet: [95508], expert_idx: 0\n",
      "subnet: [95508], loss: [1.04150543], acc: 0.5625, samples: 32\n",
      "subnet: [103645], expert_idx: 0\n",
      "subnet: [103645], loss: [1.37339211], acc: 0.375, samples: 32\n",
      "subnet: [284901], expert_idx: 0\n",
      "subnet: [284901], loss: [1.17382602], acc: 0.4375, samples: 32\n",
      "subnet: [31337], expert_idx: 0\n",
      "subnet: [31337], loss: [1.10152107], acc: 0.4375, samples: 32\n",
      "subnet: [59082], expert_idx: 0\n",
      "subnet: [59082], loss: [1.34412034], acc: 0.34375, samples: 32\n",
      "subnet: [303076], expert_idx: 0\n",
      "subnet: [303076], loss: [1.13645253], acc: 0.59375, samples: 32\n",
      "subnet: [312540], expert_idx: 0\n",
      "subnet: [312540], loss: [1.09174556], acc: 0.53125, samples: 32\n",
      "subnet: [316773], expert_idx: 0\n",
      "subnet: [316773], loss: [1.08977944], acc: 0.46875, samples: 32\n",
      "subnet: [236115], expert_idx: 0\n",
      "subnet: [236115], loss: [1.22513742], acc: 0.46875, samples: 32\n",
      "subnet: [212685], expert_idx: 0\n",
      "subnet: [212685], loss: [1.1632177], acc: 0.53125, samples: 32\n",
      "subnet: [158456], expert_idx: 0\n",
      "subnet: [158456], loss: [1.03820402], acc: 0.4375, samples: 32\n",
      "subnet: [263202], expert_idx: 0\n",
      "subnet: [263202], loss: [1.22846131], acc: 0.4375, samples: 32\n",
      "subnet: [43150], expert_idx: 0\n",
      "subnet: [43150], loss: [0.87798989], acc: 0.6875, samples: 32\n",
      "subnet: [175017], expert_idx: 0\n",
      "subnet: [175017], loss: [1.04698203], acc: 0.40625, samples: 32\n",
      "subnet: [11090], expert_idx: 0\n",
      "subnet: [11090], loss: [1.01126001], acc: 0.4375, samples: 32\n",
      "subnet: [305503], expert_idx: 0\n",
      "subnet: [305503], loss: [1.11030557], acc: 0.5625, samples: 32\n",
      "subnet: [233094], expert_idx: 0\n",
      "subnet: [233094], loss: [1.07993227], acc: 0.5, samples: 32\n",
      "subnet: [55726], expert_idx: 0\n",
      "subnet: [55726], loss: [1.02684822], acc: 0.53125, samples: 32\n",
      "subnet: [50888], expert_idx: 0\n",
      "subnet: [50888], loss: [0.99970337], acc: 0.625, samples: 32\n",
      "subnet: [319150], expert_idx: 0\n",
      "subnet: [319150], loss: [1.10544586], acc: 0.5, samples: 32\n",
      "subnet: [65336], expert_idx: 0\n",
      "subnet: [65336], loss: [1.25536955], acc: 0.4375, samples: 32\n",
      "subnet: [56338], expert_idx: 0\n",
      "subnet: [56338], loss: [1.20448908], acc: 0.46875, samples: 32\n",
      "subnet: [16141], expert_idx: 0\n",
      "subnet: [16141], loss: [0.99477473], acc: 0.5625, samples: 32\n",
      "subnet: [69806], expert_idx: 0\n",
      "subnet: [69806], loss: [1.32524406], acc: 0.46875, samples: 32\n",
      "subnet: [273585], expert_idx: 0\n",
      "subnet: [273585], loss: [0.93291019], acc: 0.59375, samples: 32\n",
      "subnet: [277098], expert_idx: 0\n",
      "subnet: [277098], loss: [1.05257275], acc: 0.53125, samples: 32\n",
      "subnet: [83353], expert_idx: 0\n",
      "subnet: [83353], loss: [1.29253324], acc: 0.3125, samples: 32\n",
      "subnet: [58010], expert_idx: 0\n",
      "subnet: [58010], loss: [1.20668966], acc: 0.5, samples: 32\n",
      "subnet: [70264], expert_idx: 0\n",
      "subnet: [70264], loss: [1.15734299], acc: 0.46875, samples: 32\n",
      "subnet: [130814], expert_idx: 0\n",
      "subnet: [130814], loss: [1.42932233], acc: 0.34375, samples: 32\n",
      "subnet: [316652], expert_idx: 0\n",
      "subnet: [316652], loss: [1.26740812], acc: 0.40625, samples: 32\n",
      "subnet: [6856], expert_idx: 0\n",
      "subnet: [6856], loss: [1.02926919], acc: 0.5, samples: 32\n",
      "subnet: [197222], expert_idx: 0\n",
      "subnet: [197222], loss: [0.84226115], acc: 0.625, samples: 32\n",
      "subnet: [325411], expert_idx: 0\n",
      "subnet: [325411], loss: [1.13544573], acc: 0.4375, samples: 32\n",
      "subnet: [94580], expert_idx: 0\n",
      "subnet: [94580], loss: [1.33763578], acc: 0.4375, samples: 32\n",
      "subnet: [313954], expert_idx: 0\n",
      "subnet: [313954], loss: [1.45964342], acc: 0.25, samples: 32\n",
      "subnet: [236372], expert_idx: 0\n",
      "subnet: [236372], loss: [1.54106536], acc: 0.5, samples: 32\n",
      "subnet: [21997], expert_idx: 0\n",
      "subnet: [21997], loss: [1.09304039], acc: 0.375, samples: 32\n",
      "subnet: [128266], expert_idx: 0\n",
      "subnet: [128266], loss: [1.35215309], acc: 0.375, samples: 32\n",
      "subnet: [266321], expert_idx: 0\n",
      "subnet: [266321], loss: [1.08969909], acc: 0.40625, samples: 32\n",
      "subnet: [277826], expert_idx: 0\n",
      "subnet: [277826], loss: [1.25129121], acc: 0.59375, samples: 32\n",
      "subnet: [87950], expert_idx: 0\n",
      "subnet: [87950], loss: [1.01664563], acc: 0.40625, samples: 32\n",
      "subnet: [205190], expert_idx: 0\n",
      "subnet: [205190], loss: [1.21138666], acc: 0.46875, samples: 32\n",
      "subnet: [326856], expert_idx: 0\n",
      "subnet: [326856], loss: [1.15968196], acc: 0.5, samples: 32\n",
      "subnet: [2700], expert_idx: 0\n",
      "subnet: [2700], loss: [0.98814877], acc: 0.46875, samples: 32\n",
      "subnet: [64116], expert_idx: 0\n",
      "subnet: [64116], loss: [1.20554097], acc: 0.4375, samples: 32\n",
      "subnet: [103279], expert_idx: 0\n",
      "subnet: [103279], loss: [1.01444167], acc: 0.46875, samples: 32\n",
      "subnet: [202618], expert_idx: 0\n",
      "subnet: [202618], loss: [1.11096929], acc: 0.5, samples: 32\n",
      "subnet: [311996], expert_idx: 0\n",
      "subnet: [311996], loss: [1.12836934], acc: 0.53125, samples: 32\n",
      "subnet: [12357], expert_idx: 0\n",
      "subnet: [12357], loss: [1.1522503], acc: 0.4375, samples: 32\n",
      "subnet: [305288], expert_idx: 0\n",
      "subnet: [305288], loss: [1.05586522], acc: 0.5625, samples: 32\n",
      "subnet: [1137], expert_idx: 0\n",
      "subnet: [1137], loss: [1.05032672], acc: 0.375, samples: 32\n",
      "subnet: [35779], expert_idx: 0\n",
      "subnet: [35779], loss: [1.14344429], acc: 0.59375, samples: 32\n",
      "subnet: [250788], expert_idx: 0\n",
      "subnet: [250788], loss: [0.86337297], acc: 0.6875, samples: 32\n",
      "subnet: [249527], expert_idx: 0\n",
      "subnet: [249527], loss: [0.99560972], acc: 0.53125, samples: 32\n",
      "subnet: [161344], expert_idx: 0\n",
      "subnet: [161344], loss: [1.08119942], acc: 0.53125, samples: 32\n",
      "subnet: [236269], expert_idx: 0\n",
      "subnet: [236269], loss: [1.05350216], acc: 0.53125, samples: 32\n",
      "subnet: [225079], expert_idx: 0\n",
      "subnet: [225079], loss: [0.86386793], acc: 0.625, samples: 32\n",
      "subnet: [26189], expert_idx: 0\n",
      "subnet: [26189], loss: [1.20280154], acc: 0.40625, samples: 32\n",
      "subnet: [125785], expert_idx: 0\n",
      "subnet: [125785], loss: [1.14987637], acc: 0.5, samples: 32\n",
      "subnet: [233524], expert_idx: 0\n",
      "subnet: [233524], loss: [0.91824327], acc: 0.5625, samples: 32\n",
      "subnet: [145067], expert_idx: 0\n",
      "subnet: [145067], loss: [1.06043236], acc: 0.53125, samples: 32\n",
      "subnet: [81621], expert_idx: 0\n",
      "subnet: [81621], loss: [1.09341715], acc: 0.53125, samples: 32\n",
      "subnet: [252489], expert_idx: 0\n",
      "subnet: [252489], loss: [0.9882972], acc: 0.5625, samples: 32\n",
      "subnet: [107663], expert_idx: 0\n",
      "subnet: [107663], loss: [1.21167516], acc: 0.5, samples: 32\n",
      "subnet: [106767], expert_idx: 0\n",
      "subnet: [106767], loss: [1.30261298], acc: 0.4375, samples: 32\n",
      "subnet: [320950], expert_idx: 0\n",
      "subnet: [320950], loss: [1.01288073], acc: 0.46875, samples: 32\n",
      "subnet: [178114], expert_idx: 0\n",
      "subnet: [178114], loss: [1.16423863], acc: 0.46875, samples: 32\n",
      "subnet: [269922], expert_idx: 0\n",
      "subnet: [269922], loss: [1.01885864], acc: 0.4375, samples: 32\n",
      "subnet: [180170], expert_idx: 0\n",
      "subnet: [180170], loss: [1.13824059], acc: 0.5, samples: 32\n",
      "subnet: [142247], expert_idx: 0\n",
      "subnet: [142247], loss: [0.9161195], acc: 0.625, samples: 32\n",
      "subnet: [275991], expert_idx: 0\n",
      "subnet: [275991], loss: [0.98044339], acc: 0.65625, samples: 32\n",
      "subnet: [267354], expert_idx: 0\n",
      "subnet: [267354], loss: [1.0274628], acc: 0.5, samples: 32\n",
      "subnet: [261364], expert_idx: 0\n",
      "subnet: [261364], loss: [1.09117255], acc: 0.46875, samples: 32\n",
      "subnet: [262999], expert_idx: 0\n",
      "subnet: [262999], loss: [0.81635812], acc: 0.75, samples: 32\n",
      "subnet: [149596], expert_idx: 0\n",
      "subnet: [149596], loss: [0.87482331], acc: 0.6875, samples: 32\n",
      "subnet: [176918], expert_idx: 0\n",
      "subnet: [176918], loss: [1.10858851], acc: 0.53125, samples: 32\n",
      "subnet: [220196], expert_idx: 0\n",
      "subnet: [220196], loss: [0.9948773], acc: 0.625, samples: 32\n",
      "subnet: [24272], expert_idx: 0\n",
      "subnet: [24272], loss: [0.99457479], acc: 0.5625, samples: 32\n",
      "subnet: [50333], expert_idx: 0\n",
      "subnet: [50333], loss: [0.94225637], acc: 0.625, samples: 32\n",
      "subnet: [235570], expert_idx: 0\n",
      "subnet: [235570], loss: [1.04163122], acc: 0.625, samples: 32\n",
      "subnet: [186027], expert_idx: 0\n",
      "subnet: [186027], loss: [1.25995752], acc: 0.40625, samples: 32\n",
      "subnet: [155601], expert_idx: 0\n",
      "subnet: [155601], loss: [1.11903867], acc: 0.53125, samples: 32\n",
      "subnet: [35312], expert_idx: 0\n",
      "subnet: [35312], loss: [1.01066898], acc: 0.5, samples: 32\n",
      "subnet: [50903], expert_idx: 0\n",
      "subnet: [50903], loss: [1.20715549], acc: 0.59375, samples: 32\n",
      "subnet: [104932], expert_idx: 0\n",
      "subnet: [104932], loss: [1.11918913], acc: 0.53125, samples: 32\n",
      "subnet: [99444], expert_idx: 0\n",
      "subnet: [99444], loss: [1.21236761], acc: 0.40625, samples: 32\n",
      "subnet: [239232], expert_idx: 0\n",
      "subnet: [239232], loss: [1.22118211], acc: 0.5, samples: 32\n",
      "subnet: [32629], expert_idx: 0\n",
      "subnet: [32629], loss: [0.99871232], acc: 0.5, samples: 32\n",
      "subnet: [274149], expert_idx: 0\n",
      "subnet: [274149], loss: [1.06273995], acc: 0.53125, samples: 32\n",
      "subnet: [129908], expert_idx: 0\n",
      "subnet: [129908], loss: [1.01813827], acc: 0.59375, samples: 32\n",
      "subnet: [213270], expert_idx: 0\n",
      "subnet: [213270], loss: [1.14065621], acc: 0.375, samples: 32\n",
      "subnet: [94130], expert_idx: 0\n",
      "subnet: [94130], loss: [1.18458637], acc: 0.4375, samples: 32\n",
      "subnet: [86983], expert_idx: 0\n",
      "subnet: [86983], loss: [1.33614819], acc: 0.34375, samples: 32\n",
      "subnet: [54838], expert_idx: 0\n",
      "subnet: [54838], loss: [1.36002471], acc: 0.34375, samples: 32\n",
      "subnet: [32370], expert_idx: 0\n",
      "subnet: [32370], loss: [0.98614114], acc: 0.4375, samples: 32\n",
      "subnet: [134557], expert_idx: 0\n",
      "subnet: [134557], loss: [1.11558011], acc: 0.59375, samples: 32\n",
      "subnet: [331024], expert_idx: 0\n",
      "subnet: [331024], loss: [1.27297598], acc: 0.3125, samples: 32\n",
      "subnet: [165378], expert_idx: 0\n",
      "subnet: [165378], loss: [1.1855283], acc: 0.59375, samples: 32\n",
      "subnet: [305518], expert_idx: 0\n",
      "subnet: [305518], loss: [1.34358265], acc: 0.40625, samples: 32\n",
      "subnet: [274367], expert_idx: 0\n",
      "subnet: [274367], loss: [1.12358372], acc: 0.53125, samples: 32\n",
      "subnet: [212921], expert_idx: 0\n",
      "subnet: [212921], loss: [1.18614832], acc: 0.46875, samples: 32\n",
      "subnet: [171140], expert_idx: 0\n",
      "subnet: [171140], loss: [1.13858703], acc: 0.40625, samples: 32\n",
      "subnet: [252844], expert_idx: 0\n",
      "subnet: [252844], loss: [1.06994115], acc: 0.4375, samples: 32\n",
      "subnet: [139191], expert_idx: 0\n",
      "subnet: [139191], loss: [1.14557789], acc: 0.4375, samples: 32\n",
      "subnet: [268005], expert_idx: 0\n",
      "subnet: [268005], loss: [1.05259352], acc: 0.40625, samples: 32\n",
      "subnet: [250938], expert_idx: 0\n",
      "subnet: [250938], loss: [1.22728728], acc: 0.34375, samples: 32\n",
      "subnet: [101771], expert_idx: 0\n",
      "subnet: [101771], loss: [1.17317238], acc: 0.4375, samples: 32\n",
      "subnet: [187703], expert_idx: 0\n",
      "subnet: [187703], loss: [1.11610301], acc: 0.4375, samples: 32\n",
      "subnet: [59845], expert_idx: 0\n",
      "subnet: [59845], loss: [1.14093545], acc: 0.46875, samples: 32\n",
      "subnet: [230749], expert_idx: 0\n",
      "subnet: [230749], loss: [1.54049539], acc: 0.375, samples: 32\n",
      "subnet: [264392], expert_idx: 0\n",
      "subnet: [264392], loss: [0.99802884], acc: 0.53125, samples: 32\n",
      "subnet: [254976], expert_idx: 0\n",
      "subnet: [254976], loss: [0.83714424], acc: 0.71875, samples: 32\n",
      "subnet: [236629], expert_idx: 0\n",
      "subnet: [236629], loss: [1.05297867], acc: 0.53125, samples: 32\n",
      "subnet: [139179], expert_idx: 0\n",
      "subnet: [139179], loss: [1.05257581], acc: 0.53125, samples: 32\n",
      "subnet: [137274], expert_idx: 0\n",
      "subnet: [137274], loss: [0.95986569], acc: 0.5625, samples: 32\n",
      "subnet: [26788], expert_idx: 0\n",
      "subnet: [26788], loss: [0.99640221], acc: 0.53125, samples: 32\n",
      "subnet: [175295], expert_idx: 0\n",
      "subnet: [175295], loss: [0.97990881], acc: 0.53125, samples: 32\n",
      "subnet: [2078], expert_idx: 0\n",
      "subnet: [2078], loss: [1.17407794], acc: 0.46875, samples: 32\n",
      "subnet: [288320], expert_idx: 0\n",
      "subnet: [288320], loss: [1.33389784], acc: 0.34375, samples: 32\n",
      "subnet: [29406], expert_idx: 0\n",
      "subnet: [29406], loss: [0.978983], acc: 0.5625, samples: 32\n",
      "subnet: [284419], expert_idx: 0\n",
      "subnet: [284419], loss: [1.08843628], acc: 0.4375, samples: 32\n",
      "subnet: [147586], expert_idx: 0\n",
      "subnet: [147586], loss: [1.01296245], acc: 0.5, samples: 32\n",
      "subnet: [24784], expert_idx: 0\n",
      "subnet: [24784], loss: [1.12575281], acc: 0.46875, samples: 32\n",
      "subnet: [240080], expert_idx: 0\n",
      "subnet: [240080], loss: [1.01706661], acc: 0.53125, samples: 32\n",
      "subnet: [199026], expert_idx: 0\n",
      "subnet: [199026], loss: [1.45528221], acc: 0.40625, samples: 32\n",
      "subnet: [229355], expert_idx: 0\n",
      "subnet: [229355], loss: [1.06667162], acc: 0.5625, samples: 32\n",
      "subnet: [100670], expert_idx: 0\n",
      "subnet: [100670], loss: [0.98558415], acc: 0.625, samples: 32\n",
      "subnet: [113394], expert_idx: 0\n",
      "subnet: [113394], loss: [1.13503894], acc: 0.375, samples: 32\n",
      "subnet: [319725], expert_idx: 0\n",
      "subnet: [319725], loss: [1.24606916], acc: 0.40625, samples: 32\n",
      "subnet: [180240], expert_idx: 0\n",
      "subnet: [180240], loss: [1.08467046], acc: 0.625, samples: 32\n",
      "subnet: [123791], expert_idx: 0\n",
      "subnet: [123791], loss: [1.18507573], acc: 0.4375, samples: 32\n",
      "subnet: [317390], expert_idx: 0\n",
      "subnet: [317390], loss: [1.38342421], acc: 0.375, samples: 32\n",
      "subnet: [138690], expert_idx: 0\n",
      "subnet: [138690], loss: [1.10275823], acc: 0.5625, samples: 32\n",
      "subnet: [299827], expert_idx: 0\n",
      "subnet: [299827], loss: [1.23988428], acc: 0.5, samples: 32\n",
      "subnet: [254898], expert_idx: 0\n",
      "subnet: [254898], loss: [1.05735042], acc: 0.59375, samples: 32\n",
      "subnet: [183966], expert_idx: 0\n",
      "subnet: [183966], loss: [0.87599858], acc: 0.65625, samples: 32\n",
      "subnet: [24625], expert_idx: 0\n",
      "subnet: [24625], loss: [0.99825431], acc: 0.53125, samples: 32\n",
      "subnet: [11183], expert_idx: 0\n",
      "subnet: [11183], loss: [1.02564591], acc: 0.46875, samples: 32\n",
      "subnet: [70623], expert_idx: 0\n",
      "subnet: [70623], loss: [1.27739419], acc: 0.5, samples: 32\n",
      "subnet: [189824], expert_idx: 0\n",
      "subnet: [189824], loss: [1.0717198], acc: 0.59375, samples: 32\n",
      "subnet: [210751], expert_idx: 0\n",
      "subnet: [210751], loss: [1.16971521], acc: 0.4375, samples: 32\n",
      "subnet: [38836], expert_idx: 0\n",
      "subnet: [38836], loss: [1.02671534], acc: 0.53125, samples: 32\n",
      "subnet: [131870], expert_idx: 0\n",
      "subnet: [131870], loss: [1.05521516], acc: 0.5625, samples: 32\n",
      "subnet: [117028], expert_idx: 0\n",
      "subnet: [117028], loss: [0.91533527], acc: 0.65625, samples: 32\n",
      "subnet: [16093], expert_idx: 0\n",
      "subnet: [16093], loss: [1.09086371], acc: 0.5, samples: 32\n",
      "subnet: [118822], expert_idx: 0\n",
      "subnet: [118822], loss: [1.13292685], acc: 0.4375, samples: 32\n",
      "subnet: [33110], expert_idx: 0\n",
      "subnet: [33110], loss: [1.13261862], acc: 0.375, samples: 32\n",
      "subnet: [78809], expert_idx: 0\n",
      "subnet: [78809], loss: [1.441543], acc: 0.375, samples: 32\n",
      "subnet: [183694], expert_idx: 0\n",
      "subnet: [183694], loss: [0.78928204], acc: 0.6875, samples: 32\n",
      "subnet: [259105], expert_idx: 0\n",
      "subnet: [259105], loss: [1.00163015], acc: 0.5625, samples: 32\n",
      "subnet: [236509], expert_idx: 0\n",
      "subnet: [236509], loss: [1.26177], acc: 0.34375, samples: 32\n",
      "subnet: [43482], expert_idx: 0\n",
      "subnet: [43482], loss: [1.36253886], acc: 0.40625, samples: 32\n",
      "subnet: [42587], expert_idx: 0\n",
      "subnet: [42587], loss: [0.90465583], acc: 0.5625, samples: 32\n",
      "subnet: [180617], expert_idx: 0\n",
      "subnet: [180617], loss: [1.05946837], acc: 0.46875, samples: 32\n",
      "subnet: [73121], expert_idx: 0\n",
      "subnet: [73121], loss: [1.09192498], acc: 0.53125, samples: 32\n",
      "subnet: [68473], expert_idx: 0\n",
      "subnet: [68473], loss: [1.40089312], acc: 0.3125, samples: 32\n",
      "subnet: [140519], expert_idx: 0\n",
      "subnet: [140519], loss: [1.19047788], acc: 0.46875, samples: 32\n",
      "subnet: [270035], expert_idx: 0\n",
      "subnet: [270035], loss: [1.03274851], acc: 0.40625, samples: 32\n",
      "subnet: [192799], expert_idx: 0\n",
      "subnet: [192799], loss: [1.14065936], acc: 0.5, samples: 32\n",
      "subnet: [46776], expert_idx: 0\n",
      "subnet: [46776], loss: [1.10442929], acc: 0.4375, samples: 32\n",
      "subnet: [102516], expert_idx: 0\n",
      "subnet: [102516], loss: [1.13379587], acc: 0.46875, samples: 32\n",
      "subnet: [161511], expert_idx: 0\n",
      "subnet: [161511], loss: [1.24352247], acc: 0.5, samples: 32\n",
      "subnet: [169045], expert_idx: 0\n",
      "subnet: [169045], loss: [0.95696977], acc: 0.59375, samples: 32\n",
      "subnet: [172386], expert_idx: 0\n",
      "subnet: [172386], loss: [1.03182364], acc: 0.59375, samples: 32\n",
      "subnet: [165709], expert_idx: 0\n",
      "subnet: [165709], loss: [1.01817502], acc: 0.5, samples: 32\n",
      "subnet: [129658], expert_idx: 0\n",
      "subnet: [129658], loss: [1.07675143], acc: 0.46875, samples: 32\n",
      "subnet: [266106], expert_idx: 0\n",
      "subnet: [266106], loss: [1.45448085], acc: 0.4375, samples: 32\n",
      "subnet: [37082], expert_idx: 0\n",
      "subnet: [37082], loss: [0.97413861], acc: 0.625, samples: 32\n",
      "subnet: [48659], expert_idx: 0\n",
      "subnet: [48659], loss: [1.24602852], acc: 0.375, samples: 32\n",
      "subnet: [193388], expert_idx: 0\n",
      "subnet: [193388], loss: [1.45300684], acc: 0.40625, samples: 32\n",
      "subnet: [185392], expert_idx: 0\n",
      "subnet: [185392], loss: [0.96135116], acc: 0.5625, samples: 32\n",
      "subnet: [133759], expert_idx: 0\n",
      "subnet: [133759], loss: [0.99994064], acc: 0.5, samples: 32\n",
      "subnet: [103384], expert_idx: 0\n",
      "subnet: [103384], loss: [1.27089448], acc: 0.40625, samples: 32\n",
      "subnet: [296061], expert_idx: 0\n",
      "subnet: [296061], loss: [1.03736028], acc: 0.5625, samples: 32\n",
      "subnet: [81461], expert_idx: 0\n",
      "subnet: [81461], loss: [1.03244475], acc: 0.5, samples: 32\n",
      "subnet: [150466], expert_idx: 0\n",
      "subnet: [150466], loss: [1.32260521], acc: 0.40625, samples: 32\n",
      "subnet: [97001], expert_idx: 0\n",
      "subnet: [97001], loss: [1.00319653], acc: 0.5, samples: 32\n",
      "subnet: [209404], expert_idx: 0\n",
      "subnet: [209404], loss: [1.03974517], acc: 0.53125, samples: 32\n",
      "subnet: [323082], expert_idx: 0\n",
      "subnet: [323082], loss: [1.0146454], acc: 0.53125, samples: 32\n",
      "subnet: [96524], expert_idx: 0\n",
      "subnet: [96524], loss: [1.08467046], acc: 0.625, samples: 32\n",
      "subnet: [299668], expert_idx: 0\n",
      "subnet: [299668], loss: [1.04753941], acc: 0.53125, samples: 32\n",
      "subnet: [149434], expert_idx: 0\n",
      "subnet: [149434], loss: [1.04548712], acc: 0.53125, samples: 32\n",
      "subnet: [95075], expert_idx: 0\n",
      "subnet: [95075], loss: [1.15069463], acc: 0.5625, samples: 32\n",
      "subnet: [256328], expert_idx: 0\n",
      "subnet: [256328], loss: [1.29189959], acc: 0.5, samples: 32\n",
      "subnet: [152302], expert_idx: 0\n",
      "subnet: [152302], loss: [1.09737537], acc: 0.5625, samples: 32\n",
      "subnet: [72585], expert_idx: 0\n",
      "subnet: [72585], loss: [0.96684938], acc: 0.5625, samples: 32\n",
      "subnet: [41650], expert_idx: 0\n",
      "subnet: [41650], loss: [1.10918996], acc: 0.375, samples: 32\n",
      "subnet: [203293], expert_idx: 0\n",
      "subnet: [203293], loss: [1.16667298], acc: 0.5625, samples: 32\n",
      "subnet: [84235], expert_idx: 0\n",
      "subnet: [84235], loss: [1.22528335], acc: 0.46875, samples: 32\n",
      "subnet: [295723], expert_idx: 0\n",
      "subnet: [295723], loss: [1.23513989], acc: 0.5, samples: 32\n",
      "subnet: [198200], expert_idx: 0\n",
      "subnet: [198200], loss: [1.15998756], acc: 0.4375, samples: 32\n",
      "subnet: [92765], expert_idx: 0\n",
      "subnet: [92765], loss: [1.10468784], acc: 0.53125, samples: 32\n",
      "subnet: [238136], expert_idx: 0\n",
      "subnet: [238136], loss: [0.97393183], acc: 0.59375, samples: 32\n",
      "subnet: [309218], expert_idx: 0\n",
      "subnet: [309218], loss: [1.34998107], acc: 0.53125, samples: 32\n",
      "subnet: [283802], expert_idx: 0\n",
      "subnet: [283802], loss: [1.1263486], acc: 0.4375, samples: 32\n",
      "subnet: [54100], expert_idx: 0\n",
      "subnet: [54100], loss: [1.19288893], acc: 0.4375, samples: 32\n",
      "subnet: [306550], expert_idx: 0\n",
      "subnet: [306550], loss: [1.60888504], acc: 0.4375, samples: 32\n",
      "subnet: [250798], expert_idx: 0\n",
      "subnet: [250798], loss: [1.33647809], acc: 0.375, samples: 32\n",
      "subnet: [62729], expert_idx: 0\n",
      "subnet: [62729], loss: [1.02478369], acc: 0.625, samples: 32\n",
      "subnet: [247152], expert_idx: 0\n",
      "subnet: [247152], loss: [1.47059415], acc: 0.40625, samples: 32\n",
      "subnet: [293860], expert_idx: 0\n",
      "subnet: [293860], loss: [1.13482579], acc: 0.65625, samples: 32\n",
      "subnet: [79058], expert_idx: 0\n",
      "subnet: [79058], loss: [1.02754848], acc: 0.5625, samples: 32\n",
      "subnet: [277290], expert_idx: 0\n",
      "subnet: [277290], loss: [1.27061331], acc: 0.40625, samples: 32\n",
      "subnet: [196920], expert_idx: 0\n",
      "subnet: [196920], loss: [0.88672531], acc: 0.625, samples: 32\n",
      "subnet: [167806], expert_idx: 0\n",
      "subnet: [167806], loss: [1.27524017], acc: 0.46875, samples: 32\n",
      "subnet: [180264], expert_idx: 0\n",
      "subnet: [180264], loss: [1.01156544], acc: 0.59375, samples: 32\n",
      "subnet: [98097], expert_idx: 0\n",
      "subnet: [98097], loss: [1.38381169], acc: 0.375, samples: 32\n",
      "subnet: [40288], expert_idx: 0\n",
      "subnet: [40288], loss: [1.0342965], acc: 0.40625, samples: 32\n",
      "subnet: [21615], expert_idx: 0\n",
      "subnet: [21615], loss: [0.98624958], acc: 0.53125, samples: 32\n",
      "subnet: [245183], expert_idx: 0\n",
      "subnet: [245183], loss: [1.11764137], acc: 0.46875, samples: 32\n",
      "subnet: [293835], expert_idx: 0\n",
      "subnet: [293835], loss: [1.07573663], acc: 0.5, samples: 32\n",
      "subnet: [33736], expert_idx: 0\n",
      "subnet: [33736], loss: [0.87583183], acc: 0.625, samples: 32\n",
      "subnet: [39680], expert_idx: 0\n",
      "subnet: [39680], loss: [1.00872833], acc: 0.46875, samples: 32\n",
      "subnet: [253833], expert_idx: 0\n",
      "subnet: [253833], loss: [1.07808188], acc: 0.5, samples: 32\n",
      "subnet: [84841], expert_idx: 0\n",
      "subnet: [84841], loss: [1.30374861], acc: 0.4375, samples: 32\n",
      "subnet: [314184], expert_idx: 0\n",
      "subnet: [314184], loss: [1.5370653], acc: 0.3125, samples: 32\n",
      "subnet: [116485], expert_idx: 0\n",
      "subnet: [116485], loss: [1.28920355], acc: 0.53125, samples: 32\n",
      "subnet: [159959], expert_idx: 0\n",
      "subnet: [159959], loss: [1.05420577], acc: 0.625, samples: 32\n",
      "subnet: [288538], expert_idx: 0\n",
      "subnet: [288538], loss: [1.21233615], acc: 0.4375, samples: 32\n",
      "subnet: [118016], expert_idx: 0\n",
      "subnet: [118016], loss: [1.1683691], acc: 0.46875, samples: 32\n",
      "subnet: [65387], expert_idx: 0\n",
      "subnet: [65387], loss: [0.80708276], acc: 0.625, samples: 32\n",
      "subnet: [196590], expert_idx: 0\n",
      "subnet: [196590], loss: [0.99149644], acc: 0.46875, samples: 32\n",
      "subnet: [158215], expert_idx: 0\n",
      "subnet: [158215], loss: [0.9218904], acc: 0.6875, samples: 32\n",
      "subnet: [69809], expert_idx: 0\n",
      "subnet: [69809], loss: [1.14783561], acc: 0.46875, samples: 32\n",
      "subnet: [331135], expert_idx: 0\n",
      "subnet: [331135], loss: [0.92304804], acc: 0.65625, samples: 32\n",
      "subnet: [75124], expert_idx: 0\n",
      "subnet: [75124], loss: [1.06273854], acc: 0.5625, samples: 32\n",
      "subnet: [86616], expert_idx: 0\n",
      "subnet: [86616], loss: [0.90593705], acc: 0.71875, samples: 32\n",
      "subnet: [61481], expert_idx: 0\n",
      "subnet: [61481], loss: [0.88292179], acc: 0.625, samples: 32\n",
      "subnet: [249611], expert_idx: 0\n",
      "subnet: [249611], loss: [1.00606815], acc: 0.53125, samples: 32\n",
      "subnet: [176120], expert_idx: 0\n",
      "subnet: [176120], loss: [1.46456687], acc: 0.3125, samples: 32\n",
      "subnet: [239379], expert_idx: 0\n",
      "subnet: [239379], loss: [1.18201084], acc: 0.53125, samples: 32\n",
      "subnet: [310124], expert_idx: 0\n",
      "subnet: [310124], loss: [1.23152516], acc: 0.46875, samples: 32\n",
      "subnet: [130371], expert_idx: 0\n",
      "subnet: [130371], loss: [1.03840604], acc: 0.53125, samples: 32\n",
      "subnet: [70386], expert_idx: 0\n",
      "subnet: [70386], loss: [1.09237855], acc: 0.5, samples: 32\n",
      "subnet: [303592], expert_idx: 0\n",
      "subnet: [303592], loss: [1.2016772], acc: 0.40625, samples: 32\n",
      "subnet: [111027], expert_idx: 0\n",
      "subnet: [111027], loss: [1.18566773], acc: 0.53125, samples: 32\n",
      "subnet: [65337], expert_idx: 0\n",
      "subnet: [65337], loss: [1.07846262], acc: 0.53125, samples: 32\n",
      "subnet: [158141], expert_idx: 0\n",
      "subnet: [158141], loss: [0.91680133], acc: 0.65625, samples: 32\n",
      "subnet: [144084], expert_idx: 0\n",
      "subnet: [144084], loss: [1.05958928], acc: 0.5, samples: 32\n",
      "subnet: [191340], expert_idx: 0\n",
      "subnet: [191340], loss: [0.99323853], acc: 0.625, samples: 32\n",
      "subnet: [291469], expert_idx: 0\n",
      "subnet: [291469], loss: [1.33658639], acc: 0.34375, samples: 32\n",
      "subnet: [227176], expert_idx: 0\n",
      "subnet: [227176], loss: [0.98806772], acc: 0.5, samples: 32\n",
      "subnet: [119993], expert_idx: 0\n",
      "subnet: [119993], loss: [1.02677996], acc: 0.59375, samples: 32\n",
      "subnet: [138524], expert_idx: 0\n",
      "subnet: [138524], loss: [1.09719431], acc: 0.46875, samples: 32\n",
      "subnet: [238004], expert_idx: 0\n",
      "subnet: [238004], loss: [1.09689716], acc: 0.40625, samples: 32\n",
      "subnet: [19790], expert_idx: 0\n",
      "subnet: [19790], loss: [0.97444224], acc: 0.5625, samples: 32\n",
      "subnet: [310848], expert_idx: 0\n",
      "subnet: [310848], loss: [1.41005341], acc: 0.34375, samples: 32\n",
      "subnet: [233267], expert_idx: 0\n",
      "subnet: [233267], loss: [0.91356361], acc: 0.59375, samples: 32\n",
      "subnet: [168597], expert_idx: 0\n",
      "subnet: [168597], loss: [1.29334745], acc: 0.375, samples: 32\n",
      "subnet: [79816], expert_idx: 0\n",
      "subnet: [79816], loss: [1.33215076], acc: 0.40625, samples: 32\n",
      "subnet: [59024], expert_idx: 0\n",
      "subnet: [59024], loss: [1.12694386], acc: 0.4375, samples: 32\n",
      "subnet: [153897], expert_idx: 0\n",
      "subnet: [153897], loss: [1.25627071], acc: 0.40625, samples: 32\n",
      "subnet: [204975], expert_idx: 0\n",
      "subnet: [204975], loss: [1.41811096], acc: 0.4375, samples: 32\n",
      "subnet: [115814], expert_idx: 0\n",
      "subnet: [115814], loss: [0.85072761], acc: 0.59375, samples: 32\n",
      "subnet: [232493], expert_idx: 0\n",
      "subnet: [232493], loss: [1.2565191], acc: 0.46875, samples: 32\n",
      "subnet: [110124], expert_idx: 0\n",
      "subnet: [110124], loss: [1.01156544], acc: 0.59375, samples: 32\n",
      "subnet: [163413], expert_idx: 0\n",
      "subnet: [163413], loss: [1.18759657], acc: 0.46875, samples: 32\n",
      "subnet: [42717], expert_idx: 0\n",
      "subnet: [42717], loss: [1.09089109], acc: 0.40625, samples: 32\n",
      "subnet: [244883], expert_idx: 0\n",
      "subnet: [244883], loss: [1.06392665], acc: 0.5625, samples: 32\n",
      "subnet: [130077], expert_idx: 0\n",
      "subnet: [130077], loss: [1.30498793], acc: 0.40625, samples: 32\n",
      "subnet: [96766], expert_idx: 0\n",
      "subnet: [96766], loss: [0.99649008], acc: 0.5, samples: 32\n",
      "subnet: [270338], expert_idx: 0\n",
      "subnet: [270338], loss: [0.8372574], acc: 0.71875, samples: 32\n",
      "subnet: [3224], expert_idx: 0\n",
      "subnet: [3224], loss: [1.07049894], acc: 0.46875, samples: 32\n",
      "subnet: [165623], expert_idx: 0\n",
      "subnet: [165623], loss: [1.09594844], acc: 0.5625, samples: 32\n",
      "subnet: [81711], expert_idx: 0\n",
      "subnet: [81711], loss: [1.19284016], acc: 0.53125, samples: 32\n",
      "subnet: [141242], expert_idx: 0\n",
      "subnet: [141242], loss: [1.16751269], acc: 0.46875, samples: 32\n",
      "subnet: [122944], expert_idx: 0\n",
      "subnet: [122944], loss: [0.99180687], acc: 0.4375, samples: 32\n",
      "subnet: [320299], expert_idx: 0\n",
      "subnet: [320299], loss: [1.01577995], acc: 0.46875, samples: 32\n",
      "subnet: [65975], expert_idx: 0\n",
      "subnet: [65975], loss: [1.19824234], acc: 0.375, samples: 32\n",
      "subnet: [263055], expert_idx: 0\n",
      "subnet: [263055], loss: [1.07628615], acc: 0.53125, samples: 32\n",
      "subnet: [283467], expert_idx: 0\n",
      "subnet: [283467], loss: [1.36606012], acc: 0.375, samples: 32\n",
      "subnet: [101000], expert_idx: 0\n",
      "subnet: [101000], loss: [1.09493997], acc: 0.53125, samples: 32\n",
      "subnet: [310377], expert_idx: 0\n",
      "subnet: [310377], loss: [1.1639792], acc: 0.5, samples: 32\n",
      "subnet: [246366], expert_idx: 0\n",
      "subnet: [246366], loss: [0.94469484], acc: 0.5625, samples: 32\n",
      "subnet: [3330], expert_idx: 0\n",
      "subnet: [3330], loss: [1.13203649], acc: 0.46875, samples: 32\n",
      "subnet: [52029], expert_idx: 0\n",
      "subnet: [52029], loss: [1.0206005], acc: 0.4375, samples: 32\n",
      "subnet: [197644], expert_idx: 0\n",
      "subnet: [197644], loss: [1.42251827], acc: 0.375, samples: 32\n",
      "subnet: [260208], expert_idx: 0\n",
      "subnet: [260208], loss: [1.11686756], acc: 0.53125, samples: 32\n",
      "subnet: [326162], expert_idx: 0\n",
      "subnet: [326162], loss: [1.17856559], acc: 0.5625, samples: 32\n",
      "subnet: [185582], expert_idx: 0\n",
      "subnet: [185582], loss: [1.08059173], acc: 0.46875, samples: 32\n",
      "subnet: [40521], expert_idx: 0\n",
      "subnet: [40521], loss: [0.96552161], acc: 0.59375, samples: 32\n",
      "subnet: [81797], expert_idx: 0\n",
      "subnet: [81797], loss: [1.17218676], acc: 0.4375, samples: 32\n",
      "subnet: [11053], expert_idx: 0\n",
      "subnet: [11053], loss: [1.08070147], acc: 0.53125, samples: 32\n",
      "subnet: [279905], expert_idx: 0\n",
      "subnet: [279905], loss: [1.12282254], acc: 0.59375, samples: 32\n",
      "subnet: [79077], expert_idx: 0\n",
      "subnet: [79077], loss: [1.3375787], acc: 0.34375, samples: 32\n",
      "subnet: [296594], expert_idx: 0\n",
      "subnet: [296594], loss: [1.27650765], acc: 0.375, samples: 32\n",
      "subnet: [312918], expert_idx: 0\n",
      "subnet: [312918], loss: [1.22725622], acc: 0.59375, samples: 32\n",
      "subnet: [32863], expert_idx: 0\n",
      "subnet: [32863], loss: [1.20019111], acc: 0.4375, samples: 32\n",
      "subnet: [115178], expert_idx: 0\n",
      "subnet: [115178], loss: [1.12815186], acc: 0.46875, samples: 32\n",
      "subnet: [229502], expert_idx: 0\n",
      "subnet: [229502], loss: [1.31580496], acc: 0.34375, samples: 32\n",
      "subnet: [173774], expert_idx: 0\n",
      "subnet: [173774], loss: [1.01375864], acc: 0.5, samples: 32\n",
      "subnet: [272251], expert_idx: 0\n",
      "subnet: [272251], loss: [0.98806772], acc: 0.5, samples: 32\n",
      "subnet: [39907], expert_idx: 0\n",
      "subnet: [39907], loss: [1.02368037], acc: 0.5, samples: 32\n",
      "subnet: [329961], expert_idx: 0\n",
      "subnet: [329961], loss: [1.2222008], acc: 0.46875, samples: 32\n",
      "subnet: [203435], expert_idx: 0\n",
      "subnet: [203435], loss: [1.16511353], acc: 0.4375, samples: 32\n",
      "subnet: [167121], expert_idx: 0\n",
      "subnet: [167121], loss: [1.09041803], acc: 0.46875, samples: 32\n",
      "subnet: [44881], expert_idx: 0\n",
      "subnet: [44881], loss: [1.09546655], acc: 0.5625, samples: 32\n",
      "subnet: [4545], expert_idx: 0\n",
      "subnet: [4545], loss: [1.30317895], acc: 0.375, samples: 32\n",
      "subnet: [202632], expert_idx: 0\n",
      "subnet: [202632], loss: [1.40678298], acc: 0.40625, samples: 32\n",
      "subnet: [67561], expert_idx: 0\n",
      "subnet: [67561], loss: [1.36323699], acc: 0.40625, samples: 32\n",
      "subnet: [79749], expert_idx: 0\n",
      "subnet: [79749], loss: [1.07558878], acc: 0.5, samples: 32\n",
      "subnet: [322062], expert_idx: 0\n",
      "subnet: [322062], loss: [0.98553945], acc: 0.59375, samples: 32\n",
      "subnet: [280090], expert_idx: 0\n",
      "subnet: [280090], loss: [0.99785004], acc: 0.5625, samples: 32\n",
      "subnet: [277417], expert_idx: 0\n",
      "subnet: [277417], loss: [1.37924943], acc: 0.34375, samples: 32\n",
      "subnet: [133561], expert_idx: 0\n",
      "subnet: [133561], loss: [0.99755738], acc: 0.59375, samples: 32\n",
      "subnet: [307950], expert_idx: 0\n",
      "subnet: [307950], loss: [0.75111825], acc: 0.6875, samples: 32\n",
      "subnet: [287983], expert_idx: 0\n",
      "subnet: [287983], loss: [1.19047788], acc: 0.46875, samples: 32\n",
      "subnet: [40988], expert_idx: 0\n",
      "subnet: [40988], loss: [0.82958357], acc: 0.71875, samples: 32\n",
      "subnet: [116757], expert_idx: 0\n",
      "subnet: [116757], loss: [1.00400025], acc: 0.59375, samples: 32\n",
      "subnet: [144667], expert_idx: 0\n",
      "subnet: [144667], loss: [0.92186459], acc: 0.5625, samples: 32\n",
      "subnet: [60377], expert_idx: 0\n",
      "subnet: [60377], loss: [1.0879415], acc: 0.5, samples: 32\n",
      "subnet: [51609], expert_idx: 0\n",
      "subnet: [51609], loss: [0.94733813], acc: 0.6875, samples: 32\n",
      "subnet: [149852], expert_idx: 0\n",
      "subnet: [149852], loss: [1.11714463], acc: 0.5, samples: 32\n",
      "subnet: [129619], expert_idx: 0\n",
      "subnet: [129619], loss: [1.09569544], acc: 0.5, samples: 32\n",
      "subnet: [161659], expert_idx: 0\n",
      "subnet: [161659], loss: [1.10337226], acc: 0.625, samples: 32\n",
      "subnet: [100346], expert_idx: 0\n",
      "subnet: [100346], loss: [1.06362398], acc: 0.53125, samples: 32\n",
      "subnet: [73260], expert_idx: 0\n",
      "subnet: [73260], loss: [1.08060583], acc: 0.53125, samples: 32\n",
      "subnet: [209963], expert_idx: 0\n",
      "subnet: [209963], loss: [1.19135122], acc: 0.46875, samples: 32\n",
      "subnet: [296752], expert_idx: 0\n",
      "subnet: [296752], loss: [1.23663509], acc: 0.375, samples: 32\n",
      "subnet: [290591], expert_idx: 0\n",
      "subnet: [290591], loss: [1.0569336], acc: 0.46875, samples: 32\n",
      "subnet: [225266], expert_idx: 0\n",
      "subnet: [225266], loss: [1.69703121], acc: 0.375, samples: 32\n",
      "subnet: [257900], expert_idx: 0\n",
      "subnet: [257900], loss: [1.34828991], acc: 0.5, samples: 32\n",
      "subnet: [92752], expert_idx: 0\n",
      "subnet: [92752], loss: [1.05945567], acc: 0.5625, samples: 32\n",
      "subnet: [206968], expert_idx: 0\n",
      "subnet: [206968], loss: [1.16764626], acc: 0.53125, samples: 32\n",
      "subnet: [123041], expert_idx: 0\n",
      "subnet: [123041], loss: [1.17702849], acc: 0.46875, samples: 32\n",
      "subnet: [29946], expert_idx: 0\n",
      "subnet: [29946], loss: [1.10351486], acc: 0.5625, samples: 32\n",
      "subnet: [180430], expert_idx: 0\n",
      "subnet: [180430], loss: [1.33616822], acc: 0.5, samples: 32\n",
      "subnet: [33789], expert_idx: 0\n",
      "subnet: [33789], loss: [0.98715997], acc: 0.53125, samples: 32\n",
      "subnet: [110977], expert_idx: 0\n",
      "subnet: [110977], loss: [1.08748986], acc: 0.5, samples: 32\n",
      "subnet: [302241], expert_idx: 0\n",
      "subnet: [302241], loss: [1.14076735], acc: 0.40625, samples: 32\n",
      "subnet: [242339], expert_idx: 0\n",
      "subnet: [242339], loss: [1.00478236], acc: 0.5, samples: 32\n",
      "subnet: [265857], expert_idx: 0\n",
      "subnet: [265857], loss: [1.01495688], acc: 0.53125, samples: 32\n",
      "subnet: [198014], expert_idx: 0\n",
      "subnet: [198014], loss: [1.20280554], acc: 0.40625, samples: 32\n",
      "subnet: [245093], expert_idx: 0\n",
      "subnet: [245093], loss: [1.12282254], acc: 0.59375, samples: 32\n",
      "subnet: [171571], expert_idx: 0\n",
      "subnet: [171571], loss: [1.12292483], acc: 0.5, samples: 32\n",
      "subnet: [44616], expert_idx: 0\n",
      "subnet: [44616], loss: [0.88987208], acc: 0.6875, samples: 32\n",
      "subnet: [181785], expert_idx: 0\n",
      "subnet: [181785], loss: [1.08986822], acc: 0.53125, samples: 32\n",
      "subnet: [310051], expert_idx: 0\n",
      "subnet: [310051], loss: [1.26843332], acc: 0.34375, samples: 32\n",
      "subnet: [14009], expert_idx: 0\n",
      "subnet: [14009], loss: [1.07613093], acc: 0.65625, samples: 32\n",
      "subnet: [286669], expert_idx: 0\n",
      "subnet: [286669], loss: [1.25129121], acc: 0.59375, samples: 32\n",
      "subnet: [331322], expert_idx: 0\n",
      "subnet: [331322], loss: [1.11123319], acc: 0.5625, samples: 32\n",
      "subnet: [76069], expert_idx: 0\n",
      "subnet: [76069], loss: [1.02520884], acc: 0.53125, samples: 32\n",
      "subnet: [261034], expert_idx: 0\n",
      "subnet: [261034], loss: [0.9532486], acc: 0.625, samples: 32\n",
      "subnet: [207014], expert_idx: 0\n",
      "subnet: [207014], loss: [1.13409271], acc: 0.4375, samples: 32\n",
      "subnet: [88897], expert_idx: 0\n",
      "subnet: [88897], loss: [1.00202231], acc: 0.4375, samples: 32\n",
      "subnet: [14904], expert_idx: 0\n",
      "subnet: [14904], loss: [1.07732061], acc: 0.4375, samples: 32\n",
      "subnet: [164171], expert_idx: 0\n",
      "subnet: [164171], loss: [1.27375498], acc: 0.46875, samples: 32\n",
      "subnet: [214704], expert_idx: 0\n",
      "subnet: [214704], loss: [1.10868927], acc: 0.4375, samples: 32\n",
      "subnet: [32827], expert_idx: 0\n",
      "subnet: [32827], loss: [1.03534643], acc: 0.625, samples: 32\n",
      "subnet: [200255], expert_idx: 0\n",
      "subnet: [200255], loss: [1.14182818], acc: 0.4375, samples: 32\n",
      "subnet: [922], expert_idx: 0\n",
      "subnet: [922], loss: [1.09462389], acc: 0.4375, samples: 32\n",
      "subnet: [176219], expert_idx: 0\n",
      "subnet: [176219], loss: [1.17382602], acc: 0.4375, samples: 32\n",
      "subnet: [296919], expert_idx: 0\n",
      "subnet: [296919], loss: [1.17663291], acc: 0.53125, samples: 32\n",
      "subnet: [58652], expert_idx: 0\n",
      "subnet: [58652], loss: [1.42543714], acc: 0.375, samples: 32\n",
      "subnet: [224385], expert_idx: 0\n",
      "subnet: [224385], loss: [1.19873039], acc: 0.5, samples: 32\n",
      "subnet: [172353], expert_idx: 0\n",
      "subnet: [172353], loss: [1.2069114], acc: 0.53125, samples: 32\n",
      "subnet: [303088], expert_idx: 0\n",
      "subnet: [303088], loss: [1.08977944], acc: 0.46875, samples: 32\n",
      "subnet: [308349], expert_idx: 0\n",
      "subnet: [308349], loss: [1.29596719], acc: 0.40625, samples: 32\n",
      "subnet: [187903], expert_idx: 0\n",
      "subnet: [187903], loss: [1.22496301], acc: 0.5, samples: 32\n",
      "subnet: [28919], expert_idx: 0\n",
      "subnet: [28919], loss: [1.0257159], acc: 0.5, samples: 32\n",
      "subnet: [82324], expert_idx: 0\n",
      "subnet: [82324], loss: [1.07581593], acc: 0.59375, samples: 32\n",
      "subnet: [178167], expert_idx: 0\n",
      "subnet: [178167], loss: [1.10744165], acc: 0.4375, samples: 32\n",
      "subnet: [30086], expert_idx: 0\n",
      "subnet: [30086], loss: [0.99982358], acc: 0.5625, samples: 32\n",
      "subnet: [193046], expert_idx: 0\n",
      "subnet: [193046], loss: [1.38358663], acc: 0.34375, samples: 32\n",
      "subnet: [125188], expert_idx: 0\n",
      "subnet: [125188], loss: [0.98001124], acc: 0.5625, samples: 32\n",
      "subnet: [61209], expert_idx: 0\n",
      "subnet: [61209], loss: [1.02185082], acc: 0.5625, samples: 32\n",
      "subnet: [185671], expert_idx: 0\n",
      "subnet: [185671], loss: [0.9936563], acc: 0.53125, samples: 32\n",
      "subnet: [307421], expert_idx: 0\n",
      "subnet: [307421], loss: [1.31323537], acc: 0.375, samples: 32\n",
      "subnet: [110821], expert_idx: 0\n",
      "subnet: [110821], loss: [1.11571377], acc: 0.46875, samples: 32\n",
      "subnet: [302321], expert_idx: 0\n",
      "subnet: [302321], loss: [1.08259874], acc: 0.53125, samples: 32\n",
      "subnet: [290553], expert_idx: 0\n",
      "subnet: [290553], loss: [1.33698252], acc: 0.375, samples: 32\n",
      "subnet: [239496], expert_idx: 0\n",
      "subnet: [239496], loss: [1.01390792], acc: 0.5625, samples: 32\n",
      "subnet: [299559], expert_idx: 0\n",
      "subnet: [299559], loss: [1.03573057], acc: 0.53125, samples: 32\n",
      "subnet: [66450], expert_idx: 0\n",
      "subnet: [66450], loss: [1.08717859], acc: 0.5, samples: 32\n",
      "subnet: [203918], expert_idx: 0\n",
      "subnet: [203918], loss: [1.20228257], acc: 0.375, samples: 32\n",
      "subnet: [223782], expert_idx: 0\n",
      "subnet: [223782], loss: [1.11255514], acc: 0.5625, samples: 32\n",
      "subnet: [69827], expert_idx: 0\n",
      "subnet: [69827], loss: [1.33616822], acc: 0.5, samples: 32\n",
      "subnet: [272915], expert_idx: 0\n",
      "subnet: [272915], loss: [0.8840349], acc: 0.625, samples: 32\n",
      "subnet: [128721], expert_idx: 0\n",
      "subnet: [128721], loss: [1.42777811], acc: 0.375, samples: 32\n",
      "subnet: [200637], expert_idx: 0\n",
      "subnet: [200637], loss: [1.23368516], acc: 0.4375, samples: 32\n",
      "subnet: [58871], expert_idx: 0\n",
      "subnet: [58871], loss: [1.49789548], acc: 0.4375, samples: 32\n",
      "subnet: [89802], expert_idx: 0\n",
      "subnet: [89802], loss: [0.9999303], acc: 0.65625, samples: 32\n",
      "subnet: [222760], expert_idx: 0\n",
      "subnet: [222760], loss: [1.09427356], acc: 0.46875, samples: 32\n",
      "subnet: [254284], expert_idx: 0\n",
      "subnet: [254284], loss: [1.33666156], acc: 0.4375, samples: 32\n",
      "subnet: [295872], expert_idx: 0\n",
      "subnet: [295872], loss: [1.2016772], acc: 0.40625, samples: 32\n",
      "subnet: [216651], expert_idx: 0\n",
      "subnet: [216651], loss: [1.16365631], acc: 0.5, samples: 32\n",
      "subnet: [298338], expert_idx: 0\n",
      "subnet: [298338], loss: [1.04970347], acc: 0.625, samples: 32\n",
      "subnet: [226634], expert_idx: 0\n",
      "subnet: [226634], loss: [1.29063857], acc: 0.5, samples: 32\n",
      "subnet: [180749], expert_idx: 0\n",
      "subnet: [180749], loss: [1.22528335], acc: 0.46875, samples: 32\n",
      "subnet: [305965], expert_idx: 0\n",
      "subnet: [305965], loss: [1.24608577], acc: 0.5, samples: 32\n",
      "subnet: [263288], expert_idx: 0\n",
      "subnet: [263288], loss: [0.98064278], acc: 0.5625, samples: 32\n",
      "subnet: [76931], expert_idx: 0\n",
      "subnet: [76931], loss: [1.007648], acc: 0.625, samples: 32\n",
      "subnet: [311062], expert_idx: 0\n",
      "subnet: [311062], loss: [1.16493991], acc: 0.4375, samples: 32\n",
      "subnet: [90338], expert_idx: 0\n",
      "subnet: [90338], loss: [1.2278008], acc: 0.4375, samples: 32\n",
      "subnet: [232431], expert_idx: 0\n",
      "subnet: [232431], loss: [1.16287544], acc: 0.4375, samples: 32\n",
      "subnet: [311589], expert_idx: 0\n",
      "subnet: [311589], loss: [1.04261959], acc: 0.59375, samples: 32\n",
      "subnet: [325507], expert_idx: 0\n",
      "subnet: [325507], loss: [0.97640763], acc: 0.59375, samples: 32\n",
      "subnet: [256579], expert_idx: 0\n",
      "subnet: [256579], loss: [1.3841465], acc: 0.4375, samples: 32\n",
      "subnet: [4791], expert_idx: 0\n",
      "subnet: [4791], loss: [1.11391899], acc: 0.4375, samples: 32\n",
      "subnet: [66240], expert_idx: 0\n",
      "subnet: [66240], loss: [1.08552142], acc: 0.4375, samples: 32\n",
      "subnet: [237938], expert_idx: 0\n",
      "subnet: [237938], loss: [1.04590378], acc: 0.40625, samples: 32\n",
      "subnet: [218003], expert_idx: 0\n",
      "subnet: [218003], loss: [1.22157801], acc: 0.53125, samples: 32\n",
      "subnet: [225423], expert_idx: 0\n",
      "subnet: [225423], loss: [1.07417474], acc: 0.5, samples: 32\n",
      "subnet: [188936], expert_idx: 0\n",
      "subnet: [188936], loss: [0.88679495], acc: 0.625, samples: 32\n",
      "subnet: [204574], expert_idx: 0\n",
      "subnet: [204574], loss: [1.16751535], acc: 0.46875, samples: 32\n",
      "subnet: [291939], expert_idx: 0\n",
      "subnet: [291939], loss: [1.19542615], acc: 0.4375, samples: 32\n",
      "subnet: [222998], expert_idx: 0\n",
      "subnet: [222998], loss: [0.86476167], acc: 0.625, samples: 32\n",
      "subnet: [329118], expert_idx: 0\n",
      "subnet: [329118], loss: [0.95087958], acc: 0.5625, samples: 32\n",
      "subnet: [82627], expert_idx: 0\n",
      "subnet: [82627], loss: [1.00600768], acc: 0.375, samples: 32\n",
      "subnet: [34770], expert_idx: 0\n",
      "subnet: [34770], loss: [0.90703271], acc: 0.625, samples: 32\n",
      "subnet: [92217], expert_idx: 0\n",
      "subnet: [92217], loss: [1.23480634], acc: 0.46875, samples: 32\n",
      "subnet: [91427], expert_idx: 0\n",
      "subnet: [91427], loss: [0.82953967], acc: 0.65625, samples: 32\n",
      "subnet: [193431], expert_idx: 0\n",
      "subnet: [193431], loss: [1.39648413], acc: 0.375, samples: 32\n",
      "subnet: [213535], expert_idx: 0\n",
      "subnet: [213535], loss: [1.22121396], acc: 0.46875, samples: 32\n",
      "subnet: [158119], expert_idx: 0\n",
      "subnet: [158119], loss: [1.12018688], acc: 0.40625, samples: 32\n",
      "subnet: [132275], expert_idx: 0\n",
      "subnet: [132275], loss: [1.01751448], acc: 0.53125, samples: 32\n",
      "subnet: [179564], expert_idx: 0\n",
      "subnet: [179564], loss: [1.15573666], acc: 0.46875, samples: 32\n",
      "subnet: [38905], expert_idx: 0\n",
      "subnet: [38905], loss: [0.98776439], acc: 0.5625, samples: 32\n",
      "subnet: [135611], expert_idx: 0\n",
      "subnet: [135611], loss: [1.19609], acc: 0.40625, samples: 32\n",
      "subnet: [204440], expert_idx: 0\n",
      "subnet: [204440], loss: [1.14628597], acc: 0.46875, samples: 32\n",
      "subnet: [315029], expert_idx: 0\n",
      "subnet: [315029], loss: [1.17024448], acc: 0.46875, samples: 32\n",
      "subnet: [8345], expert_idx: 0\n",
      "subnet: [8345], loss: [1.02879457], acc: 0.5, samples: 32\n",
      "subnet: [152880], expert_idx: 0\n",
      "subnet: [152880], loss: [0.97048884], acc: 0.65625, samples: 32\n",
      "subnet: [311707], expert_idx: 0\n",
      "subnet: [311707], loss: [1.11918824], acc: 0.46875, samples: 32\n",
      "subnet: [33216], expert_idx: 0\n",
      "subnet: [33216], loss: [1.02711827], acc: 0.59375, samples: 32\n",
      "subnet: [148227], expert_idx: 0\n",
      "subnet: [148227], loss: [1.0985662], acc: 0.4375, samples: 32\n",
      "subnet: [39124], expert_idx: 0\n",
      "subnet: [39124], loss: [0.99811581], acc: 0.5, samples: 32\n",
      "subnet: [329726], expert_idx: 0\n",
      "subnet: [329726], loss: [1.21925084], acc: 0.4375, samples: 32\n",
      "subnet: [232437], expert_idx: 0\n",
      "subnet: [232437], loss: [1.02308431], acc: 0.53125, samples: 32\n",
      "subnet: [288595], expert_idx: 0\n",
      "subnet: [288595], loss: [1.61597072], acc: 0.375, samples: 32\n",
      "subnet: [140362], expert_idx: 0\n",
      "subnet: [140362], loss: [1.44083295], acc: 0.25, samples: 32\n",
      "subnet: [52535], expert_idx: 0\n",
      "subnet: [52535], loss: [1.06927713], acc: 0.4375, samples: 32\n",
      "subnet: [280018], expert_idx: 0\n",
      "subnet: [280018], loss: [1.09543122], acc: 0.59375, samples: 32\n",
      "subnet: [107874], expert_idx: 0\n",
      "subnet: [107874], loss: [1.06545257], acc: 0.59375, samples: 32\n",
      "subnet: [153838], expert_idx: 0\n",
      "subnet: [153838], loss: [1.47864337], acc: 0.34375, samples: 32\n",
      "subnet: [160796], expert_idx: 0\n",
      "subnet: [160796], loss: [1.39433847], acc: 0.46875, samples: 32\n",
      "subnet: [195182], expert_idx: 0\n",
      "subnet: [195182], loss: [1.15787072], acc: 0.46875, samples: 32\n",
      "subnet: [286627], expert_idx: 0\n",
      "subnet: [286627], loss: [1.13674966], acc: 0.34375, samples: 32\n",
      "subnet: [22299], expert_idx: 0\n",
      "subnet: [22299], loss: [1.22528335], acc: 0.46875, samples: 32\n",
      "subnet: [67752], expert_idx: 0\n",
      "subnet: [67752], loss: [1.0374964], acc: 0.59375, samples: 32\n",
      "subnet: [26914], expert_idx: 0\n",
      "subnet: [26914], loss: [0.99893373], acc: 0.5625, samples: 32\n",
      "subnet: [301353], expert_idx: 0\n",
      "subnet: [301353], loss: [0.96238656], acc: 0.59375, samples: 32\n",
      "subnet: [204885], expert_idx: 0\n",
      "subnet: [204885], loss: [0.952703], acc: 0.625, samples: 32\n",
      "subnet: [293725], expert_idx: 0\n",
      "subnet: [293725], loss: [1.09365457], acc: 0.5, samples: 32\n",
      "subnet: [130485], expert_idx: 0\n",
      "subnet: [130485], loss: [1.17864747], acc: 0.46875, samples: 32\n",
      "subnet: [117639], expert_idx: 0\n",
      "subnet: [117639], loss: [0.86869237], acc: 0.71875, samples: 32\n",
      "subnet: [166607], expert_idx: 0\n",
      "subnet: [166607], loss: [0.95861338], acc: 0.59375, samples: 32\n",
      "subnet: [186906], expert_idx: 0\n",
      "subnet: [186906], loss: [0.90094754], acc: 0.59375, samples: 32\n",
      "subnet: [58887], expert_idx: 0\n",
      "subnet: [58887], loss: [1.03534643], acc: 0.625, samples: 32\n",
      "subnet: [255087], expert_idx: 0\n",
      "subnet: [255087], loss: [1.19512534], acc: 0.4375, samples: 32\n",
      "subnet: [54602], expert_idx: 0\n",
      "subnet: [54602], loss: [1.17644535], acc: 0.4375, samples: 32\n",
      "subnet: [220101], expert_idx: 0\n",
      "subnet: [220101], loss: [0.95836435], acc: 0.5625, samples: 32\n",
      "subnet: [328273], expert_idx: 0\n",
      "subnet: [328273], loss: [1.25675954], acc: 0.46875, samples: 32\n",
      "subnet: [218304], expert_idx: 0\n",
      "subnet: [218304], loss: [1.471781], acc: 0.40625, samples: 32\n",
      "subnet: [219954], expert_idx: 0\n",
      "subnet: [219954], loss: [1.2565191], acc: 0.46875, samples: 32\n",
      "subnet: [184873], expert_idx: 0\n",
      "subnet: [184873], loss: [0.9790849], acc: 0.59375, samples: 32\n",
      "subnet: [50582], expert_idx: 0\n",
      "subnet: [50582], loss: [1.05248005], acc: 0.46875, samples: 32\n",
      "subnet: [296753], expert_idx: 0\n",
      "subnet: [296753], loss: [1.25067392], acc: 0.4375, samples: 32\n",
      "subnet: [62201], expert_idx: 0\n",
      "subnet: [62201], loss: [0.91087579], acc: 0.59375, samples: 32\n",
      "subnet: [193512], expert_idx: 0\n",
      "subnet: [193512], loss: [1.6346102], acc: 0.40625, samples: 32\n",
      "subnet: [113151], expert_idx: 0\n",
      "subnet: [113151], loss: [1.02707139], acc: 0.53125, samples: 32\n",
      "subnet: [13362], expert_idx: 0\n",
      "subnet: [13362], loss: [0.85030524], acc: 0.71875, samples: 32\n",
      "subnet: [210271], expert_idx: 0\n",
      "subnet: [210271], loss: [1.49275838], acc: 0.40625, samples: 32\n",
      "subnet: [121291], expert_idx: 0\n",
      "subnet: [121291], loss: [1.00831349], acc: 0.5625, samples: 32\n",
      "subnet: [9213], expert_idx: 0\n",
      "subnet: [9213], loss: [1.01216197], acc: 0.40625, samples: 32\n",
      "subnet: [129497], expert_idx: 0\n",
      "subnet: [129497], loss: [0.95345455], acc: 0.59375, samples: 32\n",
      "subnet: [53982], expert_idx: 0\n",
      "subnet: [53982], loss: [1.19238763], acc: 0.46875, samples: 32\n",
      "subnet: [182806], expert_idx: 0\n",
      "subnet: [182806], loss: [1.09453733], acc: 0.53125, samples: 32\n",
      "subnet: [129747], expert_idx: 0\n",
      "subnet: [129747], loss: [1.26149232], acc: 0.375, samples: 32\n",
      "subnet: [19396], expert_idx: 0\n",
      "subnet: [19396], loss: [1.2952072], acc: 0.40625, samples: 32\n",
      "subnet: [47178], expert_idx: 0\n",
      "subnet: [47178], loss: [1.24977138], acc: 0.46875, samples: 32\n",
      "subnet: [41831], expert_idx: 0\n",
      "subnet: [41831], loss: [1.30328667], acc: 0.40625, samples: 32\n",
      "subnet: [142277], expert_idx: 0\n",
      "subnet: [142277], loss: [0.98825837], acc: 0.59375, samples: 32\n",
      "subnet: [137618], expert_idx: 0\n",
      "subnet: [137618], loss: [1.13863783], acc: 0.375, samples: 32\n",
      "subnet: [160819], expert_idx: 0\n",
      "subnet: [160819], loss: [1.16780056], acc: 0.53125, samples: 32\n",
      "subnet: [8984], expert_idx: 0\n",
      "subnet: [8984], loss: [1.12937331], acc: 0.4375, samples: 32\n",
      "subnet: [74756], expert_idx: 0\n",
      "subnet: [74756], loss: [1.08267551], acc: 0.375, samples: 32\n",
      "subnet: [167899], expert_idx: 0\n",
      "subnet: [167899], loss: [1.07173325], acc: 0.5, samples: 32\n",
      "subnet: [182660], expert_idx: 0\n",
      "subnet: [182660], loss: [1.16080322], acc: 0.46875, samples: 32\n",
      "subnet: [310791], expert_idx: 0\n",
      "subnet: [310791], loss: [1.14443769], acc: 0.375, samples: 32\n",
      "subnet: [21351], expert_idx: 0\n",
      "subnet: [21351], loss: [0.98517972], acc: 0.59375, samples: 32\n",
      "subnet: [15812], expert_idx: 0\n",
      "subnet: [15812], loss: [1.0925356], acc: 0.625, samples: 32\n",
      "subnet: [4663], expert_idx: 0\n",
      "subnet: [4663], loss: [1.13864339], acc: 0.46875, samples: 32\n",
      "subnet: [276904], expert_idx: 0\n",
      "subnet: [276904], loss: [1.13383286], acc: 0.53125, samples: 32\n",
      "subnet: [110101], expert_idx: 0\n",
      "subnet: [110101], loss: [1.12292483], acc: 0.5, samples: 32\n",
      "subnet: [292248], expert_idx: 0\n",
      "subnet: [292248], loss: [0.96641018], acc: 0.6875, samples: 32\n",
      "subnet: [7497], expert_idx: 0\n",
      "subnet: [7497], loss: [1.03151811], acc: 0.625, samples: 32\n",
      "subnet: [30981], expert_idx: 0\n",
      "subnet: [30981], loss: [0.99857917], acc: 0.5625, samples: 32\n",
      "subnet: [303657], expert_idx: 0\n",
      "subnet: [303657], loss: [1.22528335], acc: 0.46875, samples: 32\n",
      "subnet: [180454], expert_idx: 0\n",
      "subnet: [180454], loss: [1.44381571], acc: 0.34375, samples: 32\n",
      "subnet: [160842], expert_idx: 0\n",
      "subnet: [160842], loss: [1.11369136], acc: 0.5, samples: 32\n",
      "subnet: [302306], expert_idx: 0\n",
      "subnet: [302306], loss: [1.04723943], acc: 0.4375, samples: 32\n",
      "subnet: [170533], expert_idx: 0\n",
      "subnet: [170533], loss: [1.05521516], acc: 0.5625, samples: 32\n",
      "subnet: [240773], expert_idx: 0\n",
      "subnet: [240773], loss: [1.06536005], acc: 0.46875, samples: 32\n",
      "subnet: [219708], expert_idx: 0\n",
      "subnet: [219708], loss: [0.94793743], acc: 0.5625, samples: 32\n",
      "subnet: [16196], expert_idx: 0\n",
      "subnet: [16196], loss: [1.33521328], acc: 0.4375, samples: 32\n",
      "subnet: [34588], expert_idx: 0\n",
      "subnet: [34588], loss: [1.27297598], acc: 0.3125, samples: 32\n",
      "subnet: [82594], expert_idx: 0\n",
      "subnet: [82594], loss: [1.00020916], acc: 0.53125, samples: 32\n",
      "subnet: [205198], expert_idx: 0\n",
      "subnet: [205198], loss: [0.99041092], acc: 0.59375, samples: 32\n",
      "subnet: [105277], expert_idx: 0\n",
      "subnet: [105277], loss: [1.08317314], acc: 0.53125, samples: 32\n",
      "subnet: [155485], expert_idx: 0\n",
      "subnet: [155485], loss: [1.14499274], acc: 0.5, samples: 32\n",
      "subnet: [125811], expert_idx: 0\n",
      "subnet: [125811], loss: [1.21807859], acc: 0.46875, samples: 32\n",
      "subnet: [170372], expert_idx: 0\n",
      "subnet: [170372], loss: [1.05973556], acc: 0.5, samples: 32\n",
      "subnet: [275753], expert_idx: 0\n",
      "subnet: [275753], loss: [1.56675653], acc: 0.375, samples: 32\n",
      "subnet: [170578], expert_idx: 0\n",
      "subnet: [170578], loss: [1.05660251], acc: 0.59375, samples: 32\n",
      "subnet: [210550], expert_idx: 0\n",
      "subnet: [210550], loss: [1.15579909], acc: 0.59375, samples: 32\n",
      "subnet: [9464], expert_idx: 0\n",
      "subnet: [9464], loss: [1.03967449], acc: 0.53125, samples: 32\n",
      "subnet: [237520], expert_idx: 0\n",
      "subnet: [237520], loss: [1.14060429], acc: 0.59375, samples: 32\n",
      "subnet: [273032], expert_idx: 0\n",
      "subnet: [273032], loss: [0.91656571], acc: 0.6875, samples: 32\n",
      "subnet: [140214], expert_idx: 0\n",
      "subnet: [140214], loss: [0.9495133], acc: 0.625, samples: 32\n",
      "subnet: [80067], expert_idx: 0\n",
      "subnet: [80067], loss: [1.02779509], acc: 0.53125, samples: 32\n",
      "subnet: [84188], expert_idx: 0\n",
      "subnet: [84188], loss: [1.00102674], acc: 0.34375, samples: 32\n",
      "subnet: [289521], expert_idx: 0\n",
      "subnet: [289521], loss: [1.33116928], acc: 0.4375, samples: 32\n",
      "subnet: [135584], expert_idx: 0\n",
      "subnet: [135584], loss: [1.02110783], acc: 0.53125, samples: 32\n",
      "subnet: [53530], expert_idx: 0\n",
      "subnet: [53530], loss: [1.13538024], acc: 0.53125, samples: 32\n",
      "subnet: [236180], expert_idx: 0\n",
      "subnet: [236180], loss: [1.06267714], acc: 0.46875, samples: 32\n",
      "subnet: [192818], expert_idx: 0\n",
      "subnet: [192818], loss: [1.08060583], acc: 0.53125, samples: 32\n",
      "subnet: [89497], expert_idx: 0\n",
      "subnet: [89497], loss: [1.08096843], acc: 0.53125, samples: 32\n",
      "subnet: [29325], expert_idx: 0\n",
      "subnet: [29325], loss: [1.03066692], acc: 0.46875, samples: 32\n",
      "subnet: [179203], expert_idx: 0\n",
      "subnet: [179203], loss: [1.16102263], acc: 0.53125, samples: 32\n",
      "subnet: [302583], expert_idx: 0\n",
      "subnet: [302583], loss: [1.69703121], acc: 0.375, samples: 32\n",
      "subnet: [244683], expert_idx: 0\n",
      "subnet: [244683], loss: [1.05297229], acc: 0.4375, samples: 32\n",
      "subnet: [303515], expert_idx: 0\n",
      "subnet: [303515], loss: [0.82650371], acc: 0.75, samples: 32\n",
      "subnet: [230769], expert_idx: 0\n",
      "subnet: [230769], loss: [1.13380416], acc: 0.59375, samples: 32\n",
      "subnet: [311879], expert_idx: 0\n",
      "subnet: [311879], loss: [1.44808609], acc: 0.3125, samples: 32\n",
      "subnet: [167407], expert_idx: 0\n",
      "subnet: [167407], loss: [1.06919807], acc: 0.65625, samples: 32\n",
      "subnet: [57151], expert_idx: 0\n",
      "subnet: [57151], loss: [0.845525], acc: 0.65625, samples: 32\n",
      "subnet: [184883], expert_idx: 0\n",
      "subnet: [184883], loss: [0.89758385], acc: 0.625, samples: 32\n",
      "subnet: [198301], expert_idx: 0\n",
      "subnet: [198301], loss: [1.23826658], acc: 0.5, samples: 32\n",
      "subnet: [171311], expert_idx: 0\n",
      "subnet: [171311], loss: [1.08467046], acc: 0.625, samples: 32\n",
      "subnet: [221056], expert_idx: 0\n",
      "subnet: [221056], loss: [1.06695654], acc: 0.5, samples: 32\n",
      "subnet: [272217], expert_idx: 0\n",
      "subnet: [272217], loss: [1.38342421], acc: 0.375, samples: 32\n",
      "subnet: [302362], expert_idx: 0\n",
      "subnet: [302362], loss: [1.14824178], acc: 0.5625, samples: 32\n",
      "subnet: [287484], expert_idx: 0\n",
      "subnet: [287484], loss: [1.2155311], acc: 0.375, samples: 32\n",
      "subnet: [98690], expert_idx: 0\n",
      "subnet: [98690], loss: [1.04955621], acc: 0.59375, samples: 32\n",
      "subnet: [38194], expert_idx: 0\n",
      "subnet: [38194], loss: [0.98896276], acc: 0.53125, samples: 32\n",
      "subnet: [13493], expert_idx: 0\n",
      "subnet: [13493], loss: [0.95969171], acc: 0.625, samples: 32\n",
      "subnet: [170837], expert_idx: 0\n",
      "subnet: [170837], loss: [1.04522237], acc: 0.53125, samples: 32\n",
      "subnet: [227956], expert_idx: 0\n",
      "subnet: [227956], loss: [1.08274764], acc: 0.5, samples: 32\n",
      "subnet: [149780], expert_idx: 0\n",
      "subnet: [149780], loss: [1.1958152], acc: 0.5, samples: 32\n",
      "subnet: [152103], expert_idx: 0\n",
      "subnet: [152103], loss: [1.0968978], acc: 0.5, samples: 32\n",
      "subnet: [292448], expert_idx: 0\n",
      "subnet: [292448], loss: [1.06121316], acc: 0.625, samples: 32\n",
      "subnet: [114249], expert_idx: 0\n",
      "subnet: [114249], loss: [1.10744374], acc: 0.46875, samples: 32\n",
      "subnet: [240920], expert_idx: 0\n",
      "subnet: [240920], loss: [1.27288301], acc: 0.3125, samples: 32\n",
      "subnet: [302833], expert_idx: 0\n",
      "subnet: [302833], loss: [0.9928714], acc: 0.4375, samples: 32\n",
      "subnet: [67497], expert_idx: 0\n",
      "subnet: [67497], loss: [1.13816305], acc: 0.375, samples: 32\n",
      "subnet: [223506], expert_idx: 0\n",
      "subnet: [223506], loss: [1.04965729], acc: 0.6875, samples: 32\n",
      "subnet: [82268], expert_idx: 0\n",
      "subnet: [82268], loss: [1.441543], acc: 0.375, samples: 32\n",
      "subnet: [69633], expert_idx: 0\n",
      "subnet: [69633], loss: [1.03534643], acc: 0.625, samples: 32\n",
      "subnet: [189630], expert_idx: 0\n",
      "subnet: [189630], loss: [1.01676646], acc: 0.53125, samples: 32\n",
      "subnet: [25691], expert_idx: 0\n",
      "subnet: [25691], loss: [1.37339211], acc: 0.375, samples: 32\n",
      "subnet: [1364], expert_idx: 0\n",
      "subnet: [1364], loss: [1.17347914], acc: 0.5, samples: 32\n",
      "subnet: [25986], expert_idx: 0\n",
      "subnet: [25986], loss: [0.93264056], acc: 0.65625, samples: 32\n",
      "subnet: [41996], expert_idx: 0\n",
      "subnet: [41996], loss: [1.22098108], acc: 0.4375, samples: 32\n",
      "subnet: [232612], expert_idx: 0\n",
      "subnet: [232612], loss: [1.18464821], acc: 0.5625, samples: 32\n",
      "subnet: [249831], expert_idx: 0\n",
      "subnet: [249831], loss: [1.3427646], acc: 0.5, samples: 32\n",
      "subnet: [147710], expert_idx: 0\n",
      "subnet: [147710], loss: [1.02228801], acc: 0.53125, samples: 32\n",
      "subnet: [118382], expert_idx: 0\n",
      "subnet: [118382], loss: [1.02891103], acc: 0.625, samples: 32\n",
      "subnet: [154426], expert_idx: 0\n",
      "subnet: [154426], loss: [1.15870178], acc: 0.5, samples: 32\n",
      "subnet: [27056], expert_idx: 0\n",
      "subnet: [27056], loss: [1.05646017], acc: 0.4375, samples: 32\n",
      "subnet: [202774], expert_idx: 0\n",
      "subnet: [202774], loss: [1.21141924], acc: 0.46875, samples: 32\n",
      "subnet: [67455], expert_idx: 0\n",
      "subnet: [67455], loss: [1.08044256], acc: 0.53125, samples: 32\n",
      "subnet: [247484], expert_idx: 0\n",
      "subnet: [247484], loss: [1.00327195], acc: 0.46875, samples: 32\n",
      "subnet: [312192], expert_idx: 0\n",
      "subnet: [312192], loss: [1.17279064], acc: 0.40625, samples: 32\n",
      "subnet: [289973], expert_idx: 0\n",
      "subnet: [289973], loss: [1.49450783], acc: 0.34375, samples: 32\n",
      "subnet: [113060], expert_idx: 0\n",
      "subnet: [113060], loss: [1.31560483], acc: 0.25, samples: 32\n",
      "subnet: [168562], expert_idx: 0\n",
      "subnet: [168562], loss: [0.99890074], acc: 0.59375, samples: 32\n",
      "subnet: [276549], expert_idx: 0\n",
      "subnet: [276549], loss: [1.17091522], acc: 0.53125, samples: 32\n",
      "subnet: [209286], expert_idx: 0\n",
      "subnet: [209286], loss: [1.07685047], acc: 0.4375, samples: 32\n",
      "subnet: [252944], expert_idx: 0\n",
      "subnet: [252944], loss: [1.03780547], acc: 0.53125, samples: 32\n",
      "subnet: [124936], expert_idx: 0\n",
      "subnet: [124936], loss: [1.25143456], acc: 0.4375, samples: 32\n",
      "subnet: [120041], expert_idx: 0\n",
      "subnet: [120041], loss: [1.03789225], acc: 0.5, samples: 32\n",
      "subnet: [183774], expert_idx: 0\n",
      "subnet: [183774], loss: [1.08917852], acc: 0.5625, samples: 32\n",
      "subnet: [234336], expert_idx: 0\n",
      "subnet: [234336], loss: [1.27900221], acc: 0.40625, samples: 32\n",
      "subnet: [241392], expert_idx: 0\n",
      "subnet: [241392], loss: [1.02571193], acc: 0.53125, samples: 32\n",
      "subnet: [159805], expert_idx: 0\n",
      "subnet: [159805], loss: [1.12828462], acc: 0.4375, samples: 32\n",
      "subnet: [51136], expert_idx: 0\n",
      "subnet: [51136], loss: [1.08980614], acc: 0.53125, samples: 32\n",
      "subnet: [80696], expert_idx: 0\n",
      "subnet: [80696], loss: [0.91892226], acc: 0.5625, samples: 32\n",
      "subnet: [309406], expert_idx: 0\n",
      "subnet: [309406], loss: [1.08111181], acc: 0.53125, samples: 32\n",
      "subnet: [54687], expert_idx: 0\n",
      "subnet: [54687], loss: [1.21255212], acc: 0.40625, samples: 32\n",
      "subnet: [2681], expert_idx: 0\n",
      "subnet: [2681], loss: [1.16631418], acc: 0.4375, samples: 32\n",
      "subnet: [237893], expert_idx: 0\n",
      "subnet: [237893], loss: [1.01002846], acc: 0.53125, samples: 32\n",
      "subnet: [33812], expert_idx: 0\n",
      "subnet: [33812], loss: [1.08070147], acc: 0.53125, samples: 32\n",
      "subnet: [262429], expert_idx: 0\n",
      "subnet: [262429], loss: [1.23198741], acc: 0.5, samples: 32\n",
      "subnet: [78564], expert_idx: 0\n",
      "subnet: [78564], loss: [1.08505823], acc: 0.40625, samples: 32\n",
      "subnet: [166395], expert_idx: 0\n",
      "subnet: [166395], loss: [0.93008632], acc: 0.65625, samples: 32\n",
      "subnet: [293229], expert_idx: 0\n",
      "subnet: [293229], loss: [1.19476367], acc: 0.5625, samples: 32\n",
      "subnet: [81897], expert_idx: 0\n",
      "subnet: [81897], loss: [1.18249986], acc: 0.5, samples: 32\n",
      "subnet: [325700], expert_idx: 0\n",
      "subnet: [325700], loss: [1.35268094], acc: 0.40625, samples: 32\n",
      "subnet: [123581], expert_idx: 0\n",
      "subnet: [123581], loss: [1.14858525], acc: 0.4375, samples: 32\n",
      "subnet: [141413], expert_idx: 0\n",
      "subnet: [141413], loss: [1.24157685], acc: 0.4375, samples: 32\n",
      "subnet: [107616], expert_idx: 0\n",
      "subnet: [107616], loss: [1.04934919], acc: 0.5625, samples: 32\n",
      "subnet: [222477], expert_idx: 0\n",
      "subnet: [222477], loss: [1.11312132], acc: 0.4375, samples: 32\n",
      "subnet: [70396], expert_idx: 0\n",
      "subnet: [70396], loss: [1.23609814], acc: 0.4375, samples: 32\n",
      "subnet: [287229], expert_idx: 0\n",
      "subnet: [287229], loss: [1.17744914], acc: 0.5, samples: 32\n",
      "subnet: [292791], expert_idx: 0\n",
      "subnet: [292791], loss: [1.02297142], acc: 0.53125, samples: 32\n",
      "subnet: [128797], expert_idx: 0\n",
      "subnet: [128797], loss: [1.10130863], acc: 0.65625, samples: 32\n",
      "subnet: [191435], expert_idx: 0\n",
      "subnet: [191435], loss: [0.95925418], acc: 0.6875, samples: 32\n",
      "subnet: [231037], expert_idx: 0\n",
      "subnet: [231037], loss: [1.29703244], acc: 0.40625, samples: 32\n",
      "subnet: [215121], expert_idx: 0\n",
      "subnet: [215121], loss: [1.19512534], acc: 0.4375, samples: 32\n",
      "subnet: [142939], expert_idx: 0\n",
      "subnet: [142939], loss: [1.12345288], acc: 0.4375, samples: 32\n",
      "subnet: [124835], expert_idx: 0\n",
      "subnet: [124835], loss: [1.24297187], acc: 0.40625, samples: 32\n",
      "subnet: [73828], expert_idx: 0\n",
      "subnet: [73828], loss: [1.25378921], acc: 0.46875, samples: 32\n",
      "subnet: [323664], expert_idx: 0\n",
      "subnet: [323664], loss: [1.471781], acc: 0.40625, samples: 32\n",
      "subnet: [82245], expert_idx: 0\n",
      "subnet: [82245], loss: [1.07543458], acc: 0.5625, samples: 32\n",
      "subnet: [318657], expert_idx: 0\n",
      "subnet: [318657], loss: [1.19115545], acc: 0.53125, samples: 32\n",
      "subnet: [186528], expert_idx: 0\n",
      "subnet: [186528], loss: [1.29004023], acc: 0.3125, samples: 32\n",
      "subnet: [98068], expert_idx: 0\n",
      "subnet: [98068], loss: [1.26155673], acc: 0.4375, samples: 32\n",
      "subnet: [130016], expert_idx: 0\n",
      "subnet: [130016], loss: [1.01432151], acc: 0.5625, samples: 32\n",
      "subnet: [62798], expert_idx: 0\n",
      "subnet: [62798], loss: [0.92578197], acc: 0.625, samples: 32\n",
      "subnet: [183482], expert_idx: 0\n",
      "subnet: [183482], loss: [1.10352914], acc: 0.5, samples: 32\n",
      "subnet: [50749], expert_idx: 0\n",
      "subnet: [50749], loss: [0.89362221], acc: 0.71875, samples: 32\n",
      "subnet: [184223], expert_idx: 0\n",
      "subnet: [184223], loss: [1.28922395], acc: 0.4375, samples: 32\n",
      "subnet: [26559], expert_idx: 0\n",
      "subnet: [26559], loss: [1.01629467], acc: 0.4375, samples: 32\n",
      "subnet: [302413], expert_idx: 0\n",
      "subnet: [302413], loss: [1.17251991], acc: 0.53125, samples: 32\n",
      "subnet: [54372], expert_idx: 0\n",
      "subnet: [54372], loss: [1.46278699], acc: 0.34375, samples: 32\n",
      "subnet: [235409], expert_idx: 0\n",
      "subnet: [235409], loss: [1.03675594], acc: 0.5, samples: 32\n",
      "subnet: [82199], expert_idx: 0\n",
      "subnet: [82199], loss: [1.09453733], acc: 0.53125, samples: 32\n",
      "subnet: [249907], expert_idx: 0\n",
      "subnet: [249907], loss: [0.96686288], acc: 0.65625, samples: 32\n",
      "subnet: [194031], expert_idx: 0\n",
      "subnet: [194031], loss: [0.90674312], acc: 0.6875, samples: 32\n",
      "subnet: [66396], expert_idx: 0\n",
      "subnet: [66396], loss: [1.36524528], acc: 0.5, samples: 32\n",
      "subnet: [273490], expert_idx: 0\n",
      "subnet: [273490], loss: [1.14037248], acc: 0.53125, samples: 32\n",
      "subnet: [112131], expert_idx: 0\n",
      "subnet: [112131], loss: [0.9215998], acc: 0.6875, samples: 32\n",
      "subnet: [268447], expert_idx: 0\n",
      "subnet: [268447], loss: [1.0202986], acc: 0.46875, samples: 32\n",
      "subnet: [280702], expert_idx: 0\n",
      "subnet: [280702], loss: [1.24053654], acc: 0.34375, samples: 32\n",
      "subnet: [34255], expert_idx: 0\n",
      "subnet: [34255], loss: [1.25715424], acc: 0.4375, samples: 32\n",
      "subnet: [114870], expert_idx: 0\n",
      "subnet: [114870], loss: [1.10107564], acc: 0.46875, samples: 32\n",
      "subnet: [204823], expert_idx: 0\n",
      "subnet: [204823], loss: [0.88434517], acc: 0.625, samples: 32\n",
      "subnet: [232620], expert_idx: 0\n",
      "subnet: [232620], loss: [1.11432801], acc: 0.5, samples: 32\n",
      "subnet: [314613], expert_idx: 0\n",
      "subnet: [314613], loss: [0.99815918], acc: 0.59375, samples: 32\n",
      "subnet: [110297], expert_idx: 0\n",
      "subnet: [110297], loss: [1.07909305], acc: 0.53125, samples: 32\n",
      "subnet: [87675], expert_idx: 0\n",
      "subnet: [87675], loss: [0.99791606], acc: 0.65625, samples: 32\n",
      "subnet: [221322], expert_idx: 0\n",
      "subnet: [221322], loss: [1.02077351], acc: 0.46875, samples: 32\n",
      "subnet: [79885], expert_idx: 0\n",
      "subnet: [79885], loss: [1.09094719], acc: 0.59375, samples: 32\n",
      "subnet: [14714], expert_idx: 0\n",
      "subnet: [14714], loss: [0.93583177], acc: 0.65625, samples: 32\n",
      "subnet: [122692], expert_idx: 0\n",
      "subnet: [122692], loss: [1.09223623], acc: 0.53125, samples: 32\n",
      "subnet: [283209], expert_idx: 0\n",
      "subnet: [283209], loss: [1.43861778], acc: 0.375, samples: 32\n",
      "subnet: [103739], expert_idx: 0\n",
      "subnet: [103739], loss: [0.9315643], acc: 0.625, samples: 32\n",
      "subnet: [20899], expert_idx: 0\n",
      "subnet: [20899], loss: [1.15390951], acc: 0.46875, samples: 32\n",
      "subnet: [46807], expert_idx: 0\n",
      "subnet: [46807], loss: [0.93360938], acc: 0.65625, samples: 32\n",
      "subnet: [183197], expert_idx: 0\n",
      "subnet: [183197], loss: [1.12229158], acc: 0.46875, samples: 32\n",
      "subnet: [315744], expert_idx: 0\n",
      "subnet: [315744], loss: [1.26719837], acc: 0.4375, samples: 32\n",
      "subnet: [1567], expert_idx: 0\n",
      "subnet: [1567], loss: [1.14930216], acc: 0.4375, samples: 32\n",
      "subnet: [243548], expert_idx: 0\n",
      "subnet: [243548], loss: [1.34998107], acc: 0.53125, samples: 32\n",
      "subnet: [265730], expert_idx: 0\n",
      "subnet: [265730], loss: [0.83881747], acc: 0.6875, samples: 32\n",
      "subnet: [58853], expert_idx: 0\n",
      "subnet: [58853], loss: [1.35407662], acc: 0.4375, samples: 32\n",
      "subnet: [301100], expert_idx: 0\n",
      "subnet: [301100], loss: [1.22341989], acc: 0.5, samples: 32\n",
      "subnet: [76540], expert_idx: 0\n",
      "subnet: [76540], loss: [1.37784048], acc: 0.4375, samples: 32\n",
      "subnet: [66590], expert_idx: 0\n",
      "subnet: [66590], loss: [1.42609338], acc: 0.3125, samples: 32\n",
      "subnet: [215352], expert_idx: 0\n",
      "subnet: [215352], loss: [1.12636417], acc: 0.59375, samples: 32\n",
      "subnet: [180455], expert_idx: 0\n",
      "subnet: [180455], loss: [1.37339211], acc: 0.375, samples: 32\n",
      "subnet: [99896], expert_idx: 0\n",
      "subnet: [99896], loss: [1.23678616], acc: 0.40625, samples: 32\n",
      "subnet: [315393], expert_idx: 0\n",
      "subnet: [315393], loss: [0.99785004], acc: 0.5625, samples: 32\n",
      "subnet: [78674], expert_idx: 0\n",
      "subnet: [78674], loss: [1.13639473], acc: 0.5625, samples: 32\n",
      "subnet: [119106], expert_idx: 0\n",
      "subnet: [119106], loss: [0.99473179], acc: 0.5625, samples: 32\n",
      "subnet: [194070], expert_idx: 0\n",
      "subnet: [194070], loss: [1.01156544], acc: 0.59375, samples: 32\n",
      "subnet: [291867], expert_idx: 0\n",
      "subnet: [291867], loss: [1.38381169], acc: 0.375, samples: 32\n",
      "subnet: [20766], expert_idx: 0\n",
      "subnet: [20766], loss: [1.26155673], acc: 0.4375, samples: 32\n",
      "subnet: [270507], expert_idx: 0\n",
      "subnet: [270507], loss: [1.30392625], acc: 0.40625, samples: 32\n",
      "subnet: [249479], expert_idx: 0\n",
      "subnet: [249479], loss: [1.00555217], acc: 0.46875, samples: 32\n",
      "subnet: [271251], expert_idx: 0\n",
      "subnet: [271251], loss: [1.08768382], acc: 0.5625, samples: 32\n",
      "subnet: [75214], expert_idx: 0\n",
      "subnet: [75214], loss: [1.13508072], acc: 0.53125, samples: 32\n",
      "subnet: [114061], expert_idx: 0\n",
      "subnet: [114061], loss: [1.2433863], acc: 0.4375, samples: 32\n",
      "subnet: [100797], expert_idx: 0\n",
      "subnet: [100797], loss: [0.92968844], acc: 0.59375, samples: 32\n",
      "subnet: [71418], expert_idx: 0\n",
      "subnet: [71418], loss: [1.04778141], acc: 0.5625, samples: 32\n",
      "subnet: [249535], expert_idx: 0\n",
      "subnet: [249535], loss: [0.997505], acc: 0.5, samples: 32\n",
      "subnet: [204278], expert_idx: 0\n",
      "subnet: [204278], loss: [0.94988224], acc: 0.59375, samples: 32\n",
      "subnet: [195593], expert_idx: 0\n",
      "subnet: [195593], loss: [1.09453733], acc: 0.53125, samples: 32\n",
      "subnet: [9494], expert_idx: 0\n",
      "subnet: [9494], loss: [1.24602852], acc: 0.375, samples: 32\n",
      "subnet: [303699], expert_idx: 0\n",
      "subnet: [303699], loss: [0.98151631], acc: 0.5625, samples: 32\n",
      "subnet: [60999], expert_idx: 0\n",
      "subnet: [60999], loss: [0.86394529], acc: 0.59375, samples: 32\n",
      "subnet: [227772], expert_idx: 0\n",
      "subnet: [227772], loss: [0.85629468], acc: 0.71875, samples: 32\n",
      "subnet: [203853], expert_idx: 0\n",
      "subnet: [203853], loss: [1.22446469], acc: 0.5, samples: 32\n",
      "subnet: [155075], expert_idx: 0\n",
      "subnet: [155075], loss: [1.52091584], acc: 0.40625, samples: 32\n",
      "subnet: [157480], expert_idx: 0\n",
      "subnet: [157480], loss: [1.16904343], acc: 0.53125, samples: 32\n",
      "subnet: [280659], expert_idx: 0\n",
      "subnet: [280659], loss: [1.47710652], acc: 0.34375, samples: 32\n",
      "subnet: [193543], expert_idx: 0\n",
      "subnet: [193543], loss: [0.9948773], acc: 0.625, samples: 32\n",
      "subnet: [206672], expert_idx: 0\n",
      "subnet: [206672], loss: [1.18105648], acc: 0.5, samples: 32\n",
      "subnet: [102337], expert_idx: 0\n",
      "subnet: [102337], loss: [1.15046565], acc: 0.53125, samples: 32\n",
      "subnet: [111562], expert_idx: 0\n",
      "subnet: [111562], loss: [0.9752752], acc: 0.53125, samples: 32\n",
      "subnet: [180152], expert_idx: 0\n",
      "subnet: [180152], loss: [1.11311116], acc: 0.375, samples: 32\n",
      "subnet: [163439], expert_idx: 0\n",
      "subnet: [163439], loss: [1.12904117], acc: 0.5625, samples: 32\n",
      "subnet: [91966], expert_idx: 0\n",
      "subnet: [91966], loss: [1.12134708], acc: 0.5625, samples: 32\n",
      "subnet: [163040], expert_idx: 0\n",
      "subnet: [163040], loss: [1.0628727], acc: 0.5, samples: 32\n",
      "subnet: [291182], expert_idx: 0\n",
      "subnet: [291182], loss: [1.20733381], acc: 0.4375, samples: 32\n",
      "subnet: [161402], expert_idx: 0\n",
      "subnet: [161402], loss: [1.44893227], acc: 0.3125, samples: 32\n",
      "subnet: [289425], expert_idx: 0\n",
      "subnet: [289425], loss: [1.02046406], acc: 0.5625, samples: 32\n",
      "subnet: [90061], expert_idx: 0\n",
      "subnet: [90061], loss: [0.99869508], acc: 0.5, samples: 32\n",
      "subnet: [275357], expert_idx: 0\n",
      "subnet: [275357], loss: [1.03189988], acc: 0.59375, samples: 32\n",
      "subnet: [64445], expert_idx: 0\n",
      "subnet: [64445], loss: [1.3368382], acc: 0.375, samples: 32\n",
      "subnet: [167938], expert_idx: 0\n",
      "subnet: [167938], loss: [1.11123319], acc: 0.5625, samples: 32\n",
      "subnet: [124670], expert_idx: 0\n",
      "subnet: [124670], loss: [1.45662873], acc: 0.3125, samples: 32\n",
      "subnet: [77698], expert_idx: 0\n",
      "subnet: [77698], loss: [1.04072569], acc: 0.4375, samples: 32\n",
      "subnet: [330656], expert_idx: 0\n",
      "subnet: [330656], loss: [1.06176478], acc: 0.53125, samples: 32\n",
      "subnet: [243676], expert_idx: 0\n",
      "subnet: [243676], loss: [1.31829733], acc: 0.53125, samples: 32\n",
      "subnet: [258723], expert_idx: 0\n",
      "subnet: [258723], loss: [1.02851447], acc: 0.5, samples: 32\n",
      "subnet: [168067], expert_idx: 0\n",
      "subnet: [168067], loss: [1.2273058], acc: 0.46875, samples: 32\n",
      "subnet: [204698], expert_idx: 0\n",
      "subnet: [204698], loss: [1.08406009], acc: 0.46875, samples: 32\n",
      "subnet: [114462], expert_idx: 0\n",
      "subnet: [114462], loss: [1.03306199], acc: 0.625, samples: 32\n",
      "subnet: [288962], expert_idx: 0\n",
      "subnet: [288962], loss: [1.19512534], acc: 0.4375, samples: 32\n",
      "subnet: [284855], expert_idx: 0\n",
      "subnet: [284855], loss: [1.15690372], acc: 0.4375, samples: 32\n",
      "subnet: [109121], expert_idx: 0\n",
      "subnet: [109121], loss: [1.14265416], acc: 0.46875, samples: 32\n",
      "subnet: [13514], expert_idx: 0\n",
      "subnet: [13514], loss: [1.25971153], acc: 0.46875, samples: 32\n",
      "subnet: [55324], expert_idx: 0\n",
      "subnet: [55324], loss: [1.12013632], acc: 0.5625, samples: 32\n",
      "subnet: [166413], expert_idx: 0\n",
      "subnet: [166413], loss: [1.09271124], acc: 0.5, samples: 32\n",
      "subnet: [110368], expert_idx: 0\n",
      "subnet: [110368], loss: [1.01156544], acc: 0.59375, samples: 32\n",
      "subnet: [212073], expert_idx: 0\n",
      "subnet: [212073], loss: [1.19047788], acc: 0.46875, samples: 32\n",
      "subnet: [207042], expert_idx: 0\n",
      "subnet: [207042], loss: [1.08108673], acc: 0.5625, samples: 32\n",
      "subnet: [129502], expert_idx: 0\n",
      "subnet: [129502], loss: [1.01427978], acc: 0.5625, samples: 32\n",
      "subnet: [259638], expert_idx: 0\n",
      "subnet: [259638], loss: [0.92362774], acc: 0.5625, samples: 32\n",
      "subnet: [284447], expert_idx: 0\n",
      "subnet: [284447], loss: [0.96497686], acc: 0.53125, samples: 32\n",
      "subnet: [1664], expert_idx: 0\n",
      "subnet: [1664], loss: [1.0501301], acc: 0.53125, samples: 32\n",
      "subnet: [242173], expert_idx: 0\n",
      "subnet: [242173], loss: [0.93950648], acc: 0.59375, samples: 32\n",
      "subnet: [91760], expert_idx: 0\n",
      "subnet: [91760], loss: [1.13280145], acc: 0.5625, samples: 32\n",
      "subnet: [285685], expert_idx: 0\n",
      "subnet: [285685], loss: [1.3830638], acc: 0.375, samples: 32\n",
      "subnet: [253055], expert_idx: 0\n",
      "subnet: [253055], loss: [1.37392393], acc: 0.25, samples: 32\n",
      "subnet: [67703], expert_idx: 0\n",
      "subnet: [67703], loss: [1.4293122], acc: 0.3125, samples: 32\n",
      "subnet: [205833], expert_idx: 0\n",
      "subnet: [205833], loss: [1.09453733], acc: 0.53125, samples: 32\n",
      "subnet: [43342], expert_idx: 0\n",
      "subnet: [43342], loss: [1.30093956], acc: 0.34375, samples: 32\n",
      "subnet: [133904], expert_idx: 0\n",
      "subnet: [133904], loss: [1.2750087], acc: 0.4375, samples: 32\n",
      "subnet: [200108], expert_idx: 0\n",
      "subnet: [200108], loss: [1.00490125], acc: 0.5625, samples: 32\n",
      "subnet: [214751], expert_idx: 0\n",
      "subnet: [214751], loss: [1.06621332], acc: 0.5, samples: 32\n",
      "subnet: [308405], expert_idx: 0\n",
      "subnet: [308405], loss: [0.97621237], acc: 0.59375, samples: 32\n",
      "subnet: [86484], expert_idx: 0\n",
      "subnet: [86484], loss: [1.07517924], acc: 0.59375, samples: 32\n",
      "subnet: [107534], expert_idx: 0\n",
      "subnet: [107534], loss: [1.16904343], acc: 0.53125, samples: 32\n",
      "subnet: [108506], expert_idx: 0\n",
      "subnet: [108506], loss: [1.0152315], acc: 0.5625, samples: 32\n",
      "subnet: [210215], expert_idx: 0\n",
      "subnet: [210215], loss: [1.44366414], acc: 0.375, samples: 32\n",
      "subnet: [267254], expert_idx: 0\n",
      "subnet: [267254], loss: [1.20013732], acc: 0.375, samples: 32\n",
      "subnet: [297914], expert_idx: 0\n",
      "subnet: [297914], loss: [1.20385923], acc: 0.46875, samples: 32\n",
      "subnet: [317363], expert_idx: 0\n",
      "subnet: [317363], loss: [1.25539014], acc: 0.4375, samples: 32\n",
      "subnet: [51997], expert_idx: 0\n",
      "subnet: [51997], loss: [0.99438649], acc: 0.46875, samples: 32\n",
      "subnet: [146784], expert_idx: 0\n",
      "subnet: [146784], loss: [1.29358512], acc: 0.3125, samples: 32\n",
      "subnet: [20469], expert_idx: 0\n",
      "subnet: [20469], loss: [0.99485043], acc: 0.59375, samples: 32\n",
      "subnet: [282593], expert_idx: 0\n",
      "subnet: [282593], loss: [1.12282254], acc: 0.59375, samples: 32\n",
      "subnet: [237251], expert_idx: 0\n",
      "subnet: [237251], loss: [1.31218718], acc: 0.375, samples: 32\n",
      "subnet: [48388], expert_idx: 0\n",
      "subnet: [48388], loss: [1.39672997], acc: 0.46875, samples: 32\n",
      "subnet: [84974], expert_idx: 0\n",
      "subnet: [84974], loss: [1.1157339], acc: 0.4375, samples: 32\n",
      "subnet: [227938], expert_idx: 0\n",
      "subnet: [227938], loss: [1.16591413], acc: 0.40625, samples: 32\n",
      "subnet: [150769], expert_idx: 0\n",
      "subnet: [150769], loss: [0.90472957], acc: 0.59375, samples: 32\n",
      "subnet: [65139], expert_idx: 0\n",
      "subnet: [65139], loss: [0.93522431], acc: 0.6875, samples: 32\n",
      "subnet: [83528], expert_idx: 0\n",
      "subnet: [83528], loss: [1.07485144], acc: 0.4375, samples: 32\n",
      "subnet: [96731], expert_idx: 0\n",
      "subnet: [96731], loss: [1.00279216], acc: 0.53125, samples: 32\n",
      "subnet: [218553], expert_idx: 0\n",
      "subnet: [218553], loss: [1.07390661], acc: 0.6875, samples: 32\n",
      "subnet: [126253], expert_idx: 0\n",
      "subnet: [126253], loss: [1.15493796], acc: 0.46875, samples: 32\n",
      "subnet: [3625], expert_idx: 0\n",
      "subnet: [3625], loss: [1.2461984], acc: 0.4375, samples: 32\n",
      "subnet: [249998], expert_idx: 0\n",
      "subnet: [249998], loss: [1.32488816], acc: 0.375, samples: 32\n",
      "subnet: [68885], expert_idx: 0\n",
      "subnet: [68885], loss: [1.13147465], acc: 0.5, samples: 32\n",
      "subnet: [175525], expert_idx: 0\n",
      "subnet: [175525], loss: [1.03858662], acc: 0.4375, samples: 32\n",
      "subnet: [262132], expert_idx: 0\n",
      "subnet: [262132], loss: [1.21100168], acc: 0.59375, samples: 32\n",
      "subnet: [170797], expert_idx: 0\n",
      "subnet: [170797], loss: [1.04150543], acc: 0.5625, samples: 32\n",
      "subnet: [282599], expert_idx: 0\n",
      "subnet: [282599], loss: [1.35741653], acc: 0.4375, samples: 32\n",
      "subnet: [309803], expert_idx: 0\n",
      "subnet: [309803], loss: [1.04503297], acc: 0.59375, samples: 32\n",
      "subnet: [192283], expert_idx: 0\n",
      "subnet: [192283], loss: [1.1086173], acc: 0.53125, samples: 32\n",
      "subnet: [309707], expert_idx: 0\n",
      "subnet: [309707], loss: [1.16900232], acc: 0.46875, samples: 32\n",
      "subnet: [90089], expert_idx: 0\n",
      "subnet: [90089], loss: [1.00419333], acc: 0.40625, samples: 32\n",
      "subnet: [311259], expert_idx: 0\n",
      "subnet: [311259], loss: [1.3427646], acc: 0.5, samples: 32\n",
      "subnet: [305453], expert_idx: 0\n",
      "subnet: [305453], loss: [1.38381169], acc: 0.375, samples: 32\n",
      "subnet: [84368], expert_idx: 0\n",
      "subnet: [84368], loss: [1.02715717], acc: 0.5625, samples: 32\n",
      "subnet: [243739], expert_idx: 0\n",
      "subnet: [243739], loss: [1.14065936], acc: 0.5, samples: 32\n",
      "subnet: [76885], expert_idx: 0\n",
      "subnet: [76885], loss: [1.02571172], acc: 0.5, samples: 32\n",
      "subnet: [178869], expert_idx: 0\n",
      "subnet: [178869], loss: [1.3155563], acc: 0.34375, samples: 32\n",
      "subnet: [298104], expert_idx: 0\n",
      "subnet: [298104], loss: [1.01175751], acc: 0.65625, samples: 32\n",
      "subnet: [62456], expert_idx: 0\n",
      "subnet: [62456], loss: [0.99649478], acc: 0.5625, samples: 32\n",
      "subnet: [328587], expert_idx: 0\n",
      "subnet: [328587], loss: [0.91937863], acc: 0.65625, samples: 32\n",
      "subnet: [21257], expert_idx: 0\n",
      "subnet: [21257], loss: [1.30261298], acc: 0.4375, samples: 32\n",
      "subnet: [111396], expert_idx: 0\n",
      "subnet: [111396], loss: [1.02030842], acc: 0.5625, samples: 32\n",
      "subnet: [141502], expert_idx: 0\n",
      "subnet: [141502], loss: [1.37507776], acc: 0.375, samples: 32\n",
      "subnet: [18621], expert_idx: 0\n",
      "subnet: [18621], loss: [0.86134474], acc: 0.625, samples: 32\n",
      "subnet: [177421], expert_idx: 0\n",
      "subnet: [177421], loss: [1.25357404], acc: 0.375, samples: 32\n",
      "subnet: [276118], expert_idx: 0\n",
      "subnet: [276118], loss: [1.2023893], acc: 0.4375, samples: 32\n",
      "subnet: [66035], expert_idx: 0\n",
      "subnet: [66035], loss: [1.36383008], acc: 0.5, samples: 32\n",
      "subnet: [16422], expert_idx: 0\n",
      "subnet: [16422], loss: [1.21990545], acc: 0.40625, samples: 32\n",
      "subnet: [304357], expert_idx: 0\n",
      "subnet: [304357], loss: [1.034195], acc: 0.46875, samples: 32\n",
      "subnet: [328682], expert_idx: 0\n",
      "subnet: [328682], loss: [1.07517924], acc: 0.59375, samples: 32\n",
      "subnet: [294897], expert_idx: 0\n",
      "subnet: [294897], loss: [0.92304804], acc: 0.65625, samples: 32\n",
      "subnet: [245034], expert_idx: 0\n",
      "subnet: [245034], loss: [1.12937331], acc: 0.4375, samples: 32\n",
      "subnet: [209838], expert_idx: 0\n",
      "subnet: [209838], loss: [1.15730039], acc: 0.46875, samples: 32\n",
      "subnet: [231085], expert_idx: 0\n",
      "subnet: [231085], loss: [0.896507], acc: 0.65625, samples: 32\n",
      "subnet: [51260], expert_idx: 0\n",
      "subnet: [51260], loss: [1.0186017], acc: 0.46875, samples: 32\n",
      "subnet: [176161], expert_idx: 0\n",
      "subnet: [176161], loss: [1.28428287], acc: 0.4375, samples: 32\n",
      "subnet: [76792], expert_idx: 0\n",
      "subnet: [76792], loss: [1.07372123], acc: 0.46875, samples: 32\n",
      "subnet: [116903], expert_idx: 0\n",
      "subnet: [116903], loss: [1.13145979], acc: 0.4375, samples: 32\n",
      "subnet: [9948], expert_idx: 0\n",
      "subnet: [9948], loss: [1.06570125], acc: 0.5625, samples: 32\n",
      "subnet: [141731], expert_idx: 0\n",
      "subnet: [141731], loss: [0.93168115], acc: 0.6875, samples: 32\n",
      "subnet: [97646], expert_idx: 0\n",
      "subnet: [97646], loss: [0.97692556], acc: 0.5625, samples: 32\n",
      "subnet: [321269], expert_idx: 0\n",
      "subnet: [321269], loss: [0.85821865], acc: 0.65625, samples: 32\n",
      "subnet: [5946], expert_idx: 0\n",
      "subnet: [5946], loss: [1.22102498], acc: 0.46875, samples: 32\n",
      "subnet: [13517], expert_idx: 0\n",
      "subnet: [13517], loss: [1.21612677], acc: 0.34375, samples: 32\n",
      "subnet: [108923], expert_idx: 0\n",
      "subnet: [108923], loss: [1.08777128], acc: 0.53125, samples: 32\n",
      "subnet: [324214], expert_idx: 0\n",
      "subnet: [324214], loss: [1.04214467], acc: 0.53125, samples: 32\n",
      "subnet: [88834], expert_idx: 0\n",
      "subnet: [88834], loss: [1.07013192], acc: 0.5625, samples: 32\n",
      "subnet: [45286], expert_idx: 0\n",
      "subnet: [45286], loss: [1.04676005], acc: 0.5625, samples: 32\n",
      "subnet: [239337], expert_idx: 0\n",
      "subnet: [239337], loss: [1.22835221], acc: 0.46875, samples: 32\n",
      "subnet: [115262], expert_idx: 0\n",
      "subnet: [115262], loss: [0.91299501], acc: 0.65625, samples: 32\n",
      "subnet: [192632], expert_idx: 0\n",
      "subnet: [192632], loss: [1.23373984], acc: 0.4375, samples: 32\n",
      "subnet: [319799], expert_idx: 0\n",
      "subnet: [319799], loss: [1.00046012], acc: 0.5625, samples: 32\n",
      "subnet: [219645], expert_idx: 0\n",
      "subnet: [219645], loss: [1.33044705], acc: 0.375, samples: 32\n",
      "subnet: [9850], expert_idx: 0\n",
      "subnet: [9850], loss: [1.10598302], acc: 0.5, samples: 32\n",
      "subnet: [241919], expert_idx: 0\n",
      "subnet: [241919], loss: [1.08491245], acc: 0.53125, samples: 32\n",
      "subnet: [223085], expert_idx: 0\n",
      "subnet: [223085], loss: [1.47059415], acc: 0.40625, samples: 32\n",
      "subnet: [65656], expert_idx: 0\n",
      "subnet: [65656], loss: [1.37339211], acc: 0.375, samples: 32\n",
      "subnet: [63993], expert_idx: 0\n",
      "subnet: [63993], loss: [1.29841435], acc: 0.40625, samples: 32\n",
      "subnet: [24834], expert_idx: 0\n",
      "subnet: [24834], loss: [0.99648628], acc: 0.5625, samples: 32\n",
      "subnet: [143550], expert_idx: 0\n",
      "subnet: [143550], loss: [1.28461265], acc: 0.3125, samples: 32\n",
      "subnet: [257740], expert_idx: 0\n",
      "subnet: [257740], loss: [1.28499377], acc: 0.40625, samples: 32\n",
      "subnet: [61271], expert_idx: 0\n",
      "subnet: [61271], loss: [1.09524737], acc: 0.5, samples: 32\n",
      "subnet: [81274], expert_idx: 0\n",
      "subnet: [81274], loss: [1.03608148], acc: 0.46875, samples: 32\n",
      "subnet: [125574], expert_idx: 0\n",
      "subnet: [125574], loss: [1.21365906], acc: 0.34375, samples: 32\n",
      "subnet: [111133], expert_idx: 0\n",
      "subnet: [111133], loss: [1.13768745], acc: 0.5, samples: 32\n",
      "subnet: [160846], expert_idx: 0\n",
      "subnet: [160846], loss: [1.2230859], acc: 0.53125, samples: 32\n",
      "subnet: [161727], expert_idx: 0\n",
      "subnet: [161727], loss: [1.07251697], acc: 0.53125, samples: 32\n",
      "subnet: [107600], expert_idx: 0\n",
      "subnet: [107600], loss: [1.21117134], acc: 0.34375, samples: 32\n",
      "subnet: [88394], expert_idx: 0\n",
      "subnet: [88394], loss: [0.99646955], acc: 0.625, samples: 32\n",
      "subnet: [105565], expert_idx: 0\n",
      "subnet: [105565], loss: [1.24963887], acc: 0.375, samples: 32\n",
      "subnet: [276338], expert_idx: 0\n",
      "subnet: [276338], loss: [1.17251991], acc: 0.53125, samples: 32\n",
      "subnet: [197877], expert_idx: 0\n",
      "subnet: [197877], loss: [1.14978776], acc: 0.5625, samples: 32\n",
      "subnet: [310234], expert_idx: 0\n",
      "subnet: [310234], loss: [1.51716672], acc: 0.25, samples: 32\n",
      "subnet: [56447], expert_idx: 0\n",
      "subnet: [56447], loss: [1.26507568], acc: 0.5, samples: 32\n",
      "subnet: [218372], expert_idx: 0\n",
      "subnet: [218372], loss: [1.24602852], acc: 0.375, samples: 32\n",
      "subnet: [126893], expert_idx: 0\n",
      "subnet: [126893], loss: [1.24663102], acc: 0.46875, samples: 32\n",
      "subnet: [267526], expert_idx: 0\n",
      "subnet: [267526], loss: [0.98302411], acc: 0.5625, samples: 32\n",
      "subnet: [89084], expert_idx: 0\n",
      "subnet: [89084], loss: [1.00375341], acc: 0.4375, samples: 32\n",
      "subnet: [98246], expert_idx: 0\n",
      "subnet: [98246], loss: [0.96640804], acc: 0.5625, samples: 32\n",
      "subnet: [251485], expert_idx: 0\n",
      "subnet: [251485], loss: [1.13693893], acc: 0.5, samples: 32\n",
      "subnet: [277161], expert_idx: 0\n",
      "subnet: [277161], loss: [1.05762259], acc: 0.5625, samples: 32\n",
      "subnet: [11332], expert_idx: 0\n",
      "subnet: [11332], loss: [1.08858843], acc: 0.4375, samples: 32\n",
      "subnet: [282450], expert_idx: 0\n",
      "subnet: [282450], loss: [1.40551534], acc: 0.46875, samples: 32\n",
      "subnet: [38268], expert_idx: 0\n",
      "subnet: [38268], loss: [1.02687024], acc: 0.5625, samples: 32\n",
      "subnet: [270357], expert_idx: 0\n",
      "subnet: [270357], loss: [1.14333197], acc: 0.46875, samples: 32\n",
      "subnet: [97793], expert_idx: 0\n",
      "subnet: [97793], loss: [1.08467046], acc: 0.625, samples: 32\n",
      "subnet: [17334], expert_idx: 0\n",
      "subnet: [17334], loss: [1.00778087], acc: 0.5, samples: 32\n",
      "subnet: [101885], expert_idx: 0\n",
      "subnet: [101885], loss: [1.05668088], acc: 0.625, samples: 32\n",
      "subnet: [200693], expert_idx: 0\n",
      "subnet: [200693], loss: [1.20033731], acc: 0.40625, samples: 32\n",
      "subnet: [88395], expert_idx: 0\n",
      "subnet: [88395], loss: [0.99979595], acc: 0.5, samples: 32\n",
      "subnet: [198171], expert_idx: 0\n",
      "subnet: [198171], loss: [1.1617826], acc: 0.5, samples: 32\n",
      "subnet: [172808], expert_idx: 0\n",
      "subnet: [172808], loss: [1.04663687], acc: 0.46875, samples: 32\n",
      "subnet: [159178], expert_idx: 0\n",
      "subnet: [159178], loss: [1.00295304], acc: 0.53125, samples: 32\n",
      "subnet: [157368], expert_idx: 0\n",
      "subnet: [157368], loss: [0.97534857], acc: 0.625, samples: 32\n",
      "subnet: [221805], expert_idx: 0\n",
      "subnet: [221805], loss: [0.95583031], acc: 0.5, samples: 32\n",
      "subnet: [130173], expert_idx: 0\n",
      "subnet: [130173], loss: [1.10870956], acc: 0.46875, samples: 32\n",
      "subnet: [160428], expert_idx: 0\n",
      "subnet: [160428], loss: [0.95364997], acc: 0.53125, samples: 32\n",
      "subnet: [320388], expert_idx: 0\n",
      "subnet: [320388], loss: [0.99778551], acc: 0.53125, samples: 32\n",
      "subnet: [259923], expert_idx: 0\n",
      "subnet: [259923], loss: [1.22175955], acc: 0.53125, samples: 32\n",
      "subnet: [328206], expert_idx: 0\n",
      "subnet: [328206], loss: [1.09799329], acc: 0.53125, samples: 32\n",
      "subnet: [243661], expert_idx: 0\n",
      "subnet: [243661], loss: [1.31829733], acc: 0.53125, samples: 32\n",
      "subnet: [281838], expert_idx: 0\n",
      "subnet: [281838], loss: [0.87581867], acc: 0.65625, samples: 32\n",
      "subnet: [241491], expert_idx: 0\n",
      "subnet: [241491], loss: [1.60888504], acc: 0.4375, samples: 32\n",
      "subnet: [254676], expert_idx: 0\n",
      "subnet: [254676], loss: [1.08165979], acc: 0.53125, samples: 32\n",
      "subnet: [202713], expert_idx: 0\n",
      "subnet: [202713], loss: [1.31726741], acc: 0.5, samples: 32\n",
      "subnet: [275191], expert_idx: 0\n",
      "subnet: [275191], loss: [1.13336357], acc: 0.53125, samples: 32\n",
      "subnet: [74208], expert_idx: 0\n",
      "subnet: [74208], loss: [1.22009304], acc: 0.40625, samples: 32\n",
      "subnet: [703], expert_idx: 0\n",
      "subnet: [703], loss: [1.15990116], acc: 0.40625, samples: 32\n",
      "subnet: [319437], expert_idx: 0\n",
      "subnet: [319437], loss: [1.17663291], acc: 0.53125, samples: 32\n",
      "subnet: [235591], expert_idx: 0\n",
      "subnet: [235591], loss: [1.43054564], acc: 0.3125, samples: 32\n",
      "subnet: [252664], expert_idx: 0\n",
      "subnet: [252664], loss: [1.09676434], acc: 0.5, samples: 32\n",
      "subnet: [216637], expert_idx: 0\n",
      "subnet: [216637], loss: [1.06600312], acc: 0.46875, samples: 32\n",
      "subnet: [114652], expert_idx: 0\n",
      "subnet: [114652], loss: [0.9831854], acc: 0.5625, samples: 32\n",
      "subnet: [169679], expert_idx: 0\n",
      "subnet: [169679], loss: [1.02234761], acc: 0.53125, samples: 32\n",
      "subnet: [214307], expert_idx: 0\n",
      "subnet: [214307], loss: [1.09472143], acc: 0.59375, samples: 32\n",
      "subnet: [161236], expert_idx: 0\n",
      "subnet: [161236], loss: [1.29366051], acc: 0.4375, samples: 32\n",
      "subnet: [181724], expert_idx: 0\n",
      "subnet: [181724], loss: [1.12517776], acc: 0.5625, samples: 32\n",
      "subnet: [33089], expert_idx: 0\n",
      "subnet: [33089], loss: [0.87630738], acc: 0.625, samples: 32\n",
      "subnet: [222962], expert_idx: 0\n",
      "subnet: [222962], loss: [0.86164818], acc: 0.65625, samples: 32\n",
      "subnet: [223842], expert_idx: 0\n",
      "subnet: [223842], loss: [0.98213002], acc: 0.59375, samples: 32\n",
      "subnet: [209226], expert_idx: 0\n",
      "subnet: [209226], loss: [1.17205071], acc: 0.59375, samples: 32\n",
      "subnet: [181279], expert_idx: 0\n",
      "subnet: [181279], loss: [1.02185082], acc: 0.5625, samples: 32\n",
      "subnet: [251533], expert_idx: 0\n",
      "subnet: [251533], loss: [1.17204473], acc: 0.5, samples: 32\n",
      "subnet: [147552], expert_idx: 0\n",
      "subnet: [147552], loss: [0.95911808], acc: 0.625, samples: 32\n",
      "subnet: [136879], expert_idx: 0\n",
      "subnet: [136879], loss: [1.12903045], acc: 0.46875, samples: 32\n",
      "subnet: [303689], expert_idx: 0\n",
      "subnet: [303689], loss: [0.98127711], acc: 0.5625, samples: 32\n",
      "subnet: [148372], expert_idx: 0\n",
      "subnet: [148372], loss: [0.961314], acc: 0.625, samples: 32\n",
      "subnet: [32943], expert_idx: 0\n",
      "subnet: [32943], loss: [1.1123239], acc: 0.5, samples: 32\n",
      "subnet: [146516], expert_idx: 0\n",
      "subnet: [146516], loss: [1.12575281], acc: 0.46875, samples: 32\n",
      "subnet: [176465], expert_idx: 0\n",
      "subnet: [176465], loss: [0.82497209], acc: 0.6875, samples: 32\n",
      "subnet: [85288], expert_idx: 0\n",
      "subnet: [85288], loss: [1.16904343], acc: 0.53125, samples: 32\n",
      "subnet: [313967], expert_idx: 0\n",
      "subnet: [313967], loss: [1.16459744], acc: 0.53125, samples: 32\n",
      "subnet: [54530], expert_idx: 0\n",
      "subnet: [54530], loss: [0.84520927], acc: 0.65625, samples: 32\n",
      "subnet: [121060], expert_idx: 0\n",
      "subnet: [121060], loss: [1.05414679], acc: 0.46875, samples: 32\n",
      "subnet: [189685], expert_idx: 0\n",
      "subnet: [189685], loss: [1.23758112], acc: 0.40625, samples: 32\n",
      "subnet: [227063], expert_idx: 0\n",
      "subnet: [227063], loss: [1.26854314], acc: 0.46875, samples: 32\n",
      "subnet: [180732], expert_idx: 0\n",
      "subnet: [180732], loss: [1.25006025], acc: 0.40625, samples: 32\n",
      "subnet: [234494], expert_idx: 0\n",
      "subnet: [234494], loss: [1.23513676], acc: 0.46875, samples: 32\n",
      "subnet: [227743], expert_idx: 0\n",
      "subnet: [227743], loss: [0.95903207], acc: 0.59375, samples: 32\n",
      "subnet: [88467], expert_idx: 0\n",
      "subnet: [88467], loss: [0.98272237], acc: 0.71875, samples: 32\n",
      "subnet: [204615], expert_idx: 0\n",
      "subnet: [204615], loss: [1.41364938], acc: 0.4375, samples: 32\n",
      "subnet: [188679], expert_idx: 0\n",
      "subnet: [188679], loss: [0.90261222], acc: 0.65625, samples: 32\n",
      "subnet: [213974], expert_idx: 0\n",
      "subnet: [213974], loss: [1.11148043], acc: 0.53125, samples: 32\n",
      "subnet: [64330], expert_idx: 0\n",
      "subnet: [64330], loss: [1.26735525], acc: 0.375, samples: 32\n",
      "subnet: [329289], expert_idx: 0\n",
      "subnet: [329289], loss: [1.10860642], acc: 0.625, samples: 32\n",
      "subnet: [270611], expert_idx: 0\n",
      "subnet: [270611], loss: [1.24169317], acc: 0.5, samples: 32\n",
      "subnet: [325849], expert_idx: 0\n",
      "subnet: [325849], loss: [1.24157685], acc: 0.4375, samples: 32\n",
      "subnet: [183811], expert_idx: 0\n",
      "subnet: [183811], loss: [1.10463592], acc: 0.46875, samples: 32\n",
      "subnet: [239346], expert_idx: 0\n",
      "subnet: [239346], loss: [1.22755591], acc: 0.46875, samples: 32\n",
      "subnet: [244628], expert_idx: 0\n",
      "subnet: [244628], loss: [1.02061372], acc: 0.46875, samples: 32\n",
      "subnet: [263298], expert_idx: 0\n",
      "subnet: [263298], loss: [1.0347577], acc: 0.46875, samples: 32\n",
      "subnet: [13492], expert_idx: 0\n",
      "subnet: [13492], loss: [0.98385835], acc: 0.53125, samples: 32\n",
      "subnet: [68632], expert_idx: 0\n",
      "subnet: [68632], loss: [1.131199], acc: 0.5, samples: 32\n",
      "subnet: [200346], expert_idx: 0\n",
      "subnet: [200346], loss: [1.28783415], acc: 0.40625, samples: 32\n",
      "subnet: [273084], expert_idx: 0\n",
      "subnet: [273084], loss: [1.17645234], acc: 0.46875, samples: 32\n",
      "subnet: [253942], expert_idx: 0\n",
      "subnet: [253942], loss: [1.47059415], acc: 0.40625, samples: 32\n",
      "subnet: [318297], expert_idx: 0\n",
      "subnet: [318297], loss: [1.13399473], acc: 0.65625, samples: 32\n",
      "subnet: [18088], expert_idx: 0\n",
      "subnet: [18088], loss: [1.01649094], acc: 0.3125, samples: 32\n",
      "subnet: [70861], expert_idx: 0\n",
      "subnet: [70861], loss: [1.25379728], acc: 0.46875, samples: 32\n",
      "subnet: [77122], expert_idx: 0\n",
      "subnet: [77122], loss: [0.82058191], acc: 0.65625, samples: 32\n",
      "subnet: [263001], expert_idx: 0\n",
      "subnet: [263001], loss: [0.99213957], acc: 0.5625, samples: 32\n",
      "subnet: [83001], expert_idx: 0\n",
      "subnet: [83001], loss: [1.01156544], acc: 0.59375, samples: 32\n",
      "subnet: [272004], expert_idx: 0\n",
      "subnet: [272004], loss: [1.46839168], acc: 0.3125, samples: 32\n",
      "subnet: [225188], expert_idx: 0\n",
      "subnet: [225188], loss: [1.18612851], acc: 0.4375, samples: 32\n",
      "subnet: [277536], expert_idx: 0\n",
      "subnet: [277536], loss: [1.09412636], acc: 0.59375, samples: 32\n",
      "subnet: [47562], expert_idx: 0\n",
      "subnet: [47562], loss: [1.07286933], acc: 0.53125, samples: 32\n",
      "subnet: [135940], expert_idx: 0\n",
      "subnet: [135940], loss: [0.95685566], acc: 0.53125, samples: 32\n",
      "subnet: [87990], expert_idx: 0\n",
      "subnet: [87990], loss: [1.00827352], acc: 0.5, samples: 32\n",
      "subnet: [112047], expert_idx: 0\n",
      "subnet: [112047], loss: [1.24254394], acc: 0.4375, samples: 32\n",
      "subnet: [266246], expert_idx: 0\n",
      "subnet: [266246], loss: [1.38358663], acc: 0.34375, samples: 32\n",
      "subnet: [97855], expert_idx: 0\n",
      "subnet: [97855], loss: [1.30261298], acc: 0.4375, samples: 32\n",
      "subnet: [248754], expert_idx: 0\n",
      "subnet: [248754], loss: [1.1274202], acc: 0.4375, samples: 32\n",
      "subnet: [56734], expert_idx: 0\n",
      "subnet: [56734], loss: [1.07658097], acc: 0.46875, samples: 32\n",
      "subnet: [79679], expert_idx: 0\n",
      "subnet: [79679], loss: [1.14637137], acc: 0.5, samples: 32\n",
      "subnet: [178936], expert_idx: 0\n",
      "subnet: [178936], loss: [0.84884092], acc: 0.59375, samples: 32\n",
      "subnet: [184090], expert_idx: 0\n",
      "subnet: [184090], loss: [1.2750087], acc: 0.4375, samples: 32\n",
      "subnet: [130482], expert_idx: 0\n",
      "subnet: [130482], loss: [1.19485336], acc: 0.46875, samples: 32\n",
      "subnet: [145098], expert_idx: 0\n",
      "subnet: [145098], loss: [1.25928949], acc: 0.46875, samples: 32\n",
      "subnet: [226729], expert_idx: 0\n",
      "subnet: [226729], loss: [1.37534797], acc: 0.3125, samples: 32\n",
      "subnet: [288927], expert_idx: 0\n",
      "subnet: [288927], loss: [1.27904015], acc: 0.5, samples: 32\n",
      "subnet: [27628], expert_idx: 0\n",
      "subnet: [27628], loss: [1.02828088], acc: 0.59375, samples: 32\n",
      "subnet: [305038], expert_idx: 0\n",
      "subnet: [305038], loss: [0.9261543], acc: 0.6875, samples: 32\n",
      "subnet: [53201], expert_idx: 0\n",
      "subnet: [53201], loss: [1.08977861], acc: 0.53125, samples: 32\n",
      "subnet: [158078], expert_idx: 0\n",
      "subnet: [158078], loss: [0.93346729], acc: 0.65625, samples: 32\n",
      "subnet: [296419], expert_idx: 0\n",
      "subnet: [296419], loss: [1.2435569], acc: 0.46875, samples: 32\n",
      "subnet: [5718], expert_idx: 0\n",
      "subnet: [5718], loss: [1.02569651], acc: 0.46875, samples: 32\n",
      "subnet: [221447], expert_idx: 0\n",
      "subnet: [221447], loss: [1.138794], acc: 0.53125, samples: 32\n",
      "subnet: [307404], expert_idx: 0\n",
      "subnet: [307404], loss: [1.26321584], acc: 0.5, samples: 32\n",
      "subnet: [264132], expert_idx: 0\n",
      "subnet: [264132], loss: [1.09543122], acc: 0.59375, samples: 32\n",
      "subnet: [35497], expert_idx: 0\n",
      "subnet: [35497], loss: [1.01371535], acc: 0.4375, samples: 32\n",
      "subnet: [52083], expert_idx: 0\n",
      "subnet: [52083], loss: [1.24605763], acc: 0.40625, samples: 32\n",
      "subnet: [133239], expert_idx: 0\n",
      "subnet: [133239], loss: [1.20750143], acc: 0.375, samples: 32\n",
      "subnet: [91231], expert_idx: 0\n",
      "subnet: [91231], loss: [0.90794819], acc: 0.65625, samples: 32\n",
      "subnet: [120162], expert_idx: 0\n",
      "subnet: [120162], loss: [1.26035678], acc: 0.5, samples: 32\n",
      "subnet: [32175], expert_idx: 0\n",
      "subnet: [32175], loss: [1.07326664], acc: 0.34375, samples: 32\n",
      "subnet: [115357], expert_idx: 0\n",
      "subnet: [115357], loss: [1.06264707], acc: 0.46875, samples: 32\n",
      "subnet: [115851], expert_idx: 0\n",
      "subnet: [115851], loss: [1.19832706], acc: 0.3125, samples: 32\n",
      "subnet: [327378], expert_idx: 0\n",
      "subnet: [327378], loss: [1.06121316], acc: 0.625, samples: 32\n",
      "subnet: [185343], expert_idx: 0\n",
      "subnet: [185343], loss: [1.33614819], acc: 0.34375, samples: 32\n",
      "subnet: [22075], expert_idx: 0\n",
      "subnet: [22075], loss: [1.09799329], acc: 0.53125, samples: 32\n",
      "subnet: [186536], expert_idx: 0\n",
      "subnet: [186536], loss: [1.08689079], acc: 0.53125, samples: 32\n",
      "subnet: [10916], expert_idx: 0\n",
      "subnet: [10916], loss: [1.00552855], acc: 0.53125, samples: 32\n",
      "subnet: [212384], expert_idx: 0\n",
      "subnet: [212384], loss: [1.06633383], acc: 0.53125, samples: 32\n",
      "subnet: [277492], expert_idx: 0\n",
      "subnet: [277492], loss: [1.23900999], acc: 0.375, samples: 32\n",
      "subnet: [132571], expert_idx: 0\n",
      "subnet: [132571], loss: [1.11387817], acc: 0.4375, samples: 32\n",
      "subnet: [56505], expert_idx: 0\n",
      "subnet: [56505], loss: [1.02134101], acc: 0.53125, samples: 32\n",
      "subnet: [88915], expert_idx: 0\n",
      "subnet: [88915], loss: [1.00158622], acc: 0.34375, samples: 32\n",
      "subnet: [72570], expert_idx: 0\n",
      "subnet: [72570], loss: [1.21635207], acc: 0.375, samples: 32\n",
      "subnet: [121077], expert_idx: 0\n",
      "subnet: [121077], loss: [1.10465024], acc: 0.53125, samples: 32\n",
      "subnet: [202554], expert_idx: 0\n",
      "subnet: [202554], loss: [1.19114754], acc: 0.46875, samples: 32\n",
      "subnet: [220600], expert_idx: 0\n",
      "subnet: [220600], loss: [1.00499293], acc: 0.59375, samples: 32\n",
      "subnet: [311688], expert_idx: 0\n",
      "subnet: [311688], loss: [1.12632714], acc: 0.5, samples: 32\n",
      "subnet: [851], expert_idx: 0\n",
      "subnet: [851], loss: [1.16385702], acc: 0.4375, samples: 32\n",
      "subnet: [111337], expert_idx: 0\n",
      "subnet: [111337], loss: [0.9658453], acc: 0.5, samples: 32\n",
      "subnet: [140622], expert_idx: 0\n",
      "subnet: [140622], loss: [1.03832203], acc: 0.53125, samples: 32\n",
      "subnet: [326873], expert_idx: 0\n",
      "subnet: [326873], loss: [1.0257159], acc: 0.5, samples: 32\n",
      "subnet: [196327], expert_idx: 0\n",
      "subnet: [196327], loss: [1.04059614], acc: 0.5, samples: 32\n",
      "subnet: [264667], expert_idx: 0\n",
      "subnet: [264667], loss: [1.0755099], acc: 0.53125, samples: 32\n",
      "subnet: [130646], expert_idx: 0\n",
      "subnet: [130646], loss: [0.93056556], acc: 0.65625, samples: 32\n",
      "subnet: [117995], expert_idx: 0\n",
      "subnet: [117995], loss: [0.91748027], acc: 0.625, samples: 32\n",
      "subnet: [95473], expert_idx: 0\n",
      "subnet: [95473], loss: [1.01465461], acc: 0.5625, samples: 32\n",
      "subnet: [102936], expert_idx: 0\n",
      "subnet: [102936], loss: [1.00331102], acc: 0.65625, samples: 32\n",
      "subnet: [32228], expert_idx: 0\n",
      "subnet: [32228], loss: [0.95426657], acc: 0.5625, samples: 32\n",
      "subnet: [270011], expert_idx: 0\n",
      "subnet: [270011], loss: [0.99759598], acc: 0.53125, samples: 32\n",
      "subnet: [75939], expert_idx: 0\n",
      "subnet: [75939], loss: [1.05176777], acc: 0.40625, samples: 32\n",
      "subnet: [312201], expert_idx: 0\n",
      "subnet: [312201], loss: [1.120498], acc: 0.4375, samples: 32\n",
      "subnet: [64983], expert_idx: 0\n",
      "subnet: [64983], loss: [0.80677624], acc: 0.625, samples: 32\n",
      "subnet: [213097], expert_idx: 0\n",
      "subnet: [213097], loss: [1.15941165], acc: 0.5, samples: 32\n",
      "subnet: [236195], expert_idx: 0\n",
      "subnet: [236195], loss: [1.00379613], acc: 0.5, samples: 32\n",
      "subnet: [28059], expert_idx: 0\n",
      "subnet: [28059], loss: [1.02915603], acc: 0.46875, samples: 32\n",
      "subnet: [324602], expert_idx: 0\n",
      "subnet: [324602], loss: [1.16574892], acc: 0.59375, samples: 32\n",
      "subnet: [168688], expert_idx: 0\n",
      "subnet: [168688], loss: [1.36200753], acc: 0.3125, samples: 32\n",
      "subnet: [172079], expert_idx: 0\n",
      "subnet: [172079], loss: [0.95801571], acc: 0.625, samples: 32\n",
      "subnet: [204842], expert_idx: 0\n",
      "subnet: [204842], loss: [0.88434517], acc: 0.625, samples: 32\n",
      "subnet: [253829], expert_idx: 0\n",
      "subnet: [253829], loss: [1.23145173], acc: 0.4375, samples: 32\n",
      "subnet: [83306], expert_idx: 0\n",
      "subnet: [83306], loss: [1.03552915], acc: 0.625, samples: 32\n",
      "subnet: [126493], expert_idx: 0\n",
      "subnet: [126493], loss: [1.20795831], acc: 0.53125, samples: 32\n",
      "subnet: [298638], expert_idx: 0\n",
      "subnet: [298638], loss: [1.16227144], acc: 0.625, samples: 32\n",
      "subnet: [10634], expert_idx: 0\n",
      "subnet: [10634], loss: [0.96843012], acc: 0.65625, samples: 32\n",
      "subnet: [157044], expert_idx: 0\n",
      "subnet: [157044], loss: [0.972866], acc: 0.5, samples: 32\n",
      "subnet: [28760], expert_idx: 0\n",
      "subnet: [28760], loss: [0.84941544], acc: 0.65625, samples: 32\n",
      "subnet: [248562], expert_idx: 0\n",
      "subnet: [248562], loss: [1.05145042], acc: 0.59375, samples: 32\n",
      "subnet: [268893], expert_idx: 0\n",
      "subnet: [268893], loss: [1.02035055], acc: 0.375, samples: 32\n",
      "subnet: [153385], expert_idx: 0\n",
      "subnet: [153385], loss: [1.11663975], acc: 0.5625, samples: 32\n",
      "subnet: [258822], expert_idx: 0\n",
      "subnet: [258822], loss: [1.09757014], acc: 0.46875, samples: 32\n",
      "subnet: [60829], expert_idx: 0\n",
      "subnet: [60829], loss: [0.92259062], acc: 0.65625, samples: 32\n",
      "subnet: [176669], expert_idx: 0\n",
      "subnet: [176669], loss: [1.08408289], acc: 0.59375, samples: 32\n",
      "subnet: [175847], expert_idx: 0\n",
      "subnet: [175847], loss: [0.98597441], acc: 0.65625, samples: 32\n",
      "subnet: [66194], expert_idx: 0\n",
      "subnet: [66194], loss: [0.9846772], acc: 0.5, samples: 32\n",
      "subnet: [148210], expert_idx: 0\n",
      "subnet: [148210], loss: [1.08948745], acc: 0.4375, samples: 32\n",
      "subnet: [286265], expert_idx: 0\n",
      "subnet: [286265], loss: [1.08451337], acc: 0.4375, samples: 32\n",
      "subnet: [117051], expert_idx: 0\n",
      "subnet: [117051], loss: [1.04373545], acc: 0.59375, samples: 32\n",
      "subnet: [17713], expert_idx: 0\n",
      "subnet: [17713], loss: [1.13880085], acc: 0.5625, samples: 32\n",
      "subnet: [234819], expert_idx: 0\n",
      "subnet: [234819], loss: [1.1455441], acc: 0.59375, samples: 32\n",
      "subnet: [147430], expert_idx: 0\n",
      "subnet: [147430], loss: [1.05893769], acc: 0.5, samples: 32\n",
      "subnet: [165234], expert_idx: 0\n",
      "subnet: [165234], loss: [1.27417047], acc: 0.34375, samples: 32\n",
      "subnet: [229877], expert_idx: 0\n",
      "subnet: [229877], loss: [1.07146923], acc: 0.40625, samples: 32\n",
      "subnet: [105202], expert_idx: 0\n",
      "subnet: [105202], loss: [1.09948227], acc: 0.53125, samples: 32\n",
      "subnet: [266439], expert_idx: 0\n",
      "subnet: [266439], loss: [1.03262305], acc: 0.46875, samples: 32\n",
      "subnet: [172702], expert_idx: 0\n",
      "subnet: [172702], loss: [1.02232022], acc: 0.46875, samples: 32\n",
      "subnet: [312744], expert_idx: 0\n",
      "subnet: [312744], loss: [1.15376758], acc: 0.53125, samples: 32\n",
      "subnet: [148860], expert_idx: 0\n",
      "subnet: [148860], loss: [1.2560522], acc: 0.5, samples: 32\n",
      "subnet: [123979], expert_idx: 0\n",
      "subnet: [123979], loss: [1.0484682], acc: 0.4375, samples: 32\n",
      "subnet: [49748], expert_idx: 0\n",
      "subnet: [49748], loss: [1.11406294], acc: 0.53125, samples: 32\n",
      "subnet: [161677], expert_idx: 0\n",
      "subnet: [161677], loss: [1.0403335], acc: 0.5625, samples: 32\n",
      "subnet: [168034], expert_idx: 0\n",
      "subnet: [168034], loss: [1.13221927], acc: 0.625, samples: 32\n",
      "subnet: [318566], expert_idx: 0\n",
      "subnet: [318566], loss: [1.14330104], acc: 0.46875, samples: 32\n",
      "subnet: [75658], expert_idx: 0\n",
      "subnet: [75658], loss: [1.03405552], acc: 0.4375, samples: 32\n",
      "subnet: [227500], expert_idx: 0\n",
      "subnet: [227500], loss: [1.03749151], acc: 0.5625, samples: 32\n",
      "subnet: [143191], expert_idx: 0\n",
      "subnet: [143191], loss: [1.26830679], acc: 0.46875, samples: 32\n",
      "subnet: [3379], expert_idx: 0\n",
      "subnet: [3379], loss: [0.97266489], acc: 0.5625, samples: 32\n",
      "subnet: [75184], expert_idx: 0\n",
      "subnet: [75184], loss: [0.99913951], acc: 0.53125, samples: 32\n",
      "subnet: [38873], expert_idx: 0\n",
      "subnet: [38873], loss: [0.96326618], acc: 0.5625, samples: 32\n",
      "subnet: [170131], expert_idx: 0\n",
      "subnet: [170131], loss: [1.05048022], acc: 0.53125, samples: 32\n",
      "subnet: [328257], expert_idx: 0\n",
      "subnet: [328257], loss: [0.93139848], acc: 0.6875, samples: 32\n",
      "subnet: [209770], expert_idx: 0\n"
     ]
    }
   ],
   "source": [
    "!python supernet/train_search_mnist.py --warmup_epochs 0 --steps 25000 --n_search 1 --n_qubits 4 --n_experts 1 --n_layers 1 --n_encode_layers 4 --save \"4_params\" --data \"./experiment_data/mnist_2/\" --save_dir \"./supernet/mnist_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python supernet/train_search_mnist.py --warmup_epochs 0 --steps 25000 --n_search 1 --n_qubits 4 --n_experts 1 --n_layers 2 --n_encode_layers 4 --save \"8_params\" --data \"./experiment_data/mnist_2/\" --save_dir \"./supernet/mnist_2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ranndom circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Sliding Loss Window : 1.9999999999999998\n",
      "Step 51 | Sliding Loss Window : 0.84\n",
      "Step 101 | Sliding Loss Window : 0.96\n",
      "Step 151 | Sliding Loss Window : 1.0\n",
      "Step 201 | Sliding Loss Window : 1.08\n",
      "Step 251 | Sliding Loss Window : 1.24\n",
      "Step 301 | Sliding Loss Window : 0.72\n",
      "Step 351 | Sliding Loss Window : 0.88\n",
      "Step 401 | Sliding Loss Window : 0.96\n",
      "Step 451 | Sliding Loss Window : 1.0\n",
      "Step 501 | Sliding Loss Window : 0.76\n",
      "Step 551 | Sliding Loss Window : 1.12\n",
      "Step 601 | Sliding Loss Window : 0.68\n",
      "Step 651 | Sliding Loss Window : 1.2\n",
      "Step 701 | Sliding Loss Window : 0.92\n",
      "Step 751 | Sliding Loss Window : 1.2\n",
      "Step 801 | Sliding Loss Window : 1.04\n",
      "Step 851 | Sliding Loss Window : 1.04\n",
      "Step 901 | Sliding Loss Window : 1.16\n",
      "Step 951 | Sliding Loss Window : 1.2\n",
      "Step 1001 | Sliding Loss Window : 0.96\n",
      "Step 1051 | Sliding Loss Window : 0.92\n",
      "Step 1101 | Sliding Loss Window : 1.12\n",
      "Step 1151 | Sliding Loss Window : 0.8\n",
      "Step 1201 | Sliding Loss Window : 1.0\n",
      "Step 1251 | Sliding Loss Window : 1.04\n",
      "Step 1301 | Sliding Loss Window : 1.04\n",
      "Step 1351 | Sliding Loss Window : 1.2\n",
      "Step 1401 | Sliding Loss Window : 0.72\n",
      "Step 1451 | Sliding Loss Window : 0.88\n",
      "Step 1501 | Sliding Loss Window : 1.0\n",
      "Step 1551 | Sliding Loss Window : 0.96\n",
      "Step 1601 | Sliding Loss Window : 0.8\n",
      "Step 1651 | Sliding Loss Window : 1.08\n",
      "Step 1701 | Sliding Loss Window : 0.72\n",
      "Step 1751 | Sliding Loss Window : 1.2\n",
      "Step 1801 | Sliding Loss Window : 0.92\n",
      "Step 1851 | Sliding Loss Window : 1.16\n",
      "Step 1901 | Sliding Loss Window : 1.12\n",
      "Step 1951 | Sliding Loss Window : 0.96\n",
      "Step 2001 | Sliding Loss Window : 1.24\n",
      "Step 2051 | Sliding Loss Window : 1.12\n",
      "Step 2101 | Sliding Loss Window : 1.0\n",
      "Step 2151 | Sliding Loss Window : 0.92\n",
      "Step 2201 | Sliding Loss Window : 1.08\n",
      "Step 2251 | Sliding Loss Window : 0.8\n",
      "Step 2301 | Sliding Loss Window : 1.08\n",
      "Step 2351 | Sliding Loss Window : 0.96\n",
      "Step 2401 | Sliding Loss Window : 1.04\n",
      "Step 2451 | Sliding Loss Window : 1.2\n",
      "Step 2501 | Sliding Loss Window : 0.72\n",
      "Step 2551 | Sliding Loss Window : 0.88\n",
      "Step 2601 | Sliding Loss Window : 1.0\n",
      "Step 2651 | Sliding Loss Window : 1.0\n",
      "Step 2701 | Sliding Loss Window : 0.8\n",
      "Step 2751 | Sliding Loss Window : 1.04\n",
      "Step 2801 | Sliding Loss Window : 0.8\n",
      "Step 2851 | Sliding Loss Window : 1.12\n",
      "Step 2901 | Sliding Loss Window : 1.0\n",
      "Step 2951 | Sliding Loss Window : 1.12\n",
      "Step 3001 | Sliding Loss Window : 1.08\n",
      "Step 3051 | Sliding Loss Window : 1.0\n",
      "Step 3101 | Sliding Loss Window : 1.28\n",
      "Step 3151 | Sliding Loss Window : 1.08\n",
      "Step 3201 | Sliding Loss Window : 1.0\n",
      "Step 3251 | Sliding Loss Window : 0.96\n",
      "Step 3301 | Sliding Loss Window : 1.0\n",
      "Step 3351 | Sliding Loss Window : 0.84\n",
      "Step 3401 | Sliding Loss Window : 1.04\n",
      "Step 3451 | Sliding Loss Window : 1.0\n",
      "Step 3501 | Sliding Loss Window : 1.04\n",
      "Step 3551 | Sliding Loss Window : 1.16\n",
      "Step 3601 | Sliding Loss Window : 0.76\n",
      "Step 3651 | Sliding Loss Window : 0.92\n",
      "Step 3701 | Sliding Loss Window : 0.92\n",
      "Step 3751 | Sliding Loss Window : 1.04\n",
      "Step 3801 | Sliding Loss Window : 0.8\n",
      "Step 3851 | Sliding Loss Window : 1.0\n",
      "Step 3901 | Sliding Loss Window : 0.88\n",
      "Step 3951 | Sliding Loss Window : 1.12\n",
      "Step 4001 | Sliding Loss Window : 0.96\n",
      "Step 4051 | Sliding Loss Window : 1.12\n",
      "Step 4101 | Sliding Loss Window : 1.12\n",
      "Step 4151 | Sliding Loss Window : 0.96\n",
      "Step 4201 | Sliding Loss Window : 1.28\n",
      "Step 4251 | Sliding Loss Window : 1.12\n",
      "Step 4301 | Sliding Loss Window : 0.96\n",
      "Step 4351 | Sliding Loss Window : 1.0\n",
      "Step 4401 | Sliding Loss Window : 0.96\n",
      "Step 4451 | Sliding Loss Window : 0.8\n",
      "Step 4501 | Sliding Loss Window : 1.12\n",
      "Step 4551 | Sliding Loss Window : 1.0\n",
      "Step 4601 | Sliding Loss Window : 1.04\n",
      "Step 4651 | Sliding Loss Window : 1.12\n",
      "Step 4701 | Sliding Loss Window : 0.72\n",
      "Step 4751 | Sliding Loss Window : 0.96\n",
      "Step 4801 | Sliding Loss Window : 0.88\n",
      "Step 4851 | Sliding Loss Window : 1.04\n",
      "Step 4901 | Sliding Loss Window : 0.84\n",
      "Step 4951 | Sliding Loss Window : 1.0\n",
      "Step 5001 | Sliding Loss Window : 0.84\n",
      "Step 5051 | Sliding Loss Window : 1.16\n",
      "Step 5101 | Sliding Loss Window : 0.96\n",
      "Step 5151 | Sliding Loss Window : 1.12\n",
      "Step 5201 | Sliding Loss Window : 1.08\n",
      "Step 5251 | Sliding Loss Window : 1.0\n",
      "Step 5301 | Sliding Loss Window : 1.28\n",
      "Step 5351 | Sliding Loss Window : 1.08\n",
      "Step 5401 | Sliding Loss Window : 0.96\n",
      "Step 5451 | Sliding Loss Window : 1.0\n",
      "Step 1 | Sliding Loss Window : 2.220446049250313e-16\n",
      "Step 51 | Sliding Loss Window : 0.92\n",
      "Step 101 | Sliding Loss Window : 0.96\n",
      "Step 151 | Sliding Loss Window : 0.72\n",
      "Step 201 | Sliding Loss Window : 1.16\n",
      "Step 251 | Sliding Loss Window : 1.2\n",
      "Step 301 | Sliding Loss Window : 0.96\n",
      "Step 351 | Sliding Loss Window : 1.12\n",
      "Step 401 | Sliding Loss Window : 1.04\n",
      "Step 451 | Sliding Loss Window : 0.88\n",
      "Step 501 | Sliding Loss Window : 0.8\n",
      "Step 551 | Sliding Loss Window : 1.16\n",
      "Step 601 | Sliding Loss Window : 1.04\n",
      "Step 651 | Sliding Loss Window : 1.04\n",
      "Step 701 | Sliding Loss Window : 0.8\n",
      "Step 751 | Sliding Loss Window : 0.96\n",
      "Step 801 | Sliding Loss Window : 0.96\n",
      "Step 851 | Sliding Loss Window : 1.0\n",
      "Step 901 | Sliding Loss Window : 1.0\n",
      "Step 951 | Sliding Loss Window : 1.24\n",
      "Step 1001 | Sliding Loss Window : 1.16\n",
      "Step 1051 | Sliding Loss Window : 0.76\n",
      "Step 1101 | Sliding Loss Window : 1.08\n",
      "Step 1151 | Sliding Loss Window : 1.0\n",
      "Step 1201 | Sliding Loss Window : 0.88\n",
      "Step 1251 | Sliding Loss Window : 0.76\n",
      "Step 1301 | Sliding Loss Window : 1.2\n",
      "Step 1351 | Sliding Loss Window : 1.16\n",
      "Step 1401 | Sliding Loss Window : 1.0\n",
      "Step 1451 | Sliding Loss Window : 1.12\n",
      "Step 1501 | Sliding Loss Window : 1.04\n",
      "Step 1551 | Sliding Loss Window : 0.84\n",
      "Step 1601 | Sliding Loss Window : 0.8\n",
      "Step 1651 | Sliding Loss Window : 1.2\n",
      "Step 1701 | Sliding Loss Window : 1.0\n",
      "Step 1751 | Sliding Loss Window : 1.04\n",
      "Step 1801 | Sliding Loss Window : 0.8\n",
      "Step 1851 | Sliding Loss Window : 0.92\n",
      "Step 1901 | Sliding Loss Window : 1.04\n",
      "Step 1951 | Sliding Loss Window : 0.92\n",
      "Step 2001 | Sliding Loss Window : 1.08\n",
      "Step 2051 | Sliding Loss Window : 1.2\n",
      "Step 2101 | Sliding Loss Window : 1.2\n",
      "Step 2151 | Sliding Loss Window : 0.72\n",
      "Step 2201 | Sliding Loss Window : 1.08\n",
      "Step 2251 | Sliding Loss Window : 1.0\n",
      "Step 2301 | Sliding Loss Window : 0.84\n",
      "Step 2351 | Sliding Loss Window : 0.8\n",
      "Step 2401 | Sliding Loss Window : 1.24\n",
      "Step 2451 | Sliding Loss Window : 1.08\n",
      "Step 2501 | Sliding Loss Window : 1.04\n",
      "Step 2551 | Sliding Loss Window : 1.16\n",
      "Step 2601 | Sliding Loss Window : 1.04\n",
      "Step 2651 | Sliding Loss Window : 0.76\n",
      "Step 2701 | Sliding Loss Window : 0.84\n",
      "Step 2751 | Sliding Loss Window : 1.16\n",
      "Step 2801 | Sliding Loss Window : 1.04\n",
      "Step 2851 | Sliding Loss Window : 1.04\n",
      "Step 2901 | Sliding Loss Window : 0.76\n",
      "Step 2951 | Sliding Loss Window : 1.0\n",
      "Step 3001 | Sliding Loss Window : 1.04\n",
      "Step 3051 | Sliding Loss Window : 0.92\n",
      "Step 3101 | Sliding Loss Window : 1.0\n",
      "Step 3151 | Sliding Loss Window : 1.28\n",
      "Step 3201 | Sliding Loss Window : 1.16\n",
      "Step 3251 | Sliding Loss Window : 0.68\n",
      "Step 3301 | Sliding Loss Window : 1.12\n",
      "Step 3351 | Sliding Loss Window : 1.0\n",
      "Step 3401 | Sliding Loss Window : 0.84\n",
      "Step 3451 | Sliding Loss Window : 0.8\n",
      "Step 3501 | Sliding Loss Window : 1.24\n",
      "Step 3551 | Sliding Loss Window : 1.12\n",
      "Step 3601 | Sliding Loss Window : 0.96\n",
      "Step 3651 | Sliding Loss Window : 1.2\n",
      "Step 3701 | Sliding Loss Window : 1.04\n",
      "Step 3751 | Sliding Loss Window : 0.76\n",
      "Step 3801 | Sliding Loss Window : 0.84\n",
      "Step 3851 | Sliding Loss Window : 1.16\n",
      "Step 3901 | Sliding Loss Window : 1.0\n",
      "Step 3951 | Sliding Loss Window : 1.12\n",
      "Step 4001 | Sliding Loss Window : 0.72\n",
      "Step 4051 | Sliding Loss Window : 1.0\n",
      "Step 4101 | Sliding Loss Window : 1.08\n",
      "Step 4151 | Sliding Loss Window : 0.84\n",
      "Step 4201 | Sliding Loss Window : 1.04\n",
      "Step 4251 | Sliding Loss Window : 1.32\n",
      "Step 4301 | Sliding Loss Window : 1.08\n",
      "Step 4351 | Sliding Loss Window : 0.72\n",
      "Step 4401 | Sliding Loss Window : 1.08\n",
      "Step 4451 | Sliding Loss Window : 1.0\n",
      "Step 4501 | Sliding Loss Window : 0.88\n",
      "Step 4551 | Sliding Loss Window : 0.8\n",
      "Step 4601 | Sliding Loss Window : 1.24\n",
      "Step 4651 | Sliding Loss Window : 1.08\n",
      "Step 4701 | Sliding Loss Window : 1.0\n",
      "Step 4751 | Sliding Loss Window : 1.16\n",
      "Step 4801 | Sliding Loss Window : 1.04\n",
      "Step 4851 | Sliding Loss Window : 0.8\n",
      "Step 4901 | Sliding Loss Window : 0.84\n",
      "Step 4951 | Sliding Loss Window : 1.16\n",
      "Step 5001 | Sliding Loss Window : 1.04\n",
      "Step 5051 | Sliding Loss Window : 1.12\n",
      "Step 5101 | Sliding Loss Window : 0.64\n",
      "Step 5151 | Sliding Loss Window : 1.04\n",
      "Step 5201 | Sliding Loss Window : 1.04\n",
      "Step 5251 | Sliding Loss Window : 0.84\n",
      "Step 5301 | Sliding Loss Window : 1.12\n",
      "Step 5351 | Sliding Loss Window : 1.28\n",
      "Step 5401 | Sliding Loss Window : 1.08\n",
      "Step 5451 | Sliding Loss Window : 0.76\n",
      "Step 1 | Sliding Loss Window : 0.0\n",
      "Step 51 | Sliding Loss Window : 1.08\n",
      "Step 101 | Sliding Loss Window : 1.12\n",
      "Step 151 | Sliding Loss Window : 1.04\n",
      "Step 201 | Sliding Loss Window : 1.0\n",
      "Step 251 | Sliding Loss Window : 1.08\n",
      "Step 301 | Sliding Loss Window : 0.96\n",
      "Step 351 | Sliding Loss Window : 1.08\n",
      "Step 401 | Sliding Loss Window : 0.92\n",
      "Step 451 | Sliding Loss Window : 1.12\n",
      "Step 501 | Sliding Loss Window : 1.04\n",
      "Step 551 | Sliding Loss Window : 0.96\n",
      "Step 601 | Sliding Loss Window : 0.96\n",
      "Step 651 | Sliding Loss Window : 1.08\n",
      "Step 701 | Sliding Loss Window : 0.88\n",
      "Step 751 | Sliding Loss Window : 1.16\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 0.72\n",
      "Step 901 | Sliding Loss Window : 0.72\n",
      "Step 951 | Sliding Loss Window : 1.24\n",
      "Step 1001 | Sliding Loss Window : 0.88\n",
      "Step 1051 | Sliding Loss Window : 1.12\n",
      "Step 1101 | Sliding Loss Window : 1.04\n",
      "Step 1151 | Sliding Loss Window : 1.04\n",
      "Step 1201 | Sliding Loss Window : 1.12\n",
      "Step 1251 | Sliding Loss Window : 1.04\n",
      "Step 1301 | Sliding Loss Window : 1.04\n",
      "Step 1351 | Sliding Loss Window : 1.04\n",
      "Step 1401 | Sliding Loss Window : 0.96\n",
      "Step 1451 | Sliding Loss Window : 1.04\n",
      "Step 1501 | Sliding Loss Window : 0.96\n",
      "Step 1551 | Sliding Loss Window : 1.12\n",
      "Step 1601 | Sliding Loss Window : 1.04\n",
      "Step 1651 | Sliding Loss Window : 1.0\n",
      "Step 1701 | Sliding Loss Window : 0.92\n",
      "Step 1751 | Sliding Loss Window : 1.12\n",
      "Step 1801 | Sliding Loss Window : 0.8\n",
      "Step 1851 | Sliding Loss Window : 1.16\n",
      "Step 1901 | Sliding Loss Window : 0.88\n",
      "Step 1951 | Sliding Loss Window : 0.72\n",
      "Step 2001 | Sliding Loss Window : 0.76\n",
      "Step 2051 | Sliding Loss Window : 1.2\n",
      "Step 2101 | Sliding Loss Window : 0.92\n",
      "Step 2151 | Sliding Loss Window : 1.12\n",
      "Step 2201 | Sliding Loss Window : 0.96\n",
      "Step 2251 | Sliding Loss Window : 1.08\n",
      "Step 2301 | Sliding Loss Window : 1.16\n",
      "Step 2351 | Sliding Loss Window : 1.0\n",
      "Step 2401 | Sliding Loss Window : 1.04\n",
      "Step 2451 | Sliding Loss Window : 1.04\n",
      "Step 2501 | Sliding Loss Window : 0.96\n",
      "Step 2551 | Sliding Loss Window : 1.08\n",
      "Step 2601 | Sliding Loss Window : 0.88\n",
      "Step 2651 | Sliding Loss Window : 1.2\n",
      "Step 2701 | Sliding Loss Window : 1.04\n",
      "Step 2751 | Sliding Loss Window : 0.96\n",
      "Step 2801 | Sliding Loss Window : 0.92\n",
      "Step 2851 | Sliding Loss Window : 1.12\n",
      "Step 2901 | Sliding Loss Window : 0.8\n",
      "Step 2951 | Sliding Loss Window : 1.2\n",
      "Step 3001 | Sliding Loss Window : 0.8\n",
      "Step 3051 | Sliding Loss Window : 0.8\n",
      "Step 3101 | Sliding Loss Window : 0.76\n",
      "Step 3151 | Sliding Loss Window : 1.16\n",
      "Step 3201 | Sliding Loss Window : 0.88\n",
      "Step 3251 | Sliding Loss Window : 1.12\n",
      "Step 3301 | Sliding Loss Window : 1.0\n",
      "Step 3351 | Sliding Loss Window : 1.12\n",
      "Step 3401 | Sliding Loss Window : 1.16\n",
      "Step 3451 | Sliding Loss Window : 1.0\n",
      "Step 3501 | Sliding Loss Window : 0.96\n",
      "Step 3551 | Sliding Loss Window : 1.08\n",
      "Step 3601 | Sliding Loss Window : 1.0\n",
      "Step 3651 | Sliding Loss Window : 1.04\n",
      "Step 3701 | Sliding Loss Window : 0.88\n",
      "Step 3751 | Sliding Loss Window : 1.16\n",
      "Step 3801 | Sliding Loss Window : 1.08\n",
      "Step 3851 | Sliding Loss Window : 0.96\n",
      "Step 3901 | Sliding Loss Window : 0.88\n",
      "Step 3951 | Sliding Loss Window : 1.12\n",
      "Step 4001 | Sliding Loss Window : 0.84\n",
      "Step 4051 | Sliding Loss Window : 1.2\n",
      "Step 4101 | Sliding Loss Window : 0.8\n",
      "Step 4151 | Sliding Loss Window : 0.84\n",
      "Step 4201 | Sliding Loss Window : 0.72\n",
      "Step 4251 | Sliding Loss Window : 1.16\n",
      "Step 4301 | Sliding Loss Window : 0.88\n",
      "Step 4351 | Sliding Loss Window : 1.12\n",
      "Step 4401 | Sliding Loss Window : 0.96\n",
      "Step 4451 | Sliding Loss Window : 1.2\n",
      "Step 4501 | Sliding Loss Window : 1.08\n",
      "Step 4551 | Sliding Loss Window : 1.0\n",
      "Step 4601 | Sliding Loss Window : 0.96\n",
      "Step 4651 | Sliding Loss Window : 1.12\n",
      "Step 4701 | Sliding Loss Window : 1.04\n",
      "Step 4751 | Sliding Loss Window : 1.0\n",
      "Step 4801 | Sliding Loss Window : 0.92\n",
      "Step 4851 | Sliding Loss Window : 1.12\n",
      "Step 4901 | Sliding Loss Window : 1.08\n",
      "Step 4951 | Sliding Loss Window : 1.0\n",
      "Step 5001 | Sliding Loss Window : 0.88\n",
      "Step 5051 | Sliding Loss Window : 1.12\n",
      "Step 5101 | Sliding Loss Window : 0.8\n",
      "Step 5151 | Sliding Loss Window : 1.2\n",
      "Step 5201 | Sliding Loss Window : 0.76\n",
      "Step 5251 | Sliding Loss Window : 0.88\n",
      "Step 5301 | Sliding Loss Window : 0.72\n",
      "Step 5351 | Sliding Loss Window : 1.16\n",
      "Step 5401 | Sliding Loss Window : 0.88\n",
      "Step 5451 | Sliding Loss Window : 1.12\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.12\n",
      "Step 101 | Sliding Loss Window : 1.16\n",
      "Step 151 | Sliding Loss Window : 1.0\n",
      "Step 201 | Sliding Loss Window : 1.08\n",
      "Step 251 | Sliding Loss Window : 1.04\n",
      "Step 301 | Sliding Loss Window : 1.24\n",
      "Step 351 | Sliding Loss Window : 0.92\n",
      "Step 401 | Sliding Loss Window : 0.96\n",
      "Step 451 | Sliding Loss Window : 0.92\n",
      "Step 501 | Sliding Loss Window : 0.8\n",
      "Step 551 | Sliding Loss Window : 1.12\n",
      "Step 601 | Sliding Loss Window : 1.04\n",
      "Step 651 | Sliding Loss Window : 1.12\n",
      "Step 701 | Sliding Loss Window : 1.08\n",
      "Step 751 | Sliding Loss Window : 0.8\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 0.8\n",
      "Step 901 | Sliding Loss Window : 0.88\n",
      "Step 951 | Sliding Loss Window : 1.16\n",
      "Step 1001 | Sliding Loss Window : 1.0\n",
      "Step 1051 | Sliding Loss Window : 0.96\n",
      "Step 1101 | Sliding Loss Window : 0.96\n",
      "Step 1151 | Sliding Loss Window : 1.16\n",
      "Step 1201 | Sliding Loss Window : 1.08\n",
      "Step 1251 | Sliding Loss Window : 1.04\n",
      "Step 1301 | Sliding Loss Window : 1.08\n",
      "Step 1351 | Sliding Loss Window : 1.04\n",
      "Step 1401 | Sliding Loss Window : 1.28\n",
      "Step 1451 | Sliding Loss Window : 0.88\n",
      "Step 1501 | Sliding Loss Window : 0.96\n",
      "Step 1551 | Sliding Loss Window : 0.96\n",
      "Step 1601 | Sliding Loss Window : 0.72\n",
      "Step 1651 | Sliding Loss Window : 1.16\n",
      "Step 1701 | Sliding Loss Window : 1.0\n",
      "Step 1751 | Sliding Loss Window : 1.16\n",
      "Step 1801 | Sliding Loss Window : 1.12\n",
      "Step 1851 | Sliding Loss Window : 0.76\n",
      "Step 1901 | Sliding Loss Window : 0.88\n",
      "Step 1951 | Sliding Loss Window : 0.76\n",
      "Step 2001 | Sliding Loss Window : 0.92\n",
      "Step 2051 | Sliding Loss Window : 1.08\n",
      "Step 2101 | Sliding Loss Window : 1.0\n",
      "Step 2151 | Sliding Loss Window : 1.04\n",
      "Step 2201 | Sliding Loss Window : 0.92\n",
      "Step 2251 | Sliding Loss Window : 1.2\n",
      "Step 2301 | Sliding Loss Window : 1.04\n",
      "Step 2351 | Sliding Loss Window : 1.08\n",
      "Step 2401 | Sliding Loss Window : 1.04\n",
      "Step 2451 | Sliding Loss Window : 1.08\n",
      "Step 2501 | Sliding Loss Window : 1.24\n",
      "Step 2551 | Sliding Loss Window : 0.92\n",
      "Step 2601 | Sliding Loss Window : 0.96\n",
      "Step 2651 | Sliding Loss Window : 0.88\n",
      "Step 2701 | Sliding Loss Window : 0.76\n",
      "Step 2751 | Sliding Loss Window : 1.16\n",
      "Step 2801 | Sliding Loss Window : 1.0\n",
      "Step 2851 | Sliding Loss Window : 1.16\n",
      "Step 2901 | Sliding Loss Window : 1.12\n",
      "Step 2951 | Sliding Loss Window : 0.76\n",
      "Step 3001 | Sliding Loss Window : 0.88\n",
      "Step 3051 | Sliding Loss Window : 0.76\n",
      "Step 3101 | Sliding Loss Window : 0.92\n",
      "Step 3151 | Sliding Loss Window : 1.12\n",
      "Step 3201 | Sliding Loss Window : 0.96\n",
      "Step 3251 | Sliding Loss Window : 1.0\n",
      "Step 3301 | Sliding Loss Window : 0.96\n",
      "Step 3351 | Sliding Loss Window : 1.16\n",
      "Step 3401 | Sliding Loss Window : 1.08\n",
      "Step 3451 | Sliding Loss Window : 1.04\n",
      "Step 3501 | Sliding Loss Window : 1.12\n",
      "Step 3551 | Sliding Loss Window : 1.04\n",
      "Step 3601 | Sliding Loss Window : 1.24\n",
      "Step 3651 | Sliding Loss Window : 0.88\n",
      "Step 3701 | Sliding Loss Window : 1.0\n",
      "Step 3751 | Sliding Loss Window : 0.88\n",
      "Step 3801 | Sliding Loss Window : 0.8\n",
      "Step 3851 | Sliding Loss Window : 1.12\n",
      "Step 3901 | Sliding Loss Window : 1.04\n",
      "Step 3951 | Sliding Loss Window : 1.16\n",
      "Step 4001 | Sliding Loss Window : 1.04\n",
      "Step 4051 | Sliding Loss Window : 0.76\n",
      "Step 4101 | Sliding Loss Window : 0.92\n",
      "Step 4151 | Sliding Loss Window : 0.72\n",
      "Step 4201 | Sliding Loss Window : 0.96\n",
      "Step 4251 | Sliding Loss Window : 1.08\n",
      "Step 4301 | Sliding Loss Window : 1.0\n",
      "Step 4351 | Sliding Loss Window : 1.0\n",
      "Step 4401 | Sliding Loss Window : 0.96\n",
      "Step 4451 | Sliding Loss Window : 1.16\n",
      "Step 4501 | Sliding Loss Window : 1.08\n",
      "Step 4551 | Sliding Loss Window : 1.08\n",
      "Step 4601 | Sliding Loss Window : 1.04\n",
      "Step 4651 | Sliding Loss Window : 1.12\n",
      "Step 4701 | Sliding Loss Window : 1.2\n",
      "Step 4751 | Sliding Loss Window : 0.84\n",
      "Step 4801 | Sliding Loss Window : 1.0\n",
      "Step 4851 | Sliding Loss Window : 0.96\n",
      "Step 4901 | Sliding Loss Window : 0.72\n",
      "Step 4951 | Sliding Loss Window : 1.12\n",
      "Step 5001 | Sliding Loss Window : 1.08\n",
      "Step 5051 | Sliding Loss Window : 1.12\n",
      "Step 5101 | Sliding Loss Window : 1.04\n",
      "Step 5151 | Sliding Loss Window : 0.8\n",
      "Step 5201 | Sliding Loss Window : 0.92\n",
      "Step 5251 | Sliding Loss Window : 0.72\n",
      "Step 5301 | Sliding Loss Window : 0.96\n",
      "Step 5351 | Sliding Loss Window : 1.08\n",
      "Step 5401 | Sliding Loss Window : 0.96\n",
      "Step 5451 | Sliding Loss Window : 1.04\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.08\n",
      "Step 101 | Sliding Loss Window : 0.96\n",
      "Step 151 | Sliding Loss Window : 0.84\n",
      "Step 201 | Sliding Loss Window : 1.12\n",
      "Step 251 | Sliding Loss Window : 0.8\n",
      "Step 301 | Sliding Loss Window : 0.84\n",
      "Step 351 | Sliding Loss Window : 0.84\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 1.36\n",
      "Step 501 | Sliding Loss Window : 1.08\n",
      "Step 551 | Sliding Loss Window : 1.08\n",
      "Step 601 | Sliding Loss Window : 0.72\n",
      "Step 651 | Sliding Loss Window : 1.0\n",
      "Step 701 | Sliding Loss Window : 1.08\n",
      "Step 751 | Sliding Loss Window : 1.04\n",
      "Step 801 | Sliding Loss Window : 0.96\n",
      "Step 851 | Sliding Loss Window : 0.96\n",
      "Step 901 | Sliding Loss Window : 1.08\n",
      "Step 951 | Sliding Loss Window : 1.04\n",
      "Step 1001 | Sliding Loss Window : 1.08\n",
      "Step 1051 | Sliding Loss Window : 0.92\n",
      "Step 1101 | Sliding Loss Window : 1.0\n",
      "Step 1151 | Sliding Loss Window : 1.08\n",
      "Step 1201 | Sliding Loss Window : 0.96\n",
      "Step 1251 | Sliding Loss Window : 0.8400000000000002\n",
      "Step 1301 | Sliding Loss Window : 1.16\n",
      "Step 1351 | Sliding Loss Window : 0.76\n",
      "Step 1401 | Sliding Loss Window : 0.88\n",
      "Step 1451 | Sliding Loss Window : 0.84\n",
      "Step 1501 | Sliding Loss Window : 1.12\n",
      "Step 1551 | Sliding Loss Window : 1.32\n",
      "Step 1601 | Sliding Loss Window : 1.12\n",
      "Step 1651 | Sliding Loss Window : 1.0\n",
      "Step 1701 | Sliding Loss Window : 0.72\n",
      "Step 1751 | Sliding Loss Window : 1.04\n",
      "Step 1801 | Sliding Loss Window : 1.08\n",
      "Step 1851 | Sliding Loss Window : 1.08\n",
      "Step 1901 | Sliding Loss Window : 0.92\n",
      "Step 1951 | Sliding Loss Window : 0.92\n",
      "Step 2001 | Sliding Loss Window : 1.08\n",
      "Step 2051 | Sliding Loss Window : 1.08\n",
      "Step 2101 | Sliding Loss Window : 1.04\n",
      "Step 2151 | Sliding Loss Window : 0.96\n",
      "Step 2201 | Sliding Loss Window : 1.0\n",
      "Step 2251 | Sliding Loss Window : 1.08\n",
      "Step 2301 | Sliding Loss Window : 0.96\n",
      "Step 2351 | Sliding Loss Window : 0.84\n",
      "Step 2401 | Sliding Loss Window : 1.16\n",
      "Step 2451 | Sliding Loss Window : 0.76\n",
      "Step 2501 | Sliding Loss Window : 0.84\n",
      "Step 2551 | Sliding Loss Window : 0.88\n",
      "Step 2601 | Sliding Loss Window : 1.16\n",
      "Step 2651 | Sliding Loss Window : 1.28\n",
      "Step 2701 | Sliding Loss Window : 1.12\n",
      "Step 2751 | Sliding Loss Window : 1.04\n",
      "Step 2801 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 2851 | Sliding Loss Window : 1.04\n",
      "Step 2901 | Sliding Loss Window : 1.16\n",
      "Step 2951 | Sliding Loss Window : 1.0\n",
      "Step 3001 | Sliding Loss Window : 0.96\n",
      "Step 3051 | Sliding Loss Window : 0.92\n",
      "Step 3101 | Sliding Loss Window : 1.08\n",
      "Step 3151 | Sliding Loss Window : 1.08\n",
      "Step 3201 | Sliding Loss Window : 1.04\n",
      "Step 3251 | Sliding Loss Window : 0.96\n",
      "Step 3301 | Sliding Loss Window : 0.96\n",
      "Step 3351 | Sliding Loss Window : 1.12\n",
      "Step 3401 | Sliding Loss Window : 1.0\n",
      "Step 3451 | Sliding Loss Window : 0.8400000000000002\n",
      "Step 3501 | Sliding Loss Window : 1.12\n",
      "Step 3551 | Sliding Loss Window : 0.76\n",
      "Step 3601 | Sliding Loss Window : 0.84\n",
      "Step 3651 | Sliding Loss Window : 0.92\n",
      "Step 3701 | Sliding Loss Window : 1.12\n",
      "Step 3751 | Sliding Loss Window : 1.28\n",
      "Step 3801 | Sliding Loss Window : 1.12\n",
      "Step 3851 | Sliding Loss Window : 1.08\n",
      "Step 3901 | Sliding Loss Window : 0.6000000000000001\n",
      "Step 3951 | Sliding Loss Window : 1.04\n",
      "Step 4001 | Sliding Loss Window : 1.2\n",
      "Step 4051 | Sliding Loss Window : 0.96\n",
      "Step 4101 | Sliding Loss Window : 1.0\n",
      "Step 4151 | Sliding Loss Window : 0.88\n",
      "Step 4201 | Sliding Loss Window : 1.08\n",
      "Step 4251 | Sliding Loss Window : 1.04\n",
      "Step 4301 | Sliding Loss Window : 1.12\n",
      "Step 4351 | Sliding Loss Window : 0.88\n",
      "Step 4401 | Sliding Loss Window : 1.0\n",
      "Step 4451 | Sliding Loss Window : 1.16\n",
      "Step 4501 | Sliding Loss Window : 0.92\n",
      "Step 4551 | Sliding Loss Window : 0.88\n",
      "Step 4601 | Sliding Loss Window : 1.08\n",
      "Step 4651 | Sliding Loss Window : 0.8\n",
      "Step 4701 | Sliding Loss Window : 0.84\n",
      "Step 4751 | Sliding Loss Window : 0.96\n",
      "Step 4801 | Sliding Loss Window : 1.08\n",
      "Step 4851 | Sliding Loss Window : 1.28\n",
      "Step 4901 | Sliding Loss Window : 1.16\n",
      "Step 4951 | Sliding Loss Window : 1.04\n",
      "Step 5001 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 5051 | Sliding Loss Window : 1.0\n",
      "Step 5101 | Sliding Loss Window : 1.2\n",
      "Step 5151 | Sliding Loss Window : 0.96\n",
      "Step 5201 | Sliding Loss Window : 1.0\n",
      "Step 5251 | Sliding Loss Window : 0.88\n",
      "Step 5301 | Sliding Loss Window : 1.08\n",
      "Step 5351 | Sliding Loss Window : 1.04\n",
      "Step 5401 | Sliding Loss Window : 1.16\n",
      "Step 5451 | Sliding Loss Window : 0.88\n",
      "Step 1 | Sliding Loss Window : 0.0\n",
      "Step 51 | Sliding Loss Window : 0.6\n",
      "Step 101 | Sliding Loss Window : 1.04\n",
      "Step 151 | Sliding Loss Window : 1.12\n",
      "Step 201 | Sliding Loss Window : 1.04\n",
      "Step 251 | Sliding Loss Window : 0.72\n",
      "Step 301 | Sliding Loss Window : 1.0\n",
      "Step 351 | Sliding Loss Window : 1.2\n",
      "Step 401 | Sliding Loss Window : 1.08\n",
      "Step 451 | Sliding Loss Window : 0.92\n",
      "Step 501 | Sliding Loss Window : 0.88\n",
      "Step 551 | Sliding Loss Window : 1.04\n",
      "Step 601 | Sliding Loss Window : 1.08\n",
      "Step 651 | Sliding Loss Window : 1.08\n",
      "Step 701 | Sliding Loss Window : 0.96\n",
      "Step 751 | Sliding Loss Window : 0.92\n",
      "Step 801 | Sliding Loss Window : 0.92\n",
      "Step 851 | Sliding Loss Window : 1.28\n",
      "Step 901 | Sliding Loss Window : 1.12\n",
      "Step 951 | Sliding Loss Window : 0.76\n",
      "Step 1001 | Sliding Loss Window : 1.12\n",
      "Step 1051 | Sliding Loss Window : 1.12\n",
      "Step 1101 | Sliding Loss Window : 1.0\n",
      "Step 1151 | Sliding Loss Window : 0.56\n",
      "Step 1201 | Sliding Loss Window : 1.12\n",
      "Step 1251 | Sliding Loss Window : 1.08\n",
      "Step 1301 | Sliding Loss Window : 1.04\n",
      "Step 1351 | Sliding Loss Window : 0.72\n",
      "Step 1401 | Sliding Loss Window : 1.0\n",
      "Step 1451 | Sliding Loss Window : 1.16\n",
      "Step 1501 | Sliding Loss Window : 1.12\n",
      "Step 1551 | Sliding Loss Window : 0.92\n",
      "Step 1601 | Sliding Loss Window : 0.88\n",
      "Step 1651 | Sliding Loss Window : 1.04\n",
      "Step 1701 | Sliding Loss Window : 1.08\n",
      "Step 1751 | Sliding Loss Window : 1.12\n",
      "Step 1801 | Sliding Loss Window : 0.96\n",
      "Step 1851 | Sliding Loss Window : 0.88\n",
      "Step 1901 | Sliding Loss Window : 0.96\n",
      "Step 1951 | Sliding Loss Window : 1.24\n",
      "Step 2001 | Sliding Loss Window : 1.16\n",
      "Step 2051 | Sliding Loss Window : 0.72\n",
      "Step 2101 | Sliding Loss Window : 1.16\n",
      "Step 2151 | Sliding Loss Window : 1.08\n",
      "Step 2201 | Sliding Loss Window : 1.0\n",
      "Step 2251 | Sliding Loss Window : 0.52\n",
      "Step 2301 | Sliding Loss Window : 1.16\n",
      "Step 2351 | Sliding Loss Window : 1.08\n",
      "Step 2401 | Sliding Loss Window : 1.04\n",
      "Step 2451 | Sliding Loss Window : 0.72\n",
      "Step 2501 | Sliding Loss Window : 0.96\n",
      "Step 2551 | Sliding Loss Window : 1.2\n",
      "Step 2601 | Sliding Loss Window : 1.08\n",
      "Step 2651 | Sliding Loss Window : 0.96\n",
      "Step 2701 | Sliding Loss Window : 0.92\n",
      "Step 2751 | Sliding Loss Window : 0.96\n",
      "Step 2801 | Sliding Loss Window : 1.16\n",
      "Step 2851 | Sliding Loss Window : 1.04\n",
      "Step 2901 | Sliding Loss Window : 0.96\n",
      "Step 2951 | Sliding Loss Window : 0.92\n",
      "Step 3001 | Sliding Loss Window : 0.96\n",
      "Step 3051 | Sliding Loss Window : 1.24\n",
      "Step 3101 | Sliding Loss Window : 1.16\n",
      "Step 3151 | Sliding Loss Window : 0.72\n",
      "Step 3201 | Sliding Loss Window : 1.12\n",
      "Step 3251 | Sliding Loss Window : 1.16\n",
      "Step 3301 | Sliding Loss Window : 1.0\n",
      "Step 3351 | Sliding Loss Window : 0.48\n",
      "Step 3401 | Sliding Loss Window : 1.2\n",
      "Step 3451 | Sliding Loss Window : 1.0\n",
      "Step 3501 | Sliding Loss Window : 1.08\n",
      "Step 3551 | Sliding Loss Window : 0.72\n",
      "Step 3601 | Sliding Loss Window : 1.0\n",
      "Step 3651 | Sliding Loss Window : 1.2\n",
      "Step 3701 | Sliding Loss Window : 1.08\n",
      "Step 3751 | Sliding Loss Window : 0.96\n",
      "Step 3801 | Sliding Loss Window : 0.88\n",
      "Step 3851 | Sliding Loss Window : 0.96\n",
      "Step 3901 | Sliding Loss Window : 1.16\n",
      "Step 3951 | Sliding Loss Window : 1.04\n",
      "Step 4001 | Sliding Loss Window : 0.92\n",
      "Step 4051 | Sliding Loss Window : 0.92\n",
      "Step 4101 | Sliding Loss Window : 1.04\n",
      "Step 4151 | Sliding Loss Window : 1.24\n",
      "Step 4201 | Sliding Loss Window : 1.12\n",
      "Step 4251 | Sliding Loss Window : 0.72\n",
      "Step 4301 | Sliding Loss Window : 1.08\n",
      "Step 4351 | Sliding Loss Window : 1.24\n",
      "Step 4401 | Sliding Loss Window : 0.96\n",
      "Step 4451 | Sliding Loss Window : 0.44\n",
      "Step 4501 | Sliding Loss Window : 1.28\n",
      "Step 4551 | Sliding Loss Window : 0.96\n",
      "Step 4601 | Sliding Loss Window : 1.08\n",
      "Step 4651 | Sliding Loss Window : 0.76\n",
      "Step 4701 | Sliding Loss Window : 0.96\n",
      "Step 4751 | Sliding Loss Window : 1.24\n",
      "Step 4801 | Sliding Loss Window : 1.04\n",
      "Step 4851 | Sliding Loss Window : 0.96\n",
      "Step 4901 | Sliding Loss Window : 0.88\n",
      "Step 4951 | Sliding Loss Window : 0.96\n",
      "Step 5001 | Sliding Loss Window : 1.16\n",
      "Step 5051 | Sliding Loss Window : 1.04\n",
      "Step 5101 | Sliding Loss Window : 0.92\n",
      "Step 5151 | Sliding Loss Window : 0.92\n",
      "Step 5201 | Sliding Loss Window : 1.04\n",
      "Step 5251 | Sliding Loss Window : 1.24\n",
      "Step 5301 | Sliding Loss Window : 1.12\n",
      "Step 5351 | Sliding Loss Window : 0.76\n",
      "Step 5401 | Sliding Loss Window : 1.0\n",
      "Step 5451 | Sliding Loss Window : 1.24\n",
      "Step 1 | Sliding Loss Window : 2.220446049250313e-16\n",
      "Step 51 | Sliding Loss Window : 0.92\n",
      "Step 101 | Sliding Loss Window : 1.16\n",
      "Step 151 | Sliding Loss Window : 0.72\n",
      "Step 201 | Sliding Loss Window : 1.12\n",
      "Step 251 | Sliding Loss Window : 0.64\n",
      "Step 301 | Sliding Loss Window : 0.8\n",
      "Step 351 | Sliding Loss Window : 0.88\n",
      "Step 401 | Sliding Loss Window : 0.76\n",
      "Step 451 | Sliding Loss Window : 0.76\n",
      "Step 501 | Sliding Loss Window : 1.08\n",
      "Step 551 | Sliding Loss Window : 1.0\n",
      "Step 601 | Sliding Loss Window : 1.2\n",
      "Step 651 | Sliding Loss Window : 1.28\n",
      "Step 701 | Sliding Loss Window : 1.16\n",
      "Step 751 | Sliding Loss Window : 1.12\n",
      "Step 801 | Sliding Loss Window : 1.08\n",
      "Step 851 | Sliding Loss Window : 1.08\n",
      "Step 901 | Sliding Loss Window : 1.08\n",
      "Step 951 | Sliding Loss Window : 1.0\n",
      "Step 1001 | Sliding Loss Window : 1.0\n",
      "Step 1051 | Sliding Loss Window : 1.12\n",
      "Step 1101 | Sliding Loss Window : 1.04\n",
      "Step 1151 | Sliding Loss Window : 0.92\n",
      "Step 1201 | Sliding Loss Window : 1.12\n",
      "Step 1251 | Sliding Loss Window : 0.8\n",
      "Step 1301 | Sliding Loss Window : 1.04\n",
      "Step 1351 | Sliding Loss Window : 0.68\n",
      "Step 1401 | Sliding Loss Window : 0.8\n",
      "Step 1451 | Sliding Loss Window : 0.88\n",
      "Step 1501 | Sliding Loss Window : 0.76\n",
      "Step 1551 | Sliding Loss Window : 0.8\n",
      "Step 1601 | Sliding Loss Window : 1.04\n",
      "Step 1651 | Sliding Loss Window : 0.96\n",
      "Step 1701 | Sliding Loss Window : 1.28\n",
      "Step 1751 | Sliding Loss Window : 1.2\n",
      "Step 1801 | Sliding Loss Window : 1.16\n",
      "Step 1851 | Sliding Loss Window : 1.16\n",
      "Step 1901 | Sliding Loss Window : 1.08\n",
      "Step 1951 | Sliding Loss Window : 1.12\n",
      "Step 2001 | Sliding Loss Window : 1.0\n",
      "Step 2051 | Sliding Loss Window : 1.04\n",
      "Step 2101 | Sliding Loss Window : 0.96\n",
      "Step 2151 | Sliding Loss Window : 1.16\n",
      "Step 2201 | Sliding Loss Window : 1.04\n",
      "Step 2251 | Sliding Loss Window : 0.96\n",
      "Step 2301 | Sliding Loss Window : 1.04\n",
      "Step 2351 | Sliding Loss Window : 0.8\n",
      "Step 2401 | Sliding Loss Window : 1.08\n",
      "Step 2451 | Sliding Loss Window : 0.64\n",
      "Step 2501 | Sliding Loss Window : 0.84\n",
      "Step 2551 | Sliding Loss Window : 0.88\n",
      "Step 2601 | Sliding Loss Window : 0.72\n",
      "Step 2651 | Sliding Loss Window : 0.8\n",
      "Step 2701 | Sliding Loss Window : 1.12\n",
      "Step 2751 | Sliding Loss Window : 0.92\n",
      "Step 2801 | Sliding Loss Window : 1.32\n",
      "Step 2851 | Sliding Loss Window : 1.2\n",
      "Step 2901 | Sliding Loss Window : 1.08\n",
      "Step 2951 | Sliding Loss Window : 1.2\n",
      "Step 3001 | Sliding Loss Window : 1.12\n",
      "Step 3051 | Sliding Loss Window : 1.12\n",
      "Step 3101 | Sliding Loss Window : 0.96\n",
      "Step 3151 | Sliding Loss Window : 1.04\n",
      "Step 3201 | Sliding Loss Window : 0.96\n",
      "Step 3251 | Sliding Loss Window : 1.2\n",
      "Step 3301 | Sliding Loss Window : 1.0\n",
      "Step 3351 | Sliding Loss Window : 0.96\n",
      "Step 3401 | Sliding Loss Window : 1.08\n",
      "Step 3451 | Sliding Loss Window : 0.76\n",
      "Step 3501 | Sliding Loss Window : 1.04\n",
      "Step 3551 | Sliding Loss Window : 0.68\n",
      "Step 3601 | Sliding Loss Window : 0.88\n",
      "Step 3651 | Sliding Loss Window : 0.8\n",
      "Step 3701 | Sliding Loss Window : 0.76\n",
      "Step 3751 | Sliding Loss Window : 0.8\n",
      "Step 3801 | Sliding Loss Window : 1.12\n",
      "Step 3851 | Sliding Loss Window : 0.88\n",
      "Step 3901 | Sliding Loss Window : 1.36\n",
      "Step 3951 | Sliding Loss Window : 1.2\n",
      "Step 4001 | Sliding Loss Window : 1.12\n",
      "Step 4051 | Sliding Loss Window : 1.12\n",
      "Step 4101 | Sliding Loss Window : 1.16\n",
      "Step 4151 | Sliding Loss Window : 1.08\n",
      "Step 4201 | Sliding Loss Window : 0.96\n",
      "Step 4251 | Sliding Loss Window : 1.04\n",
      "Step 4301 | Sliding Loss Window : 1.04\n",
      "Step 4351 | Sliding Loss Window : 1.16\n",
      "Step 4401 | Sliding Loss Window : 0.96\n",
      "Step 4451 | Sliding Loss Window : 1.0\n",
      "Step 4501 | Sliding Loss Window : 1.08\n",
      "Step 4551 | Sliding Loss Window : 0.72\n",
      "Step 4601 | Sliding Loss Window : 1.04\n",
      "Step 4651 | Sliding Loss Window : 0.76\n",
      "Step 4701 | Sliding Loss Window : 0.84\n",
      "Step 4751 | Sliding Loss Window : 0.8\n",
      "Step 4801 | Sliding Loss Window : 0.76\n",
      "Step 4851 | Sliding Loss Window : 0.76\n",
      "Step 4901 | Sliding Loss Window : 1.16\n",
      "Step 4951 | Sliding Loss Window : 0.92\n",
      "Step 5001 | Sliding Loss Window : 1.28\n",
      "Step 5051 | Sliding Loss Window : 1.24\n",
      "Step 5101 | Sliding Loss Window : 1.16\n",
      "Step 5151 | Sliding Loss Window : 1.08\n",
      "Step 5201 | Sliding Loss Window : 1.12\n",
      "Step 5251 | Sliding Loss Window : 1.08\n",
      "Step 5301 | Sliding Loss Window : 1.04\n",
      "Step 5351 | Sliding Loss Window : 0.96\n",
      "Step 5401 | Sliding Loss Window : 1.08\n",
      "Step 5451 | Sliding Loss Window : 1.12\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 0.92\n",
      "Step 101 | Sliding Loss Window : 0.92\n",
      "Step 151 | Sliding Loss Window : 1.12\n",
      "Step 201 | Sliding Loss Window : 1.04\n",
      "Step 251 | Sliding Loss Window : 1.08\n",
      "Step 301 | Sliding Loss Window : 1.12\n",
      "Step 351 | Sliding Loss Window : 0.92\n",
      "Step 401 | Sliding Loss Window : 1.0\n",
      "Step 451 | Sliding Loss Window : 1.24\n",
      "Step 501 | Sliding Loss Window : 1.24\n",
      "Step 551 | Sliding Loss Window : 1.24\n",
      "Step 601 | Sliding Loss Window : 0.96\n",
      "Step 651 | Sliding Loss Window : 0.92\n",
      "Step 701 | Sliding Loss Window : 1.08\n",
      "Step 751 | Sliding Loss Window : 0.8\n",
      "Step 801 | Sliding Loss Window : 0.92\n",
      "Step 851 | Sliding Loss Window : 1.16\n",
      "Step 901 | Sliding Loss Window : 0.8\n",
      "Step 951 | Sliding Loss Window : 0.8\n",
      "Step 1001 | Sliding Loss Window : 1.08\n",
      "Step 1051 | Sliding Loss Window : 0.84\n",
      "Step 1101 | Sliding Loss Window : 0.8\n",
      "Step 1151 | Sliding Loss Window : 0.88\n",
      "Step 1201 | Sliding Loss Window : 0.96\n",
      "Step 1251 | Sliding Loss Window : 1.16\n",
      "Step 1301 | Sliding Loss Window : 1.0\n",
      "Step 1351 | Sliding Loss Window : 1.12\n",
      "Step 1401 | Sliding Loss Window : 1.04\n",
      "Step 1451 | Sliding Loss Window : 0.92\n",
      "Step 1501 | Sliding Loss Window : 1.08\n",
      "Step 1551 | Sliding Loss Window : 1.2\n",
      "Step 1601 | Sliding Loss Window : 1.28\n",
      "Step 1651 | Sliding Loss Window : 1.2\n",
      "Step 1701 | Sliding Loss Window : 0.92\n",
      "Step 1751 | Sliding Loss Window : 0.96\n",
      "Step 1801 | Sliding Loss Window : 1.08\n",
      "Step 1851 | Sliding Loss Window : 0.8\n",
      "Step 1901 | Sliding Loss Window : 0.96\n",
      "Step 1951 | Sliding Loss Window : 1.08\n",
      "Step 2001 | Sliding Loss Window : 0.84\n",
      "Step 2051 | Sliding Loss Window : 0.76\n",
      "Step 2101 | Sliding Loss Window : 1.08\n",
      "Step 2151 | Sliding Loss Window : 0.88\n",
      "Step 2201 | Sliding Loss Window : 0.76\n",
      "Step 2251 | Sliding Loss Window : 0.96\n",
      "Step 2301 | Sliding Loss Window : 0.96\n",
      "Step 2351 | Sliding Loss Window : 1.16\n",
      "Step 2401 | Sliding Loss Window : 0.96\n",
      "Step 2451 | Sliding Loss Window : 1.12\n",
      "Step 2501 | Sliding Loss Window : 1.0\n",
      "Step 2551 | Sliding Loss Window : 0.92\n",
      "Step 2601 | Sliding Loss Window : 1.12\n",
      "Step 2651 | Sliding Loss Window : 1.24\n",
      "Step 2701 | Sliding Loss Window : 1.24\n",
      "Step 2751 | Sliding Loss Window : 1.2\n",
      "Step 2801 | Sliding Loss Window : 0.96\n",
      "Step 2851 | Sliding Loss Window : 0.92\n",
      "Step 2901 | Sliding Loss Window : 1.12\n",
      "Step 2951 | Sliding Loss Window : 0.76\n",
      "Step 3001 | Sliding Loss Window : 0.92\n",
      "Step 3051 | Sliding Loss Window : 1.12\n",
      "Step 3101 | Sliding Loss Window : 0.84\n",
      "Step 3151 | Sliding Loss Window : 0.72\n",
      "Step 3201 | Sliding Loss Window : 1.12\n",
      "Step 3251 | Sliding Loss Window : 0.84\n",
      "Step 3301 | Sliding Loss Window : 0.76\n",
      "Step 3351 | Sliding Loss Window : 1.04\n",
      "Step 3401 | Sliding Loss Window : 0.96\n",
      "Step 3451 | Sliding Loss Window : 1.12\n",
      "Step 3501 | Sliding Loss Window : 0.92\n",
      "Step 3551 | Sliding Loss Window : 1.16\n",
      "Step 3601 | Sliding Loss Window : 1.04\n",
      "Step 3651 | Sliding Loss Window : 0.84\n",
      "Step 3701 | Sliding Loss Window : 1.16\n",
      "Step 3751 | Sliding Loss Window : 1.28\n",
      "Step 3801 | Sliding Loss Window : 1.2\n",
      "Step 3851 | Sliding Loss Window : 1.24\n",
      "Step 3901 | Sliding Loss Window : 0.88\n",
      "Step 3951 | Sliding Loss Window : 1.0\n",
      "Step 4001 | Sliding Loss Window : 1.04\n",
      "Step 4051 | Sliding Loss Window : 0.8\n",
      "Step 4101 | Sliding Loss Window : 0.96\n",
      "Step 4151 | Sliding Loss Window : 1.12\n",
      "Step 4201 | Sliding Loss Window : 0.76\n",
      "Step 4251 | Sliding Loss Window : 0.72\n",
      "Step 4301 | Sliding Loss Window : 1.16\n",
      "Step 4351 | Sliding Loss Window : 0.84\n",
      "Step 4401 | Sliding Loss Window : 0.76\n",
      "Step 4451 | Sliding Loss Window : 1.04\n",
      "Step 4501 | Sliding Loss Window : 1.0\n",
      "Step 4551 | Sliding Loss Window : 1.08\n",
      "Step 4601 | Sliding Loss Window : 0.92\n",
      "Step 4651 | Sliding Loss Window : 1.16\n",
      "Step 4701 | Sliding Loss Window : 1.04\n",
      "Step 4751 | Sliding Loss Window : 0.88\n",
      "Step 4801 | Sliding Loss Window : 1.16\n",
      "Step 4851 | Sliding Loss Window : 1.24\n",
      "Step 4901 | Sliding Loss Window : 1.2\n",
      "Step 4951 | Sliding Loss Window : 1.24\n",
      "Step 5001 | Sliding Loss Window : 0.84\n",
      "Step 5051 | Sliding Loss Window : 1.04\n",
      "Step 5101 | Sliding Loss Window : 1.08\n",
      "Step 5151 | Sliding Loss Window : 0.8\n",
      "Step 5201 | Sliding Loss Window : 0.88\n",
      "Step 5251 | Sliding Loss Window : 1.16\n",
      "Step 5301 | Sliding Loss Window : 0.8\n",
      "Step 5351 | Sliding Loss Window : 0.72\n",
      "Step 5401 | Sliding Loss Window : 1.12\n",
      "Step 5451 | Sliding Loss Window : 0.8\n",
      "Step 1 | Sliding Loss Window : 4.440892098500626e-16\n",
      "Step 51 | Sliding Loss Window : 1.2\n",
      "Step 101 | Sliding Loss Window : 1.2\n",
      "Step 151 | Sliding Loss Window : 0.96\n",
      "Step 201 | Sliding Loss Window : 0.92\n",
      "Step 251 | Sliding Loss Window : 1.0\n",
      "Step 301 | Sliding Loss Window : 0.88\n",
      "Step 351 | Sliding Loss Window : 0.92\n",
      "Step 401 | Sliding Loss Window : 1.08\n",
      "Step 451 | Sliding Loss Window : 0.92\n",
      "Step 501 | Sliding Loss Window : 1.12\n",
      "Step 551 | Sliding Loss Window : 0.88\n",
      "Step 601 | Sliding Loss Window : 1.2\n",
      "Step 651 | Sliding Loss Window : 1.0\n",
      "Step 701 | Sliding Loss Window : 1.12\n",
      "Step 751 | Sliding Loss Window : 1.04\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 1.08\n",
      "Step 901 | Sliding Loss Window : 1.08\n",
      "Step 951 | Sliding Loss Window : 0.92\n",
      "Step 1001 | Sliding Loss Window : 0.84\n",
      "Step 1051 | Sliding Loss Window : 1.16\n",
      "Step 1101 | Sliding Loss Window : 0.6\n",
      "Step 1151 | Sliding Loss Window : 1.24\n",
      "Step 1201 | Sliding Loss Window : 1.2\n",
      "Step 1251 | Sliding Loss Window : 0.96\n",
      "Step 1301 | Sliding Loss Window : 0.92\n",
      "Step 1351 | Sliding Loss Window : 1.0\n",
      "Step 1401 | Sliding Loss Window : 0.88\n",
      "Step 1451 | Sliding Loss Window : 0.96\n",
      "Step 1501 | Sliding Loss Window : 1.08\n",
      "Step 1551 | Sliding Loss Window : 0.92\n",
      "Step 1601 | Sliding Loss Window : 1.08\n",
      "Step 1651 | Sliding Loss Window : 0.88\n",
      "Step 1701 | Sliding Loss Window : 1.2\n",
      "Step 1751 | Sliding Loss Window : 1.0\n",
      "Step 1801 | Sliding Loss Window : 1.16\n",
      "Step 1851 | Sliding Loss Window : 0.96\n",
      "Step 1901 | Sliding Loss Window : 0.84\n",
      "Step 1951 | Sliding Loss Window : 1.16\n",
      "Step 2001 | Sliding Loss Window : 1.04\n",
      "Step 2051 | Sliding Loss Window : 0.96\n",
      "Step 2101 | Sliding Loss Window : 0.8\n",
      "Step 2151 | Sliding Loss Window : 1.2\n",
      "Step 2201 | Sliding Loss Window : 0.6\n",
      "Step 2251 | Sliding Loss Window : 1.24\n",
      "Step 2301 | Sliding Loss Window : 1.12\n",
      "Step 2351 | Sliding Loss Window : 1.0\n",
      "Step 2401 | Sliding Loss Window : 0.92\n",
      "Step 2451 | Sliding Loss Window : 1.0\n",
      "Step 2501 | Sliding Loss Window : 0.92\n",
      "Step 2551 | Sliding Loss Window : 0.92\n",
      "Step 2601 | Sliding Loss Window : 1.08\n",
      "Step 2651 | Sliding Loss Window : 0.96\n",
      "Step 2701 | Sliding Loss Window : 1.04\n",
      "Step 2751 | Sliding Loss Window : 0.92\n",
      "Step 2801 | Sliding Loss Window : 1.16\n",
      "Step 2851 | Sliding Loss Window : 1.0\n",
      "Step 2901 | Sliding Loss Window : 1.2\n",
      "Step 2951 | Sliding Loss Window : 0.88\n",
      "Step 3001 | Sliding Loss Window : 0.84\n",
      "Step 3051 | Sliding Loss Window : 1.24\n",
      "Step 3101 | Sliding Loss Window : 0.96\n",
      "Step 3151 | Sliding Loss Window : 1.04\n",
      "Step 3201 | Sliding Loss Window : 0.76\n",
      "Step 3251 | Sliding Loss Window : 1.2\n",
      "Step 3301 | Sliding Loss Window : 0.6\n",
      "Step 3351 | Sliding Loss Window : 1.28\n",
      "Step 3401 | Sliding Loss Window : 1.08\n",
      "Step 3451 | Sliding Loss Window : 0.96\n",
      "Step 3501 | Sliding Loss Window : 0.96\n",
      "Step 3551 | Sliding Loss Window : 0.96\n",
      "Step 3601 | Sliding Loss Window : 0.92\n",
      "Step 3651 | Sliding Loss Window : 0.92\n",
      "Step 3701 | Sliding Loss Window : 1.08\n",
      "Step 3751 | Sliding Loss Window : 1.04\n",
      "Step 3801 | Sliding Loss Window : 1.0\n",
      "Step 3851 | Sliding Loss Window : 0.88\n",
      "Step 3901 | Sliding Loss Window : 1.16\n",
      "Step 3951 | Sliding Loss Window : 1.04\n",
      "Step 4001 | Sliding Loss Window : 1.16\n",
      "Step 4051 | Sliding Loss Window : 0.88\n",
      "Step 4101 | Sliding Loss Window : 0.92\n",
      "Step 4151 | Sliding Loss Window : 1.16\n",
      "Step 4201 | Sliding Loss Window : 1.0\n",
      "Step 4251 | Sliding Loss Window : 1.04\n",
      "Step 4301 | Sliding Loss Window : 0.76\n",
      "Step 4351 | Sliding Loss Window : 1.16\n",
      "Step 4401 | Sliding Loss Window : 0.6\n",
      "Step 4451 | Sliding Loss Window : 1.32\n",
      "Step 4501 | Sliding Loss Window : 1.04\n",
      "Step 4551 | Sliding Loss Window : 1.0\n",
      "Step 4601 | Sliding Loss Window : 0.96\n",
      "Step 4651 | Sliding Loss Window : 0.96\n",
      "Step 4701 | Sliding Loss Window : 0.92\n",
      "Step 4751 | Sliding Loss Window : 0.92\n",
      "Step 4801 | Sliding Loss Window : 1.04\n",
      "Step 4851 | Sliding Loss Window : 1.04\n",
      "Step 4901 | Sliding Loss Window : 1.0\n",
      "Step 4951 | Sliding Loss Window : 0.96\n",
      "Step 5001 | Sliding Loss Window : 1.12\n",
      "Step 5051 | Sliding Loss Window : 1.08\n",
      "Step 5101 | Sliding Loss Window : 1.16\n",
      "Step 5151 | Sliding Loss Window : 0.8\n",
      "Step 5201 | Sliding Loss Window : 1.0\n",
      "Step 5251 | Sliding Loss Window : 1.12\n",
      "Step 5301 | Sliding Loss Window : 1.0\n",
      "Step 5351 | Sliding Loss Window : 1.08\n",
      "Step 5401 | Sliding Loss Window : 0.72\n",
      "Step 5451 | Sliding Loss Window : 1.12\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.12\n",
      "Step 101 | Sliding Loss Window : 0.96\n",
      "Step 151 | Sliding Loss Window : 1.28\n",
      "Step 201 | Sliding Loss Window : 0.8\n",
      "Step 251 | Sliding Loss Window : 0.84\n",
      "Step 301 | Sliding Loss Window : 0.96\n",
      "Step 351 | Sliding Loss Window : 0.8\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 1.0\n",
      "Step 501 | Sliding Loss Window : 0.96\n",
      "Step 551 | Sliding Loss Window : 1.08\n",
      "Step 601 | Sliding Loss Window : 1.2\n",
      "Step 651 | Sliding Loss Window : 1.0\n",
      "Step 701 | Sliding Loss Window : 1.04\n",
      "Step 751 | Sliding Loss Window : 1.0\n",
      "Step 801 | Sliding Loss Window : 1.0\n",
      "Step 851 | Sliding Loss Window : 1.08\n",
      "Step 901 | Sliding Loss Window : 0.84\n",
      "Step 951 | Sliding Loss Window : 1.0\n",
      "Step 1001 | Sliding Loss Window : 1.0\n",
      "Step 1051 | Sliding Loss Window : 0.96\n",
      "Step 1101 | Sliding Loss Window : 0.92\n",
      "Step 1151 | Sliding Loss Window : 1.2\n",
      "Step 1201 | Sliding Loss Window : 0.88\n",
      "Step 1251 | Sliding Loss Window : 1.32\n",
      "Step 1301 | Sliding Loss Window : 0.8\n",
      "Step 1351 | Sliding Loss Window : 0.8\n",
      "Step 1401 | Sliding Loss Window : 1.0\n",
      "Step 1451 | Sliding Loss Window : 0.76\n",
      "Step 1501 | Sliding Loss Window : 1.16\n",
      "Step 1551 | Sliding Loss Window : 1.0\n",
      "Step 1601 | Sliding Loss Window : 0.92\n",
      "Step 1651 | Sliding Loss Window : 1.16\n",
      "Step 1701 | Sliding Loss Window : 1.2\n",
      "Step 1751 | Sliding Loss Window : 0.96\n",
      "Step 1801 | Sliding Loss Window : 1.04\n",
      "Step 1851 | Sliding Loss Window : 1.0\n",
      "Step 1901 | Sliding Loss Window : 1.0\n",
      "Step 1951 | Sliding Loss Window : 1.08\n",
      "Step 2001 | Sliding Loss Window : 0.8\n",
      "Step 2051 | Sliding Loss Window : 1.04\n",
      "Step 2101 | Sliding Loss Window : 1.0\n",
      "Step 2151 | Sliding Loss Window : 0.96\n",
      "Step 2201 | Sliding Loss Window : 0.92\n",
      "Step 2251 | Sliding Loss Window : 1.24\n",
      "Step 2301 | Sliding Loss Window : 0.88\n",
      "Step 2351 | Sliding Loss Window : 1.32\n",
      "Step 2401 | Sliding Loss Window : 0.76\n",
      "Step 2451 | Sliding Loss Window : 0.8\n",
      "Step 2501 | Sliding Loss Window : 0.96\n",
      "Step 2551 | Sliding Loss Window : 0.84\n",
      "Step 2601 | Sliding Loss Window : 1.12\n",
      "Step 2651 | Sliding Loss Window : 1.0\n",
      "Step 2701 | Sliding Loss Window : 0.96\n",
      "Step 2751 | Sliding Loss Window : 1.12\n",
      "Step 2801 | Sliding Loss Window : 1.24\n",
      "Step 2851 | Sliding Loss Window : 0.88\n",
      "Step 2901 | Sliding Loss Window : 1.08\n",
      "Step 2951 | Sliding Loss Window : 1.0\n",
      "Step 3001 | Sliding Loss Window : 0.96\n",
      "Step 3051 | Sliding Loss Window : 1.16\n",
      "Step 3101 | Sliding Loss Window : 0.76\n",
      "Step 3151 | Sliding Loss Window : 1.0\n",
      "Step 3201 | Sliding Loss Window : 1.0\n",
      "Step 3251 | Sliding Loss Window : 0.96\n",
      "Step 3301 | Sliding Loss Window : 0.96\n",
      "Step 3351 | Sliding Loss Window : 1.2\n",
      "Step 3401 | Sliding Loss Window : 0.92\n",
      "Step 3451 | Sliding Loss Window : 1.36\n",
      "Step 3501 | Sliding Loss Window : 0.76\n",
      "Step 3551 | Sliding Loss Window : 0.76\n",
      "Step 3601 | Sliding Loss Window : 0.92\n",
      "Step 3651 | Sliding Loss Window : 0.92\n",
      "Step 3701 | Sliding Loss Window : 1.08\n",
      "Step 3751 | Sliding Loss Window : 1.0\n",
      "Step 3801 | Sliding Loss Window : 0.96\n",
      "Step 3851 | Sliding Loss Window : 1.12\n",
      "Step 3901 | Sliding Loss Window : 1.24\n",
      "Step 3951 | Sliding Loss Window : 0.88\n",
      "Step 4001 | Sliding Loss Window : 1.04\n",
      "Step 4051 | Sliding Loss Window : 1.0\n",
      "Step 4101 | Sliding Loss Window : 1.0\n",
      "Step 4151 | Sliding Loss Window : 1.12\n",
      "Step 4201 | Sliding Loss Window : 0.76\n",
      "Step 4251 | Sliding Loss Window : 1.08\n",
      "Step 4301 | Sliding Loss Window : 0.92\n",
      "Step 4351 | Sliding Loss Window : 1.0\n",
      "Step 4401 | Sliding Loss Window : 1.0\n",
      "Step 4451 | Sliding Loss Window : 1.16\n",
      "Step 4501 | Sliding Loss Window : 0.96\n",
      "Step 4551 | Sliding Loss Window : 1.32\n",
      "Step 4601 | Sliding Loss Window : 0.76\n",
      "Step 4651 | Sliding Loss Window : 0.72\n",
      "Step 4701 | Sliding Loss Window : 0.96\n",
      "Step 4751 | Sliding Loss Window : 0.92\n",
      "Step 4801 | Sliding Loss Window : 1.08\n",
      "Step 4851 | Sliding Loss Window : 1.0\n",
      "Step 4901 | Sliding Loss Window : 1.0\n",
      "Step 4951 | Sliding Loss Window : 1.12\n",
      "Step 5001 | Sliding Loss Window : 1.16\n",
      "Step 5051 | Sliding Loss Window : 0.96\n",
      "Step 5101 | Sliding Loss Window : 1.0\n",
      "Step 5151 | Sliding Loss Window : 1.0\n",
      "Step 5201 | Sliding Loss Window : 1.0\n",
      "Step 5251 | Sliding Loss Window : 1.08\n",
      "Step 5301 | Sliding Loss Window : 0.76\n",
      "Step 5351 | Sliding Loss Window : 1.12\n",
      "Step 5401 | Sliding Loss Window : 0.96\n",
      "Step 5451 | Sliding Loss Window : 0.92\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.0\n",
      "Step 101 | Sliding Loss Window : 0.92\n",
      "Step 151 | Sliding Loss Window : 0.8\n",
      "Step 201 | Sliding Loss Window : 0.8400000000000002\n",
      "Step 251 | Sliding Loss Window : 1.36\n",
      "Step 301 | Sliding Loss Window : 1.12\n",
      "Step 351 | Sliding Loss Window : 1.12\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 501 | Sliding Loss Window : 1.2\n",
      "Step 551 | Sliding Loss Window : 1.24\n",
      "Step 601 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 651 | Sliding Loss Window : 0.88\n",
      "Step 701 | Sliding Loss Window : 0.96\n",
      "Step 751 | Sliding Loss Window : 0.88\n",
      "Step 801 | Sliding Loss Window : 1.0\n",
      "Step 851 | Sliding Loss Window : 0.7600000000000001\n",
      "Step 901 | Sliding Loss Window : 0.96\n",
      "Step 951 | Sliding Loss Window : 0.88\n",
      "Step 1001 | Sliding Loss Window : 1.04\n",
      "Step 1051 | Sliding Loss Window : 1.08\n",
      "Step 1101 | Sliding Loss Window : 0.92\n",
      "Step 1151 | Sliding Loss Window : 1.08\n",
      "Step 1201 | Sliding Loss Window : 0.84\n",
      "Step 1251 | Sliding Loss Window : 0.8\n",
      "Step 1301 | Sliding Loss Window : 0.88\n",
      "Step 1351 | Sliding Loss Window : 1.4\n",
      "Step 1401 | Sliding Loss Window : 1.12\n",
      "Step 1451 | Sliding Loss Window : 1.08\n",
      "Step 1501 | Sliding Loss Window : 1.12\n",
      "Step 1551 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 1601 | Sliding Loss Window : 1.24\n",
      "Step 1651 | Sliding Loss Window : 1.2\n",
      "Step 1701 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 1751 | Sliding Loss Window : 0.84\n",
      "Step 1801 | Sliding Loss Window : 1.0\n",
      "Step 1851 | Sliding Loss Window : 0.84\n",
      "Step 1901 | Sliding Loss Window : 1.0\n",
      "Step 1951 | Sliding Loss Window : 0.7600000000000001\n",
      "Step 2001 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 2051 | Sliding Loss Window : 0.92\n",
      "Step 2101 | Sliding Loss Window : 1.04\n",
      "Step 2151 | Sliding Loss Window : 1.04\n",
      "Step 2201 | Sliding Loss Window : 0.92\n",
      "Step 2251 | Sliding Loss Window : 1.12\n",
      "Step 2301 | Sliding Loss Window : 0.8\n",
      "Step 2351 | Sliding Loss Window : 0.84\n",
      "Step 2401 | Sliding Loss Window : 0.8\n",
      "Step 2451 | Sliding Loss Window : 1.44\n",
      "Step 2501 | Sliding Loss Window : 1.12\n",
      "Step 2551 | Sliding Loss Window : 1.08\n",
      "Step 2601 | Sliding Loss Window : 1.16\n",
      "Step 2651 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 2701 | Sliding Loss Window : 1.2\n",
      "Step 2751 | Sliding Loss Window : 1.2\n",
      "Step 2801 | Sliding Loss Window : 1.0\n",
      "Step 2851 | Sliding Loss Window : 0.84\n",
      "Step 2901 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 2951 | Sliding Loss Window : 0.8\n",
      "Step 3001 | Sliding Loss Window : 1.0\n",
      "Step 3051 | Sliding Loss Window : 0.76\n",
      "Step 3101 | Sliding Loss Window : 1.0\n",
      "Step 3151 | Sliding Loss Window : 0.88\n",
      "Step 3201 | Sliding Loss Window : 1.04\n",
      "Step 3251 | Sliding Loss Window : 1.12\n",
      "Step 3301 | Sliding Loss Window : 0.84\n",
      "Step 3351 | Sliding Loss Window : 1.12\n",
      "Step 3401 | Sliding Loss Window : 0.8\n",
      "Step 3451 | Sliding Loss Window : 0.84\n",
      "Step 3501 | Sliding Loss Window : 0.88\n",
      "Step 3551 | Sliding Loss Window : 1.44\n",
      "Step 3601 | Sliding Loss Window : 1.08\n",
      "Step 3651 | Sliding Loss Window : 1.04\n",
      "Step 3701 | Sliding Loss Window : 1.2\n",
      "Step 3751 | Sliding Loss Window : 0.92\n",
      "Step 3801 | Sliding Loss Window : 1.24\n",
      "Step 3851 | Sliding Loss Window : 1.16\n",
      "Step 3901 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 3951 | Sliding Loss Window : 0.84\n",
      "Step 4001 | Sliding Loss Window : 1.04\n",
      "Step 4051 | Sliding Loss Window : 0.7600000000000001\n",
      "Step 4101 | Sliding Loss Window : 0.96\n",
      "Step 4151 | Sliding Loss Window : 0.84\n",
      "Step 4201 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 4251 | Sliding Loss Window : 0.96\n",
      "Step 4301 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 4351 | Sliding Loss Window : 1.12\n",
      "Step 4401 | Sliding Loss Window : 0.8\n",
      "Step 4451 | Sliding Loss Window : 1.16\n",
      "Step 4501 | Sliding Loss Window : 0.76\n",
      "Step 4551 | Sliding Loss Window : 0.92\n",
      "Step 4601 | Sliding Loss Window : 0.84\n",
      "Step 4651 | Sliding Loss Window : 1.44\n",
      "Step 4701 | Sliding Loss Window : 1.12\n",
      "Step 4751 | Sliding Loss Window : 0.96\n",
      "Step 4801 | Sliding Loss Window : 1.24\n",
      "Step 4851 | Sliding Loss Window : 0.92\n",
      "Step 4901 | Sliding Loss Window : 1.28\n",
      "Step 4951 | Sliding Loss Window : 1.12\n",
      "Step 5001 | Sliding Loss Window : 1.04\n",
      "Step 5051 | Sliding Loss Window : 0.8000000000000002\n",
      "Step 5101 | Sliding Loss Window : 1.04\n",
      "Step 5151 | Sliding Loss Window : 0.8000000000000002\n",
      "Step 5201 | Sliding Loss Window : 0.92\n",
      "Step 5251 | Sliding Loss Window : 0.8\n",
      "Step 5301 | Sliding Loss Window : 0.96\n",
      "Step 5351 | Sliding Loss Window : 0.92\n",
      "Step 5401 | Sliding Loss Window : 1.04\n",
      "Step 5451 | Sliding Loss Window : 1.08\n",
      "Step 1 | Sliding Loss Window : 2.0000000000000004\n",
      "Step 51 | Sliding Loss Window : 0.8\n",
      "Step 101 | Sliding Loss Window : 1.08\n",
      "Step 151 | Sliding Loss Window : 1.2400000000000002\n",
      "Step 201 | Sliding Loss Window : 1.04\n",
      "Step 251 | Sliding Loss Window : 0.8\n",
      "Step 301 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 351 | Sliding Loss Window : 1.08\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 0.96\n",
      "Step 501 | Sliding Loss Window : 1.0\n",
      "Step 551 | Sliding Loss Window : 1.04\n",
      "Step 601 | Sliding Loss Window : 1.08\n",
      "Step 651 | Sliding Loss Window : 1.0\n",
      "Step 701 | Sliding Loss Window : 0.96\n",
      "Step 751 | Sliding Loss Window : 1.08\n",
      "Step 801 | Sliding Loss Window : 0.6000000000000001\n",
      "Step 851 | Sliding Loss Window : 1.12\n",
      "Step 901 | Sliding Loss Window : 1.16\n",
      "Step 951 | Sliding Loss Window : 0.8\n",
      "Step 1001 | Sliding Loss Window : 0.88\n",
      "Step 1051 | Sliding Loss Window : 1.16\n",
      "Step 1101 | Sliding Loss Window : 1.0\n",
      "Step 1151 | Sliding Loss Window : 0.84\n",
      "Step 1201 | Sliding Loss Window : 1.08\n",
      "Step 1251 | Sliding Loss Window : 1.28\n",
      "Step 1301 | Sliding Loss Window : 0.96\n",
      "Step 1351 | Sliding Loss Window : 0.84\n",
      "Step 1401 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 1451 | Sliding Loss Window : 1.0800000000000003\n",
      "Step 1501 | Sliding Loss Window : 1.04\n",
      "Step 1551 | Sliding Loss Window : 1.04\n",
      "Step 1601 | Sliding Loss Window : 0.92\n",
      "Step 1651 | Sliding Loss Window : 1.12\n",
      "Step 1701 | Sliding Loss Window : 1.04\n",
      "Step 1751 | Sliding Loss Window : 0.96\n",
      "Step 1801 | Sliding Loss Window : 1.0\n",
      "Step 1851 | Sliding Loss Window : 1.04\n",
      "Step 1901 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 1951 | Sliding Loss Window : 1.16\n",
      "Step 2001 | Sliding Loss Window : 1.12\n",
      "Step 2051 | Sliding Loss Window : 0.8\n",
      "Step 2101 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 2151 | Sliding Loss Window : 1.2\n",
      "Step 2201 | Sliding Loss Window : 0.96\n",
      "Step 2251 | Sliding Loss Window : 0.8\n",
      "Step 2301 | Sliding Loss Window : 1.16\n",
      "Step 2351 | Sliding Loss Window : 1.28\n",
      "Step 2401 | Sliding Loss Window : 0.96\n",
      "Step 2451 | Sliding Loss Window : 0.8\n",
      "Step 2501 | Sliding Loss Window : 1.04\n",
      "Step 2551 | Sliding Loss Window : 1.0400000000000003\n",
      "Step 2601 | Sliding Loss Window : 1.08\n",
      "Step 2651 | Sliding Loss Window : 1.0\n",
      "Step 2701 | Sliding Loss Window : 0.92\n",
      "Step 2751 | Sliding Loss Window : 1.08\n",
      "Step 2801 | Sliding Loss Window : 1.08\n",
      "Step 2851 | Sliding Loss Window : 0.96\n",
      "Step 2901 | Sliding Loss Window : 1.04\n",
      "Step 2951 | Sliding Loss Window : 0.96\n",
      "Step 3001 | Sliding Loss Window : 0.68\n",
      "Step 3051 | Sliding Loss Window : 1.2\n",
      "Step 3101 | Sliding Loss Window : 1.08\n",
      "Step 3151 | Sliding Loss Window : 0.76\n",
      "Step 3201 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 3251 | Sliding Loss Window : 1.2\n",
      "Step 3301 | Sliding Loss Window : 0.92\n",
      "Step 3351 | Sliding Loss Window : 0.8\n",
      "Step 3401 | Sliding Loss Window : 1.16\n",
      "Step 3451 | Sliding Loss Window : 1.36\n",
      "Step 3501 | Sliding Loss Window : 0.92\n",
      "Step 3551 | Sliding Loss Window : 0.8\n",
      "Step 3601 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 3651 | Sliding Loss Window : 1.0400000000000003\n",
      "Step 3701 | Sliding Loss Window : 1.08\n",
      "Step 3751 | Sliding Loss Window : 1.04\n",
      "Step 3801 | Sliding Loss Window : 0.92\n",
      "Step 3851 | Sliding Loss Window : 1.04\n",
      "Step 3901 | Sliding Loss Window : 1.08\n",
      "Step 3951 | Sliding Loss Window : 0.96\n",
      "Step 4001 | Sliding Loss Window : 1.08\n",
      "Step 4051 | Sliding Loss Window : 1.0\n",
      "Step 4101 | Sliding Loss Window : 0.64\n",
      "Step 4151 | Sliding Loss Window : 1.2\n",
      "Step 4201 | Sliding Loss Window : 1.12\n",
      "Step 4251 | Sliding Loss Window : 0.76\n",
      "Step 4301 | Sliding Loss Window : 0.8800000000000003\n",
      "Step 4351 | Sliding Loss Window : 1.16\n",
      "Step 4401 | Sliding Loss Window : 0.92\n",
      "Step 4451 | Sliding Loss Window : 0.88\n",
      "Step 4501 | Sliding Loss Window : 1.12\n",
      "Step 4551 | Sliding Loss Window : 1.36\n",
      "Step 4601 | Sliding Loss Window : 0.92\n",
      "Step 4651 | Sliding Loss Window : 0.8\n",
      "Step 4701 | Sliding Loss Window : 1.04\n",
      "Step 4751 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 4801 | Sliding Loss Window : 1.12\n",
      "Step 4851 | Sliding Loss Window : 1.0\n",
      "Step 4901 | Sliding Loss Window : 0.92\n",
      "Step 4951 | Sliding Loss Window : 1.04\n",
      "Step 5001 | Sliding Loss Window : 1.12\n",
      "Step 5051 | Sliding Loss Window : 0.88\n",
      "Step 5101 | Sliding Loss Window : 1.16\n",
      "Step 5151 | Sliding Loss Window : 0.92\n",
      "Step 5201 | Sliding Loss Window : 0.68\n",
      "Step 5251 | Sliding Loss Window : 1.24\n",
      "Step 5301 | Sliding Loss Window : 1.08\n",
      "Step 5351 | Sliding Loss Window : 0.8\n",
      "Step 5401 | Sliding Loss Window : 0.8800000000000003\n",
      "Step 5451 | Sliding Loss Window : 1.16\n",
      "Step 1 | Sliding Loss Window : 0.0\n",
      "Step 51 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 101 | Sliding Loss Window : 0.8400000000000002\n",
      "Step 151 | Sliding Loss Window : 1.12\n",
      "Step 201 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 251 | Sliding Loss Window : 0.8800000000000003\n",
      "Step 301 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 351 | Sliding Loss Window : 1.04\n",
      "Step 401 | Sliding Loss Window : 0.8400000000000003\n",
      "Step 451 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 501 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 551 | Sliding Loss Window : 1.08\n",
      "Step 601 | Sliding Loss Window : 1.0400000000000003\n",
      "Step 651 | Sliding Loss Window : 0.8000000000000003\n",
      "Step 701 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 751 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 801 | Sliding Loss Window : 1.0400000000000003\n",
      "Step 851 | Sliding Loss Window : 1.28\n",
      "Step 901 | Sliding Loss Window : 1.1600000000000001\n",
      "Step 951 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 1001 | Sliding Loss Window : 1.24\n",
      "Step 1051 | Sliding Loss Window : 1.12\n",
      "Step 1101 | Sliding Loss Window : 1.04\n",
      "Step 1151 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 1201 | Sliding Loss Window : 0.8000000000000003\n",
      "Step 1251 | Sliding Loss Window : 1.2\n",
      "Step 1301 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 1351 | Sliding Loss Window : 0.8400000000000003\n",
      "Step 1401 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 1451 | Sliding Loss Window : 0.96\n",
      "Step 1501 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 1551 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 1601 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 1651 | Sliding Loss Window : 1.12\n",
      "Step 1701 | Sliding Loss Window : 1.08\n",
      "Step 1751 | Sliding Loss Window : 0.7600000000000002\n",
      "Step 1801 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 1851 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 1901 | Sliding Loss Window : 1.08\n",
      "Step 1951 | Sliding Loss Window : 1.24\n",
      "Step 2001 | Sliding Loss Window : 1.2000000000000002\n",
      "Step 2051 | Sliding Loss Window : 0.9200000000000002\n",
      "Step 2101 | Sliding Loss Window : 1.2\n",
      "Step 2151 | Sliding Loss Window : 1.1600000000000001\n",
      "Step 2201 | Sliding Loss Window : 1.0\n",
      "Step 2251 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 2301 | Sliding Loss Window : 0.8000000000000002\n",
      "Step 2351 | Sliding Loss Window : 1.28\n",
      "Step 2401 | Sliding Loss Window : 0.8000000000000003\n",
      "Step 2451 | Sliding Loss Window : 0.8400000000000003\n",
      "Step 2501 | Sliding Loss Window : 1.04\n",
      "Step 2551 | Sliding Loss Window : 0.96\n",
      "Step 2601 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 2651 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 2701 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 2751 | Sliding Loss Window : 1.12\n",
      "Step 2801 | Sliding Loss Window : 1.12\n",
      "Step 2851 | Sliding Loss Window : 0.7200000000000003\n",
      "Step 2901 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 2951 | Sliding Loss Window : 0.9200000000000003\n",
      "Step 3001 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 3051 | Sliding Loss Window : 1.28\n",
      "Step 3101 | Sliding Loss Window : 1.2400000000000002\n",
      "Step 3151 | Sliding Loss Window : 0.88\n",
      "Step 3201 | Sliding Loss Window : 1.24\n",
      "Step 3251 | Sliding Loss Window : 1.1600000000000001\n",
      "Step 3301 | Sliding Loss Window : 0.92\n",
      "Step 3351 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 3401 | Sliding Loss Window : 0.8000000000000003\n",
      "Step 3451 | Sliding Loss Window : 1.28\n",
      "Step 3501 | Sliding Loss Window : 0.7600000000000001\n",
      "Step 3551 | Sliding Loss Window : 0.8400000000000003\n",
      "Step 3601 | Sliding Loss Window : 1.04\n",
      "Step 3651 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 3701 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 3751 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 3801 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 3851 | Sliding Loss Window : 1.12\n",
      "Step 3901 | Sliding Loss Window : 1.12\n",
      "Step 3951 | Sliding Loss Window : 0.6800000000000003\n",
      "Step 4001 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 4051 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 4101 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 4151 | Sliding Loss Window : 1.32\n",
      "Step 4201 | Sliding Loss Window : 1.2000000000000002\n",
      "Step 4251 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 4301 | Sliding Loss Window : 1.28\n",
      "Step 4351 | Sliding Loss Window : 1.12\n",
      "Step 4401 | Sliding Loss Window : 0.92\n",
      "Step 4451 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 4501 | Sliding Loss Window : 0.8400000000000002\n",
      "Step 4551 | Sliding Loss Window : 1.2\n",
      "Step 4601 | Sliding Loss Window : 0.8400000000000003\n",
      "Step 4651 | Sliding Loss Window : 0.8000000000000003\n",
      "Step 4701 | Sliding Loss Window : 1.08\n",
      "Step 4751 | Sliding Loss Window : 0.9600000000000002\n",
      "Step 4801 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 4851 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 4901 | Sliding Loss Window : 0.8800000000000003\n",
      "Step 4951 | Sliding Loss Window : 1.08\n",
      "Step 5001 | Sliding Loss Window : 1.2\n",
      "Step 5051 | Sliding Loss Window : 0.6400000000000002\n",
      "Step 5101 | Sliding Loss Window : 1.0000000000000002\n",
      "Step 5151 | Sliding Loss Window : 0.9600000000000003\n",
      "Step 5201 | Sliding Loss Window : 0.96\n",
      "Step 5251 | Sliding Loss Window : 1.32\n",
      "Step 5301 | Sliding Loss Window : 1.2400000000000002\n",
      "Step 5351 | Sliding Loss Window : 0.8800000000000001\n",
      "Step 5401 | Sliding Loss Window : 1.24\n",
      "Step 5451 | Sliding Loss Window : 1.12\n",
      "Step 1 | Sliding Loss Window : 2.220446049250313e-16\n",
      "Step 51 | Sliding Loss Window : 1.16\n",
      "Step 101 | Sliding Loss Window : 0.68\n",
      "Step 151 | Sliding Loss Window : 0.8\n",
      "Step 201 | Sliding Loss Window : 1.08\n",
      "Step 251 | Sliding Loss Window : 1.32\n",
      "Step 301 | Sliding Loss Window : 1.04\n",
      "Step 351 | Sliding Loss Window : 1.0\n",
      "Step 401 | Sliding Loss Window : 0.88\n",
      "Step 451 | Sliding Loss Window : 0.88\n",
      "Step 501 | Sliding Loss Window : 1.08\n",
      "Step 551 | Sliding Loss Window : 1.28\n",
      "Step 601 | Sliding Loss Window : 0.92\n",
      "Step 651 | Sliding Loss Window : 0.88\n",
      "Step 701 | Sliding Loss Window : 1.0\n",
      "Step 751 | Sliding Loss Window : 0.92\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 0.92\n",
      "Step 901 | Sliding Loss Window : 1.04\n",
      "Step 951 | Sliding Loss Window : 1.04\n",
      "Step 1001 | Sliding Loss Window : 1.24\n",
      "Step 1051 | Sliding Loss Window : 0.92\n",
      "Step 1101 | Sliding Loss Window : 1.12\n",
      "Step 1151 | Sliding Loss Window : 1.08\n",
      "Step 1201 | Sliding Loss Window : 0.72\n",
      "Step 1251 | Sliding Loss Window : 0.8\n",
      "Step 1301 | Sliding Loss Window : 1.08\n",
      "Step 1351 | Sliding Loss Window : 1.28\n",
      "Step 1401 | Sliding Loss Window : 1.08\n",
      "Step 1451 | Sliding Loss Window : 1.0\n",
      "Step 1501 | Sliding Loss Window : 0.88\n",
      "Step 1551 | Sliding Loss Window : 0.92\n",
      "Step 1601 | Sliding Loss Window : 1.0\n",
      "Step 1651 | Sliding Loss Window : 1.28\n",
      "Step 1701 | Sliding Loss Window : 0.92\n",
      "Step 1751 | Sliding Loss Window : 0.96\n",
      "Step 1801 | Sliding Loss Window : 0.92\n",
      "Step 1851 | Sliding Loss Window : 0.96\n",
      "Step 1901 | Sliding Loss Window : 0.84\n",
      "Step 1951 | Sliding Loss Window : 0.92\n",
      "Step 2001 | Sliding Loss Window : 1.04\n",
      "Step 2051 | Sliding Loss Window : 1.08\n",
      "Step 2101 | Sliding Loss Window : 1.2\n",
      "Step 2151 | Sliding Loss Window : 0.96\n",
      "Step 2201 | Sliding Loss Window : 1.08\n",
      "Step 2251 | Sliding Loss Window : 1.08\n",
      "Step 2301 | Sliding Loss Window : 0.68\n",
      "Step 2351 | Sliding Loss Window : 0.88\n",
      "Step 2401 | Sliding Loss Window : 1.04\n",
      "Step 2451 | Sliding Loss Window : 1.28\n",
      "Step 2501 | Sliding Loss Window : 1.08\n",
      "Step 2551 | Sliding Loss Window : 0.96\n",
      "Step 2601 | Sliding Loss Window : 0.92\n",
      "Step 2651 | Sliding Loss Window : 0.92\n",
      "Step 2701 | Sliding Loss Window : 1.0\n",
      "Step 2751 | Sliding Loss Window : 1.32\n",
      "Step 2801 | Sliding Loss Window : 0.84\n",
      "Step 2851 | Sliding Loss Window : 1.04\n",
      "Step 2901 | Sliding Loss Window : 0.92\n",
      "Step 2951 | Sliding Loss Window : 0.96\n",
      "Step 3001 | Sliding Loss Window : 0.84\n",
      "Step 3051 | Sliding Loss Window : 0.92\n",
      "Step 3101 | Sliding Loss Window : 1.0\n",
      "Step 3151 | Sliding Loss Window : 1.08\n",
      "Step 3201 | Sliding Loss Window : 1.24\n",
      "Step 3251 | Sliding Loss Window : 0.88\n",
      "Step 3301 | Sliding Loss Window : 1.12\n",
      "Step 3351 | Sliding Loss Window : 1.12\n",
      "Step 3401 | Sliding Loss Window : 0.64\n",
      "Step 3451 | Sliding Loss Window : 0.88\n",
      "Step 3501 | Sliding Loss Window : 1.04\n",
      "Step 3551 | Sliding Loss Window : 1.28\n",
      "Step 3601 | Sliding Loss Window : 1.12\n",
      "Step 3651 | Sliding Loss Window : 0.88\n",
      "Step 3701 | Sliding Loss Window : 0.92\n",
      "Step 3751 | Sliding Loss Window : 0.96\n",
      "Step 3801 | Sliding Loss Window : 1.0\n",
      "Step 3851 | Sliding Loss Window : 1.28\n",
      "Step 3901 | Sliding Loss Window : 0.84\n",
      "Step 3951 | Sliding Loss Window : 1.04\n",
      "Step 4001 | Sliding Loss Window : 0.96\n",
      "Step 4051 | Sliding Loss Window : 0.96\n",
      "Step 4101 | Sliding Loss Window : 0.84\n",
      "Step 4151 | Sliding Loss Window : 0.96\n",
      "Step 4201 | Sliding Loss Window : 1.0\n",
      "Step 4251 | Sliding Loss Window : 1.08\n",
      "Step 4301 | Sliding Loss Window : 1.16\n",
      "Step 4351 | Sliding Loss Window : 0.92\n",
      "Step 4401 | Sliding Loss Window : 1.16\n",
      "Step 4451 | Sliding Loss Window : 1.08\n",
      "Step 4501 | Sliding Loss Window : 0.64\n",
      "Step 4551 | Sliding Loss Window : 0.92\n",
      "Step 4601 | Sliding Loss Window : 1.04\n",
      "Step 4651 | Sliding Loss Window : 1.24\n",
      "Step 4701 | Sliding Loss Window : 1.12\n",
      "Step 4751 | Sliding Loss Window : 0.92\n",
      "Step 4801 | Sliding Loss Window : 0.88\n",
      "Step 4851 | Sliding Loss Window : 0.96\n",
      "Step 4901 | Sliding Loss Window : 1.04\n",
      "Step 4951 | Sliding Loss Window : 1.24\n",
      "Step 5001 | Sliding Loss Window : 0.8\n",
      "Step 5051 | Sliding Loss Window : 1.08\n",
      "Step 5101 | Sliding Loss Window : 0.96\n",
      "Step 5151 | Sliding Loss Window : 0.96\n",
      "Step 5201 | Sliding Loss Window : 0.84\n",
      "Step 5251 | Sliding Loss Window : 0.96\n",
      "Step 5301 | Sliding Loss Window : 1.04\n",
      "Step 5351 | Sliding Loss Window : 1.04\n",
      "Step 5401 | Sliding Loss Window : 1.2\n",
      "Step 5451 | Sliding Loss Window : 0.92\n",
      "Step 1 | Sliding Loss Window : 1.9999999999999998\n",
      "Step 51 | Sliding Loss Window : 1.08\n",
      "Step 101 | Sliding Loss Window : 1.24\n",
      "Step 151 | Sliding Loss Window : 1.24\n",
      "Step 201 | Sliding Loss Window : 1.08\n",
      "Step 251 | Sliding Loss Window : 1.04\n",
      "Step 301 | Sliding Loss Window : 0.92\n",
      "Step 351 | Sliding Loss Window : 1.12\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 0.88\n",
      "Step 501 | Sliding Loss Window : 0.8\n",
      "Step 551 | Sliding Loss Window : 1.16\n",
      "Step 601 | Sliding Loss Window : 0.92\n",
      "Step 651 | Sliding Loss Window : 1.12\n",
      "Step 701 | Sliding Loss Window : 1.0\n",
      "Step 751 | Sliding Loss Window : 0.6800000000000002\n",
      "Step 801 | Sliding Loss Window : 1.12\n",
      "Step 851 | Sliding Loss Window : 1.04\n",
      "Step 901 | Sliding Loss Window : 0.6800000000000002\n",
      "Step 951 | Sliding Loss Window : 0.96\n",
      "Step 1001 | Sliding Loss Window : 0.88\n",
      "Step 1051 | Sliding Loss Window : 1.16\n",
      "Step 1101 | Sliding Loss Window : 0.72\n",
      "Step 1151 | Sliding Loss Window : 1.08\n",
      "Step 1201 | Sliding Loss Window : 1.32\n",
      "Step 1251 | Sliding Loss Window : 1.2\n",
      "Step 1301 | Sliding Loss Window : 1.04\n",
      "Step 1351 | Sliding Loss Window : 1.12\n",
      "Step 1401 | Sliding Loss Window : 0.84\n",
      "Step 1451 | Sliding Loss Window : 1.16\n",
      "Step 1501 | Sliding Loss Window : 1.08\n",
      "Step 1551 | Sliding Loss Window : 0.92\n",
      "Step 1601 | Sliding Loss Window : 0.84\n",
      "Step 1651 | Sliding Loss Window : 1.12\n",
      "Step 1701 | Sliding Loss Window : 0.92\n",
      "Step 1751 | Sliding Loss Window : 1.12\n",
      "Step 1801 | Sliding Loss Window : 0.96\n",
      "Step 1851 | Sliding Loss Window : 0.7200000000000002\n",
      "Step 1901 | Sliding Loss Window : 1.12\n",
      "Step 1951 | Sliding Loss Window : 1.04\n",
      "Step 2001 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 2051 | Sliding Loss Window : 1.0\n",
      "Step 2101 | Sliding Loss Window : 0.88\n",
      "Step 2151 | Sliding Loss Window : 1.16\n",
      "Step 2201 | Sliding Loss Window : 0.72\n",
      "Step 2251 | Sliding Loss Window : 1.04\n",
      "Step 2301 | Sliding Loss Window : 1.36\n",
      "Step 2351 | Sliding Loss Window : 1.24\n",
      "Step 2401 | Sliding Loss Window : 1.0\n",
      "Step 2451 | Sliding Loss Window : 1.08\n",
      "Step 2501 | Sliding Loss Window : 0.88\n",
      "Step 2551 | Sliding Loss Window : 1.2\n",
      "Step 2601 | Sliding Loss Window : 1.0\n",
      "Step 2651 | Sliding Loss Window : 0.96\n",
      "Step 2701 | Sliding Loss Window : 0.8\n",
      "Step 2751 | Sliding Loss Window : 1.12\n",
      "Step 2801 | Sliding Loss Window : 0.96\n",
      "Step 2851 | Sliding Loss Window : 1.12\n",
      "Step 2901 | Sliding Loss Window : 0.96\n",
      "Step 2951 | Sliding Loss Window : 0.76\n",
      "Step 3001 | Sliding Loss Window : 1.12\n",
      "Step 3051 | Sliding Loss Window : 1.0\n",
      "Step 3101 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 3151 | Sliding Loss Window : 0.96\n",
      "Step 3201 | Sliding Loss Window : 0.92\n",
      "Step 3251 | Sliding Loss Window : 1.16\n",
      "Step 3301 | Sliding Loss Window : 0.72\n",
      "Step 3351 | Sliding Loss Window : 1.0\n",
      "Step 3401 | Sliding Loss Window : 1.44\n",
      "Step 3451 | Sliding Loss Window : 1.16\n",
      "Step 3501 | Sliding Loss Window : 1.04\n",
      "Step 3551 | Sliding Loss Window : 1.04\n",
      "Step 3601 | Sliding Loss Window : 0.88\n",
      "Step 3651 | Sliding Loss Window : 1.24\n",
      "Step 3701 | Sliding Loss Window : 0.96\n",
      "Step 3751 | Sliding Loss Window : 1.0\n",
      "Step 3801 | Sliding Loss Window : 0.8\n",
      "Step 3851 | Sliding Loss Window : 1.12\n",
      "Step 3901 | Sliding Loss Window : 0.92\n",
      "Step 3951 | Sliding Loss Window : 1.16\n",
      "Step 4001 | Sliding Loss Window : 1.0\n",
      "Step 4051 | Sliding Loss Window : 0.7200000000000002\n",
      "Step 4101 | Sliding Loss Window : 1.08\n",
      "Step 4151 | Sliding Loss Window : 1.04\n",
      "Step 4201 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 4251 | Sliding Loss Window : 0.92\n",
      "Step 4301 | Sliding Loss Window : 0.92\n",
      "Step 4351 | Sliding Loss Window : 1.16\n",
      "Step 4401 | Sliding Loss Window : 0.76\n",
      "Step 4451 | Sliding Loss Window : 1.04\n",
      "Step 4501 | Sliding Loss Window : 1.44\n",
      "Step 4551 | Sliding Loss Window : 1.16\n",
      "Step 4601 | Sliding Loss Window : 1.0\n",
      "Step 4651 | Sliding Loss Window : 1.04\n",
      "Step 4701 | Sliding Loss Window : 0.84\n",
      "Step 4751 | Sliding Loss Window : 1.24\n",
      "Step 4801 | Sliding Loss Window : 0.96\n",
      "Step 4851 | Sliding Loss Window : 1.08\n",
      "Step 4901 | Sliding Loss Window : 0.76\n",
      "Step 4951 | Sliding Loss Window : 1.08\n",
      "Step 5001 | Sliding Loss Window : 1.0\n",
      "Step 5051 | Sliding Loss Window : 1.12\n",
      "Step 5101 | Sliding Loss Window : 0.96\n",
      "Step 5151 | Sliding Loss Window : 0.72\n",
      "Step 5201 | Sliding Loss Window : 1.08\n",
      "Step 5251 | Sliding Loss Window : 1.04\n",
      "Step 5301 | Sliding Loss Window : 0.6400000000000001\n",
      "Step 5351 | Sliding Loss Window : 0.96\n",
      "Step 5401 | Sliding Loss Window : 0.96\n",
      "Step 5451 | Sliding Loss Window : 1.16\n",
      "Step 1 | Sliding Loss Window : 0.061060505044843394\n",
      "Step 51 | Sliding Loss Window : 0.8560188803005421\n",
      "Step 101 | Sliding Loss Window : 0.7417670951278538\n",
      "Step 151 | Sliding Loss Window : 0.7113875846838673\n",
      "Step 201 | Sliding Loss Window : 0.7818671409442132\n",
      "Step 251 | Sliding Loss Window : 0.7868956025177966\n",
      "Step 301 | Sliding Loss Window : 0.7047405989544516\n",
      "Step 351 | Sliding Loss Window : 0.8237585109778738\n",
      "Step 401 | Sliding Loss Window : 0.7671131004949213\n",
      "Step 451 | Sliding Loss Window : 0.7323425931460301\n",
      "Step 501 | Sliding Loss Window : 0.7090973544242435\n",
      "Step 551 | Sliding Loss Window : 0.7792873984023339\n",
      "Step 601 | Sliding Loss Window : 0.8148937926019554\n",
      "Step 651 | Sliding Loss Window : 0.7983154418356843\n",
      "Step 701 | Sliding Loss Window : 0.6905635705802893\n",
      "Step 751 | Sliding Loss Window : 0.6864806485841616\n",
      "Step 801 | Sliding Loss Window : 0.8010328660366878\n",
      "Step 851 | Sliding Loss Window : 0.7025099298239327\n",
      "Step 901 | Sliding Loss Window : 0.7392729244097825\n",
      "Step 951 | Sliding Loss Window : 0.7109093264202501\n",
      "Step 1001 | Sliding Loss Window : 0.7577448515033279\n",
      "Step 1051 | Sliding Loss Window : 0.7286194265365346\n",
      "Step 1101 | Sliding Loss Window : 0.8313357675428054\n",
      "Step 1151 | Sliding Loss Window : 0.7856608056551825\n",
      "Step 1201 | Sliding Loss Window : 0.7153650910083975\n",
      "Step 1251 | Sliding Loss Window : 0.7038320378698859\n",
      "Step 1301 | Sliding Loss Window : 0.7388010799267262\n",
      "Step 1351 | Sliding Loss Window : 0.7957184007585378\n",
      "Step 1401 | Sliding Loss Window : 0.7084254029039271\n",
      "Step 1451 | Sliding Loss Window : 0.8387827132189198\n",
      "Step 1501 | Sliding Loss Window : 0.7429565548157254\n",
      "Step 1551 | Sliding Loss Window : 0.7400131718486463\n",
      "Step 1601 | Sliding Loss Window : 0.699791004420014\n",
      "Step 1651 | Sliding Loss Window : 0.8033671312923434\n",
      "Step 1701 | Sliding Loss Window : 0.8009607597609484\n",
      "Step 1751 | Sliding Loss Window : 0.7712697954704038\n",
      "Step 1801 | Sliding Loss Window : 0.7003469762550988\n",
      "Step 1851 | Sliding Loss Window : 0.6930898640370537\n",
      "Step 1901 | Sliding Loss Window : 0.823734631808947\n",
      "Step 1951 | Sliding Loss Window : 0.6815207257468653\n",
      "Step 2001 | Sliding Loss Window : 0.7503035244255197\n",
      "Step 2051 | Sliding Loss Window : 0.702468515097589\n",
      "Step 2101 | Sliding Loss Window : 0.7346632354768835\n",
      "Step 2151 | Sliding Loss Window : 0.7666482644203793\n",
      "Step 2201 | Sliding Loss Window : 0.7955790014129162\n",
      "Step 2251 | Sliding Loss Window : 0.7988902220490308\n",
      "Step 2301 | Sliding Loss Window : 0.7226862721462076\n",
      "Step 2351 | Sliding Loss Window : 0.7124727201424262\n",
      "Step 2401 | Sliding Loss Window : 0.7300251491958124\n",
      "Step 2451 | Sliding Loss Window : 0.8174873154259514\n",
      "Step 2501 | Sliding Loss Window : 0.7051576401381203\n",
      "Step 2551 | Sliding Loss Window : 0.8142803434030383\n",
      "Step 2601 | Sliding Loss Window : 0.7532104918034672\n",
      "Step 2651 | Sliding Loss Window : 0.7350058685981615\n",
      "Step 2701 | Sliding Loss Window : 0.7112933054690807\n",
      "Step 2751 | Sliding Loss Window : 0.7984471270382524\n",
      "Step 2801 | Sliding Loss Window : 0.7928922870222369\n",
      "Step 2851 | Sliding Loss Window : 0.7568268625390131\n",
      "Step 2901 | Sliding Loss Window : 0.7208163135695979\n",
      "Step 2951 | Sliding Loss Window : 0.7025443203309062\n",
      "Step 3001 | Sliding Loss Window : 0.7963143307499068\n",
      "Step 3051 | Sliding Loss Window : 0.7078523122886056\n",
      "Step 3101 | Sliding Loss Window : 0.7391738659092988\n",
      "Step 3151 | Sliding Loss Window : 0.6915490726899955\n",
      "Step 3201 | Sliding Loss Window : 0.7469113237009875\n",
      "Step 3251 | Sliding Loss Window : 0.7767990015570746\n",
      "Step 3301 | Sliding Loss Window : 0.7833503583855355\n",
      "Step 3351 | Sliding Loss Window : 0.7972767842333296\n",
      "Step 3401 | Sliding Loss Window : 0.7366625254192921\n",
      "Step 3451 | Sliding Loss Window : 0.7097596377987988\n",
      "Step 3501 | Sliding Loss Window : 0.7248943005804622\n",
      "Step 3551 | Sliding Loss Window : 0.8275219529107177\n",
      "Step 3601 | Sliding Loss Window : 0.6841094396114403\n",
      "Step 3651 | Sliding Loss Window : 0.8290670905421794\n",
      "Step 3701 | Sliding Loss Window : 0.769348238209617\n",
      "Step 3751 | Sliding Loss Window : 0.698734007524085\n",
      "Step 3801 | Sliding Loss Window : 0.72315844604706\n",
      "Step 3851 | Sliding Loss Window : 0.8123266387578271\n",
      "Step 3901 | Sliding Loss Window : 0.7956752133200107\n",
      "Step 3951 | Sliding Loss Window : 0.7453962370338115\n",
      "Step 4001 | Sliding Loss Window : 0.7210689929601899\n",
      "Step 4051 | Sliding Loss Window : 0.7097721904172444\n",
      "Step 4101 | Sliding Loss Window : 0.7779104397923837\n",
      "Step 4151 | Sliding Loss Window : 0.6951740273883271\n",
      "Step 4201 | Sliding Loss Window : 0.7424339589827724\n",
      "Step 4251 | Sliding Loss Window : 0.7040857119417939\n",
      "Step 4301 | Sliding Loss Window : 0.7568255405476285\n",
      "Step 4351 | Sliding Loss Window : 0.7812213509261509\n",
      "Step 4401 | Sliding Loss Window : 0.7984850131926331\n",
      "Step 4451 | Sliding Loss Window : 0.7789580796707202\n",
      "Step 4501 | Sliding Loss Window : 0.7282270159918762\n",
      "Step 4551 | Sliding Loss Window : 0.6981866899613459\n",
      "Step 4601 | Sliding Loss Window : 0.7116856804450125\n",
      "Step 4651 | Sliding Loss Window : 0.8314067701892698\n",
      "Step 4701 | Sliding Loss Window : 0.7250673020490704\n",
      "Step 4751 | Sliding Loss Window : 0.8135063493564438\n",
      "Step 4801 | Sliding Loss Window : 0.7557809531910762\n",
      "Step 4851 | Sliding Loss Window : 0.7061428355358089\n",
      "Step 4901 | Sliding Loss Window : 0.7189854691570864\n",
      "Step 4951 | Sliding Loss Window : 0.8119816488215079\n",
      "Step 5001 | Sliding Loss Window : 0.8212833757647101\n",
      "Step 5051 | Sliding Loss Window : 0.7039906387265817\n",
      "Step 5101 | Sliding Loss Window : 0.7277041314342764\n",
      "Step 5151 | Sliding Loss Window : 0.7175544839145219\n",
      "Step 5201 | Sliding Loss Window : 0.7818711682826606\n",
      "Step 5251 | Sliding Loss Window : 0.7019149826071234\n",
      "Step 5301 | Sliding Loss Window : 0.7417695398609141\n",
      "Step 5351 | Sliding Loss Window : 0.7087835064983513\n",
      "Step 5401 | Sliding Loss Window : 0.7655619455786092\n",
      "Step 5451 | Sliding Loss Window : 0.7840137694039554\n",
      "Step 1 | Sliding Loss Window : 0.07707051719442093\n",
      "Step 51 | Sliding Loss Window : 0.8438738528572434\n",
      "Step 101 | Sliding Loss Window : 0.807246637429939\n",
      "Step 151 | Sliding Loss Window : 0.6974344634822595\n",
      "Step 201 | Sliding Loss Window : 0.7638761027840981\n",
      "Step 251 | Sliding Loss Window : 0.82695925787381\n",
      "Step 301 | Sliding Loss Window : 0.6949354374726129\n",
      "Step 351 | Sliding Loss Window : 0.7617501334368261\n",
      "Step 401 | Sliding Loss Window : 0.8042025104894764\n",
      "Step 451 | Sliding Loss Window : 0.6732666293126112\n",
      "Step 501 | Sliding Loss Window : 0.6349775607725828\n",
      "Step 551 | Sliding Loss Window : 0.6799833152777616\n",
      "Step 601 | Sliding Loss Window : 0.8786021177261177\n",
      "Step 651 | Sliding Loss Window : 0.8000330071114128\n",
      "Step 701 | Sliding Loss Window : 0.7291824871909134\n",
      "Step 751 | Sliding Loss Window : 0.6569247561735466\n",
      "Step 801 | Sliding Loss Window : 0.7512616285498521\n",
      "Step 851 | Sliding Loss Window : 0.759999456195536\n",
      "Step 901 | Sliding Loss Window : 0.604426569030638\n",
      "Step 951 | Sliding Loss Window : 0.7357136380200652\n",
      "Step 1001 | Sliding Loss Window : 0.8478614252723602\n",
      "Step 1051 | Sliding Loss Window : 0.7922492558723124\n",
      "Step 1101 | Sliding Loss Window : 0.8827766803483992\n",
      "Step 1151 | Sliding Loss Window : 0.7114125101131344\n",
      "Step 1201 | Sliding Loss Window : 0.7845714609539833\n",
      "Step 1251 | Sliding Loss Window : 0.7442623019655134\n",
      "Step 1301 | Sliding Loss Window : 0.7630657976501322\n",
      "Step 1351 | Sliding Loss Window : 0.8225853705437021\n",
      "Step 1401 | Sliding Loss Window : 0.689395500278772\n",
      "Step 1451 | Sliding Loss Window : 0.7748445010750434\n",
      "Step 1501 | Sliding Loss Window : 0.789346253994998\n",
      "Step 1551 | Sliding Loss Window : 0.6805963782323415\n",
      "Step 1601 | Sliding Loss Window : 0.6311153316716748\n",
      "Step 1651 | Sliding Loss Window : 0.7027311509991826\n",
      "Step 1701 | Sliding Loss Window : 0.8286442944001395\n",
      "Step 1751 | Sliding Loss Window : 0.8156318855740181\n",
      "Step 1801 | Sliding Loss Window : 0.7287940780934002\n",
      "Step 1851 | Sliding Loss Window : 0.6874323825638132\n",
      "Step 1901 | Sliding Loss Window : 0.7168423264038114\n",
      "Step 1951 | Sliding Loss Window : 0.7728506715901535\n",
      "Step 2001 | Sliding Loss Window : 0.6169758860039878\n",
      "Step 2051 | Sliding Loss Window : 0.736450807795402\n",
      "Step 2101 | Sliding Loss Window : 0.8136555181099555\n",
      "Step 2151 | Sliding Loss Window : 0.8155256544417778\n",
      "Step 2201 | Sliding Loss Window : 0.8632372722989679\n",
      "Step 2251 | Sliding Loss Window : 0.7286710784759033\n",
      "Step 2301 | Sliding Loss Window : 0.813960027087393\n",
      "Step 2351 | Sliding Loss Window : 0.7401759917484168\n",
      "Step 2401 | Sliding Loss Window : 0.7457828476666418\n",
      "Step 2451 | Sliding Loss Window : 0.8041807814725219\n",
      "Step 2501 | Sliding Loss Window : 0.7122752708263481\n",
      "Step 2551 | Sliding Loss Window : 0.7460411718808868\n",
      "Step 2601 | Sliding Loss Window : 0.8096485464347124\n",
      "Step 2651 | Sliding Loss Window : 0.6961068851072898\n",
      "Step 2701 | Sliding Loss Window : 0.5999180314709764\n",
      "Step 2751 | Sliding Loss Window : 0.6917759722714575\n",
      "Step 2801 | Sliding Loss Window : 0.8415554042380758\n",
      "Step 2851 | Sliding Loss Window : 0.8314327134694373\n",
      "Step 2901 | Sliding Loss Window : 0.7012123384688898\n",
      "Step 2951 | Sliding Loss Window : 0.7048263793453482\n",
      "Step 3001 | Sliding Loss Window : 0.7395686753745158\n",
      "Step 3051 | Sliding Loss Window : 0.7705898068983841\n",
      "Step 3101 | Sliding Loss Window : 0.6235064695221986\n",
      "Step 3151 | Sliding Loss Window : 0.7135467935720198\n",
      "Step 3201 | Sliding Loss Window : 0.8111851241538602\n",
      "Step 3251 | Sliding Loss Window : 0.8173947861258952\n",
      "Step 3301 | Sliding Loss Window : 0.8623481327022788\n",
      "Step 3351 | Sliding Loss Window : 0.7401192814875818\n",
      "Step 3401 | Sliding Loss Window : 0.8047515005359882\n",
      "Step 3451 | Sliding Loss Window : 0.7300722346297432\n",
      "Step 3501 | Sliding Loss Window : 0.7478128813154123\n",
      "Step 3551 | Sliding Loss Window : 0.7907557290889495\n",
      "Step 3601 | Sliding Loss Window : 0.7472397143451736\n",
      "Step 3651 | Sliding Loss Window : 0.750519355853817\n",
      "Step 3701 | Sliding Loss Window : 0.7849752034862557\n",
      "Step 3751 | Sliding Loss Window : 0.6806265637699813\n",
      "Step 3801 | Sliding Loss Window : 0.6056835356574656\n",
      "Step 3851 | Sliding Loss Window : 0.686666465085337\n",
      "Step 3901 | Sliding Loss Window : 0.8504121730421147\n",
      "Step 3951 | Sliding Loss Window : 0.8425019861459565\n",
      "Step 4001 | Sliding Loss Window : 0.6916884082244051\n",
      "Step 4051 | Sliding Loss Window : 0.7120226273853707\n",
      "Step 4101 | Sliding Loss Window : 0.7720173745426954\n",
      "Step 4151 | Sliding Loss Window : 0.7304798703946528\n",
      "Step 4201 | Sliding Loss Window : 0.624279074240905\n",
      "Step 4251 | Sliding Loss Window : 0.7406351669864977\n",
      "Step 4301 | Sliding Loss Window : 0.7879889441883567\n",
      "Step 4351 | Sliding Loss Window : 0.8160678418666639\n",
      "Step 4401 | Sliding Loss Window : 0.8620386677193832\n",
      "Step 4451 | Sliding Loss Window : 0.7305577610232667\n",
      "Step 4501 | Sliding Loss Window : 0.8252009984544998\n",
      "Step 4551 | Sliding Loss Window : 0.7274713313034394\n",
      "Step 4601 | Sliding Loss Window : 0.730839920325089\n",
      "Step 4651 | Sliding Loss Window : 0.811075535135067\n",
      "Step 4701 | Sliding Loss Window : 0.7283817986780751\n",
      "Step 4751 | Sliding Loss Window : 0.7627792672694841\n",
      "Step 4801 | Sliding Loss Window : 0.7946858290755914\n",
      "Step 4851 | Sliding Loss Window : 0.6728858019499444\n",
      "Step 4901 | Sliding Loss Window : 0.6306743341595035\n",
      "Step 4951 | Sliding Loss Window : 0.6812387116279388\n",
      "Step 5001 | Sliding Loss Window : 0.8213098198517556\n",
      "Step 5051 | Sliding Loss Window : 0.8540695342341711\n",
      "Step 5101 | Sliding Loss Window : 0.7054745452000142\n",
      "Step 5151 | Sliding Loss Window : 0.6961055885472496\n",
      "Step 5201 | Sliding Loss Window : 0.7692742981861663\n",
      "Step 5251 | Sliding Loss Window : 0.7580344332892686\n",
      "Step 5301 | Sliding Loss Window : 0.5999159825871935\n",
      "Step 5351 | Sliding Loss Window : 0.7293707122564244\n",
      "Step 5401 | Sliding Loss Window : 0.8246589944844761\n",
      "Step 5451 | Sliding Loss Window : 0.7863167138071772\n",
      "Step 1 | Sliding Loss Window : 1.9601423402716676\n",
      "Step 51 | Sliding Loss Window : 1.2716526751260664\n",
      "Step 101 | Sliding Loss Window : 1.2798338204806443\n",
      "Step 151 | Sliding Loss Window : 1.213214930934766\n",
      "Step 201 | Sliding Loss Window : 0.8244367931211842\n",
      "Step 251 | Sliding Loss Window : 0.8235728281601397\n",
      "Step 301 | Sliding Loss Window : 0.8350851042508297\n",
      "Step 351 | Sliding Loss Window : 0.8107321841046315\n",
      "Step 401 | Sliding Loss Window : 0.7856618094297362\n",
      "Step 451 | Sliding Loss Window : 0.7714686482685867\n",
      "Step 501 | Sliding Loss Window : 0.5965677744924769\n",
      "Step 551 | Sliding Loss Window : 0.7935249012659595\n",
      "Step 601 | Sliding Loss Window : 0.8473531201846503\n",
      "Step 651 | Sliding Loss Window : 0.8077166187127454\n",
      "Step 701 | Sliding Loss Window : 0.7692884967932353\n",
      "Step 751 | Sliding Loss Window : 0.7333690456027491\n",
      "Step 801 | Sliding Loss Window : 0.7579063090856343\n",
      "Step 851 | Sliding Loss Window : 0.6597122033361476\n",
      "Step 901 | Sliding Loss Window : 0.7711961905161394\n",
      "Step 951 | Sliding Loss Window : 0.7209293507018283\n",
      "Step 1001 | Sliding Loss Window : 0.6586843096608622\n",
      "Step 1051 | Sliding Loss Window : 0.7596738218426176\n",
      "Step 1101 | Sliding Loss Window : 0.8005671998931454\n",
      "Step 1151 | Sliding Loss Window : 0.7531511076415578\n",
      "Step 1201 | Sliding Loss Window : 0.6683764688628806\n",
      "Step 1251 | Sliding Loss Window : 0.721997707376121\n",
      "Step 1301 | Sliding Loss Window : 0.726492911484136\n",
      "Step 1351 | Sliding Loss Window : 0.8062298377109919\n",
      "Step 1401 | Sliding Loss Window : 0.8301736719624476\n",
      "Step 1451 | Sliding Loss Window : 0.7811774066016993\n",
      "Step 1501 | Sliding Loss Window : 0.8156088755583343\n",
      "Step 1551 | Sliding Loss Window : 0.72378525845686\n",
      "Step 1601 | Sliding Loss Window : 0.6179014827791145\n",
      "Step 1651 | Sliding Loss Window : 0.7790232821953366\n",
      "Step 1701 | Sliding Loss Window : 0.8702991595768848\n",
      "Step 1751 | Sliding Loss Window : 0.8171186170732446\n",
      "Step 1801 | Sliding Loss Window : 0.7574353941985765\n",
      "Step 1851 | Sliding Loss Window : 0.7368641258470334\n",
      "Step 1901 | Sliding Loss Window : 0.7398517056897519\n",
      "Step 1951 | Sliding Loss Window : 0.6683805390186948\n",
      "Step 2001 | Sliding Loss Window : 0.7824694880937614\n",
      "Step 2051 | Sliding Loss Window : 0.7124267911520472\n",
      "Step 2101 | Sliding Loss Window : 0.6613604418579483\n",
      "Step 2151 | Sliding Loss Window : 0.7536495096678373\n",
      "Step 2201 | Sliding Loss Window : 0.7875684347980514\n",
      "Step 2251 | Sliding Loss Window : 0.7560141387758432\n",
      "Step 2301 | Sliding Loss Window : 0.6697216387264746\n",
      "Step 2351 | Sliding Loss Window : 0.7420972988685886\n",
      "Step 2401 | Sliding Loss Window : 0.7295609470372321\n",
      "Step 2451 | Sliding Loss Window : 0.7744343327688094\n",
      "Step 2501 | Sliding Loss Window : 0.8441757046427422\n",
      "Step 2551 | Sliding Loss Window : 0.7951607903959435\n",
      "Step 2601 | Sliding Loss Window : 0.8024043657110337\n",
      "Step 2651 | Sliding Loss Window : 0.7383030295236174\n",
      "Step 2701 | Sliding Loss Window : 0.6124753076757338\n",
      "Step 2751 | Sliding Loss Window : 0.7668400127622115\n",
      "Step 2801 | Sliding Loss Window : 0.8758209167529865\n",
      "Step 2851 | Sliding Loss Window : 0.803804452731224\n",
      "Step 2901 | Sliding Loss Window : 0.7842247413283941\n",
      "Step 2951 | Sliding Loss Window : 0.7077868511798723\n",
      "Step 3001 | Sliding Loss Window : 0.7324634220385847\n",
      "Step 3051 | Sliding Loss Window : 0.6886718696292795\n",
      "Step 3101 | Sliding Loss Window : 0.7755593675666711\n",
      "Step 3151 | Sliding Loss Window : 0.7204796654581171\n",
      "Step 3201 | Sliding Loss Window : 0.666932238513711\n",
      "Step 3251 | Sliding Loss Window : 0.7325627791434259\n",
      "Step 3301 | Sliding Loss Window : 0.7838152240235814\n",
      "Step 3351 | Sliding Loss Window : 0.7974283938022725\n",
      "Step 3401 | Sliding Loss Window : 0.6627984730004681\n",
      "Step 3451 | Sliding Loss Window : 0.7248211470592416\n",
      "Step 3501 | Sliding Loss Window : 0.7532621773063957\n",
      "Step 3551 | Sliding Loss Window : 0.7518914365646279\n",
      "Step 3601 | Sliding Loss Window : 0.8342911780129363\n",
      "Step 3651 | Sliding Loss Window : 0.8120974096882354\n",
      "Step 3701 | Sliding Loss Window : 0.7963312502905818\n",
      "Step 3751 | Sliding Loss Window : 0.7338997778952318\n",
      "Step 3801 | Sliding Loss Window : 0.6369542102954265\n",
      "Step 3851 | Sliding Loss Window : 0.756966044304891\n",
      "Step 3901 | Sliding Loss Window : 0.8658502380767226\n",
      "Step 3951 | Sliding Loss Window : 0.7778835103768991\n",
      "Step 4001 | Sliding Loss Window : 0.7966640282188504\n",
      "Step 4051 | Sliding Loss Window : 0.7273175600245011\n",
      "Step 4101 | Sliding Loss Window : 0.7242789329314284\n",
      "Step 4151 | Sliding Loss Window : 0.7049707987679796\n",
      "Step 4201 | Sliding Loss Window : 0.7581322767540482\n",
      "Step 4251 | Sliding Loss Window : 0.7189564757990506\n",
      "Step 4301 | Sliding Loss Window : 0.6639933277394137\n",
      "Step 4351 | Sliding Loss Window : 0.734426539632357\n",
      "Step 4401 | Sliding Loss Window : 0.7958309236761746\n",
      "Step 4451 | Sliding Loss Window : 0.779253108287211\n",
      "Step 4501 | Sliding Loss Window : 0.6833523154530257\n",
      "Step 4551 | Sliding Loss Window : 0.7238927836426378\n",
      "Step 4601 | Sliding Loss Window : 0.720221642610309\n",
      "Step 4651 | Sliding Loss Window : 0.7762736126262108\n",
      "Step 4701 | Sliding Loss Window : 0.8429239487459494\n",
      "Step 4751 | Sliding Loss Window : 0.8068784789216003\n",
      "Step 4801 | Sliding Loss Window : 0.8052618452261348\n",
      "Step 4851 | Sliding Loss Window : 0.706315399215396\n",
      "Step 4901 | Sliding Loss Window : 0.6662836139183438\n",
      "Step 4951 | Sliding Loss Window : 0.7358583506447615\n",
      "Step 5001 | Sliding Loss Window : 0.8842336774725961\n",
      "Step 5051 | Sliding Loss Window : 0.7504170686346441\n",
      "Step 5101 | Sliding Loss Window : 0.8090986402937455\n",
      "Step 5151 | Sliding Loss Window : 0.7382992041746543\n",
      "Step 5201 | Sliding Loss Window : 0.7127416525507377\n",
      "Step 5251 | Sliding Loss Window : 0.7002991234635901\n",
      "Step 5301 | Sliding Loss Window : 0.7693347215760147\n",
      "Step 5351 | Sliding Loss Window : 0.7208954909247993\n",
      "Step 5401 | Sliding Loss Window : 0.6373628249802392\n",
      "Step 5451 | Sliding Loss Window : 0.7598517186276108\n",
      "Step 1 | Sliding Loss Window : 1.8705863729838699\n",
      "Step 51 | Sliding Loss Window : 1.0770942283538667\n",
      "Step 101 | Sliding Loss Window : 0.7683145183761024\n",
      "Step 151 | Sliding Loss Window : 0.6633635411352887\n",
      "Step 201 | Sliding Loss Window : 0.7755937948508318\n",
      "Step 251 | Sliding Loss Window : 0.8166217207005669\n",
      "Step 301 | Sliding Loss Window : 0.6050362619172722\n",
      "Step 351 | Sliding Loss Window : 0.7263392907933622\n",
      "Step 401 | Sliding Loss Window : 0.7416255628720476\n",
      "Step 451 | Sliding Loss Window : 0.7830394715846692\n",
      "Step 501 | Sliding Loss Window : 0.7821169969996579\n",
      "Step 551 | Sliding Loss Window : 0.8278811746432722\n",
      "Step 601 | Sliding Loss Window : 0.8566925730448858\n",
      "Step 651 | Sliding Loss Window : 0.7443083398726159\n",
      "Step 701 | Sliding Loss Window : 0.8071090893154136\n",
      "Step 751 | Sliding Loss Window : 0.7420326551753215\n",
      "Step 801 | Sliding Loss Window : 0.7482181418629485\n",
      "Step 851 | Sliding Loss Window : 0.7595285962425684\n",
      "Step 901 | Sliding Loss Window : 0.8646542332897524\n",
      "Step 951 | Sliding Loss Window : 0.6410346117939058\n",
      "Step 1001 | Sliding Loss Window : 0.7226018758447704\n",
      "Step 1051 | Sliding Loss Window : 0.6451127638587473\n",
      "Step 1101 | Sliding Loss Window : 0.7319731309067872\n",
      "Step 1151 | Sliding Loss Window : 0.7838265764056714\n",
      "Step 1201 | Sliding Loss Window : 0.8251507166057854\n",
      "Step 1251 | Sliding Loss Window : 0.6437286204676532\n",
      "Step 1301 | Sliding Loss Window : 0.7400230404304751\n",
      "Step 1351 | Sliding Loss Window : 0.784922488868218\n",
      "Step 1401 | Sliding Loss Window : 0.6348377430005242\n",
      "Step 1451 | Sliding Loss Window : 0.7093112877269943\n",
      "Step 1501 | Sliding Loss Window : 0.7333863789778097\n",
      "Step 1551 | Sliding Loss Window : 0.7817325266969484\n",
      "Step 1601 | Sliding Loss Window : 0.8227440013615245\n",
      "Step 1651 | Sliding Loss Window : 0.7992590219281124\n",
      "Step 1701 | Sliding Loss Window : 0.8327291337177556\n",
      "Step 1751 | Sliding Loss Window : 0.7717618376399316\n",
      "Step 1801 | Sliding Loss Window : 0.8134608695331298\n",
      "Step 1851 | Sliding Loss Window : 0.7282825336281938\n",
      "Step 1901 | Sliding Loss Window : 0.7608778442008706\n",
      "Step 1951 | Sliding Loss Window : 0.7341088450514264\n",
      "Step 2001 | Sliding Loss Window : 0.861917217049179\n",
      "Step 2051 | Sliding Loss Window : 0.6660734487505927\n",
      "Step 2101 | Sliding Loss Window : 0.7263197338393422\n",
      "Step 2151 | Sliding Loss Window : 0.6154497603413455\n",
      "Step 2201 | Sliding Loss Window : 0.7623545151600459\n",
      "Step 2251 | Sliding Loss Window : 0.783369622616794\n",
      "Step 2301 | Sliding Loss Window : 0.8342040660619644\n",
      "Step 2351 | Sliding Loss Window : 0.6352057413464153\n",
      "Step 2401 | Sliding Loss Window : 0.7490152531223121\n",
      "Step 2451 | Sliding Loss Window : 0.7538250577440199\n",
      "Step 2501 | Sliding Loss Window : 0.6208332516357357\n",
      "Step 2551 | Sliding Loss Window : 0.7445433590146877\n",
      "Step 2601 | Sliding Loss Window : 0.7250296633437208\n",
      "Step 2651 | Sliding Loss Window : 0.7999297698387982\n",
      "Step 2701 | Sliding Loss Window : 0.7959423402787766\n",
      "Step 2751 | Sliding Loss Window : 0.8138350907851485\n",
      "Step 2801 | Sliding Loss Window : 0.8227076029023452\n",
      "Step 2851 | Sliding Loss Window : 0.7910375763175541\n",
      "Step 2901 | Sliding Loss Window : 0.8049432325158423\n",
      "Step 2951 | Sliding Loss Window : 0.7269232962749874\n",
      "Step 3001 | Sliding Loss Window : 0.7292043075044029\n",
      "Step 3051 | Sliding Loss Window : 0.7497280781005058\n",
      "Step 3101 | Sliding Loss Window : 0.8615236824318322\n",
      "Step 3151 | Sliding Loss Window : 0.6834126565620652\n",
      "Step 3201 | Sliding Loss Window : 0.7160681574505939\n",
      "Step 3251 | Sliding Loss Window : 0.6429407782339045\n",
      "Step 3301 | Sliding Loss Window : 0.7498649947439722\n",
      "Step 3351 | Sliding Loss Window : 0.771226492930227\n",
      "Step 3401 | Sliding Loss Window : 0.8634306485003747\n",
      "Step 3451 | Sliding Loss Window : 0.5866499027200788\n",
      "Step 3501 | Sliding Loss Window : 0.770684978149692\n",
      "Step 3551 | Sliding Loss Window : 0.7612788807113273\n",
      "Step 3601 | Sliding Loss Window : 0.6124441517218913\n",
      "Step 3651 | Sliding Loss Window : 0.7229126058967092\n",
      "Step 3701 | Sliding Loss Window : 0.7381046506872692\n",
      "Step 3751 | Sliding Loss Window : 0.7856212225332125\n",
      "Step 3801 | Sliding Loss Window : 0.8236465595631904\n",
      "Step 3851 | Sliding Loss Window : 0.795828070604461\n",
      "Step 3901 | Sliding Loss Window : 0.8155219310111588\n",
      "Step 3951 | Sliding Loss Window : 0.8023308806011322\n",
      "Step 4001 | Sliding Loss Window : 0.7992876800177231\n",
      "Step 4051 | Sliding Loss Window : 0.7644069456560264\n",
      "Step 4101 | Sliding Loss Window : 0.7223933324707938\n",
      "Step 4151 | Sliding Loss Window : 0.7607414351759363\n",
      "Step 4201 | Sliding Loss Window : 0.8313926457781973\n",
      "Step 4251 | Sliding Loss Window : 0.7110711246124306\n",
      "Step 4301 | Sliding Loss Window : 0.673694385737748\n",
      "Step 4351 | Sliding Loss Window : 0.65416349618731\n",
      "Step 4401 | Sliding Loss Window : 0.7544211736466165\n",
      "Step 4451 | Sliding Loss Window : 0.7633670842242277\n",
      "Step 4501 | Sliding Loss Window : 0.8616270698664437\n",
      "Step 4551 | Sliding Loss Window : 0.6032472907937385\n",
      "Step 4601 | Sliding Loss Window : 0.7625377116029571\n",
      "Step 4651 | Sliding Loss Window : 0.782881912793322\n",
      "Step 4701 | Sliding Loss Window : 0.6207560418743691\n",
      "Step 4751 | Sliding Loss Window : 0.6885177373812295\n",
      "Step 4801 | Sliding Loss Window : 0.75057422111267\n",
      "Step 4851 | Sliding Loss Window : 0.7633767464469037\n",
      "Step 4901 | Sliding Loss Window : 0.8438271420909109\n",
      "Step 4951 | Sliding Loss Window : 0.8180108412959594\n",
      "Step 5001 | Sliding Loss Window : 0.7982822763768227\n",
      "Step 5051 | Sliding Loss Window : 0.7836084432020517\n",
      "Step 5101 | Sliding Loss Window : 0.8043702928559656\n",
      "Step 5151 | Sliding Loss Window : 0.7602571087924826\n",
      "Step 5201 | Sliding Loss Window : 0.7229246133708581\n",
      "Step 5251 | Sliding Loss Window : 0.7522812187674295\n",
      "Step 5301 | Sliding Loss Window : 0.8597282718956049\n",
      "Step 5351 | Sliding Loss Window : 0.7039394988523497\n",
      "Step 5401 | Sliding Loss Window : 0.6655056326063815\n",
      "Step 5451 | Sliding Loss Window : 0.6635010417412278\n",
      "Step 1 | Sliding Loss Window : 1.993514921128718\n",
      "Step 51 | Sliding Loss Window : 1.2460671222755313\n",
      "Step 101 | Sliding Loss Window : 1.1293736740363962\n",
      "Step 151 | Sliding Loss Window : 1.022323084952073\n",
      "Step 201 | Sliding Loss Window : 0.7970652594236618\n",
      "Step 251 | Sliding Loss Window : 0.8117854066118185\n",
      "Step 301 | Sliding Loss Window : 0.6975395659294236\n",
      "Step 351 | Sliding Loss Window : 0.7294401345055348\n",
      "Step 401 | Sliding Loss Window : 0.654359619093678\n",
      "Step 451 | Sliding Loss Window : 0.7405101374908262\n",
      "Step 501 | Sliding Loss Window : 0.7318005602011423\n",
      "Step 551 | Sliding Loss Window : 0.7437520287442786\n",
      "Step 601 | Sliding Loss Window : 0.8167906102768785\n",
      "Step 651 | Sliding Loss Window : 0.6521565197743707\n",
      "Step 701 | Sliding Loss Window : 0.7675557778427324\n",
      "Step 751 | Sliding Loss Window : 0.6872372916669309\n",
      "Step 801 | Sliding Loss Window : 0.7943386243149004\n",
      "Step 851 | Sliding Loss Window : 0.8627206943309628\n",
      "Step 901 | Sliding Loss Window : 0.8165927154590241\n",
      "Step 951 | Sliding Loss Window : 0.7891149218322581\n",
      "Step 1001 | Sliding Loss Window : 0.6924072041419901\n",
      "Step 1051 | Sliding Loss Window : 0.8390493446330598\n",
      "Step 1101 | Sliding Loss Window : 0.7799839633853878\n",
      "Step 1151 | Sliding Loss Window : 0.7142819169542336\n",
      "Step 1201 | Sliding Loss Window : 0.8311946626870838\n",
      "Step 1251 | Sliding Loss Window : 0.7991827892475313\n",
      "Step 1301 | Sliding Loss Window : 0.6357368573663941\n",
      "Step 1351 | Sliding Loss Window : 0.78763386558641\n",
      "Step 1401 | Sliding Loss Window : 0.685739103253097\n",
      "Step 1451 | Sliding Loss Window : 0.7415241057386923\n",
      "Step 1501 | Sliding Loss Window : 0.6535266467910549\n",
      "Step 1551 | Sliding Loss Window : 0.7308472409643944\n",
      "Step 1601 | Sliding Loss Window : 0.7346738917496097\n",
      "Step 1651 | Sliding Loss Window : 0.7400816963501896\n",
      "Step 1701 | Sliding Loss Window : 0.8343352260920242\n",
      "Step 1751 | Sliding Loss Window : 0.6682923450728169\n",
      "Step 1801 | Sliding Loss Window : 0.736125233163969\n",
      "Step 1851 | Sliding Loss Window : 0.691537507380689\n",
      "Step 1901 | Sliding Loss Window : 0.7887822526783694\n",
      "Step 1951 | Sliding Loss Window : 0.8838014333933178\n",
      "Step 2001 | Sliding Loss Window : 0.8174350630337027\n",
      "Step 2051 | Sliding Loss Window : 0.7754193808444099\n",
      "Step 2101 | Sliding Loss Window : 0.6869745986424725\n",
      "Step 2151 | Sliding Loss Window : 0.8323984163369309\n",
      "Step 2201 | Sliding Loss Window : 0.7769711741045613\n",
      "Step 2251 | Sliding Loss Window : 0.7500446422367285\n",
      "Step 2301 | Sliding Loss Window : 0.8359980082457787\n",
      "Step 2351 | Sliding Loss Window : 0.7616616641140505\n",
      "Step 2401 | Sliding Loss Window : 0.6647021967649256\n",
      "Step 2451 | Sliding Loss Window : 0.7922396217755838\n",
      "Step 2501 | Sliding Loss Window : 0.6453583812457856\n",
      "Step 2551 | Sliding Loss Window : 0.7600757716883092\n",
      "Step 2601 | Sliding Loss Window : 0.6480801699904571\n",
      "Step 2651 | Sliding Loss Window : 0.7484089917651405\n",
      "Step 2701 | Sliding Loss Window : 0.7253498718604411\n",
      "Step 2751 | Sliding Loss Window : 0.7568579999781133\n",
      "Step 2801 | Sliding Loss Window : 0.8075260695035925\n",
      "Step 2851 | Sliding Loss Window : 0.6856646179675766\n",
      "Step 2901 | Sliding Loss Window : 0.7405186575824391\n",
      "Step 2951 | Sliding Loss Window : 0.6963536565010285\n",
      "Step 3001 | Sliding Loss Window : 0.7722401015821742\n",
      "Step 3051 | Sliding Loss Window : 0.8709962733694965\n",
      "Step 3101 | Sliding Loss Window : 0.8503165097759622\n",
      "Step 3151 | Sliding Loss Window : 0.7393800019086082\n",
      "Step 3201 | Sliding Loss Window : 0.7003661821430479\n",
      "Step 3251 | Sliding Loss Window : 0.8375859257403034\n",
      "Step 3301 | Sliding Loss Window : 0.7525649643109403\n",
      "Step 3351 | Sliding Loss Window : 0.7612374023432972\n",
      "Step 3401 | Sliding Loss Window : 0.8275690578622794\n",
      "Step 3451 | Sliding Loss Window : 0.7595454242665027\n",
      "Step 3501 | Sliding Loss Window : 0.7095369569882547\n",
      "Step 3551 | Sliding Loss Window : 0.7606363556587342\n",
      "Step 3601 | Sliding Loss Window : 0.6494490599044663\n",
      "Step 3651 | Sliding Loss Window : 0.7571268582058543\n",
      "Step 3701 | Sliding Loss Window : 0.6574081311230302\n",
      "Step 3751 | Sliding Loss Window : 0.7377615722499047\n",
      "Step 3801 | Sliding Loss Window : 0.7506591573628656\n",
      "Step 3851 | Sliding Loss Window : 0.7428847965432595\n",
      "Step 3901 | Sliding Loss Window : 0.8059046633110823\n",
      "Step 3951 | Sliding Loss Window : 0.6704057261890704\n",
      "Step 4001 | Sliding Loss Window : 0.7427299827197779\n",
      "Step 4051 | Sliding Loss Window : 0.7122271244745251\n",
      "Step 4101 | Sliding Loss Window : 0.7610420422549167\n",
      "Step 4151 | Sliding Loss Window : 0.8944824859163317\n",
      "Step 4201 | Sliding Loss Window : 0.8340093026571345\n",
      "Step 4251 | Sliding Loss Window : 0.7564263796150463\n",
      "Step 4301 | Sliding Loss Window : 0.6639659360446664\n",
      "Step 4351 | Sliding Loss Window : 0.8744415347027189\n",
      "Step 4401 | Sliding Loss Window : 0.7307210915956637\n",
      "Step 4451 | Sliding Loss Window : 0.7882491459471456\n",
      "Step 4501 | Sliding Loss Window : 0.8155676990103907\n",
      "Step 4551 | Sliding Loss Window : 0.7397338894993734\n",
      "Step 4601 | Sliding Loss Window : 0.7198868556630582\n",
      "Step 4651 | Sliding Loss Window : 0.7495042261813731\n",
      "Step 4701 | Sliding Loss Window : 0.682853923415444\n",
      "Step 4751 | Sliding Loss Window : 0.747715131896698\n",
      "Step 4801 | Sliding Loss Window : 0.6364950819528002\n",
      "Step 4851 | Sliding Loss Window : 0.7529019394463502\n",
      "Step 4901 | Sliding Loss Window : 0.7181328676525447\n",
      "Step 4951 | Sliding Loss Window : 0.7542191851988796\n",
      "Step 5001 | Sliding Loss Window : 0.8050848151905816\n",
      "Step 5051 | Sliding Loss Window : 0.6704944747304756\n",
      "Step 5101 | Sliding Loss Window : 0.7738857157316906\n",
      "Step 5151 | Sliding Loss Window : 0.6940776024727673\n",
      "Step 5201 | Sliding Loss Window : 0.7883011824747687\n",
      "Step 5251 | Sliding Loss Window : 0.8654875171837719\n",
      "Step 5301 | Sliding Loss Window : 0.8449009472304587\n",
      "Step 5351 | Sliding Loss Window : 0.7760998645148979\n",
      "Step 5401 | Sliding Loss Window : 0.6325136750825424\n",
      "Step 5451 | Sliding Loss Window : 0.8740498590141393\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.12\n",
      "Step 101 | Sliding Loss Window : 1.0\n",
      "Step 151 | Sliding Loss Window : 1.04\n",
      "Step 201 | Sliding Loss Window : 0.76\n",
      "Step 251 | Sliding Loss Window : 1.0\n",
      "Step 301 | Sliding Loss Window : 0.96\n",
      "Step 351 | Sliding Loss Window : 0.96\n",
      "Step 401 | Sliding Loss Window : 0.88\n",
      "Step 451 | Sliding Loss Window : 0.96\n",
      "Step 501 | Sliding Loss Window : 1.0\n",
      "Step 551 | Sliding Loss Window : 1.32\n",
      "Step 601 | Sliding Loss Window : 1.0\n",
      "Step 651 | Sliding Loss Window : 1.04\n",
      "Step 701 | Sliding Loss Window : 1.2\n",
      "Step 751 | Sliding Loss Window : 0.88\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 1.04\n",
      "Step 901 | Sliding Loss Window : 1.04\n",
      "Step 951 | Sliding Loss Window : 0.92\n",
      "Step 1001 | Sliding Loss Window : 0.92\n",
      "Step 1051 | Sliding Loss Window : 1.16\n",
      "Step 1101 | Sliding Loss Window : 0.96\n",
      "Step 1151 | Sliding Loss Window : 1.12\n",
      "Step 1201 | Sliding Loss Window : 1.04\n",
      "Step 1251 | Sliding Loss Window : 1.0\n",
      "Step 1301 | Sliding Loss Window : 0.76\n",
      "Step 1351 | Sliding Loss Window : 1.0\n",
      "Step 1401 | Sliding Loss Window : 0.96\n",
      "Step 1451 | Sliding Loss Window : 0.92\n",
      "Step 1501 | Sliding Loss Window : 0.92\n",
      "Step 1551 | Sliding Loss Window : 0.96\n",
      "Step 1601 | Sliding Loss Window : 1.04\n",
      "Step 1651 | Sliding Loss Window : 1.28\n",
      "Step 1701 | Sliding Loss Window : 1.04\n",
      "Step 1751 | Sliding Loss Window : 1.04\n",
      "Step 1801 | Sliding Loss Window : 1.2\n",
      "Step 1851 | Sliding Loss Window : 0.88\n",
      "Step 1901 | Sliding Loss Window : 0.76\n",
      "Step 1951 | Sliding Loss Window : 1.04\n",
      "Step 2001 | Sliding Loss Window : 1.08\n",
      "Step 2051 | Sliding Loss Window : 0.92\n",
      "Step 2101 | Sliding Loss Window : 0.96\n",
      "Step 2151 | Sliding Loss Window : 1.12\n",
      "Step 2201 | Sliding Loss Window : 1.0\n",
      "Step 2251 | Sliding Loss Window : 1.08\n",
      "Step 2301 | Sliding Loss Window : 1.08\n",
      "Step 2351 | Sliding Loss Window : 1.0\n",
      "Step 2401 | Sliding Loss Window : 0.68\n",
      "Step 2451 | Sliding Loss Window : 1.0\n",
      "Step 2501 | Sliding Loss Window : 0.96\n",
      "Step 2551 | Sliding Loss Window : 0.92\n",
      "Step 2601 | Sliding Loss Window : 1.0\n",
      "Step 2651 | Sliding Loss Window : 0.88\n",
      "Step 2701 | Sliding Loss Window : 1.12\n",
      "Step 2751 | Sliding Loss Window : 1.2\n",
      "Step 2801 | Sliding Loss Window : 1.08\n",
      "Step 2851 | Sliding Loss Window : 1.08\n",
      "Step 2901 | Sliding Loss Window : 1.12\n",
      "Step 2951 | Sliding Loss Window : 0.92\n",
      "Step 3001 | Sliding Loss Window : 0.8\n",
      "Step 3051 | Sliding Loss Window : 1.0\n",
      "Step 3101 | Sliding Loss Window : 1.12\n",
      "Step 3151 | Sliding Loss Window : 0.84\n",
      "Step 3201 | Sliding Loss Window : 1.04\n",
      "Step 3251 | Sliding Loss Window : 1.08\n",
      "Step 3301 | Sliding Loss Window : 1.04\n",
      "Step 3351 | Sliding Loss Window : 1.04\n",
      "Step 3401 | Sliding Loss Window : 1.12\n",
      "Step 3451 | Sliding Loss Window : 0.96\n",
      "Step 3501 | Sliding Loss Window : 0.72\n",
      "Step 3551 | Sliding Loss Window : 0.96\n",
      "Step 3601 | Sliding Loss Window : 0.92\n",
      "Step 3651 | Sliding Loss Window : 1.0\n",
      "Step 3701 | Sliding Loss Window : 0.96\n",
      "Step 3751 | Sliding Loss Window : 0.92\n",
      "Step 3801 | Sliding Loss Window : 1.08\n",
      "Step 3851 | Sliding Loss Window : 1.2\n",
      "Step 3901 | Sliding Loss Window : 1.08\n",
      "Step 3951 | Sliding Loss Window : 1.08\n",
      "Step 4001 | Sliding Loss Window : 1.12\n",
      "Step 4051 | Sliding Loss Window : 0.88\n",
      "Step 4101 | Sliding Loss Window : 0.84\n",
      "Step 4151 | Sliding Loss Window : 1.0\n",
      "Step 4201 | Sliding Loss Window : 1.08\n",
      "Step 4251 | Sliding Loss Window : 0.84\n",
      "Step 4301 | Sliding Loss Window : 1.08\n",
      "Step 4351 | Sliding Loss Window : 1.04\n",
      "Step 4401 | Sliding Loss Window : 1.08\n",
      "Step 4451 | Sliding Loss Window : 1.08\n",
      "Step 4501 | Sliding Loss Window : 1.12\n",
      "Step 4551 | Sliding Loss Window : 0.92\n",
      "Step 4601 | Sliding Loss Window : 0.72\n",
      "Step 4651 | Sliding Loss Window : 0.96\n",
      "Step 4701 | Sliding Loss Window : 0.92\n",
      "Step 4751 | Sliding Loss Window : 0.96\n",
      "Step 4801 | Sliding Loss Window : 1.0\n",
      "Step 4851 | Sliding Loss Window : 0.92\n",
      "Step 4901 | Sliding Loss Window : 1.04\n",
      "Step 4951 | Sliding Loss Window : 1.24\n",
      "Step 5001 | Sliding Loss Window : 1.08\n",
      "Step 5051 | Sliding Loss Window : 1.12\n",
      "Step 5101 | Sliding Loss Window : 1.04\n",
      "Step 5151 | Sliding Loss Window : 0.88\n",
      "Step 5201 | Sliding Loss Window : 0.92\n",
      "Step 5251 | Sliding Loss Window : 0.92\n",
      "Step 5301 | Sliding Loss Window : 1.16\n",
      "Step 5351 | Sliding Loss Window : 0.84\n",
      "Step 5401 | Sliding Loss Window : 1.04\n",
      "Step 5451 | Sliding Loss Window : 1.04\n",
      "Step 1 | Sliding Loss Window : 1.1102230246251565e-16\n",
      "Step 51 | Sliding Loss Window : 0.8\n",
      "Step 101 | Sliding Loss Window : 1.16\n",
      "Step 151 | Sliding Loss Window : 1.0\n",
      "Step 201 | Sliding Loss Window : 1.16\n",
      "Step 251 | Sliding Loss Window : 0.8\n",
      "Step 301 | Sliding Loss Window : 1.04\n",
      "Step 351 | Sliding Loss Window : 1.12\n",
      "Step 401 | Sliding Loss Window : 1.16\n",
      "Step 451 | Sliding Loss Window : 1.12\n",
      "Step 501 | Sliding Loss Window : 1.12\n",
      "Step 551 | Sliding Loss Window : 0.96\n",
      "Step 601 | Sliding Loss Window : 0.68\n",
      "Step 651 | Sliding Loss Window : 0.96\n",
      "Step 701 | Sliding Loss Window : 0.92\n",
      "Step 751 | Sliding Loss Window : 1.12\n",
      "Step 801 | Sliding Loss Window : 1.04\n",
      "Step 851 | Sliding Loss Window : 0.8\n",
      "Step 901 | Sliding Loss Window : 1.16\n",
      "Step 951 | Sliding Loss Window : 0.92\n",
      "Step 1001 | Sliding Loss Window : 0.92\n",
      "Step 1051 | Sliding Loss Window : 1.04\n",
      "Step 1101 | Sliding Loss Window : 0.96\n",
      "Step 1151 | Sliding Loss Window : 0.88\n",
      "Step 1201 | Sliding Loss Window : 1.16\n",
      "Step 1251 | Sliding Loss Window : 1.0\n",
      "Step 1301 | Sliding Loss Window : 1.16\n",
      "Step 1351 | Sliding Loss Window : 0.76\n",
      "Step 1401 | Sliding Loss Window : 1.04\n",
      "Step 1451 | Sliding Loss Window : 1.12\n",
      "Step 1501 | Sliding Loss Window : 1.16\n",
      "Step 1551 | Sliding Loss Window : 1.12\n",
      "Step 1601 | Sliding Loss Window : 1.12\n",
      "Step 1651 | Sliding Loss Window : 1.0\n",
      "Step 1701 | Sliding Loss Window : 0.64\n",
      "Step 1751 | Sliding Loss Window : 1.0\n",
      "Step 1801 | Sliding Loss Window : 0.88\n",
      "Step 1851 | Sliding Loss Window : 1.12\n",
      "Step 1901 | Sliding Loss Window : 1.04\n",
      "Step 1951 | Sliding Loss Window : 0.8\n",
      "Step 2001 | Sliding Loss Window : 1.2\n",
      "Step 2051 | Sliding Loss Window : 0.88\n",
      "Step 2101 | Sliding Loss Window : 0.96\n",
      "Step 2151 | Sliding Loss Window : 1.0\n",
      "Step 2201 | Sliding Loss Window : 0.96\n",
      "Step 2251 | Sliding Loss Window : 0.92\n",
      "Step 2301 | Sliding Loss Window : 1.12\n",
      "Step 2351 | Sliding Loss Window : 0.96\n",
      "Step 2401 | Sliding Loss Window : 1.16\n",
      "Step 2451 | Sliding Loss Window : 0.84\n",
      "Step 2501 | Sliding Loss Window : 1.04\n",
      "Step 2551 | Sliding Loss Window : 1.08\n",
      "Step 2601 | Sliding Loss Window : 1.12\n",
      "Step 2651 | Sliding Loss Window : 1.16\n",
      "Step 2701 | Sliding Loss Window : 1.12\n",
      "Step 2751 | Sliding Loss Window : 0.96\n",
      "Step 2801 | Sliding Loss Window : 0.64\n",
      "Step 2851 | Sliding Loss Window : 1.08\n",
      "Step 2901 | Sliding Loss Window : 0.84\n",
      "Step 2951 | Sliding Loss Window : 1.12\n",
      "Step 3001 | Sliding Loss Window : 1.0\n",
      "Step 3051 | Sliding Loss Window : 0.84\n",
      "Step 3101 | Sliding Loss Window : 1.2\n",
      "Step 3151 | Sliding Loss Window : 0.88\n",
      "Step 3201 | Sliding Loss Window : 0.92\n",
      "Step 3251 | Sliding Loss Window : 1.08\n",
      "Step 3301 | Sliding Loss Window : 0.92\n",
      "Step 3351 | Sliding Loss Window : 0.96\n",
      "Step 3401 | Sliding Loss Window : 1.04\n",
      "Step 3451 | Sliding Loss Window : 1.0\n",
      "Step 3501 | Sliding Loss Window : 1.16\n",
      "Step 3551 | Sliding Loss Window : 0.8\n",
      "Step 3601 | Sliding Loss Window : 1.08\n",
      "Step 3651 | Sliding Loss Window : 1.12\n",
      "Step 3701 | Sliding Loss Window : 1.08\n",
      "Step 3751 | Sliding Loss Window : 1.2\n",
      "Step 3801 | Sliding Loss Window : 1.04\n",
      "Step 3851 | Sliding Loss Window : 0.96\n",
      "Step 3901 | Sliding Loss Window : 0.64\n",
      "Step 3951 | Sliding Loss Window : 1.12\n",
      "Step 4001 | Sliding Loss Window : 0.88\n",
      "Step 4051 | Sliding Loss Window : 1.12\n",
      "Step 4101 | Sliding Loss Window : 0.92\n",
      "Step 4151 | Sliding Loss Window : 0.84\n",
      "Step 4201 | Sliding Loss Window : 1.24\n",
      "Step 4251 | Sliding Loss Window : 0.88\n",
      "Step 4301 | Sliding Loss Window : 0.88\n",
      "Step 4351 | Sliding Loss Window : 1.16\n",
      "Step 4401 | Sliding Loss Window : 0.88\n",
      "Step 4451 | Sliding Loss Window : 0.92\n",
      "Step 4501 | Sliding Loss Window : 1.04\n",
      "Step 4551 | Sliding Loss Window : 1.0\n",
      "Step 4601 | Sliding Loss Window : 1.2\n",
      "Step 4651 | Sliding Loss Window : 0.84\n",
      "Step 4701 | Sliding Loss Window : 1.04\n",
      "Step 4751 | Sliding Loss Window : 1.16\n",
      "Step 4801 | Sliding Loss Window : 1.0\n",
      "Step 4851 | Sliding Loss Window : 1.2\n",
      "Step 4901 | Sliding Loss Window : 1.08\n",
      "Step 4951 | Sliding Loss Window : 0.96\n",
      "Step 5001 | Sliding Loss Window : 0.64\n",
      "Step 5051 | Sliding Loss Window : 1.08\n",
      "Step 5101 | Sliding Loss Window : 0.88\n",
      "Step 5151 | Sliding Loss Window : 1.16\n",
      "Step 5201 | Sliding Loss Window : 0.92\n",
      "Step 5251 | Sliding Loss Window : 0.84\n",
      "Step 5301 | Sliding Loss Window : 1.24\n",
      "Step 5351 | Sliding Loss Window : 0.84\n",
      "Step 5401 | Sliding Loss Window : 0.96\n",
      "Step 5451 | Sliding Loss Window : 1.08\n",
      "Step 1 | Sliding Loss Window : 2.0\n",
      "Step 51 | Sliding Loss Window : 1.16\n",
      "Step 101 | Sliding Loss Window : 1.0\n",
      "Step 151 | Sliding Loss Window : 1.0\n",
      "Step 201 | Sliding Loss Window : 1.16\n",
      "Step 251 | Sliding Loss Window : 0.96\n",
      "Step 301 | Sliding Loss Window : 1.16\n",
      "Step 351 | Sliding Loss Window : 0.76\n",
      "Step 401 | Sliding Loss Window : 1.12\n",
      "Step 451 | Sliding Loss Window : 1.0\n",
      "Step 501 | Sliding Loss Window : 0.96\n",
      "Step 551 | Sliding Loss Window : 1.2\n",
      "Step 601 | Sliding Loss Window : 0.96\n",
      "Step 651 | Sliding Loss Window : 0.72\n",
      "Step 701 | Sliding Loss Window : 1.04\n",
      "Step 751 | Sliding Loss Window : 0.96\n",
      "Step 801 | Sliding Loss Window : 0.84\n",
      "Step 851 | Sliding Loss Window : 1.16\n",
      "Step 901 | Sliding Loss Window : 0.76\n",
      "Step 951 | Sliding Loss Window : 0.88\n",
      "Step 1001 | Sliding Loss Window : 0.92\n",
      "Step 1051 | Sliding Loss Window : 1.24\n",
      "Step 1101 | Sliding Loss Window : 1.04\n",
      "Step 1151 | Sliding Loss Window : 1.2\n",
      "Step 1201 | Sliding Loss Window : 0.96\n",
      "Step 1251 | Sliding Loss Window : 0.96\n",
      "Step 1301 | Sliding Loss Window : 1.2\n",
      "Step 1351 | Sliding Loss Window : 0.96\n",
      "Step 1401 | Sliding Loss Window : 1.2\n",
      "Step 1451 | Sliding Loss Window : 0.68\n",
      "Step 1501 | Sliding Loss Window : 1.2\n",
      "Step 1551 | Sliding Loss Window : 1.0\n",
      "Step 1601 | Sliding Loss Window : 0.96\n",
      "Step 1651 | Sliding Loss Window : 1.16\n",
      "Step 1701 | Sliding Loss Window : 0.96\n",
      "Step 1751 | Sliding Loss Window : 0.72\n",
      "Step 1801 | Sliding Loss Window : 1.0\n",
      "Step 1851 | Sliding Loss Window : 1.04\n",
      "Step 1901 | Sliding Loss Window : 0.76\n",
      "Step 1951 | Sliding Loss Window : 1.16\n",
      "Step 2001 | Sliding Loss Window : 0.76\n",
      "Step 2051 | Sliding Loss Window : 0.92\n",
      "Step 2101 | Sliding Loss Window : 0.96\n",
      "Step 2151 | Sliding Loss Window : 1.24\n",
      "Step 2201 | Sliding Loss Window : 0.96\n",
      "Step 2251 | Sliding Loss Window : 1.24\n",
      "Step 2301 | Sliding Loss Window : 0.96\n",
      "Step 2351 | Sliding Loss Window : 1.0\n",
      "Step 2401 | Sliding Loss Window : 1.16\n",
      "Step 2451 | Sliding Loss Window : 0.96\n",
      "Step 2501 | Sliding Loss Window : 1.2\n",
      "Step 2551 | Sliding Loss Window : 0.72\n",
      "Step 2601 | Sliding Loss Window : 1.16\n",
      "Step 2651 | Sliding Loss Window : 1.0\n",
      "Step 2701 | Sliding Loss Window : 1.0\n",
      "Step 2751 | Sliding Loss Window : 1.08\n",
      "Step 2801 | Sliding Loss Window : 1.0\n",
      "Step 2851 | Sliding Loss Window : 0.72\n",
      "Step 2901 | Sliding Loss Window : 1.0\n",
      "Step 2951 | Sliding Loss Window : 1.04\n",
      "Step 3001 | Sliding Loss Window : 0.72\n",
      "Step 3051 | Sliding Loss Window : 1.16\n",
      "Step 3101 | Sliding Loss Window : 0.8\n",
      "Step 3151 | Sliding Loss Window : 0.96\n",
      "Step 3201 | Sliding Loss Window : 0.96\n",
      "Step 3251 | Sliding Loss Window : 1.2\n",
      "Step 3301 | Sliding Loss Window : 0.92\n",
      "Step 3351 | Sliding Loss Window : 1.24\n",
      "Step 3401 | Sliding Loss Window : 0.96\n",
      "Step 3451 | Sliding Loss Window : 1.08\n",
      "Step 3501 | Sliding Loss Window : 1.16\n",
      "Step 3551 | Sliding Loss Window : 0.96\n",
      "Step 3601 | Sliding Loss Window : 1.2\n",
      "Step 3651 | Sliding Loss Window : 0.68\n",
      "Step 3701 | Sliding Loss Window : 1.16\n",
      "Step 3751 | Sliding Loss Window : 1.0\n",
      "Step 3801 | Sliding Loss Window : 0.96\n",
      "Step 3851 | Sliding Loss Window : 1.12\n",
      "Step 3901 | Sliding Loss Window : 0.96\n",
      "Step 3951 | Sliding Loss Window : 0.76\n",
      "Step 4001 | Sliding Loss Window : 0.96\n",
      "Step 4051 | Sliding Loss Window : 1.12\n",
      "Step 4101 | Sliding Loss Window : 0.72\n",
      "Step 4151 | Sliding Loss Window : 1.16\n",
      "Step 4201 | Sliding Loss Window : 0.76\n",
      "Step 4251 | Sliding Loss Window : 0.96\n",
      "Step 4301 | Sliding Loss Window : 0.92\n",
      "Step 4351 | Sliding Loss Window : 1.24\n",
      "Step 4401 | Sliding Loss Window : 0.92\n",
      "Step 4451 | Sliding Loss Window : 1.24\n",
      "Step 4501 | Sliding Loss Window : 0.96\n",
      "Step 4551 | Sliding Loss Window : 1.08\n",
      "Step 4601 | Sliding Loss Window : 1.16\n",
      "Step 4651 | Sliding Loss Window : 0.96\n",
      "Step 4701 | Sliding Loss Window : 1.2\n",
      "Step 4751 | Sliding Loss Window : 0.64\n",
      "Step 4801 | Sliding Loss Window : 1.2\n",
      "Step 4851 | Sliding Loss Window : 1.0\n",
      "Step 4901 | Sliding Loss Window : 1.0\n",
      "Step 4951 | Sliding Loss Window : 1.08\n",
      "Step 5001 | Sliding Loss Window : 1.0\n",
      "Step 5051 | Sliding Loss Window : 0.76\n",
      "Step 5101 | Sliding Loss Window : 0.92\n",
      "Step 5151 | Sliding Loss Window : 1.12\n",
      "Step 5201 | Sliding Loss Window : 0.72\n",
      "Step 5251 | Sliding Loss Window : 1.12\n",
      "Step 5301 | Sliding Loss Window : 0.76\n",
      "Step 5351 | Sliding Loss Window : 1.04\n",
      "Step 5401 | Sliding Loss Window : 0.88\n",
      "Step 5451 | Sliding Loss Window : 1.2\n",
      "Step 1 | Sliding Loss Window : 0.0\n",
      "Step 51 | Sliding Loss Window : 0.84\n",
      "Step 101 | Sliding Loss Window : 0.96\n",
      "Step 151 | Sliding Loss Window : 0.84\n",
      "Step 201 | Sliding Loss Window : 1.0\n",
      "Step 251 | Sliding Loss Window : 1.04\n",
      "Step 301 | Sliding Loss Window : 0.96\n",
      "Step 351 | Sliding Loss Window : 1.04\n",
      "Step 401 | Sliding Loss Window : 1.08\n",
      "Step 451 | Sliding Loss Window : 1.2\n",
      "Step 501 | Sliding Loss Window : 1.0\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from train_circ import mse_vec_loss\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset('bank', 'angle', 1)\n",
    "\n",
    "num_params = [4, 8, 12, 16, 20]\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 4\n",
    "\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "\n",
    "for curr_num_params in num_params:\n",
    "    curr_dir = './random/bank/{}_params'.format(curr_num_params)\n",
    "    \n",
    "    if not os.path.exists(curr_dir):\n",
    "        os.mkdir(curr_dir)\n",
    "        \n",
    "    for i in range(25):\n",
    "        curr_circ_dir = curr_dir + '/circ_{}'.format(i + 1)\n",
    "        \n",
    "        if not os.path.exists(curr_circ_dir):\n",
    "            os.mkdir(curr_circ_dir)\n",
    "        \n",
    "        ent_prob = np.random.sample()\n",
    "        cxz = np.random.sample()\n",
    "        pauli = 0\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = generate_true_random_gate_circ(num_qubits, num_embeds, curr_num_params, \n",
    "                                                                                                        ent_prob=ent_prob, \n",
    "                                                                                                        cxz_prob=cxz * ent_prob,\n",
    "                                                                                                        pauli_prob=pauli * (\n",
    "                                                                                                            1 - cxz) * ent_prob)\n",
    "        \n",
    "        np.savetxt(curr_circ_dir + '/gates.txt', circ_gates, fmt=\"%s\")\n",
    "        np.savetxt(curr_circ_dir + '/gate_params.txt', gate_params, fmt=\"%s\")\n",
    "        np.savetxt(curr_circ_dir + '/inputs_bounds.txt', inputs_bounds)\n",
    "        np.savetxt(curr_circ_dir + '/weights_bounds.txt', weights_bounds)\n",
    "        \n",
    "        circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                        weights_bounds, [0], 'exp', 'adjoint') \n",
    "\n",
    "        losses_list = []\n",
    "        accs_list = []\n",
    "        \n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_circ_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "            if os.path.exists(curr_train_dir):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir(curr_train_dir)\n",
    "                        \n",
    "            info = train_qnn(circ, x_train, y_train, x_test, y_test, [weights_bounds[-1]], 5490, 0.05, 1, mse_vec_loss, verbosity=17300, \n",
    "                                                                                            loss_window=50, init_params=None, \n",
    "                                                                                            acc_thres=1.1, shuffle=True, print_loss=50)\n",
    "\n",
    "            val_exps = np.array([circ(x_test[i], info[-1][-1]) for i in range(len(x_test))])\n",
    "            val_loss = np.array([mse_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "            acc = np.mean(val_loss < 1)\n",
    "\n",
    "            np.savetxt(curr_train_dir + '/params_{}.txt'.format(j + 1), info[-1])\n",
    "            np.savetxt(curr_train_dir + '/losses_{}.txt'.format(j + 1), info[0])\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        np.savetxt(curr_circ_dir + '/accs.txt', accs_list)\n",
    "        np.savetxt(curr_circ_dir + '/val_losses.txt', losses_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## human design circs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Sliding Loss Window : 3.0588197186315202\n",
      "Step 51 | Sliding Loss Window : 1.079412809106036\n",
      "Step 101 | Sliding Loss Window : 1.0744722567510052\n",
      "Step 151 | Sliding Loss Window : 1.0677947494489604\n",
      "Step 201 | Sliding Loss Window : 1.0088584282385293\n",
      "Step 251 | Sliding Loss Window : 1.099225500444243\n",
      "Step 301 | Sliding Loss Window : 1.0816312374563983\n",
      "Step 351 | Sliding Loss Window : 1.0863032230881258\n",
      "Step 401 | Sliding Loss Window : 1.078474401461576\n",
      "Step 451 | Sliding Loss Window : 1.0012389682431304\n",
      "Step 501 | Sliding Loss Window : 1.1031971630910262\n",
      "Step 551 | Sliding Loss Window : 1.0790290740519268\n",
      "Step 601 | Sliding Loss Window : 1.0619237020170198\n",
      "Step 651 | Sliding Loss Window : 1.0720120985717785\n",
      "Step 701 | Sliding Loss Window : 1.0476675839475285\n",
      "Step 751 | Sliding Loss Window : 1.0600579865558188\n",
      "Step 801 | Sliding Loss Window : 1.083496804578453\n",
      "Step 851 | Sliding Loss Window : 1.09967027736677\n",
      "Step 901 | Sliding Loss Window : 1.069039158393921\n",
      "Step 951 | Sliding Loss Window : 1.0509105102991072\n",
      "Step 1001 | Sliding Loss Window : 1.0767490920443665\n",
      "Step 1051 | Sliding Loss Window : 1.0433972086855157\n",
      "Step 1101 | Sliding Loss Window : 1.070683017624484\n",
      "Step 1151 | Sliding Loss Window : 1.074836456747162\n",
      "Step 1201 | Sliding Loss Window : 1.0227837661887644\n",
      "Step 1251 | Sliding Loss Window : 1.0804592681142875\n",
      "Step 1301 | Sliding Loss Window : 1.097575567015263\n",
      "Step 1351 | Sliding Loss Window : 1.0850382388959365\n",
      "Step 1401 | Sliding Loss Window : 1.086051070124402\n",
      "Step 1451 | Sliding Loss Window : 1.0419145356334152\n",
      "Step 1501 | Sliding Loss Window : 1.043158465289061\n",
      "Step 1551 | Sliding Loss Window : 1.060856362934445\n",
      "Step 1601 | Sliding Loss Window : 1.0837611960899702\n",
      "Step 1651 | Sliding Loss Window : 1.0407358208157433\n",
      "Step 1701 | Sliding Loss Window : 1.0790566760533553\n",
      "Step 1751 | Sliding Loss Window : 1.0797793497113628\n",
      "Step 1801 | Sliding Loss Window : 1.0861333414140804\n",
      "Step 1851 | Sliding Loss Window : 1.0685929194806911\n",
      "Step 1901 | Sliding Loss Window : 1.0490239922577784\n",
      "Step 1951 | Sliding Loss Window : 1.0765614680521818\n",
      "Step 2001 | Sliding Loss Window : 1.0390745529115584\n",
      "Step 2051 | Sliding Loss Window : 1.0715527688071023\n",
      "Step 2101 | Sliding Loss Window : 1.0886621538567978\n",
      "Step 2151 | Sliding Loss Window : 1.0228710979335371\n",
      "Step 2201 | Sliding Loss Window : 1.0909486164221562\n",
      "Step 2251 | Sliding Loss Window : 1.0793395543153566\n",
      "Step 2301 | Sliding Loss Window : 1.0742532025838771\n",
      "Step 2351 | Sliding Loss Window : 1.095474633817012\n",
      "Step 1 | Sliding Loss Window : 3.964575118173742\n",
      "Step 51 | Sliding Loss Window : 1.4387881493486083\n",
      "Step 101 | Sliding Loss Window : 1.0534486997337036\n",
      "Step 151 | Sliding Loss Window : 1.0750844499775263\n",
      "Step 201 | Sliding Loss Window : 1.0314301420854548\n",
      "Step 251 | Sliding Loss Window : 1.06571538125685\n",
      "Step 301 | Sliding Loss Window : 1.033512794241502\n",
      "Step 351 | Sliding Loss Window : 1.0795294140826721\n",
      "Step 401 | Sliding Loss Window : 1.0098091772038686\n",
      "Step 451 | Sliding Loss Window : 1.0465049778846456\n",
      "Step 501 | Sliding Loss Window : 1.0680605347938732\n",
      "Step 551 | Sliding Loss Window : 1.070949025673114\n",
      "Step 601 | Sliding Loss Window : 1.0492016193289053\n",
      "Step 651 | Sliding Loss Window : 1.0887264623377073\n",
      "Step 701 | Sliding Loss Window : 1.0117821776017386\n",
      "Step 751 | Sliding Loss Window : 1.009472551381057\n",
      "Step 801 | Sliding Loss Window : 1.0933374805026117\n",
      "Step 851 | Sliding Loss Window : 1.082117206306379\n",
      "Step 901 | Sliding Loss Window : 1.0098634561208055\n",
      "Step 951 | Sliding Loss Window : 1.043583958859165\n",
      "Step 1001 | Sliding Loss Window : 1.075496091801177\n",
      "Step 1051 | Sliding Loss Window : 1.052215411783603\n",
      "Step 1101 | Sliding Loss Window : 1.0753207300116252\n",
      "Step 1151 | Sliding Loss Window : 1.027450247855046\n",
      "Step 1201 | Sliding Loss Window : 1.0681807304672015\n",
      "Step 1251 | Sliding Loss Window : 1.049601783221336\n",
      "Step 1301 | Sliding Loss Window : 1.0558884774441413\n",
      "Step 1351 | Sliding Loss Window : 1.0531945358033594\n",
      "Step 1401 | Sliding Loss Window : 0.9938448145465318\n",
      "Step 1451 | Sliding Loss Window : 1.0979449597399078\n",
      "Step 1501 | Sliding Loss Window : 1.065079474510073\n",
      "Step 1551 | Sliding Loss Window : 1.0333631030239574\n",
      "Step 1601 | Sliding Loss Window : 1.095386623371497\n",
      "Step 1651 | Sliding Loss Window : 1.0024006234733145\n",
      "Step 1701 | Sliding Loss Window : 1.0595339152235694\n",
      "Step 1751 | Sliding Loss Window : 1.065673082915258\n",
      "Step 1801 | Sliding Loss Window : 1.0850601681875751\n",
      "Step 1851 | Sliding Loss Window : 0.9758790412262028\n",
      "Step 1901 | Sliding Loss Window : 1.0462236694870157\n",
      "Step 1951 | Sliding Loss Window : 1.0570775075091652\n",
      "Step 2001 | Sliding Loss Window : 1.1049282296754388\n",
      "Step 2051 | Sliding Loss Window : 1.06299495474131\n",
      "Step 2101 | Sliding Loss Window : 1.0030204670519787\n",
      "Step 2151 | Sliding Loss Window : 1.098920258637416\n",
      "Step 2201 | Sliding Loss Window : 1.0423125288124917\n",
      "Step 2251 | Sliding Loss Window : 1.061939633773589\n",
      "Step 2301 | Sliding Loss Window : 1.0264232508569098\n",
      "Step 2351 | Sliding Loss Window : 1.0635163773505303\n",
      "Step 1 | Sliding Loss Window : 0.5682294152154719\n",
      "Step 51 | Sliding Loss Window : 1.0567557636677096\n",
      "Step 101 | Sliding Loss Window : 1.031164001973773\n",
      "Step 151 | Sliding Loss Window : 1.0679921292230545\n",
      "Step 201 | Sliding Loss Window : 1.047401042857157\n",
      "Step 251 | Sliding Loss Window : 1.034389897438063\n",
      "Step 301 | Sliding Loss Window : 1.0594649599525863\n",
      "Step 351 | Sliding Loss Window : 1.091895529417616\n",
      "Step 401 | Sliding Loss Window : 1.0032869865056184\n",
      "Step 451 | Sliding Loss Window : 1.0810471778675363\n",
      "Step 501 | Sliding Loss Window : 1.029192320359164\n",
      "Step 551 | Sliding Loss Window : 1.0873680061903537\n",
      "Step 601 | Sliding Loss Window : 1.0449039336256705\n",
      "Step 651 | Sliding Loss Window : 1.068746067647702\n",
      "Step 701 | Sliding Loss Window : 1.020345228737753\n",
      "Step 751 | Sliding Loss Window : 1.084069160455892\n",
      "Step 801 | Sliding Loss Window : 1.0675038372543866\n",
      "Step 851 | Sliding Loss Window : 0.9943055522574974\n",
      "Step 901 | Sliding Loss Window : 1.0974537153545711\n",
      "Step 951 | Sliding Loss Window : 1.081959417201859\n",
      "Step 1001 | Sliding Loss Window : 1.0624960836479402\n",
      "Step 1051 | Sliding Loss Window : 1.0383847761350404\n",
      "Step 1101 | Sliding Loss Window : 1.0694147232004416\n",
      "Step 1151 | Sliding Loss Window : 1.0557784737623002\n",
      "Step 1201 | Sliding Loss Window : 1.0059158035629234\n",
      "Step 1251 | Sliding Loss Window : 1.0830043152170727\n",
      "Step 1301 | Sliding Loss Window : 1.080430989431233\n",
      "Step 1351 | Sliding Loss Window : 0.979076480257161\n",
      "Step 1401 | Sliding Loss Window : 1.1144540453035754\n",
      "Step 1451 | Sliding Loss Window : 1.0843544577841677\n",
      "Step 1501 | Sliding Loss Window : 1.0575589818784756\n",
      "Step 1551 | Sliding Loss Window : 1.0415807858154644\n",
      "Step 1601 | Sliding Loss Window : 1.0562812715665222\n",
      "Step 1651 | Sliding Loss Window : 1.0405974556449724\n",
      "Step 1701 | Sliding Loss Window : 1.0432266806988186\n",
      "Step 1751 | Sliding Loss Window : 1.063970073855848\n",
      "Step 1801 | Sliding Loss Window : 1.0028768245313382\n",
      "Step 1851 | Sliding Loss Window : 1.0708818672975755\n",
      "Step 1901 | Sliding Loss Window : 1.11785518200375\n",
      "Step 1951 | Sliding Loss Window : 1.0428979848181228\n",
      "Step 2001 | Sliding Loss Window : 1.05517861193712\n",
      "Step 2051 | Sliding Loss Window : 1.0639284513568446\n",
      "Step 2101 | Sliding Loss Window : 1.05093716601989\n",
      "Step 2151 | Sliding Loss Window : 1.0406654472159227\n",
      "Step 2201 | Sliding Loss Window : 1.0683740931944508\n",
      "Step 2251 | Sliding Loss Window : 1.070343849707556\n",
      "Step 2301 | Sliding Loss Window : 1.0092776660923761\n",
      "Step 2351 | Sliding Loss Window : 1.0760102350680876\n",
      "Step 1 | Sliding Loss Window : 0.023706129331699726\n",
      "Step 51 | Sliding Loss Window : 1.2248672676476513\n",
      "Step 101 | Sliding Loss Window : 1.0608173460118822\n",
      "Step 151 | Sliding Loss Window : 1.024352801129233\n",
      "Step 201 | Sliding Loss Window : 1.0510133405875108\n",
      "Step 251 | Sliding Loss Window : 1.0930053279743028\n",
      "Step 301 | Sliding Loss Window : 1.0115520339331345\n",
      "Step 351 | Sliding Loss Window : 1.1148668101228036\n",
      "Step 401 | Sliding Loss Window : 1.0700364041018993\n",
      "Step 451 | Sliding Loss Window : 1.0370555644633457\n",
      "Step 501 | Sliding Loss Window : 1.074622983896462\n",
      "Step 551 | Sliding Loss Window : 1.0763050658946793\n",
      "Step 601 | Sliding Loss Window : 1.049803939172745\n",
      "Step 651 | Sliding Loss Window : 1.049916509289581\n",
      "Step 701 | Sliding Loss Window : 1.0559871370715805\n",
      "Step 751 | Sliding Loss Window : 1.0687876320936385\n",
      "Step 801 | Sliding Loss Window : 1.0541436559490158\n",
      "Step 851 | Sliding Loss Window : 1.085695166766145\n",
      "Step 901 | Sliding Loss Window : 1.0237043162687247\n",
      "Step 951 | Sliding Loss Window : 1.0750656962461966\n",
      "Step 1001 | Sliding Loss Window : 1.0939110246071737\n",
      "Step 1051 | Sliding Loss Window : 1.049680587712912\n",
      "Step 1101 | Sliding Loss Window : 1.0375783117514477\n",
      "Step 1151 | Sliding Loss Window : 1.0750217472456347\n",
      "Step 1201 | Sliding Loss Window : 1.0588022020033474\n",
      "Step 1251 | Sliding Loss Window : 1.0241060641226705\n",
      "Step 1301 | Sliding Loss Window : 1.1073994143575674\n",
      "Step 1351 | Sliding Loss Window : 1.0706449121877513\n",
      "Step 1401 | Sliding Loss Window : 1.0393004317060577\n",
      "Step 1451 | Sliding Loss Window : 1.0697083913363168\n",
      "Step 1501 | Sliding Loss Window : 1.0602096057653574\n",
      "Step 1551 | Sliding Loss Window : 1.0944965276234822\n",
      "Step 1601 | Sliding Loss Window : 1.0112953047830329\n",
      "Step 1651 | Sliding Loss Window : 1.052943580508226\n",
      "Step 1701 | Sliding Loss Window : 1.0805217531853488\n",
      "Step 1751 | Sliding Loss Window : 1.0497450300684235\n",
      "Step 1801 | Sliding Loss Window : 1.0974261402603929\n",
      "Step 1851 | Sliding Loss Window : 1.0578451946244491\n",
      "Step 1901 | Sliding Loss Window : 1.0284402816736224\n",
      "Step 1951 | Sliding Loss Window : 1.0937906995487574\n",
      "Step 2001 | Sliding Loss Window : 1.0640914196370213\n",
      "Step 2051 | Sliding Loss Window : 1.0188161255091541\n",
      "Step 2101 | Sliding Loss Window : 1.0784810834955973\n",
      "Step 2151 | Sliding Loss Window : 1.0616118734900248\n",
      "Step 2201 | Sliding Loss Window : 1.0614173184077604\n",
      "Step 2251 | Sliding Loss Window : 1.0620891163304675\n",
      "Step 2301 | Sliding Loss Window : 1.0811038792241545\n",
      "Step 2351 | Sliding Loss Window : 1.0314695443723105\n",
      "Step 1 | Sliding Loss Window : 3.178761975318581\n",
      "Step 51 | Sliding Loss Window : 1.182895399202508\n",
      "Step 101 | Sliding Loss Window : 1.0701170156455178\n",
      "Step 151 | Sliding Loss Window : 0.9821362212285092\n",
      "Step 201 | Sliding Loss Window : 1.1109519551978349\n",
      "Step 251 | Sliding Loss Window : 1.0826593407537126\n",
      "Step 301 | Sliding Loss Window : 1.0366727728038305\n",
      "Step 351 | Sliding Loss Window : 1.105998323568334\n",
      "Step 401 | Sliding Loss Window : 1.078631530488411\n",
      "Step 451 | Sliding Loss Window : 1.0503873028569144\n",
      "Step 501 | Sliding Loss Window : 1.0153736807931795\n",
      "Step 551 | Sliding Loss Window : 1.0293379345716722\n",
      "Step 601 | Sliding Loss Window : 1.0460564256377973\n",
      "Step 651 | Sliding Loss Window : 1.0724243754960583\n",
      "Step 701 | Sliding Loss Window : 1.090969267462907\n",
      "Step 751 | Sliding Loss Window : 1.0696356242651668\n",
      "Step 801 | Sliding Loss Window : 1.0700563805991978\n",
      "Step 851 | Sliding Loss Window : 1.047191187600956\n",
      "Step 901 | Sliding Loss Window : 1.1177590410957472\n",
      "Step 951 | Sliding Loss Window : 0.9406761660308937\n",
      "Step 1001 | Sliding Loss Window : 1.0973926781512413\n",
      "Step 1051 | Sliding Loss Window : 1.0726027989616906\n",
      "Step 1101 | Sliding Loss Window : 1.0325265716680974\n",
      "Step 1151 | Sliding Loss Window : 1.0619115764664864\n",
      "Step 1201 | Sliding Loss Window : 1.0779683503797193\n",
      "Step 1251 | Sliding Loss Window : 1.0995596546491206\n",
      "Step 1301 | Sliding Loss Window : 1.0569036986939973\n",
      "Step 1351 | Sliding Loss Window : 1.0696678109382405\n",
      "Step 1401 | Sliding Loss Window : 1.0716894904555048\n",
      "Step 1451 | Sliding Loss Window : 1.0043271079729739\n",
      "Step 1501 | Sliding Loss Window : 1.0585178738577443\n",
      "Step 1551 | Sliding Loss Window : 1.0481683877050432\n",
      "Step 1601 | Sliding Loss Window : 1.0311005415018457\n",
      "Step 1651 | Sliding Loss Window : 1.0975617422664588\n",
      "Step 1701 | Sliding Loss Window : 1.0837014211894491\n",
      "Step 1751 | Sliding Loss Window : 1.0510438136026992\n",
      "Step 1801 | Sliding Loss Window : 1.0823855285122137\n",
      "Step 1851 | Sliding Loss Window : 1.0872605637985644\n",
      "Step 1901 | Sliding Loss Window : 1.0145976431271384\n",
      "Step 1951 | Sliding Loss Window : 1.0319881695282787\n",
      "Step 2001 | Sliding Loss Window : 1.0718007079417426\n",
      "Step 2051 | Sliding Loss Window : 1.0136749605130349\n",
      "Step 2101 | Sliding Loss Window : 1.0750131975191681\n",
      "Step 2151 | Sliding Loss Window : 1.0966914579539713\n",
      "Step 2201 | Sliding Loss Window : 1.0741414429191913\n",
      "Step 2251 | Sliding Loss Window : 1.050981710943644\n",
      "Step 2301 | Sliding Loss Window : 1.0854588561192267\n",
      "Step 2351 | Sliding Loss Window : 1.0655762774620121\n",
      "Step 1 | Sliding Loss Window : 2.430837687509907\n",
      "Step 51 | Sliding Loss Window : 1.0858849370802155\n",
      "Step 101 | Sliding Loss Window : 1.04264012441209\n",
      "Step 151 | Sliding Loss Window : 1.082548310543817\n",
      "Step 201 | Sliding Loss Window : 1.055845373643566\n",
      "Step 251 | Sliding Loss Window : 1.0344800791290225\n",
      "Step 301 | Sliding Loss Window : 1.0188245596233934\n",
      "Step 351 | Sliding Loss Window : 1.0139931785694332\n",
      "Step 401 | Sliding Loss Window : 1.0359619508732205\n",
      "Step 451 | Sliding Loss Window : 1.0136029110994942\n",
      "Step 501 | Sliding Loss Window : 0.9893579593631208\n",
      "Step 551 | Sliding Loss Window : 1.021360204793013\n",
      "Step 601 | Sliding Loss Window : 1.0082843827788908\n",
      "Step 651 | Sliding Loss Window : 1.002337176483322\n",
      "Step 701 | Sliding Loss Window : 0.9991098222898124\n",
      "Step 751 | Sliding Loss Window : 1.003183059194946\n",
      "Step 801 | Sliding Loss Window : 1.0020410712816368\n",
      "Step 851 | Sliding Loss Window : 1.0005374496960373\n",
      "Step 901 | Sliding Loss Window : 1.0003252757388306\n",
      "Step 951 | Sliding Loss Window : 1.0003122991595335\n",
      "Step 1001 | Sliding Loss Window : 1.0000211905757828\n",
      "Step 1051 | Sliding Loss Window : 1.0001339912053446\n",
      "Step 1101 | Sliding Loss Window : 1.0000501938455593\n",
      "Step 1151 | Sliding Loss Window : 1.0000109004207498\n",
      "Step 1201 | Sliding Loss Window : 1.0000207131845624\n",
      "Step 1251 | Sliding Loss Window : 0.999993455151711\n",
      "Step 1301 | Sliding Loss Window : 1.000020654246132\n",
      "Step 1351 | Sliding Loss Window : 1.0000027820781114\n",
      "Step 1401 | Sliding Loss Window : 1.0000055389509552\n",
      "Step 1451 | Sliding Loss Window : 1.0000008544683043\n",
      "Step 1501 | Sliding Loss Window : 0.999999970976679\n",
      "Step 1551 | Sliding Loss Window : 1.0000012753835614\n",
      "Step 1601 | Sliding Loss Window : 1.0000001915955243\n",
      "Step 1651 | Sliding Loss Window : 1.0000000824225022\n",
      "Step 1701 | Sliding Loss Window : 1.0000001230269189\n",
      "Step 1751 | Sliding Loss Window : 1.000000085261552\n",
      "Step 1801 | Sliding Loss Window : 1.0000000473128008\n",
      "Step 1851 | Sliding Loss Window : 1.0000000190353135\n",
      "Step 1901 | Sliding Loss Window : 1.000000016277281\n",
      "Step 1951 | Sliding Loss Window : 1.000000005294874\n",
      "Step 2001 | Sliding Loss Window : 1.0000000063646866\n",
      "Step 2051 | Sliding Loss Window : 1.0000000026881128\n",
      "Step 2101 | Sliding Loss Window : 1.0000000011221897\n",
      "Step 2151 | Sliding Loss Window : 1.0000000010464523\n",
      "Step 2201 | Sliding Loss Window : 0.9999999987436885\n",
      "Step 2251 | Sliding Loss Window : 1.0000000024049414\n",
      "Step 2301 | Sliding Loss Window : 1.0000000001859548\n",
      "Step 2351 | Sliding Loss Window : 1.0000000002864975\n",
      "Step 1 | Sliding Loss Window : 1.5011957996787886\n",
      "Step 51 | Sliding Loss Window : 0.9294442733407573\n",
      "Step 101 | Sliding Loss Window : 1.116101020084229\n",
      "Step 151 | Sliding Loss Window : 1.0839909832322745\n",
      "Step 201 | Sliding Loss Window : 1.0337607923938\n",
      "Step 251 | Sliding Loss Window : 0.9546155467305519\n",
      "Step 301 | Sliding Loss Window : 1.0512913426341735\n",
      "Step 351 | Sliding Loss Window : 1.0467167843091274\n",
      "Step 401 | Sliding Loss Window : 1.014469704801617\n",
      "Step 451 | Sliding Loss Window : 1.0423976548821103\n",
      "Step 501 | Sliding Loss Window : 1.0910365704398037\n",
      "Step 551 | Sliding Loss Window : 0.9811982111357842\n",
      "Step 601 | Sliding Loss Window : 1.055151687066823\n",
      "Step 651 | Sliding Loss Window : 1.0107543571122455\n",
      "Step 701 | Sliding Loss Window : 1.066282626607412\n",
      "Step 751 | Sliding Loss Window : 0.9898523192085049\n",
      "Step 801 | Sliding Loss Window : 0.9875140351719603\n",
      "Step 851 | Sliding Loss Window : 1.042166211784274\n",
      "Step 901 | Sliding Loss Window : 1.021699630764534\n",
      "Step 951 | Sliding Loss Window : 1.0138398568632971\n",
      "Step 1001 | Sliding Loss Window : 1.0022906587160492\n",
      "Step 1051 | Sliding Loss Window : 0.9935065177729219\n",
      "Step 1101 | Sliding Loss Window : 1.0214833914601438\n",
      "Step 1151 | Sliding Loss Window : 1.0069630397589378\n",
      "Step 1201 | Sliding Loss Window : 0.9930111317554287\n",
      "Step 1251 | Sliding Loss Window : 1.0111602311686734\n",
      "Step 1301 | Sliding Loss Window : 1.0009845588195532\n",
      "Step 1351 | Sliding Loss Window : 1.000488404212204\n",
      "Step 1401 | Sliding Loss Window : 1.0001338182820487\n",
      "Step 1451 | Sliding Loss Window : 1.0003678152607862\n",
      "Step 1501 | Sliding Loss Window : 1.0001483836632412\n",
      "Step 1551 | Sliding Loss Window : 1.0000721033302258\n",
      "Step 1601 | Sliding Loss Window : 1.0000721654295743\n",
      "Step 1651 | Sliding Loss Window : 1.0000181981809428\n",
      "Step 1701 | Sliding Loss Window : 0.9999827356834231\n",
      "Step 1751 | Sliding Loss Window : 1.0000564191335135\n",
      "Step 1801 | Sliding Loss Window : 1.0000044152657985\n",
      "Step 1851 | Sliding Loss Window : 1.0000025815090645\n",
      "Step 1901 | Sliding Loss Window : 1.0000003722901263\n",
      "Step 1951 | Sliding Loss Window : 1.0000036472942104\n",
      "Step 2001 | Sliding Loss Window : 1.0000001868597064\n",
      "Step 2051 | Sliding Loss Window : 1.0000010075603756\n",
      "Step 2101 | Sliding Loss Window : 1.000000416579371\n",
      "Step 2151 | Sliding Loss Window : 1.0000000949167696\n",
      "Step 2201 | Sliding Loss Window : 1.0000002548452875\n",
      "Step 2251 | Sliding Loss Window : 1.00000005603846\n",
      "Step 2301 | Sliding Loss Window : 1.0000000220394982\n",
      "Step 2351 | Sliding Loss Window : 1.0000000225946593\n",
      "Step 1 | Sliding Loss Window : 0.1556246961034105\n",
      "Step 51 | Sliding Loss Window : 1.0479022885881106\n",
      "Step 101 | Sliding Loss Window : 1.0807717397144736\n",
      "Step 151 | Sliding Loss Window : 1.0281565683628107\n",
      "Step 201 | Sliding Loss Window : 0.9861437237887694\n",
      "Step 251 | Sliding Loss Window : 1.0150679624202474\n",
      "Step 301 | Sliding Loss Window : 1.0039800120689797\n",
      "Step 351 | Sliding Loss Window : 0.9535555020730685\n",
      "Step 401 | Sliding Loss Window : 1.025923867274375\n",
      "Step 451 | Sliding Loss Window : 1.0135480290457581\n",
      "Step 501 | Sliding Loss Window : 1.0038884850726797\n",
      "Step 551 | Sliding Loss Window : 0.9919308732882054\n",
      "Step 601 | Sliding Loss Window : 1.0133923524938653\n",
      "Step 651 | Sliding Loss Window : 1.0020118670073908\n",
      "Step 701 | Sliding Loss Window : 0.9967141631870361\n",
      "Step 751 | Sliding Loss Window : 1.0055249644515092\n",
      "Step 801 | Sliding Loss Window : 1.001204758596773\n",
      "Step 851 | Sliding Loss Window : 0.9993105827795113\n",
      "Step 901 | Sliding Loss Window : 1.0011295084172593\n",
      "Step 951 | Sliding Loss Window : 1.0000722096085226\n",
      "Step 1001 | Sliding Loss Window : 1.0000255870281536\n",
      "Step 1051 | Sliding Loss Window : 1.0000071592235176\n",
      "Step 1101 | Sliding Loss Window : 1.0000367386699325\n",
      "Step 1151 | Sliding Loss Window : 1.0000034766480845\n",
      "Step 1201 | Sliding Loss Window : 0.9999413287295631\n",
      "Step 1251 | Sliding Loss Window : 1.0000907708990305\n",
      "Step 1301 | Sliding Loss Window : 1.000004541365042\n",
      "Step 1351 | Sliding Loss Window : 1.0000012666103044\n",
      "Step 1401 | Sliding Loss Window : 1.0000019271156808\n",
      "Step 1451 | Sliding Loss Window : 1.0000007966206281\n",
      "Step 1501 | Sliding Loss Window : 1.0000000228409083\n",
      "Step 1551 | Sliding Loss Window : 1.0000004097181294\n",
      "Step 1601 | Sliding Loss Window : 1.0000001094721616\n",
      "Step 1651 | Sliding Loss Window : 0.9999999665029398\n",
      "Step 1701 | Sliding Loss Window : 1.0000002244502388\n",
      "Step 1751 | Sliding Loss Window : 1.00000008610768\n",
      "Step 1801 | Sliding Loss Window : 0.9999999910007057\n",
      "Step 1851 | Sliding Loss Window : 1.0000000356274124\n",
      "Step 1901 | Sliding Loss Window : 1.0000000142120373\n",
      "Step 1951 | Sliding Loss Window : 1.0000000019655328\n",
      "Step 2001 | Sliding Loss Window : 1.0000000018054143\n",
      "Step 2051 | Sliding Loss Window : 1.000000001198921\n",
      "Step 2101 | Sliding Loss Window : 1.0000000003994731\n",
      "Step 2151 | Sliding Loss Window : 0.9999999970226964\n",
      "Step 2201 | Sliding Loss Window : 1.0000000046214732\n",
      "Step 2251 | Sliding Loss Window : 1.0000000005362564\n",
      "Step 2301 | Sliding Loss Window : 1.0000000000987301\n",
      "Step 2351 | Sliding Loss Window : 1.000000000098463\n",
      "Step 1 | Sliding Loss Window : 2.43225958688294\n",
      "Step 51 | Sliding Loss Window : 1.1010874422123527\n",
      "Step 101 | Sliding Loss Window : 0.9949023876744064\n",
      "Step 151 | Sliding Loss Window : 1.009037627571436\n",
      "Step 201 | Sliding Loss Window : 0.999159904238016\n",
      "Step 251 | Sliding Loss Window : 1.0123049327568454\n",
      "Step 301 | Sliding Loss Window : 1.000666616314137\n",
      "Step 351 | Sliding Loss Window : 1.0002691596888957\n",
      "Step 401 | Sliding Loss Window : 1.000222349361754\n",
      "Step 451 | Sliding Loss Window : 1.0002629121119064\n",
      "Step 501 | Sliding Loss Window : 1.0001182979602772\n",
      "Step 551 | Sliding Loss Window : 1.0000466763348517\n",
      "Step 601 | Sliding Loss Window : 1.0000138958674065\n",
      "Step 651 | Sliding Loss Window : 1.0000401015725442\n",
      "Step 701 | Sliding Loss Window : 1.0000322770354442\n",
      "Step 751 | Sliding Loss Window : 1.0000305016199373\n",
      "Step 801 | Sliding Loss Window : 1.000004241800027\n",
      "Step 851 | Sliding Loss Window : 1.000002145950582\n",
      "Step 901 | Sliding Loss Window : 1.0000016013654036\n",
      "Step 951 | Sliding Loss Window : 1.0000017993849994\n",
      "Step 1001 | Sliding Loss Window : 1.0000005569276065\n",
      "Step 1051 | Sliding Loss Window : 1.0000000112692502\n",
      "Step 1101 | Sliding Loss Window : 1.00000057299151\n",
      "Step 1151 | Sliding Loss Window : 0.9999999506039482\n",
      "Step 1201 | Sliding Loss Window : 1.0000005619024601\n",
      "Step 1251 | Sliding Loss Window : 1.0000000763404293\n",
      "Step 1301 | Sliding Loss Window : 1.0000000364866313\n",
      "Step 1351 | Sliding Loss Window : 1.000000007987264\n",
      "Step 1401 | Sliding Loss Window : 1.000000019593765\n",
      "Step 1451 | Sliding Loss Window : 1.0000000082370661\n",
      "Step 1501 | Sliding Loss Window : 1.0000000029045681\n",
      "Step 1551 | Sliding Loss Window : 0.9999999994357074\n",
      "Step 1601 | Sliding Loss Window : 1.0000000048242728\n",
      "Step 1651 | Sliding Loss Window : 1.0000000010212347\n",
      "Step 1701 | Sliding Loss Window : 1.0000000034419858\n",
      "Step 1751 | Sliding Loss Window : 1.0000000003040255\n",
      "Step 1801 | Sliding Loss Window : 1.0000000001453064\n",
      "Step 1851 | Sliding Loss Window : 1.0000000001119849\n",
      "Step 1901 | Sliding Loss Window : 1.0000000001203546\n",
      "Step 1951 | Sliding Loss Window : 1.0000000000514415\n",
      "Step 2001 | Sliding Loss Window : 1.0000000000242921\n",
      "Step 2051 | Sliding Loss Window : 1.000000000020839\n",
      "Step 2101 | Sliding Loss Window : 0.999999999999773\n",
      "Step 2151 | Sliding Loss Window : 1.0000000000274298\n",
      "Step 2201 | Sliding Loss Window : 1.0000000000087017\n",
      "Step 2251 | Sliding Loss Window : 1.000000000002529\n",
      "Step 2301 | Sliding Loss Window : 1.0000000000010258\n",
      "Step 2351 | Sliding Loss Window : 1.0000000000007698\n",
      "Step 1 | Sliding Loss Window : 2.4008608444108273\n",
      "Step 51 | Sliding Loss Window : 1.0471175349977664\n",
      "Step 101 | Sliding Loss Window : 1.0262689793955015\n",
      "Step 151 | Sliding Loss Window : 1.050723836096341\n",
      "Step 201 | Sliding Loss Window : 1.0130435962593685\n",
      "Step 251 | Sliding Loss Window : 1.0321018344975632\n",
      "Step 301 | Sliding Loss Window : 1.0274185440805628\n",
      "Step 351 | Sliding Loss Window : 1.0050637566908458\n",
      "Step 401 | Sliding Loss Window : 0.9808240790561835\n",
      "Step 451 | Sliding Loss Window : 1.0038074317603474\n",
      "Step 501 | Sliding Loss Window : 1.0171180489123623\n",
      "Step 551 | Sliding Loss Window : 1.0032648785297729\n",
      "Step 601 | Sliding Loss Window : 1.0020784739080448\n",
      "Step 651 | Sliding Loss Window : 1.0003673121126573\n",
      "Step 701 | Sliding Loss Window : 0.9988521877038127\n",
      "Step 751 | Sliding Loss Window : 1.0039913071111022\n",
      "Step 801 | Sliding Loss Window : 1.0040857358843371\n",
      "Step 851 | Sliding Loss Window : 1.0006801226133624\n",
      "Step 901 | Sliding Loss Window : 1.0000921509561715\n",
      "Step 951 | Sliding Loss Window : 1.00009372934403\n",
      "Step 1001 | Sliding Loss Window : 1.000188476104604\n",
      "Step 1051 | Sliding Loss Window : 1.0000254255875962\n",
      "Step 1101 | Sliding Loss Window : 1.000009272565283\n",
      "Step 1151 | Sliding Loss Window : 0.999989765004321\n",
      "Step 1201 | Sliding Loss Window : 1.0000124511670463\n",
      "Step 1251 | Sliding Loss Window : 1.000046301451954\n",
      "Step 1301 | Sliding Loss Window : 1.000013040223633\n",
      "Step 1351 | Sliding Loss Window : 1.0000003983760388\n",
      "Step 1401 | Sliding Loss Window : 1.0000002016506437\n",
      "Step 1451 | Sliding Loss Window : 1.0000019469260584\n",
      "Step 1501 | Sliding Loss Window : 1.000001592796595\n",
      "Step 1551 | Sliding Loss Window : 1.0000001421874594\n",
      "Step 1601 | Sliding Loss Window : 1.0000000537547435\n",
      "Step 1651 | Sliding Loss Window : 0.9999997736355923\n",
      "Step 1701 | Sliding Loss Window : 1.0000004538380238\n",
      "Step 1751 | Sliding Loss Window : 1.000000204390748\n",
      "Step 1801 | Sliding Loss Window : 1.0000000514413927\n",
      "Step 1851 | Sliding Loss Window : 1.0000000020392266\n",
      "Step 1901 | Sliding Loss Window : 1.000000004038519\n",
      "Step 1951 | Sliding Loss Window : 1.0000000193136362\n",
      "Step 2001 | Sliding Loss Window : 1.0000000017018738\n",
      "Step 2051 | Sliding Loss Window : 1.0000000009236367\n",
      "Step 2101 | Sliding Loss Window : 1.000000000143658\n",
      "Step 2151 | Sliding Loss Window : 0.9999999979432999\n",
      "Step 2201 | Sliding Loss Window : 1.000000004457687\n",
      "Step 2251 | Sliding Loss Window : 1.0000000013436847\n",
      "Step 2301 | Sliding Loss Window : 1.0000000000654197\n",
      "Step 2351 | Sliding Loss Window : 1.0000000001020524\n",
      "Step 1 | Sliding Loss Window : 1.70262811343262\n",
      "Step 51 | Sliding Loss Window : 0.9787324294346547\n",
      "Step 101 | Sliding Loss Window : 1.120521323173332\n",
      "Step 151 | Sliding Loss Window : 1.0695668683496171\n",
      "Step 201 | Sliding Loss Window : 1.0011441590205634\n",
      "Step 251 | Sliding Loss Window : 1.0924919893369205\n",
      "Step 301 | Sliding Loss Window : 1.1013786747055565\n",
      "Step 351 | Sliding Loss Window : 0.9974941733775442\n",
      "Step 401 | Sliding Loss Window : 1.0861820458814404\n",
      "Step 451 | Sliding Loss Window : 1.0840983442402532\n",
      "Step 501 | Sliding Loss Window : 1.0322329771180005\n",
      "Step 551 | Sliding Loss Window : 1.0532709967559446\n",
      "Step 601 | Sliding Loss Window : 1.022194328392784\n",
      "Step 651 | Sliding Loss Window : 1.0818962623620179\n",
      "Step 701 | Sliding Loss Window : 1.0184442613015954\n",
      "Step 751 | Sliding Loss Window : 1.072938035585524\n",
      "Step 801 | Sliding Loss Window : 1.0519381412553113\n",
      "Step 851 | Sliding Loss Window : 1.0362225720376865\n",
      "Step 901 | Sliding Loss Window : 1.0799727246294517\n",
      "Step 951 | Sliding Loss Window : 1.075037127417639\n",
      "Step 1001 | Sliding Loss Window : 1.0323475299974223\n",
      "Step 1051 | Sliding Loss Window : 1.0279691005521934\n",
      "Step 1101 | Sliding Loss Window : 1.0480894079541228\n",
      "Step 1151 | Sliding Loss Window : 1.0017599865735987\n",
      "Step 1201 | Sliding Loss Window : 1.0909651710373052\n",
      "Step 1251 | Sliding Loss Window : 1.0925927448426527\n",
      "Step 1301 | Sliding Loss Window : 1.0265681723679896\n",
      "Step 1351 | Sliding Loss Window : 1.060202184203363\n",
      "Step 1401 | Sliding Loss Window : 1.0833314472744917\n",
      "Step 1451 | Sliding Loss Window : 0.9906224159752921\n",
      "Step 1501 | Sliding Loss Window : 1.0810116609753606\n",
      "Step 1551 | Sliding Loss Window : 1.0492968482396023\n",
      "Step 1601 | Sliding Loss Window : 1.066851173144077\n",
      "Step 1651 | Sliding Loss Window : 0.9980138596281953\n",
      "Step 1701 | Sliding Loss Window : 1.0731978678481184\n",
      "Step 1751 | Sliding Loss Window : 1.0496784732979139\n",
      "Step 1801 | Sliding Loss Window : 1.0568671421716964\n",
      "Step 1851 | Sliding Loss Window : 1.0682794757028804\n",
      "Step 1901 | Sliding Loss Window : 1.0803978750485843\n",
      "Step 1951 | Sliding Loss Window : 1.0011545959585526\n",
      "Step 2001 | Sliding Loss Window : 1.0465282591056284\n",
      "Step 2051 | Sliding Loss Window : 1.073553386565248\n",
      "Step 2101 | Sliding Loss Window : 1.0602785439735214\n",
      "Step 2151 | Sliding Loss Window : 1.014176367059998\n",
      "Step 2201 | Sliding Loss Window : 1.0841944333630422\n",
      "Step 2251 | Sliding Loss Window : 1.0199887771898102\n",
      "Step 2301 | Sliding Loss Window : 1.0734618911689717\n",
      "Step 2351 | Sliding Loss Window : 1.081866194295955\n",
      "Step 1 | Sliding Loss Window : 2.5374716162377045\n",
      "Step 51 | Sliding Loss Window : 1.172920858240776\n",
      "Step 101 | Sliding Loss Window : 1.056033223396308\n",
      "Step 151 | Sliding Loss Window : 1.116473562010875\n",
      "Step 201 | Sliding Loss Window : 1.0886873062488005\n",
      "Step 251 | Sliding Loss Window : 1.0650360830175267\n",
      "Step 301 | Sliding Loss Window : 1.0908905390862673\n",
      "Step 351 | Sliding Loss Window : 1.077847692921127\n",
      "Step 401 | Sliding Loss Window : 1.0446206721702043\n",
      "Step 451 | Sliding Loss Window : 1.075482671301343\n",
      "Step 501 | Sliding Loss Window : 1.1088609808150423\n",
      "Step 551 | Sliding Loss Window : 1.084703172764234\n",
      "Step 601 | Sliding Loss Window : 1.0047369285804253\n",
      "Step 651 | Sliding Loss Window : 1.0634493490200554\n",
      "Step 701 | Sliding Loss Window : 1.092093940837251\n",
      "Step 751 | Sliding Loss Window : 1.0211317941656866\n",
      "Step 801 | Sliding Loss Window : 1.0313277540908419\n",
      "Step 851 | Sliding Loss Window : 1.0895290933242168\n",
      "Step 901 | Sliding Loss Window : 1.0497498625884116\n",
      "Step 951 | Sliding Loss Window : 1.0623318990483814\n",
      "Step 1001 | Sliding Loss Window : 1.0924591743071572\n",
      "Step 1051 | Sliding Loss Window : 0.9783141403939501\n",
      "Step 1101 | Sliding Loss Window : 1.1191548786118406\n",
      "Step 1151 | Sliding Loss Window : 1.0481038306835502\n",
      "Step 1201 | Sliding Loss Window : 1.0719894549302458\n",
      "Step 1251 | Sliding Loss Window : 1.029495275556317\n",
      "Step 1301 | Sliding Loss Window : 1.0235219431200187\n",
      "Step 1351 | Sliding Loss Window : 1.031828210521497\n",
      "Step 1401 | Sliding Loss Window : 1.0607932904087582\n",
      "Step 1451 | Sliding Loss Window : 1.1212597198583196\n",
      "Step 1501 | Sliding Loss Window : 1.0767806834499258\n",
      "Step 1551 | Sliding Loss Window : 1.0118799597244192\n",
      "Step 1601 | Sliding Loss Window : 1.0427857163250422\n",
      "Step 1651 | Sliding Loss Window : 1.1065308025966345\n",
      "Step 1701 | Sliding Loss Window : 1.019911642544121\n",
      "Step 1751 | Sliding Loss Window : 1.0627380063532856\n",
      "Step 1801 | Sliding Loss Window : 1.0351338716023106\n",
      "Step 1851 | Sliding Loss Window : 1.0555398893900914\n",
      "Step 1901 | Sliding Loss Window : 1.0672057741053724\n",
      "Step 1951 | Sliding Loss Window : 1.078158419099524\n",
      "Step 2001 | Sliding Loss Window : 1.0714584790835282\n",
      "Step 2051 | Sliding Loss Window : 1.0370705329457144\n",
      "Step 2101 | Sliding Loss Window : 1.055729282126814\n",
      "Step 2151 | Sliding Loss Window : 1.0157598738785267\n",
      "Step 2201 | Sliding Loss Window : 1.0918799462308093\n",
      "Step 2251 | Sliding Loss Window : 1.0197902449428278\n",
      "Step 2301 | Sliding Loss Window : 1.0899161304309146\n",
      "Step 2351 | Sliding Loss Window : 1.034907972284517\n",
      "Step 1 | Sliding Loss Window : 0.3072889047004191\n",
      "Step 51 | Sliding Loss Window : 1.1174877654653372\n",
      "Step 101 | Sliding Loss Window : 1.0755369093207932\n",
      "Step 151 | Sliding Loss Window : 1.06812632118305\n",
      "Step 201 | Sliding Loss Window : 0.9340283321392144\n",
      "Step 251 | Sliding Loss Window : 1.058110958459201\n",
      "Step 301 | Sliding Loss Window : 1.0583931777436917\n",
      "Step 351 | Sliding Loss Window : 1.0535719212364365\n",
      "Step 401 | Sliding Loss Window : 1.0736197855239693\n",
      "Step 451 | Sliding Loss Window : 1.0567636903697026\n",
      "Step 501 | Sliding Loss Window : 1.0245746624989194\n",
      "Step 551 | Sliding Loss Window : 1.0656479846935694\n",
      "Step 601 | Sliding Loss Window : 1.0454063401812916\n",
      "Step 651 | Sliding Loss Window : 1.0873134803463662\n",
      "Step 701 | Sliding Loss Window : 0.9919009743468777\n",
      "Step 751 | Sliding Loss Window : 0.9899271316421324\n",
      "Step 801 | Sliding Loss Window : 1.0835977788111018\n",
      "Step 851 | Sliding Loss Window : 1.0297826742869376\n",
      "Step 901 | Sliding Loss Window : 1.066946557100955\n",
      "Step 951 | Sliding Loss Window : 1.0427720947610997\n",
      "Step 1001 | Sliding Loss Window : 1.0505114044249293\n",
      "Step 1051 | Sliding Loss Window : 1.08174311585833\n",
      "Step 1101 | Sliding Loss Window : 1.0583438332404334\n",
      "Step 1151 | Sliding Loss Window : 0.9703545081599575\n",
      "Step 1201 | Sliding Loss Window : 1.0247918121003188\n",
      "Step 1251 | Sliding Loss Window : 1.0512550874743172\n",
      "Step 1301 | Sliding Loss Window : 1.0455911005111556\n",
      "Step 1351 | Sliding Loss Window : 1.0786331200968968\n",
      "Step 1401 | Sliding Loss Window : 1.06013011575242\n",
      "Step 1451 | Sliding Loss Window : 1.0174219534270534\n",
      "Step 1501 | Sliding Loss Window : 1.0706146927625175\n",
      "Step 1551 | Sliding Loss Window : 1.0761499455253307\n",
      "Step 1601 | Sliding Loss Window : 1.0647887180750308\n",
      "Step 1651 | Sliding Loss Window : 0.9839473287823803\n",
      "Step 1701 | Sliding Loss Window : 0.9836522301239105\n",
      "Step 1751 | Sliding Loss Window : 1.0855744946316608\n",
      "Step 1801 | Sliding Loss Window : 1.0256097663249664\n",
      "Step 1851 | Sliding Loss Window : 1.102599204337237\n",
      "Step 1901 | Sliding Loss Window : 1.021075751217046\n",
      "Step 1951 | Sliding Loss Window : 1.0363708687439526\n",
      "Step 2001 | Sliding Loss Window : 1.086545940181783\n",
      "Step 2051 | Sliding Loss Window : 1.058260329946849\n",
      "Step 2101 | Sliding Loss Window : 1.0436666773948875\n",
      "Step 2151 | Sliding Loss Window : 0.9324019778122209\n",
      "Step 2201 | Sliding Loss Window : 1.0717374121660415\n",
      "Step 2251 | Sliding Loss Window : 1.0564991989165058\n",
      "Step 2301 | Sliding Loss Window : 1.0700735325284159\n",
      "Step 2351 | Sliding Loss Window : 1.053540766555515\n",
      "Step 1 | Sliding Loss Window : 0.6470433840168944\n",
      "Step 51 | Sliding Loss Window : 1.019392225724011\n",
      "Step 101 | Sliding Loss Window : 1.0314487759921804\n",
      "Step 151 | Sliding Loss Window : 1.076543532929749\n",
      "Step 201 | Sliding Loss Window : 1.100880131693947\n",
      "Step 251 | Sliding Loss Window : 1.076110575894659\n",
      "Step 301 | Sliding Loss Window : 1.0804562213383695\n",
      "Step 351 | Sliding Loss Window : 1.0347762595686563\n",
      "Step 401 | Sliding Loss Window : 1.0798027134051609\n",
      "Step 451 | Sliding Loss Window : 1.0158539688686241\n",
      "Step 501 | Sliding Loss Window : 1.021166621951208\n",
      "Step 551 | Sliding Loss Window : 1.1109986111018435\n",
      "Step 601 | Sliding Loss Window : 0.9853236865642485\n",
      "Step 651 | Sliding Loss Window : 1.0714340638923887\n",
      "Step 701 | Sliding Loss Window : 1.0987965326191498\n",
      "Step 751 | Sliding Loss Window : 1.0888710677674627\n",
      "Step 801 | Sliding Loss Window : 1.0074491994519807\n",
      "Step 851 | Sliding Loss Window : 1.0859365179477694\n",
      "Step 901 | Sliding Loss Window : 1.0656641656921513\n",
      "Step 951 | Sliding Loss Window : 1.0437938105179088\n",
      "Step 1001 | Sliding Loss Window : 1.019605714501785\n",
      "Step 1051 | Sliding Loss Window : 1.014138137522025\n",
      "Step 1101 | Sliding Loss Window : 1.061206567520297\n",
      "Step 1151 | Sliding Loss Window : 1.0848740558646741\n",
      "Step 1201 | Sliding Loss Window : 1.0894741514291035\n",
      "Step 1251 | Sliding Loss Window : 1.0438764086800894\n",
      "Step 1301 | Sliding Loss Window : 1.0547719390567274\n",
      "Step 1351 | Sliding Loss Window : 1.0536854712884791\n",
      "Step 1401 | Sliding Loss Window : 1.0832595319715004\n",
      "Step 1451 | Sliding Loss Window : 0.9928004350116446\n",
      "Step 1501 | Sliding Loss Window : 1.075901432227809\n",
      "Step 1551 | Sliding Loss Window : 0.9701059841345863\n",
      "Step 1601 | Sliding Loss Window : 1.1050676817900538\n",
      "Step 1651 | Sliding Loss Window : 1.0837549941154965\n",
      "Step 1701 | Sliding Loss Window : 1.0772098118142948\n",
      "Step 1751 | Sliding Loss Window : 1.0462095173456498\n",
      "Step 1801 | Sliding Loss Window : 1.0506185276636413\n",
      "Step 1851 | Sliding Loss Window : 1.0642666103081466\n",
      "Step 1901 | Sliding Loss Window : 1.0230949780669958\n",
      "Step 1951 | Sliding Loss Window : 1.043676785811605\n",
      "Step 2001 | Sliding Loss Window : 1.077803530008193\n",
      "Step 2051 | Sliding Loss Window : 0.9963912735609911\n",
      "Step 2101 | Sliding Loss Window : 1.0846945478053494\n",
      "Step 2151 | Sliding Loss Window : 1.0919785163691504\n",
      "Step 2201 | Sliding Loss Window : 1.0583789782610085\n",
      "Step 2251 | Sliding Loss Window : 1.0461210576491917\n",
      "Step 2301 | Sliding Loss Window : 1.0571845839214755\n",
      "Step 2351 | Sliding Loss Window : 1.0675075915201722\n",
      "Step 1 | Sliding Loss Window : 0.003471063606729258\n",
      "Step 51 | Sliding Loss Window : 1.2067918070967254\n",
      "Step 101 | Sliding Loss Window : 1.0604501625532612\n",
      "Step 151 | Sliding Loss Window : 1.1049583190868868\n",
      "Step 201 | Sliding Loss Window : 1.1019159246494619\n",
      "Step 251 | Sliding Loss Window : 1.0770828273973307\n",
      "Step 301 | Sliding Loss Window : 1.0775695256549866\n",
      "Step 351 | Sliding Loss Window : 1.0792320327459082\n",
      "Step 401 | Sliding Loss Window : 1.0207036567932826\n",
      "Step 451 | Sliding Loss Window : 1.0427543107001545\n",
      "Step 501 | Sliding Loss Window : 1.1041983989051527\n",
      "Step 551 | Sliding Loss Window : 0.9979881410438822\n",
      "Step 601 | Sliding Loss Window : 1.084882565659223\n",
      "Step 651 | Sliding Loss Window : 1.078490200574297\n",
      "Step 701 | Sliding Loss Window : 1.0724141085891368\n",
      "Step 751 | Sliding Loss Window : 1.0752366117047052\n",
      "Step 801 | Sliding Loss Window : 1.0721746570376682\n",
      "Step 851 | Sliding Loss Window : 1.080210039426423\n",
      "Step 901 | Sliding Loss Window : 1.0147458932591604\n",
      "Step 951 | Sliding Loss Window : 1.0214747064103613\n",
      "Step 1001 | Sliding Loss Window : 1.1134572630892667\n",
      "Step 1051 | Sliding Loss Window : 1.0346027167193323\n",
      "Step 1101 | Sliding Loss Window : 1.0395617550206595\n",
      "Step 1151 | Sliding Loss Window : 1.088943241023301\n",
      "Step 1201 | Sliding Loss Window : 1.0752518719261943\n",
      "Step 1251 | Sliding Loss Window : 1.0844411130468976\n",
      "Step 1301 | Sliding Loss Window : 1.0366540439596588\n",
      "Step 1351 | Sliding Loss Window : 1.0095980795613413\n",
      "Step 1401 | Sliding Loss Window : 1.1108588129466195\n",
      "Step 1451 | Sliding Loss Window : 1.0607771792996608\n",
      "Step 1501 | Sliding Loss Window : 1.0467707171182894\n",
      "Step 1551 | Sliding Loss Window : 1.030548622628261\n",
      "Step 1601 | Sliding Loss Window : 1.0802759911801536\n",
      "Step 1651 | Sliding Loss Window : 1.050146003551239\n",
      "Step 1701 | Sliding Loss Window : 1.0924166082035984\n",
      "Step 1751 | Sliding Loss Window : 1.0847331238557292\n",
      "Step 1801 | Sliding Loss Window : 1.0698595239758266\n",
      "Step 1851 | Sliding Loss Window : 1.0015692227076283\n",
      "Step 1901 | Sliding Loss Window : 1.0870574651637495\n",
      "Step 1951 | Sliding Loss Window : 1.070608479803042\n",
      "Step 2001 | Sliding Loss Window : 1.0265960817899347\n",
      "Step 2051 | Sliding Loss Window : 1.034034963515982\n",
      "Step 2101 | Sliding Loss Window : 1.0938686143323295\n",
      "Step 2151 | Sliding Loss Window : 1.0828639941168563\n",
      "Step 2201 | Sliding Loss Window : 1.0774585822218024\n",
      "Step 2251 | Sliding Loss Window : 1.0694781761248402\n",
      "Step 2301 | Sliding Loss Window : 1.051136994366336\n",
      "Step 2351 | Sliding Loss Window : 1.019321666290753\n",
      "Step 1 | Sliding Loss Window : 1.4409952185914718\n",
      "Step 51 | Sliding Loss Window : 1.103810283750351\n",
      "Step 101 | Sliding Loss Window : 1.0220844688939823\n",
      "Step 151 | Sliding Loss Window : 1.0325057245109162\n",
      "Step 201 | Sliding Loss Window : 1.0802654610832212\n",
      "Step 251 | Sliding Loss Window : 1.029499013672944\n",
      "Step 301 | Sliding Loss Window : 1.0218926698483473\n",
      "Step 351 | Sliding Loss Window : 1.0011988763168762\n",
      "Step 401 | Sliding Loss Window : 1.0106915383062043\n",
      "Step 451 | Sliding Loss Window : 1.007328318000554\n",
      "Step 501 | Sliding Loss Window : 0.9998837490486543\n",
      "Step 551 | Sliding Loss Window : 1.0042159624255043\n",
      "Step 601 | Sliding Loss Window : 1.0028314712893251\n",
      "Step 651 | Sliding Loss Window : 1.0014290961692949\n",
      "Step 701 | Sliding Loss Window : 1.000056547810488\n",
      "Step 751 | Sliding Loss Window : 1.0018946281255667\n",
      "Step 801 | Sliding Loss Window : 0.9999417861979428\n",
      "Step 851 | Sliding Loss Window : 1.0015300063685184\n",
      "Step 901 | Sliding Loss Window : 0.9998754368881778\n",
      "Step 951 | Sliding Loss Window : 1.0017845696478977\n",
      "Step 1001 | Sliding Loss Window : 1.0009732784626608\n",
      "Step 1051 | Sliding Loss Window : 0.9984157225008101\n",
      "Step 1101 | Sliding Loss Window : 1.002988730073695\n",
      "Step 1151 | Sliding Loss Window : 1.0002709224805937\n",
      "Step 1201 | Sliding Loss Window : 1.0008300253438636\n",
      "Step 1251 | Sliding Loss Window : 1.000447842074575\n",
      "Step 1301 | Sliding Loss Window : 0.998463221523374\n",
      "Step 1351 | Sliding Loss Window : 1.0019561805314914\n",
      "Step 1401 | Sliding Loss Window : 1.0007645233753033\n",
      "Step 1451 | Sliding Loss Window : 1.0007360180124452\n",
      "Step 1501 | Sliding Loss Window : 0.9998997821804645\n",
      "Step 1551 | Sliding Loss Window : 1.000442803723141\n",
      "Step 1601 | Sliding Loss Window : 1.0007278805520938\n",
      "Step 1651 | Sliding Loss Window : 1.0000468880861242\n",
      "Step 1701 | Sliding Loss Window : 1.0007397077651077\n",
      "Step 1751 | Sliding Loss Window : 0.9998745539889144\n",
      "Step 1801 | Sliding Loss Window : 1.0003620948459702\n",
      "Step 1851 | Sliding Loss Window : 1.0001197157983819\n",
      "Step 1901 | Sliding Loss Window : 1.0007004508089823\n",
      "Step 1951 | Sliding Loss Window : 1.0004569854878655\n",
      "Step 2001 | Sliding Loss Window : 1.0001029093369767\n",
      "Step 2051 | Sliding Loss Window : 1.0003833136319593\n",
      "Step 2101 | Sliding Loss Window : 1.0002550330670883\n",
      "Step 2151 | Sliding Loss Window : 1.0002471718364911\n",
      "Step 2201 | Sliding Loss Window : 1.0002762536506646\n",
      "Step 2251 | Sliding Loss Window : 0.9999522722203202\n",
      "Step 2301 | Sliding Loss Window : 0.9998866512115832\n",
      "Step 2351 | Sliding Loss Window : 1.0004405957673423\n",
      "Step 1 | Sliding Loss Window : 0.4513549216021602\n",
      "Step 51 | Sliding Loss Window : 1.056986549125305\n",
      "Step 101 | Sliding Loss Window : 1.0039585602628178\n",
      "Step 151 | Sliding Loss Window : 1.014079980962565\n",
      "Step 201 | Sliding Loss Window : 1.0018681441226875\n",
      "Step 251 | Sliding Loss Window : 1.0128577716450757\n",
      "Step 301 | Sliding Loss Window : 1.0137183408027313\n",
      "Step 351 | Sliding Loss Window : 1.0079747540038204\n",
      "Step 401 | Sliding Loss Window : 1.006264201803894\n",
      "Step 451 | Sliding Loss Window : 1.0005927842755298\n",
      "Step 501 | Sliding Loss Window : 0.9984836085884369\n",
      "Step 551 | Sliding Loss Window : 1.0044844311508403\n",
      "Step 601 | Sliding Loss Window : 1.0041867969250184\n",
      "Step 651 | Sliding Loss Window : 0.9996558138062491\n",
      "Step 701 | Sliding Loss Window : 1.0089982307419876\n",
      "Step 751 | Sliding Loss Window : 1.000656500723932\n",
      "Step 801 | Sliding Loss Window : 1.00316903753036\n",
      "Step 851 | Sliding Loss Window : 1.002088956110878\n",
      "Step 901 | Sliding Loss Window : 1.002554912444871\n",
      "Step 951 | Sliding Loss Window : 0.9968974349767729\n",
      "Step 1001 | Sliding Loss Window : 1.005072288533014\n",
      "Step 1051 | Sliding Loss Window : 1.0017944466016202\n",
      "Step 1101 | Sliding Loss Window : 1.0007455939674832\n",
      "Step 1151 | Sliding Loss Window : 1.0006032429022083\n",
      "Step 1201 | Sliding Loss Window : 1.000096789731404\n",
      "Step 1251 | Sliding Loss Window : 1.0003462017990323\n",
      "Step 1301 | Sliding Loss Window : 1.0009183155611203\n",
      "Step 1351 | Sliding Loss Window : 1.0002955735355703\n",
      "Step 1401 | Sliding Loss Window : 1.0001422775234656\n",
      "Step 1451 | Sliding Loss Window : 0.9996350496581315\n",
      "Step 1501 | Sliding Loss Window : 1.0003487578989654\n",
      "Step 1551 | Sliding Loss Window : 1.0007992983064613\n",
      "Step 1601 | Sliding Loss Window : 1.0000698041023073\n",
      "Step 1651 | Sliding Loss Window : 1.0000747012093025\n",
      "Step 1701 | Sliding Loss Window : 0.9998955101899341\n",
      "Step 1751 | Sliding Loss Window : 1.000214905800042\n",
      "Step 1801 | Sliding Loss Window : 1.0002157544181223\n",
      "Step 1851 | Sliding Loss Window : 1.0000581161809396\n",
      "Step 1901 | Sliding Loss Window : 0.9999744086536263\n",
      "Step 1951 | Sliding Loss Window : 1.0001052994513515\n",
      "Step 2001 | Sliding Loss Window : 0.9999819525968543\n",
      "Step 2051 | Sliding Loss Window : 1.0001012127815023\n",
      "Step 2101 | Sliding Loss Window : 0.9999920193170019\n",
      "Step 2151 | Sliding Loss Window : 1.000034131119437\n",
      "Step 2201 | Sliding Loss Window : 1.0000119415008948\n",
      "Step 2251 | Sliding Loss Window : 1.0000262972143308\n",
      "Step 2301 | Sliding Loss Window : 1.0000169022123637\n",
      "Step 2351 | Sliding Loss Window : 1.000004092265833\n",
      "Step 1 | Sliding Loss Window : 0.5807741317345974\n",
      "Step 51 | Sliding Loss Window : 0.9945573906512929\n",
      "Step 101 | Sliding Loss Window : 1.0963614187212871\n",
      "Step 151 | Sliding Loss Window : 1.0702936471019904\n",
      "Step 201 | Sliding Loss Window : 1.0432566409979542\n",
      "Step 251 | Sliding Loss Window : 1.0102428882408965\n",
      "Step 301 | Sliding Loss Window : 1.0365046446639372\n",
      "Step 351 | Sliding Loss Window : 1.0196690921549743\n",
      "Step 401 | Sliding Loss Window : 1.0162629297391137\n",
      "Step 451 | Sliding Loss Window : 0.9908303461382947\n",
      "Step 501 | Sliding Loss Window : 0.9905667386039337\n",
      "Step 551 | Sliding Loss Window : 1.026028117651564\n",
      "Step 601 | Sliding Loss Window : 1.0061633065822508\n",
      "Step 651 | Sliding Loss Window : 1.0065050242473694\n",
      "Step 701 | Sliding Loss Window : 1.0050277534232093\n",
      "Step 751 | Sliding Loss Window : 1.0024206776245803\n",
      "Step 801 | Sliding Loss Window : 1.0057358235950178\n",
      "Step 851 | Sliding Loss Window : 1.0042632148418036\n",
      "Step 901 | Sliding Loss Window : 1.0039123359837352\n",
      "Step 951 | Sliding Loss Window : 1.000308545259333\n",
      "Step 1001 | Sliding Loss Window : 1.0021857756335046\n",
      "Step 1051 | Sliding Loss Window : 1.0033429007853707\n",
      "Step 1101 | Sliding Loss Window : 1.000839205877835\n",
      "Step 1151 | Sliding Loss Window : 1.0018200518106957\n",
      "Step 1201 | Sliding Loss Window : 1.0004950925535567\n",
      "Step 1251 | Sliding Loss Window : 1.001030626405351\n",
      "Step 1301 | Sliding Loss Window : 1.0007712797359742\n",
      "Step 1351 | Sliding Loss Window : 1.0001107992512797\n",
      "Step 1401 | Sliding Loss Window : 1.0006876709214083\n",
      "Step 1451 | Sliding Loss Window : 0.9981371225069897\n",
      "Step 1501 | Sliding Loss Window : 1.0031025050630766\n",
      "Step 1551 | Sliding Loss Window : 1.000402564090063\n",
      "Step 1601 | Sliding Loss Window : 1.0002017614418761\n",
      "Step 1651 | Sliding Loss Window : 1.000167083240253\n",
      "Step 1701 | Sliding Loss Window : 0.9998275108737993\n",
      "Step 1751 | Sliding Loss Window : 1.00036640052338\n",
      "Step 1801 | Sliding Loss Window : 1.0000611917380235\n",
      "Step 1851 | Sliding Loss Window : 1.0001633136766626\n",
      "Step 1901 | Sliding Loss Window : 1.000119314407644\n",
      "Step 1951 | Sliding Loss Window : 0.999892964833641\n",
      "Step 2001 | Sliding Loss Window : 1.0002531519701798\n",
      "Step 2051 | Sliding Loss Window : 1.0000402126882444\n",
      "Step 2101 | Sliding Loss Window : 1.0000225394569022\n",
      "Step 2151 | Sliding Loss Window : 1.0000333604662712\n",
      "Step 2201 | Sliding Loss Window : 1.0000110162362987\n",
      "Step 2251 | Sliding Loss Window : 1.0000220175238825\n",
      "Step 2301 | Sliding Loss Window : 1.000024411157295\n",
      "Step 2351 | Sliding Loss Window : 1.000017760663948\n",
      "Step 1 | Sliding Loss Window : 0.7985021211956463\n",
      "Step 51 | Sliding Loss Window : 1.0578516182118824\n",
      "Step 101 | Sliding Loss Window : 1.1156089081659468\n",
      "Step 151 | Sliding Loss Window : 0.9711377011509391\n",
      "Step 201 | Sliding Loss Window : 1.0535477856579738\n",
      "Step 251 | Sliding Loss Window : 1.0535748185625464\n",
      "Step 301 | Sliding Loss Window : 1.0224112218330483\n",
      "Step 351 | Sliding Loss Window : 1.0175306305303193\n",
      "Step 401 | Sliding Loss Window : 1.0216397053569535\n",
      "Step 451 | Sliding Loss Window : 1.0121068519844925\n",
      "Step 501 | Sliding Loss Window : 1.0079753898514026\n",
      "Step 551 | Sliding Loss Window : 1.00812435115844\n",
      "Step 601 | Sliding Loss Window : 1.004491649348907\n",
      "Step 651 | Sliding Loss Window : 0.9749940351181171\n",
      "Step 701 | Sliding Loss Window : 1.022781448640893\n",
      "Step 751 | Sliding Loss Window : 1.0085907765980056\n",
      "Step 801 | Sliding Loss Window : 1.0033836201989583\n",
      "Step 851 | Sliding Loss Window : 1.001974387811539\n",
      "Step 901 | Sliding Loss Window : 0.9998423344259567\n",
      "Step 951 | Sliding Loss Window : 0.9996304519144611\n",
      "Step 1001 | Sliding Loss Window : 1.0062363473606508\n",
      "Step 1051 | Sliding Loss Window : 1.00052373907605\n",
      "Step 1101 | Sliding Loss Window : 1.0008898860574473\n",
      "Step 1151 | Sliding Loss Window : 0.9990026580877894\n",
      "Step 1201 | Sliding Loss Window : 1.0024149105497882\n",
      "Step 1251 | Sliding Loss Window : 1.0023317921898824\n",
      "Step 1301 | Sliding Loss Window : 1.0006564141301653\n",
      "Step 1351 | Sliding Loss Window : 1.0005452238719106\n",
      "Step 1401 | Sliding Loss Window : 1.0001897657938266\n",
      "Step 1451 | Sliding Loss Window : 1.0003053541415308\n",
      "Step 1501 | Sliding Loss Window : 1.0005448640717771\n",
      "Step 1551 | Sliding Loss Window : 1.0004408237864177\n",
      "Step 1601 | Sliding Loss Window : 0.999721462412396\n",
      "Step 1651 | Sliding Loss Window : 1.0006147075033034\n",
      "Step 1701 | Sliding Loss Window : 1.0002876174566748\n",
      "Step 1751 | Sliding Loss Window : 1.0001970214788782\n",
      "Step 1801 | Sliding Loss Window : 1.0001100970247496\n",
      "Step 1851 | Sliding Loss Window : 1.0000921995130696\n",
      "Step 1901 | Sliding Loss Window : 0.9999629724168335\n",
      "Step 1951 | Sliding Loss Window : 1.000120698747411\n",
      "Step 2001 | Sliding Loss Window : 1.0000286714420494\n",
      "Step 2051 | Sliding Loss Window : 1.00022499971454\n",
      "Step 2101 | Sliding Loss Window : 0.9999023499194619\n",
      "Step 2151 | Sliding Loss Window : 1.0000713420539449\n",
      "Step 2201 | Sliding Loss Window : 1.0001673996306426\n",
      "Step 2251 | Sliding Loss Window : 1.000021346641727\n",
      "Step 2301 | Sliding Loss Window : 1.0000331789207277\n",
      "Step 2351 | Sliding Loss Window : 1.000019802006481\n",
      "Step 1 | Sliding Loss Window : 0.8108699249671453\n",
      "Step 51 | Sliding Loss Window : 1.0278214669113184\n",
      "Step 101 | Sliding Loss Window : 1.0114512971514376\n",
      "Step 151 | Sliding Loss Window : 1.0311744773224096\n",
      "Step 201 | Sliding Loss Window : 0.9757171488888673\n",
      "Step 251 | Sliding Loss Window : 1.034384235570562\n",
      "Step 301 | Sliding Loss Window : 1.006986574385987\n",
      "Step 351 | Sliding Loss Window : 1.0108886181820822\n",
      "Step 401 | Sliding Loss Window : 1.0033139994963056\n",
      "Step 451 | Sliding Loss Window : 0.9936261708604355\n",
      "Step 501 | Sliding Loss Window : 1.00951066181417\n",
      "Step 551 | Sliding Loss Window : 0.9996097963323456\n",
      "Step 601 | Sliding Loss Window : 0.9993750555656535\n",
      "Step 651 | Sliding Loss Window : 0.9990745362597998\n",
      "Step 701 | Sliding Loss Window : 1.002557712598795\n",
      "Step 751 | Sliding Loss Window : 1.001610427692189\n",
      "Step 801 | Sliding Loss Window : 1.0021026363764447\n",
      "Step 851 | Sliding Loss Window : 1.0003129177049552\n",
      "Step 901 | Sliding Loss Window : 0.9994745016955732\n",
      "Step 951 | Sliding Loss Window : 0.9986195273369599\n",
      "Step 1001 | Sliding Loss Window : 1.0025063651667852\n",
      "Step 1051 | Sliding Loss Window : 1.0004548567275322\n",
      "Step 1101 | Sliding Loss Window : 0.9998779165930272\n",
      "Step 1151 | Sliding Loss Window : 0.9986237709373152\n",
      "Step 1201 | Sliding Loss Window : 1.0020366369597904\n",
      "Step 1251 | Sliding Loss Window : 1.000601708470959\n",
      "Step 1301 | Sliding Loss Window : 1.0001926542505506\n",
      "Step 1351 | Sliding Loss Window : 0.9999514035180025\n",
      "Step 1401 | Sliding Loss Window : 0.9996196834885113\n",
      "Step 1451 | Sliding Loss Window : 1.000579788695303\n",
      "Step 1501 | Sliding Loss Window : 1.0002412737256874\n",
      "Step 1551 | Sliding Loss Window : 0.9996517870203344\n",
      "Step 1601 | Sliding Loss Window : 1.0003331808612714\n",
      "Step 1651 | Sliding Loss Window : 0.9993217801487329\n",
      "Step 1701 | Sliding Loss Window : 1.00085331248145\n",
      "Step 1751 | Sliding Loss Window : 1.0003215014884084\n",
      "Step 1801 | Sliding Loss Window : 1.0000148842277468\n",
      "Step 1851 | Sliding Loss Window : 1.0000124188719008\n",
      "Step 1901 | Sliding Loss Window : 0.999790949036963\n",
      "Step 1951 | Sliding Loss Window : 1.0003198110378073\n",
      "Step 2001 | Sliding Loss Window : 1.0000608039542218\n",
      "Step 2051 | Sliding Loss Window : 0.9999570280106657\n",
      "Step 2101 | Sliding Loss Window : 0.9998510291266757\n",
      "Step 2151 | Sliding Loss Window : 1.000179447597438\n",
      "Step 2201 | Sliding Loss Window : 1.00007981380186\n",
      "Step 2251 | Sliding Loss Window : 1.0000972426845123\n",
      "Step 2301 | Sliding Loss Window : 1.0000329860715198\n",
      "Step 2351 | Sliding Loss Window : 0.9999216267499677\n",
      "Step 1 | Sliding Loss Window : 2.8371623596997453\n",
      "Step 51 | Sliding Loss Window : 1.1716176242717045\n",
      "Step 101 | Sliding Loss Window : 1.0912802981687049\n",
      "Step 151 | Sliding Loss Window : 1.125795663703866\n",
      "Step 201 | Sliding Loss Window : 1.1226943324536895\n",
      "Step 251 | Sliding Loss Window : 1.0233181573442425\n",
      "Step 301 | Sliding Loss Window : 1.0533001078490083\n",
      "Step 351 | Sliding Loss Window : 1.0598219572412728\n",
      "Step 401 | Sliding Loss Window : 1.0492369312208032\n",
      "Step 451 | Sliding Loss Window : 1.0071013697343496\n",
      "Step 501 | Sliding Loss Window : 1.068676209313612\n",
      "Step 551 | Sliding Loss Window : 0.9874254593285403\n",
      "Step 601 | Sliding Loss Window : 1.1131945542396433\n",
      "Step 651 | Sliding Loss Window : 1.0891818311447328\n",
      "Step 701 | Sliding Loss Window : 1.0505202508507288\n",
      "Step 751 | Sliding Loss Window : 1.051351079673685\n",
      "Step 801 | Sliding Loss Window : 1.0568927332236495\n",
      "Step 851 | Sliding Loss Window : 1.0512335680036322\n",
      "Step 901 | Sliding Loss Window : 0.9813485773363252\n",
      "Step 951 | Sliding Loss Window : 1.0602433891406011\n",
      "Step 1001 | Sliding Loss Window : 1.073797339102529\n",
      "Step 1051 | Sliding Loss Window : 0.9968065416324352\n",
      "Step 1101 | Sliding Loss Window : 1.1004284458922864\n",
      "Step 1151 | Sliding Loss Window : 1.0748551188269675\n",
      "Step 1201 | Sliding Loss Window : 1.0380153120918634\n",
      "Step 1251 | Sliding Loss Window : 1.0606089729781578\n",
      "Step 1301 | Sliding Loss Window : 1.007966374209982\n",
      "Step 1351 | Sliding Loss Window : 1.0766791851084603\n",
      "Step 1401 | Sliding Loss Window : 0.9682779025294955\n",
      "Step 1451 | Sliding Loss Window : 1.0714695235880873\n",
      "Step 1501 | Sliding Loss Window : 0.9574193020081965\n",
      "Step 1551 | Sliding Loss Window : 1.1397589185377766\n",
      "Step 1601 | Sliding Loss Window : 1.086936875541336\n",
      "Step 1651 | Sliding Loss Window : 1.0484499478834528\n",
      "Step 1701 | Sliding Loss Window : 1.0318033949266787\n",
      "Step 1751 | Sliding Loss Window : 1.089810721682371\n",
      "Step 1801 | Sliding Loss Window : 1.0585530967423131\n",
      "Step 1851 | Sliding Loss Window : 0.9697614580579896\n",
      "Step 1901 | Sliding Loss Window : 1.0535230500051247\n",
      "Step 1951 | Sliding Loss Window : 1.0757316509082238\n",
      "Step 2001 | Sliding Loss Window : 1.0023089976536657\n",
      "Step 2051 | Sliding Loss Window : 1.0846297283304644\n",
      "Step 2101 | Sliding Loss Window : 1.0895649624343549\n",
      "Step 2151 | Sliding Loss Window : 1.0107243088327416\n",
      "Step 2201 | Sliding Loss Window : 1.0782441070298054\n",
      "Step 2251 | Sliding Loss Window : 1.0733665591561912\n",
      "Step 2301 | Sliding Loss Window : 0.9686778407484539\n",
      "Step 2351 | Sliding Loss Window : 1.065140887751531\n",
      "Step 1 | Sliding Loss Window : 0.43067397121603485\n",
      "Step 51 | Sliding Loss Window : 1.1574186095515666\n",
      "Step 101 | Sliding Loss Window : 1.119745213314207\n",
      "Step 151 | Sliding Loss Window : 1.089017237254798\n",
      "Step 201 | Sliding Loss Window : 1.1117808611733295\n",
      "Step 251 | Sliding Loss Window : 1.064657643622312\n",
      "Step 301 | Sliding Loss Window : 1.0905413453191959\n",
      "Step 351 | Sliding Loss Window : 1.08385371588966\n",
      "Step 401 | Sliding Loss Window : 1.0958307466851493\n",
      "Step 451 | Sliding Loss Window : 1.0359301873631719\n",
      "Step 501 | Sliding Loss Window : 1.0561160711144426\n",
      "Step 551 | Sliding Loss Window : 1.1184703742976745\n",
      "Step 601 | Sliding Loss Window : 1.0373916767323241\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from create_human_design_circs import generate_human_design_circ\n",
    "\n",
    "dataset = 'moons_300'\n",
    "num_reps = 2\n",
    "\n",
    "num_qubits = 2\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "meas_qubits = [0]\n",
    "\n",
    "ours_dir = './human_design/amp_basic/moons/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', num_reps)\n",
    "x_train = x_train.reshape((len(x_train), 1, x_train.shape[1]))\n",
    "x_test = x_test.reshape((len(x_test), 1, x_test.shape[1]))\n",
    "\n",
    "num_params = [2 * i for i in range(1, 9)]\n",
    "params_per_layer = 2\n",
    "\n",
    "num_embed_layers = 1\n",
    "\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "\n",
    "var_layer_options = [[qml.RY], [qml.RX]] * 4\n",
    "enc_layer_options = [[None, True]]\n",
    "\n",
    "for curr_num_params in num_params:\n",
    "    curr_dir = ours_dir + '{}_params'.format(curr_num_params)\n",
    "    \n",
    "    if not os.path.exists(curr_dir):\n",
    "        os.mkdir(curr_dir)\n",
    "        \n",
    "    curr_num_layers = curr_num_params // params_per_layer\n",
    "    curr_weights_shape = (curr_num_layers, 1, num_qubits)\n",
    "        \n",
    "    for i in range(1):\n",
    "        curr_circ_dir = curr_dir\n",
    "\n",
    "        circ = generate_human_design_circ(dev, num_qubits, 'amp', 'basic', num_embed_layers, curr_num_layers, enc_layer_options, var_layer_options[:curr_num_layers], [0], 'exp')\n",
    "        \n",
    "        losses_list = []\n",
    "        accs_list = []\n",
    "        \n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_circ_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "            if os.path.exists(curr_train_dir):\n",
    "                pass\n",
    "            else:\n",
    "                os.mkdir(curr_train_dir)\n",
    "                        \n",
    "            info = train_qnn(circ, x_train, y_train, x_test, y_test, curr_weights_shape, 2400, 0.05, 1, mse_loss, verbosity=17300, \n",
    "                                                                                            loss_window=50, init_params=None, \n",
    "                                                                                            acc_thres=1.1, shuffle=True, print_loss=50)\n",
    "\n",
    "            val_exps = np.array([circ(x_test[i], info[-1][-1]) for i in range(len(x_test))])\n",
    "            val_loss = np.array([mse_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "            acc = np.mean(val_loss < 1)\n",
    "\n",
    "            shape = np.array(info[-1]).shape\n",
    "            \n",
    "            np.savetxt(curr_train_dir + '/params_{}.txt'.format(j + 1), np.array(info[-1]).reshape(shape[0], np.product(shape[1:])))\n",
    "            np.savetxt(curr_train_dir + '/losses_{}.txt'.format(j + 1), info[0])\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        np.savetxt(curr_circ_dir + '/accs.txt', accs_list)\n",
    "        np.savetxt(curr_circ_dir + '/val_losses.txt', losses_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## suprennt circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_supernet_circ_into_gate_circ(subnet, num_embeds, layer_rots, layer_cnots, num_qubits, angle_embed=False):\n",
    "    circ_gates = []\n",
    "    gate_params = []\n",
    "    weights_bounds = [0]\n",
    "    inputs_bounds = [0]\n",
    "\n",
    "    curr_layers = subnet\n",
    "    num_cnots = len(layer_cnots)\n",
    "    \n",
    "    for i in range(1):\n",
    "        num_qubits = len(layer_rots[0])\n",
    "\n",
    "        if angle_embed:\n",
    "            rots = ['ry', 'rx', 'rz']\n",
    "            \n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += [rots[j % 3] for i in range(num_qubits)]\n",
    "                gate_params += [[i] for i in range(num_qubits)]\n",
    "                weights_bounds += [0 for i in range(num_qubits)]\n",
    "                inputs_bounds += [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]            \n",
    "        else:\n",
    "            for j in range(num_embeds):\n",
    "                circ_gates += ['h' for i in range(num_qubits)] + ['ry' for i in range(num_qubits)] + ['cry' for i in range(num_qubits - 1)]\n",
    "                gate_params += [[i] for i in range(num_qubits)] * 2 + [[i, i + 1] for i in range(num_qubits - 1)]\n",
    "                weights_bounds += [0 for i in range(3 * num_qubits - 1)]\n",
    "                inputs_bounds += [inputs_bounds[-1] for i in range(num_qubits)] + [inputs_bounds[-1] + i + 1 for i in range(2 * num_qubits - 1)]\n",
    "\n",
    "        for j in range(len(curr_layers)):\n",
    "            circ_gates += layer_rots[curr_layers[j] // num_cnots]\n",
    "            circ_gates += ['cx' for k in layer_cnots[curr_layers[j] % num_cnots]]\n",
    "\n",
    "            gate_params += [[k] for k in range(len(layer_rots[curr_layers[j] // num_cnots]))]\n",
    "            gate_params += layer_cnots[curr_layers[j] % num_cnots]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] + k + 1 for k in range(num_qubits)]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(num_qubits)]\n",
    "\n",
    "            weights_bounds += [weights_bounds[-1] for k in range(len(layer_cnots[curr_layers[j] % num_cnots]))]\n",
    "            inputs_bounds += [inputs_bounds[-1] for k in range(len(layer_cnots[curr_layers[j] % num_cnots]))]\n",
    "            \n",
    "    return circ_gates, gate_params, inputs_bounds, weights_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-10-02 14:00:27,441: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22007 13888] 0.545 1.0312944316864014\n",
      "[313629 219683] 0.5025 1.1062126159667969\n",
      "[108118 172841] 0.49 1.0693354988098145\n",
      "[129815 182309] 0.4925 1.0710063648223878\n",
      "[165383 182834] 0.5 1.0595513916015624\n",
      "[291634 299755] 0.5225 1.0270887565612794\n",
      "[303740  61252] 0.4875 1.0382840061187744\n",
      "[147308 269710] 0.4825 1.1894860363006592\n",
      "[106918 192941] 0.4975 1.1269544219970704\n",
      "[152205 192100] 0.5175 1.0294874572753907\n",
      "[71775 92834] 0.4625 1.049727725982666\n",
      "[ 18065 162096] 0.4725 1.0946965885162354\n",
      "[138051 316763] 0.5325 1.0468227195739745\n",
      "[242647 182768] 0.4875 1.0120526790618896\n",
      "[ 33028 236133] 0.5225 1.038199815750122\n",
      "[270918 268885] 0.5225 1.0171918964385986\n",
      "[290134  38533] 0.545 1.0063713741302491\n",
      "[189965 165818] 0.5125 1.0728435230255127\n",
      "[ 26412 266388] 0.5125 1.0921786975860597\n",
      "[216357 134095] 0.485 1.026086778640747\n",
      "[258350 159413] 0.4625 1.0286151123046876\n",
      "[190492  58380] 0.5 1.0561509704589844\n",
      "[268668   8199] 0.47 1.0112923049926759\n",
      "[68699 42665] 0.48 1.0920231819152832\n",
      "[102055 253000] 0.4375 1.1787511348724364\n",
      "[58071 30768] 0.505 1.0385432052612305\n",
      "[315691  63710] 0.5275 1.0753718662261962\n",
      "[90051 61806] 0.4675 1.0167896366119384\n",
      "[330256  95934] 0.49 1.0188740730285644\n",
      "[236959 228280] 0.4925 1.0218141460418702\n",
      "[193161  47855] 0.525 1.0076004314422606\n",
      "[148653 297504] 0.5225 1.0203702354431152\n",
      "[263703 264589] 0.475 1.0908199787139892\n",
      "[147859 115385] 0.485 1.0365962123870849\n",
      "[194186 263287] 0.515 1.0218589878082276\n",
      "[207301 317944] 0.5075 1.0684480953216553\n",
      "[ 34075 306433] 0.48 1.0045617008209229\n",
      "[ 16716 172482] 0.5125 1.0428044128417968\n",
      "[301857 107412] 0.525 1.090922975540161\n",
      "[108530  41911] 0.48 1.0637263584136962\n",
      "[214012 103406] 0.4925 1.0955057430267334\n",
      "[280522 175209] 0.475 1.0805041980743408\n",
      "[267402 321430] 0.53 0.9915432739257812\n",
      "[323156 132712] 0.4875 1.0405407905578614\n",
      "[62201 41455] 0.4975 1.0576329708099366\n",
      "[  6595 192594] 0.4675 1.04718035697937\n",
      "[ 59868 159423] 0.465 1.0616452026367187\n",
      "[276291 291554] 0.5175 1.0608576393127442\n",
      "[200616 228350] 0.51 1.0460291767120362\n",
      "[243266 181260] 0.495 1.0819207000732423\n",
      "[288887 208966] 0.505 1.0915180492401122\n",
      "[20949 73561] 0.51 1.0138326930999755\n",
      "[270803 321386] 0.5475 1.0152289009094237\n",
      "[183736  96192] 0.4775 1.0977557754516603\n",
      "[57819 41682] 0.4775 1.0739888763427734\n",
      "[189626 262521] 0.525 1.0195621681213378\n",
      "[37079  2807] 0.4525 1.0183695697784423\n",
      "[177848  15308] 0.4775 1.0145218658447266\n",
      "[30834 99340] 0.4625 1.038493881225586\n",
      "[143096 196393] 0.4875 1.0803091716766358\n",
      "[189033   4888] 0.4775 1.1020162963867188\n",
      "[290222  86806] 0.4875 1.0614315032958985\n",
      "[144310  91532] 0.48 1.030563898086548\n",
      "[136576 229791] 0.4925 1.1161961555480957\n",
      "[224508  60621] 0.5225 1.06635910987854\n",
      "[216277 110515] 0.5 1.0372221851348877\n",
      "[304054 303378] 0.5075 1.0667936611175537\n",
      "[253621 299230] 0.45 1.151185655593872\n",
      "[ 14516 301783] 0.555 0.9685935688018799\n",
      "[171086 228601] 0.5175 1.019031105041504\n",
      "[279964 330411] 0.445 1.142887077331543\n",
      "[319584 246485] 0.4875 1.028750696182251\n",
      "[143420  95011] 0.5175 1.008111925125122\n",
      "[237425 145697] 0.5025 1.0097878360748291\n",
      "[178925 214221] 0.425 1.1752760791778565\n",
      "[215275 182346] 0.475 1.1294293117523193\n",
      "[134961 218430] 0.4875 1.0118975639343262\n",
      "[266258 145997] 0.5225 1.0164983654022217\n",
      "[121186  20906] 0.455 1.0327047729492187\n",
      "[115393 146667] 0.5075 1.0187810230255128\n",
      "[301834 274628] 0.51 1.0238809394836426\n",
      "[234235 109264] 0.4875 1.0115930938720703\n",
      "[ 9621 34908] 0.5225 1.0206630516052246\n",
      "[265513  29711] 0.46 1.0947807502746583\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "\n",
    "dataset = 'mnist_2'\n",
    "main_dir = './supernet/mnist_2/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'supernet', 1)\n",
    "\n",
    "num_qubits = 4\n",
    "num_cnot_configs = 4096\n",
    "num_circs = 2500\n",
    "num_embed_layers = 4\n",
    "\n",
    "device_name = 'ibmq_lima'\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "param_nums = [8, 12, 16, 20]\n",
    "\n",
    "for p in param_nums:\n",
    "    supernet_dir = main_dir + 'search-{}_params_mb'.format(p)\n",
    "\n",
    "    num_layers = p // num_qubits\n",
    "\n",
    "    layer_rots = open(supernet_dir + '/rotations.txt').read().split('\\n')[:-1]\n",
    "    layer_rots = [''.join([j for j in i if j.isupper()]) for i in layer_rots]\n",
    "    layer_rots = [[i[j * 2:j * 2 + 2].lower() for j in range(len(i) // 2)] for i in layer_rots]\n",
    "\n",
    "    layer_cnots = open(supernet_dir + '/cnots.txt').read().split('\\n')[:-1]\n",
    "    layer_cnots = [''.join([j for j in i[1:] if j not in ['[', ']', ',', ' ']]) for i in layer_cnots]\n",
    "    layer_cnots = [[[int(i[2 * j]), int(i[2 * j + 1])] for j in range(len(i) // 2)] for i in layer_cnots]\n",
    "        \n",
    "    search_space = len(layer_rots) * len(layer_cnots)\n",
    "        \n",
    "    accs = []\n",
    "    losses = []\n",
    "    circ_layers = []\n",
    "\n",
    "    params = np.genfromtxt(supernet_dir + '/training_params.txt')[-1].reshape((num_layers, search_space // num_cnot_configs, num_qubits))\n",
    "        \n",
    "    supernet_device_dir = supernet_dir + '/' + device_name\n",
    "    \n",
    "    if not os.path.exists(supernet_device_dir):\n",
    "        os.mkdir(supernet_device_dir)\n",
    "        \n",
    "    for i in range(num_circs):\n",
    "        curr_circ_desc = np.random.randint(0, search_space, num_layers)\n",
    "#         curr_circ_desc = [[272920], [163863], [258091], [172861], [126492]][i]\n",
    "\n",
    "        curr_params = np.concatenate([params[k, curr_circ_desc[k] // num_cnot_configs] for k in range(num_layers)]).flatten()\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = convert_supernet_circ_into_gate_circ(curr_circ_desc, num_embed_layers, layer_rots, layer_cnots,\n",
    "                                                                                                          num_qubits, False) \n",
    "        \n",
    "        circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, [0], 'exp')\n",
    "        \n",
    "        val_exp_list = []\n",
    "        \n",
    "        for j in range(len(x_test)):\n",
    "            val_exp_list.append(circ(x_test[j], curr_params))\n",
    "            \n",
    "        val_exps = np.array(val_exp_list).flatten()\n",
    "        val_losses = np.power(val_exps - y_test, 2)\n",
    "        val_loss = np.mean(val_losses)\n",
    "        acc = np.mean(np.multiply(val_exps, y_test) > 0)\n",
    "            \n",
    "        print(curr_circ_desc, acc, val_loss)\n",
    "\n",
    "        accs.append(acc)\n",
    "        losses.append(val_loss)\n",
    "        circ_layers.append(curr_circ_desc)\n",
    "        \n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_layers.txt', np.array(circ_layers))\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_accs.txt', accs)\n",
    "    np.savetxt(supernet_device_dir + '/searched_circ_losses.txt', losses)  \n",
    "#     np.savetxt(supernet_dir + '/searched_circ_accs.txt', ['{} {}'.format(circ_layers[i], accs[i]) for i in range(num_circs)], fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./supernet/bank/search-4_params_mb\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(supernet_dir)\n",
    "print(inputs_bounds[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## our metrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from create_gate_circs import create_batched_gate_circ\n",
    "from metrics import compute_reduced_similarity\n",
    "\n",
    "search_params = [4, 8, 12, 16, 20]\n",
    "\n",
    "ideal = np.concatenate((np.ones((16, 16)), np.zeros((16, 16))))\n",
    "ideal = np.concatenate((ideal, ideal[::-1, :]), 1)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "meas_qubits = [0]\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset('mnist_2', 'angle', 1)\n",
    "\n",
    "class_0_sel = np.random.choice(800, 16, False)\n",
    "class_1_sel = np.random.choice(800, 16, False) + 800\n",
    "sel_inds = np.concatenate((class_0_sel, class_1_sel))\n",
    "\n",
    "sel_data = x_train[sel_inds]\n",
    "\n",
    "# np.savetxt('./ours/mnist_2/sel_data.txt', sel_data)\n",
    "\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "\n",
    "for p in search_params:\n",
    "    curr_dir = './ours/mnist_2/{}_params'.format(p)\n",
    "    \n",
    "    if not os.path.exists(curr_dir):\n",
    "        os.mkdir(curr_dir)\n",
    "    \n",
    "    d2_min_scores = []\n",
    "    d2_mean_scores = []\n",
    "    d2_var_scores = []\n",
    "    \n",
    "    d2_t_min_scores = []\n",
    "    d2_t_mean_scores = []\n",
    "    d2_t_var_scores = []\n",
    "    \n",
    "    mean_mat_scores = []\n",
    "    mean_t_mat_scores = []\n",
    "\n",
    "    for i in range(2500):\n",
    "        curr_circ_dir = curr_dir + '/circ_{}'.format(i + 1)\n",
    "\n",
    "        if not os.path.exists(curr_circ_dir):\n",
    "            os.mkdir(curr_circ_dir)\n",
    "        \n",
    "        ent_prob = np.random.sample()\n",
    "        cxz = np.random.sample()\n",
    "        pauli = 0\n",
    "\n",
    "        params = 2 * np.pi * np.random.sample((32, p))\n",
    "#         np.savetxt(curr_circ_dir + '/params.txt', params)\n",
    "        \n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = generate_true_random_gate_circ(num_qubits, num_embeds, p, \n",
    "                                                                                                        ent_prob=ent_prob, \n",
    "                                                                                                        cxz_prob=cxz * ent_prob,\n",
    "                                                                                                        pauli_prob=pauli * (\n",
    "                                                                                                            1 - cxz) * ent_prob)\n",
    "        \n",
    "#         np.savetxt(curr_circ_dir + '/gates.txt', circ_gates, fmt=\"%s\")\n",
    "#         np.savetxt(curr_circ_dir + '/gate_params.txt', gate_params, fmt=\"%s\")\n",
    "#         np.savetxt(curr_circ_dir + '/inputs_bounds.txt', inputs_bounds)\n",
    "#         np.savetxt(curr_circ_dir + '/weights_bounds.txt', weights_bounds)\n",
    "\n",
    "        batched_circ = create_batched_gate_circ(qml.device('lightning.qubit', wires=num_qubits), circ_gates, gate_params, inputs_bounds,\n",
    "                                            weights_bounds, meas_qubits, 'matrix') \n",
    "\n",
    "        circ_d2_scores = []\n",
    "        circ_d2_t_scores = []\n",
    "        circ_mean_mat = np.zeros((32, 32))\n",
    "        circ_t_mean_mat = np.zeros((32, 32))\n",
    "\n",
    "        for j in range(32):\n",
    "            curr_params = np.concatenate([params[j] for k in range(32)]).reshape((32, p))\n",
    "            mat = compute_reduced_similarity(batched_circ, curr_params, sel_data)\n",
    "            \n",
    "            t_mat = mat > ((np.sum(mat) - 32) / 996)\n",
    "        \n",
    "            diff_mat = mat - ideal\n",
    "            diff_2d = np.sum(np.multiply(diff_mat, diff_mat))\n",
    "\n",
    "            diff_t_mat = t_mat - ideal\n",
    "            diff_2d_t = np.sum(np.multiply(diff_t_mat, diff_t_mat))\n",
    "\n",
    "            circ_d2_scores.append(diff_2d)\n",
    "            circ_d2_t_scores.append(diff_2d_t)\n",
    "\n",
    "            circ_mean_mat += mat / 32\n",
    "            circ_t_mean_mat += t_mat / 32\n",
    "            \n",
    "#         np.savetxt(curr_circ_dir + '/d2_scores.txt', circ_d2_scores)\n",
    "#         np.savetxt(curr_circ_dir + '/d2_t_scores.txt', circ_d2_t_scores)\n",
    "        \n",
    "        d2_min_scores.append(np.min(circ_d2_scores))\n",
    "        d2_mean_scores.append(np.mean(circ_d2_scores))\n",
    "        d2_var_scores.append(np.var(circ_d2_scores))\n",
    "        \n",
    "        d2_t_min_scores.append(np.min(circ_d2_t_scores))\n",
    "        d2_t_mean_scores.append(np.mean(circ_d2_t_scores))\n",
    "        d2_t_var_scores.append(np.var(circ_d2_t_scores))\n",
    "        \n",
    "        diff_mean_mat = ideal - circ_mean_mat\n",
    "        diff_t_mean_mat = ideal - circ_t_mean_mat\n",
    "        \n",
    "        mean_mat_scores.append(np.sum(np.multiply(diff_mean_mat, diff_mean_mat)))\n",
    "        mean_t_mat_scores.append(np.sum(np.multiply(diff_t_mean_mat, diff_t_mean_mat)))\n",
    "        \n",
    "        print(i)\n",
    "        \n",
    "#     np.savetxt(curr_dir + '/d2_mean_scores.txt', d2_mean_scores)\n",
    "#     np.savetxt(curr_dir + '/d2_min_scores.txt', d2_min_scores)\n",
    "#     np.savetxt(curr_dir + '/d2_var_scores.txt', d2_var_scores)\n",
    "    \n",
    "#     np.savetxt(curr_dir + '/d2_t_mean_scores.txt', d2_t_mean_scores)\n",
    "#     np.savetxt(curr_dir + '/d2_t_min_scores.txt', d2_t_min_scores)\n",
    "#     np.savetxt(curr_dir + '/d2_t_var_scores.txt', d2_t_var_scores)\n",
    "    \n",
    "#     np.savetxt(curr_dir + '/d2_mean_mat_scores.txt', mean_mat_scores)\n",
    "#     np.savetxt(curr_dir + '/d2_mean_t_mat_scores.txt', mean_t_mat_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train our circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-09-26 17:58:52,757: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1067 2082 1279 2370 1200   57 1131  720 1096  134  991  175 2133  455\n",
      " 1992 1644  768 1804 1429 1133 2064   92 1102  919 2063  663 1039 2121\n",
      "   27 2153 2243  179 1034  533 1877 2346  890 1832 2367 1553  825  268\n",
      "  128  671 1619  918 1800  628 1610  146  217  100 1582  104 1595 1327\n",
      " 2221 1374 1969 1990  827 2291 1360  599 2324  892 2093  467  578 1298\n",
      " 1681 2347 1742 2487  659 1095 1323 2014  759   19  526 1577 2494  630\n",
      " 2354  190 1618 1693   34 1662  243  376 2234 1591  562 2010  147  622\n",
      " 2254 1286] [0.35647452 0.44221437 0.46940085 0.50470418 0.49201727 0.4377847\n",
      " 0.57586987 0.47526027 0.42752922 0.45651078 0.54055554 0.45351791\n",
      " 0.57267149 0.56381992 0.62488468 0.45341074 0.47422796 0.55102779\n",
      " 0.56935706 0.56489674 0.44816101 0.61210878 0.44867516 0.5373931\n",
      " 0.43963069 0.58558509 0.60000924 0.6152313  0.64009358 0.4437561\n",
      " 0.57632821 0.58982131 0.55132762 0.59124615 0.615347   0.55868626\n",
      " 0.58827229 0.50437095 0.59651261 0.49384058 0.4331398  0.5064426\n",
      " 0.59331018 0.4532665  0.57546836 0.44421387 0.45083618 0.59811547\n",
      " 0.49720347 0.50458851 0.47032535 0.60685375 0.57902589 0.48161003\n",
      " 0.66493027 0.59750303 0.57227951 0.4226799  0.53281449 0.46542898\n",
      " 0.55493805 0.45443624 0.50044017 0.55609184 0.56275291 0.58877084\n",
      " 0.4889018  0.45523834 0.5566327  0.49327418 0.58835647 0.58505876\n",
      " 0.41220474 0.58880377 0.4670994  0.59572031 0.56960571 0.55659971\n",
      " 0.68144804 0.55435869 0.61103686 0.50207675 0.52429752 0.45351014\n",
      " 0.46042839 0.44574431 0.53326178 0.38337708 0.44689178 0.57276154\n",
      " 0.49696014 0.60201282 0.4825111  0.58012187 0.42696074 0.47948456\n",
      " 0.58873909 0.43344879 0.41817254 0.48957318]\n",
      "759 0.6814480360480957\n",
      "0.9041595458984375 0.7536811828613281\n",
      "Step 1 | Sliding Loss Window : 2.0197509429707656\n",
      "Step 51 | Sliding Loss Window : 1.1194579790598589\n",
      "Step 101 | Sliding Loss Window : 1.189942727188913\n",
      "Step 151 | Sliding Loss Window : 1.0356573589008269\n",
      "Step 201 | Sliding Loss Window : 1.0610440252523659\n",
      "Step 251 | Sliding Loss Window : 1.1021168061557496\n",
      "Step 301 | Sliding Loss Window : 1.0503530449895535\n",
      "Step 351 | Sliding Loss Window : 1.1044914705623592\n",
      "Step 401 | Sliding Loss Window : 1.0481111385625308\n",
      "Step 451 | Sliding Loss Window : 1.1090139788766182\n",
      "Step 501 | Sliding Loss Window : 1.073045296502808\n",
      "Step 551 | Sliding Loss Window : 1.064584744585284\n",
      "Step 601 | Sliding Loss Window : 1.0421525822434732\n",
      "Step 651 | Sliding Loss Window : 1.037499913258553\n",
      "Step 701 | Sliding Loss Window : 0.9470483860820377\n",
      "Step 751 | Sliding Loss Window : 0.9378955331022936\n",
      "Step 801 | Sliding Loss Window : 0.9219231852897838\n",
      "Step 851 | Sliding Loss Window : 1.020884269094159\n",
      "Step 901 | Sliding Loss Window : 1.0403760023976474\n",
      "Step 951 | Sliding Loss Window : 0.9197151057595394\n",
      "Step 1001 | Sliding Loss Window : 0.941134180770108\n",
      "Step 1051 | Sliding Loss Window : 0.9558084400873526\n",
      "Step 1101 | Sliding Loss Window : 1.044176222587024\n",
      "Step 1151 | Sliding Loss Window : 0.8525145057937342\n",
      "Step 1201 | Sliding Loss Window : 1.0113809950474144\n",
      "Step 1251 | Sliding Loss Window : 0.9002654962553265\n",
      "Step 1301 | Sliding Loss Window : 0.8727809185589861\n",
      "Step 1351 | Sliding Loss Window : 1.0603034385769363\n",
      "Step 1401 | Sliding Loss Window : 0.9685717659560212\n",
      "Step 1451 | Sliding Loss Window : 1.0142321730171209\n",
      "Step 1501 | Sliding Loss Window : 1.0291289597575861\n",
      "Step 1551 | Sliding Loss Window : 0.956165895764459\n",
      "Step 1601 | Sliding Loss Window : 0.9606085654173515\n",
      "Step 1651 | Sliding Loss Window : 0.974634076884794\n",
      "Step 1701 | Sliding Loss Window : 1.025259613926934\n",
      "Step 1751 | Sliding Loss Window : 0.9551060516822798\n",
      "Step 1801 | Sliding Loss Window : 0.8997520055171832\n",
      "Step 1851 | Sliding Loss Window : 0.9196313321313678\n",
      "Step 1901 | Sliding Loss Window : 0.8934271214975156\n",
      "Step 1951 | Sliding Loss Window : 0.9888472649182006\n",
      "Step 2001 | Sliding Loss Window : 1.0525993754993217\n",
      "Step 2051 | Sliding Loss Window : 0.949673666502558\n",
      "Step 2101 | Sliding Loss Window : 0.9411915812230279\n",
      "Step 2151 | Sliding Loss Window : 0.9526489858761618\n",
      "Step 2201 | Sliding Loss Window : 1.009599504063969\n",
      "Step 2251 | Sliding Loss Window : 0.8874329304830111\n",
      "Step 2301 | Sliding Loss Window : 0.9966567060186183\n",
      "Step 2351 | Sliding Loss Window : 0.9018076683266542\n",
      "Step 2401 | Sliding Loss Window : 0.8601027926392867\n",
      "Step 2451 | Sliding Loss Window : 1.0655409559453588\n",
      "Step 2501 | Sliding Loss Window : 0.9825778682340147\n",
      "Step 2551 | Sliding Loss Window : 1.009304160788338\n",
      "Step 2601 | Sliding Loss Window : 1.0334095586331022\n",
      "Step 2651 | Sliding Loss Window : 0.9550505925590757\n",
      "Step 2701 | Sliding Loss Window : 0.9617407943044296\n",
      "Step 2751 | Sliding Loss Window : 0.9864473675435121\n",
      "Step 2801 | Sliding Loss Window : 1.0044846613716898\n",
      "Step 2851 | Sliding Loss Window : 0.9376150525278619\n",
      "Step 2901 | Sliding Loss Window : 0.9042962407681325\n",
      "Step 2951 | Sliding Loss Window : 0.9382539132992569\n",
      "Step 3001 | Sliding Loss Window : 0.9101751137464217\n",
      "Step 3051 | Sliding Loss Window : 0.9773579710915193\n",
      "Step 3101 | Sliding Loss Window : 1.0543902453079546\n",
      "Step 3151 | Sliding Loss Window : 0.9404006179588694\n",
      "Step 3201 | Sliding Loss Window : 0.958409493647109\n",
      "Step 3251 | Sliding Loss Window : 0.9520231724930457\n",
      "Step 3301 | Sliding Loss Window : 0.981383395325468\n",
      "Step 3351 | Sliding Loss Window : 0.9092110200542826\n",
      "Step 3401 | Sliding Loss Window : 0.9784035368233505\n",
      "Step 3451 | Sliding Loss Window : 0.9303457353696932\n",
      "Step 3501 | Sliding Loss Window : 0.8559303853747977\n",
      "Step 3551 | Sliding Loss Window : 1.0355267058486475\n",
      "Step 3601 | Sliding Loss Window : 1.016894092965445\n",
      "Step 3651 | Sliding Loss Window : 0.9858830023336255\n",
      "Step 3701 | Sliding Loss Window : 1.0367761130213142\n",
      "Step 3751 | Sliding Loss Window : 0.9420864513416793\n",
      "Step 3801 | Sliding Loss Window : 0.9677521609116861\n",
      "Step 3851 | Sliding Loss Window : 1.0070238494684902\n",
      "Step 3901 | Sliding Loss Window : 0.9865841129336316\n",
      "Step 3951 | Sliding Loss Window : 0.9320436912945919\n",
      "Step 4001 | Sliding Loss Window : 0.9298167096417306\n",
      "Step 4051 | Sliding Loss Window : 0.9132204729839053\n",
      "Step 4101 | Sliding Loss Window : 0.9303081560292346\n",
      "Step 4151 | Sliding Loss Window : 0.9680251043817504\n",
      "Step 4201 | Sliding Loss Window : 1.0674273917246073\n",
      "Step 4251 | Sliding Loss Window : 0.9032885209429959\n",
      "Step 4301 | Sliding Loss Window : 0.9747330329534175\n",
      "Step 4351 | Sliding Loss Window : 0.9534020419483276\n",
      "Step 4401 | Sliding Loss Window : 0.9796594042018117\n",
      "Step 4451 | Sliding Loss Window : 0.9158196706869962\n",
      "Step 4501 | Sliding Loss Window : 0.9713941877893637\n",
      "Step 4551 | Sliding Loss Window : 0.9618551268648358\n",
      "Step 4601 | Sliding Loss Window : 0.8903022635436513\n",
      "Step 4651 | Sliding Loss Window : 0.9734595587983218\n",
      "Step 4701 | Sliding Loss Window : 1.0331602293161362\n",
      "Step 4751 | Sliding Loss Window : 0.9843702450786763\n",
      "Step 4801 | Sliding Loss Window : 1.034932397273105\n",
      "Step 4851 | Sliding Loss Window : 0.9207917281845256\n",
      "Step 4901 | Sliding Loss Window : 0.9907906992457091\n",
      "Step 4951 | Sliding Loss Window : 0.9979911266067855\n",
      "Step 5001 | Sliding Loss Window : 0.9703096063828321\n",
      "Step 5051 | Sliding Loss Window : 0.9361678208736806\n",
      "Step 5101 | Sliding Loss Window : 0.9321963231624948\n",
      "Step 5151 | Sliding Loss Window : 0.9312380444582137\n",
      "Step 5201 | Sliding Loss Window : 0.9342592987192718\n",
      "Step 5251 | Sliding Loss Window : 1.0109860695583461\n",
      "Step 5301 | Sliding Loss Window : 1.0263157526622886\n",
      "Step 5351 | Sliding Loss Window : 0.8916988629941442\n",
      "Step 5401 | Sliding Loss Window : 0.9614647028761527\n",
      "Step 5451 | Sliding Loss Window : 0.9958932289774285\n",
      "Step 1 | Sliding Loss Window : 1.0121314104513242\n",
      "Step 51 | Sliding Loss Window : 1.0935140717013636\n",
      "Step 101 | Sliding Loss Window : 1.0405937490406054\n",
      "Step 151 | Sliding Loss Window : 1.0066325904970699\n",
      "Step 201 | Sliding Loss Window : 0.9534759318860989\n",
      "Step 251 | Sliding Loss Window : 0.930734950432056\n",
      "Step 301 | Sliding Loss Window : 0.978998536304909\n",
      "Step 351 | Sliding Loss Window : 0.9744005043080106\n",
      "Step 401 | Sliding Loss Window : 0.9985284487055205\n",
      "Step 451 | Sliding Loss Window : 0.9358914651330699\n",
      "Step 501 | Sliding Loss Window : 1.0029701056082392\n",
      "Step 551 | Sliding Loss Window : 0.9665803094726756\n",
      "Step 601 | Sliding Loss Window : 0.9057426962379681\n",
      "Step 651 | Sliding Loss Window : 0.8767256884406242\n",
      "Step 701 | Sliding Loss Window : 1.027399760882163\n",
      "Step 751 | Sliding Loss Window : 0.9860778068035393\n",
      "Step 801 | Sliding Loss Window : 0.9434857237983943\n",
      "Step 851 | Sliding Loss Window : 0.9347324853820496\n",
      "Step 901 | Sliding Loss Window : 0.9008803001471177\n",
      "Step 951 | Sliding Loss Window : 0.9115598969500737\n",
      "Step 1001 | Sliding Loss Window : 1.0619081858179424\n",
      "Step 1051 | Sliding Loss Window : 1.0318211665356185\n",
      "Step 1101 | Sliding Loss Window : 1.01706064229687\n",
      "Step 1151 | Sliding Loss Window : 0.9400464776881725\n",
      "Step 1201 | Sliding Loss Window : 0.967629168703147\n",
      "Step 1251 | Sliding Loss Window : 1.0151019051909125\n",
      "Step 1301 | Sliding Loss Window : 0.892638407815958\n",
      "Step 1351 | Sliding Loss Window : 0.8805728302159475\n",
      "Step 1401 | Sliding Loss Window : 0.9589960727513052\n",
      "Step 1451 | Sliding Loss Window : 1.089906596081861\n",
      "Step 1501 | Sliding Loss Window : 0.9348763667998722\n",
      "Step 1551 | Sliding Loss Window : 0.9399953240268651\n",
      "Step 1601 | Sliding Loss Window : 1.0050713389444859\n",
      "Step 1651 | Sliding Loss Window : 0.9737352610633856\n",
      "Step 1701 | Sliding Loss Window : 0.8998240203566238\n",
      "Step 1751 | Sliding Loss Window : 0.8668474621073868\n",
      "Step 1801 | Sliding Loss Window : 1.0520170834002152\n",
      "Step 1851 | Sliding Loss Window : 0.972867307643846\n",
      "Step 1901 | Sliding Loss Window : 0.9432038904738319\n",
      "Step 1951 | Sliding Loss Window : 0.9382530874198213\n",
      "Step 2001 | Sliding Loss Window : 0.9066806665322357\n",
      "Step 2051 | Sliding Loss Window : 0.8988387549695881\n",
      "Step 2101 | Sliding Loss Window : 1.0384759430337545\n",
      "Step 2151 | Sliding Loss Window : 1.0522502433813161\n",
      "Step 2201 | Sliding Loss Window : 1.0204588745298913\n",
      "Step 2251 | Sliding Loss Window : 0.9452120603064682\n",
      "Step 2301 | Sliding Loss Window : 0.9427275086950014\n",
      "Step 2351 | Sliding Loss Window : 1.039612645872194\n",
      "Step 2401 | Sliding Loss Window : 0.9052439904889482\n",
      "Step 2451 | Sliding Loss Window : 0.8548030540905661\n",
      "Step 2501 | Sliding Loss Window : 0.9745090662890794\n",
      "Step 2551 | Sliding Loss Window : 1.0818050906286245\n",
      "Step 2601 | Sliding Loss Window : 0.9281192793575176\n",
      "Step 2651 | Sliding Loss Window : 0.9336261754679929\n",
      "Step 2701 | Sliding Loss Window : 1.0284281058669118\n",
      "Step 2751 | Sliding Loss Window : 0.9725821641805051\n",
      "Step 2801 | Sliding Loss Window : 0.8914023982107909\n",
      "Step 2851 | Sliding Loss Window : 0.8686130953725494\n",
      "Step 2901 | Sliding Loss Window : 1.0496035194181317\n",
      "Step 2951 | Sliding Loss Window : 0.9732250189486527\n",
      "Step 3001 | Sliding Loss Window : 0.9479224698335138\n",
      "Step 3051 | Sliding Loss Window : 0.9383645571967917\n",
      "Step 3101 | Sliding Loss Window : 0.9040934892234113\n",
      "Step 3151 | Sliding Loss Window : 0.8884072338366089\n",
      "Step 3201 | Sliding Loss Window : 1.100083342809827\n",
      "Step 3251 | Sliding Loss Window : 1.032631976381299\n",
      "Step 3301 | Sliding Loss Window : 0.9886140413885162\n",
      "Step 3351 | Sliding Loss Window : 0.9417378379011082\n",
      "Step 3401 | Sliding Loss Window : 0.9369234044517043\n",
      "Step 3451 | Sliding Loss Window : 1.0619530414326894\n",
      "Step 3501 | Sliding Loss Window : 0.9032390105558424\n",
      "Step 3551 | Sliding Loss Window : 0.8496690854358924\n",
      "Step 3601 | Sliding Loss Window : 0.9613919490659195\n",
      "Step 3651 | Sliding Loss Window : 1.0685915564916377\n",
      "Step 3701 | Sliding Loss Window : 0.9406285395352976\n",
      "Step 3751 | Sliding Loss Window : 0.9478891292470153\n",
      "Step 3801 | Sliding Loss Window : 1.0304530914726515\n",
      "Step 3851 | Sliding Loss Window : 0.9611932812910028\n",
      "Step 3901 | Sliding Loss Window : 0.913123468304192\n",
      "Step 3951 | Sliding Loss Window : 0.8438902094955352\n",
      "Step 4001 | Sliding Loss Window : 1.0547517430585942\n",
      "Step 4051 | Sliding Loss Window : 0.9665488717932322\n",
      "Step 4101 | Sliding Loss Window : 0.9566322281873257\n",
      "Step 4151 | Sliding Loss Window : 0.9363782373758874\n",
      "Step 4201 | Sliding Loss Window : 0.9543871997137973\n",
      "Step 4251 | Sliding Loss Window : 0.8425375115820537\n",
      "Step 4301 | Sliding Loss Window : 1.1089748318164911\n",
      "Step 4351 | Sliding Loss Window : 1.0111617950551557\n",
      "Step 4401 | Sliding Loss Window : 0.983669773354992\n",
      "Step 4451 | Sliding Loss Window : 0.9378744873736694\n",
      "Step 4501 | Sliding Loss Window : 0.9468431745819889\n",
      "Step 4551 | Sliding Loss Window : 1.06007690500789\n",
      "Step 4601 | Sliding Loss Window : 0.9113435968063825\n",
      "Step 4651 | Sliding Loss Window : 0.8380526856443226\n",
      "Step 4701 | Sliding Loss Window : 0.9794439608650845\n",
      "Step 4751 | Sliding Loss Window : 1.0655167583671257\n",
      "Step 4801 | Sliding Loss Window : 0.9365245807223468\n",
      "Step 4851 | Sliding Loss Window : 0.9708397825829215\n",
      "Step 4901 | Sliding Loss Window : 1.0121835685061062\n",
      "Step 4951 | Sliding Loss Window : 0.9443903780250718\n",
      "Step 5001 | Sliding Loss Window : 0.9296019549862709\n",
      "Step 5051 | Sliding Loss Window : 0.8231059963210443\n",
      "Step 5101 | Sliding Loss Window : 1.0868690724520633\n",
      "Step 5151 | Sliding Loss Window : 0.9453938081154449\n",
      "Step 5201 | Sliding Loss Window : 0.9579505640084843\n",
      "Step 5251 | Sliding Loss Window : 0.9471014167438832\n",
      "Step 5301 | Sliding Loss Window : 0.9421486669100629\n",
      "Step 5351 | Sliding Loss Window : 0.8411106999749032\n",
      "Step 5401 | Sliding Loss Window : 1.1257345829175858\n",
      "Step 5451 | Sliding Loss Window : 0.9941041277797469\n",
      "Step 1 | Sliding Loss Window : 0.7614008841297983\n",
      "Step 51 | Sliding Loss Window : 1.1533223913007258\n",
      "Step 101 | Sliding Loss Window : 1.0896348717301199\n",
      "Step 151 | Sliding Loss Window : 1.0789374250428507\n",
      "Step 201 | Sliding Loss Window : 1.0534263329846112\n",
      "Step 251 | Sliding Loss Window : 1.113930891059272\n",
      "Step 301 | Sliding Loss Window : 1.0238256926259623\n",
      "Step 351 | Sliding Loss Window : 1.0281062519158408\n",
      "Step 401 | Sliding Loss Window : 0.9464258735029287\n",
      "Step 451 | Sliding Loss Window : 1.1239241024903897\n",
      "Step 501 | Sliding Loss Window : 0.916519319970672\n",
      "Step 551 | Sliding Loss Window : 1.1004544972082249\n",
      "Step 601 | Sliding Loss Window : 0.91779060493332\n",
      "Step 651 | Sliding Loss Window : 1.0228918611235498\n",
      "Step 701 | Sliding Loss Window : 0.9493279621679378\n",
      "Step 751 | Sliding Loss Window : 1.1476725708423947\n",
      "Step 801 | Sliding Loss Window : 1.0188905596104825\n",
      "Step 851 | Sliding Loss Window : 0.9728894216637437\n",
      "Step 901 | Sliding Loss Window : 1.0106349785153281\n",
      "Step 951 | Sliding Loss Window : 1.0673315823060947\n",
      "Step 1001 | Sliding Loss Window : 1.0967866660971815\n",
      "Step 1051 | Sliding Loss Window : 0.8507056580910135\n",
      "Step 1101 | Sliding Loss Window : 0.9744670934236163\n",
      "Step 1151 | Sliding Loss Window : 1.092767593648896\n",
      "Step 1201 | Sliding Loss Window : 1.0762867119289574\n",
      "Step 1251 | Sliding Loss Window : 1.0981320500802776\n",
      "Step 1301 | Sliding Loss Window : 1.024821678612981\n",
      "Step 1351 | Sliding Loss Window : 1.079764534224058\n",
      "Step 1401 | Sliding Loss Window : 1.0584898764289823\n",
      "Step 1451 | Sliding Loss Window : 0.9936316191581112\n",
      "Step 1501 | Sliding Loss Window : 0.9616298449498675\n",
      "Step 1551 | Sliding Loss Window : 1.0820076389821802\n",
      "Step 1601 | Sliding Loss Window : 0.947054365781274\n",
      "Step 1651 | Sliding Loss Window : 1.0774133865162812\n",
      "Step 1701 | Sliding Loss Window : 0.9239269470686021\n",
      "Step 1751 | Sliding Loss Window : 1.0043364647905038\n",
      "Step 1801 | Sliding Loss Window : 0.9969113171895627\n",
      "Step 1851 | Sliding Loss Window : 1.099278933131409\n",
      "Step 1901 | Sliding Loss Window : 1.013402965184087\n",
      "Step 1951 | Sliding Loss Window : 0.9645544150745738\n",
      "Step 2001 | Sliding Loss Window : 1.0130220054460595\n",
      "Step 2051 | Sliding Loss Window : 1.0741406505515518\n",
      "Step 2101 | Sliding Loss Window : 1.0839365135481391\n",
      "Step 2151 | Sliding Loss Window : 0.8318025267013119\n",
      "Step 2201 | Sliding Loss Window : 0.9940599751471398\n",
      "Step 2251 | Sliding Loss Window : 1.0804139504035917\n",
      "Step 2301 | Sliding Loss Window : 1.0583245306906612\n",
      "Step 2351 | Sliding Loss Window : 1.1172791884189817\n",
      "Step 2401 | Sliding Loss Window : 1.0126341271930561\n",
      "Step 2451 | Sliding Loss Window : 1.0646659958121614\n",
      "Step 2501 | Sliding Loss Window : 1.051930283581907\n",
      "Step 2551 | Sliding Loss Window : 1.0187314379970518\n",
      "Step 2601 | Sliding Loss Window : 0.9339154929599599\n",
      "Step 2651 | Sliding Loss Window : 1.0957948998989397\n",
      "Step 2701 | Sliding Loss Window : 0.9477315763474904\n",
      "Step 2751 | Sliding Loss Window : 1.0576110178486995\n",
      "Step 2801 | Sliding Loss Window : 0.9248948703147444\n",
      "Step 2851 | Sliding Loss Window : 1.0105224327774176\n",
      "Step 2901 | Sliding Loss Window : 1.034141692520129\n",
      "Step 2951 | Sliding Loss Window : 1.0706422478365285\n",
      "Step 3001 | Sliding Loss Window : 0.9933572127801085\n",
      "Step 3051 | Sliding Loss Window : 1.0078056612584003\n",
      "Step 3101 | Sliding Loss Window : 0.9799452541197905\n",
      "Step 3151 | Sliding Loss Window : 1.0716148743882006\n",
      "Step 3201 | Sliding Loss Window : 1.0892232762002236\n",
      "Step 3251 | Sliding Loss Window : 0.8249071401666477\n",
      "Step 3301 | Sliding Loss Window : 0.9929344122859557\n",
      "Step 3351 | Sliding Loss Window : 1.0860601896658089\n",
      "Step 3401 | Sliding Loss Window : 1.0504311977717158\n",
      "Step 3451 | Sliding Loss Window : 1.1411237883565457\n",
      "Step 3501 | Sliding Loss Window : 0.9952921417502096\n",
      "Step 3551 | Sliding Loss Window : 1.064154937631145\n",
      "Step 3601 | Sliding Loss Window : 1.0374419323736674\n",
      "Step 3651 | Sliding Loss Window : 1.0418899255587868\n",
      "Step 3701 | Sliding Loss Window : 0.905111447168958\n",
      "Step 3751 | Sliding Loss Window : 1.1066981657058648\n",
      "Step 3801 | Sliding Loss Window : 0.957613030934008\n",
      "Step 3851 | Sliding Loss Window : 1.0618743724563853\n",
      "Step 3901 | Sliding Loss Window : 0.8982174776544515\n",
      "Step 3951 | Sliding Loss Window : 1.031414604482048\n",
      "Step 4001 | Sliding Loss Window : 1.0315595362993981\n",
      "Step 4051 | Sliding Loss Window : 1.0443044024162846\n",
      "Step 4101 | Sliding Loss Window : 1.0163541805976517\n",
      "Step 4151 | Sliding Loss Window : 1.0229600110633528\n",
      "Step 4201 | Sliding Loss Window : 0.9794274481044457\n",
      "Step 4251 | Sliding Loss Window : 1.0789277538601914\n",
      "Step 4301 | Sliding Loss Window : 1.079164015437661\n",
      "Step 4351 | Sliding Loss Window : 0.8211920070746394\n",
      "Step 4401 | Sliding Loss Window : 0.9820280282122362\n",
      "Step 4451 | Sliding Loss Window : 1.0790872733119843\n",
      "Step 4501 | Sliding Loss Window : 1.0854922496370307\n",
      "Step 4551 | Sliding Loss Window : 1.1107117267644997\n",
      "Step 4601 | Sliding Loss Window : 1.0176437927407467\n",
      "Step 4651 | Sliding Loss Window : 1.0682642035010652\n",
      "Step 4701 | Sliding Loss Window : 1.0305578314306059\n",
      "Step 4751 | Sliding Loss Window : 1.0177710446358197\n",
      "Step 4801 | Sliding Loss Window : 0.8897423177385864\n",
      "Step 4851 | Sliding Loss Window : 1.1207400334406075\n",
      "Step 4901 | Sliding Loss Window : 0.9627663776561682\n",
      "Step 4951 | Sliding Loss Window : 1.0768299667943455\n",
      "Step 5001 | Sliding Loss Window : 0.8910534244396938\n",
      "Step 5051 | Sliding Loss Window : 1.0184063047184742\n",
      "Step 5101 | Sliding Loss Window : 1.038388718914441\n",
      "Step 5151 | Sliding Loss Window : 1.0163885931497432\n",
      "Step 5201 | Sliding Loss Window : 1.0467411492715604\n",
      "Step 5251 | Sliding Loss Window : 1.0116680377976452\n",
      "Step 5301 | Sliding Loss Window : 0.9896430766329428\n",
      "Step 5351 | Sliding Loss Window : 1.086636063303358\n",
      "Step 5401 | Sliding Loss Window : 1.0782616886002783\n",
      "Step 5451 | Sliding Loss Window : 0.7843276884691691\n",
      "Step 1 | Sliding Loss Window : 1.9142909140949367\n",
      "Step 51 | Sliding Loss Window : 1.1229381524531485\n",
      "Step 101 | Sliding Loss Window : 1.0125618868202597\n",
      "Step 151 | Sliding Loss Window : 1.0961453053962606\n",
      "Step 201 | Sliding Loss Window : 1.0425441691773438\n",
      "Step 251 | Sliding Loss Window : 0.9524968067099974\n",
      "Step 301 | Sliding Loss Window : 1.0719810370899432\n",
      "Step 351 | Sliding Loss Window : 1.0046095354784104\n",
      "Step 401 | Sliding Loss Window : 0.9928117886392336\n",
      "Step 451 | Sliding Loss Window : 0.915883504503527\n",
      "Step 501 | Sliding Loss Window : 1.1027288566805824\n",
      "Step 551 | Sliding Loss Window : 1.0028708303685165\n",
      "Step 601 | Sliding Loss Window : 0.8869515187994951\n",
      "Step 651 | Sliding Loss Window : 1.0313010432754577\n",
      "Step 701 | Sliding Loss Window : 1.0124067779134132\n",
      "Step 751 | Sliding Loss Window : 0.9384130762285539\n",
      "Step 801 | Sliding Loss Window : 0.8918622142599087\n",
      "Step 851 | Sliding Loss Window : 0.9297763316821386\n",
      "Step 901 | Sliding Loss Window : 0.8245458213791124\n",
      "Step 951 | Sliding Loss Window : 0.8707704706118763\n",
      "Step 1001 | Sliding Loss Window : 0.9862350920527797\n",
      "Step 1051 | Sliding Loss Window : 0.958856037222834\n",
      "Step 1101 | Sliding Loss Window : 0.9165678454131659\n",
      "Step 1151 | Sliding Loss Window : 1.0708339536365765\n",
      "Step 1201 | Sliding Loss Window : 1.1837525661029027\n",
      "Step 1251 | Sliding Loss Window : 0.942248168767988\n",
      "Step 1301 | Sliding Loss Window : 0.9859834037972678\n",
      "Step 1351 | Sliding Loss Window : 0.8932198701960552\n",
      "Step 1401 | Sliding Loss Window : 1.0219851302859722\n",
      "Step 1451 | Sliding Loss Window : 0.917695251246993\n",
      "Step 1501 | Sliding Loss Window : 0.9558706407635198\n",
      "Step 1551 | Sliding Loss Window : 0.9183426506345616\n",
      "Step 1601 | Sliding Loss Window : 1.061697849817635\n",
      "Step 1651 | Sliding Loss Window : 0.9779737017810209\n",
      "Step 1701 | Sliding Loss Window : 0.8695896233901186\n",
      "Step 1751 | Sliding Loss Window : 1.068078312620421\n",
      "Step 1801 | Sliding Loss Window : 0.9385671001855136\n",
      "Step 1851 | Sliding Loss Window : 0.911769513481635\n",
      "Step 1901 | Sliding Loss Window : 0.9061316063345558\n",
      "Step 1951 | Sliding Loss Window : 0.9123109563550049\n",
      "Step 2001 | Sliding Loss Window : 0.8369337257045346\n",
      "Step 2051 | Sliding Loss Window : 0.8522283184178835\n",
      "Step 2101 | Sliding Loss Window : 1.0338246052587414\n",
      "Step 2151 | Sliding Loss Window : 0.9191790439249715\n",
      "Step 2201 | Sliding Loss Window : 0.9395326861444542\n",
      "Step 2251 | Sliding Loss Window : 1.0652575391821348\n",
      "Step 2301 | Sliding Loss Window : 1.1921037067991664\n",
      "Step 2351 | Sliding Loss Window : 0.9501943479507845\n",
      "Step 2401 | Sliding Loss Window : 0.9856542916603185\n",
      "Step 2451 | Sliding Loss Window : 0.9386091319752419\n",
      "Step 2501 | Sliding Loss Window : 0.9728051727800139\n",
      "Step 2551 | Sliding Loss Window : 0.9186254242869016\n",
      "Step 2601 | Sliding Loss Window : 0.9593664611474552\n",
      "Step 2651 | Sliding Loss Window : 0.9210378772866582\n",
      "Step 2701 | Sliding Loss Window : 1.073956017133933\n",
      "Step 2751 | Sliding Loss Window : 0.9435843610883607\n",
      "Step 2801 | Sliding Loss Window : 0.8948224236932468\n",
      "Step 2851 | Sliding Loss Window : 1.0716726877376113\n",
      "Step 2901 | Sliding Loss Window : 0.9192175779880286\n",
      "Step 2951 | Sliding Loss Window : 0.9208506419090979\n",
      "Step 3001 | Sliding Loss Window : 0.8961496113065327\n",
      "Step 3051 | Sliding Loss Window : 0.9102842110691841\n",
      "Step 3101 | Sliding Loss Window : 0.8623350440981813\n",
      "Step 3151 | Sliding Loss Window : 0.8788257977771058\n",
      "Step 3201 | Sliding Loss Window : 1.0156922042070666\n",
      "Step 3251 | Sliding Loss Window : 0.8811910434647313\n",
      "Step 3301 | Sliding Loss Window : 0.9488125466921593\n",
      "Step 3351 | Sliding Loss Window : 1.0787974983634165\n",
      "Step 3401 | Sliding Loss Window : 1.1935525395642839\n",
      "Step 3451 | Sliding Loss Window : 0.9340345467902428\n",
      "Step 3501 | Sliding Loss Window : 0.9894437020938112\n",
      "Step 3551 | Sliding Loss Window : 0.9225540753872822\n",
      "Step 3601 | Sliding Loss Window : 1.0097100135984234\n",
      "Step 3651 | Sliding Loss Window : 0.908240749900124\n",
      "Step 3701 | Sliding Loss Window : 0.9720667412604961\n",
      "Step 3751 | Sliding Loss Window : 0.9543158045575595\n",
      "Step 3801 | Sliding Loss Window : 1.045868647371452\n",
      "Step 3851 | Sliding Loss Window : 0.9259025628853906\n",
      "Step 3901 | Sliding Loss Window : 0.9526261614462542\n",
      "Step 3951 | Sliding Loss Window : 1.0032933509491515\n",
      "Step 4001 | Sliding Loss Window : 0.9275357522705732\n",
      "Step 4051 | Sliding Loss Window : 0.9117779058376998\n",
      "Step 4101 | Sliding Loss Window : 0.8970865699067297\n",
      "Step 4151 | Sliding Loss Window : 0.9122922009404547\n",
      "Step 4201 | Sliding Loss Window : 0.8782554536187744\n",
      "Step 4251 | Sliding Loss Window : 0.8805967959802296\n",
      "Step 4301 | Sliding Loss Window : 1.0181186546629402\n",
      "Step 4351 | Sliding Loss Window : 0.8718808088877616\n",
      "Step 4401 | Sliding Loss Window : 0.9511455747522842\n",
      "Step 4451 | Sliding Loss Window : 1.0868777219119587\n",
      "Step 4501 | Sliding Loss Window : 1.1756061804955984\n",
      "Step 4551 | Sliding Loss Window : 0.9295340375723541\n",
      "Step 4601 | Sliding Loss Window : 0.9933747643866196\n",
      "Step 4651 | Sliding Loss Window : 0.9083881955432278\n",
      "Step 4701 | Sliding Loss Window : 1.0278972446077355\n",
      "Step 4751 | Sliding Loss Window : 0.9094483303261898\n",
      "Step 4801 | Sliding Loss Window : 0.9690142354332819\n",
      "Step 4851 | Sliding Loss Window : 0.9569839760424236\n",
      "Step 4901 | Sliding Loss Window : 1.0427579757149552\n",
      "Step 4951 | Sliding Loss Window : 0.9227596824063353\n",
      "Step 5001 | Sliding Loss Window : 0.9459365225234211\n",
      "Step 5051 | Sliding Loss Window : 1.0276355002854496\n",
      "Step 5101 | Sliding Loss Window : 0.9232170406926526\n",
      "Step 5151 | Sliding Loss Window : 0.9028017700519594\n",
      "Step 5201 | Sliding Loss Window : 0.8842653485358266\n",
      "Step 5251 | Sliding Loss Window : 0.9249185403285393\n",
      "Step 5301 | Sliding Loss Window : 0.8798834670623387\n",
      "Step 5351 | Sliding Loss Window : 0.8874948945347512\n",
      "Step 5401 | Sliding Loss Window : 1.003655474637244\n",
      "Step 5451 | Sliding Loss Window : 0.8875172667127107\n",
      "Step 1 | Sliding Loss Window : 0.8829129014802999\n",
      "Step 51 | Sliding Loss Window : 1.1890311657830759\n",
      "Step 101 | Sliding Loss Window : 1.1001758883720738\n",
      "Step 151 | Sliding Loss Window : 1.078900109189235\n",
      "Step 201 | Sliding Loss Window : 1.085851462086252\n",
      "Step 251 | Sliding Loss Window : 1.0712178008446767\n",
      "Step 301 | Sliding Loss Window : 1.0399626267362572\n",
      "Step 351 | Sliding Loss Window : 1.0346288030376494\n",
      "Step 401 | Sliding Loss Window : 1.0125397822019737\n",
      "Step 451 | Sliding Loss Window : 0.9202680875273989\n",
      "Step 501 | Sliding Loss Window : 1.0736553619297895\n",
      "Step 551 | Sliding Loss Window : 0.9960350823480577\n",
      "Step 601 | Sliding Loss Window : 0.9894302626432371\n",
      "Step 651 | Sliding Loss Window : 1.1031795834753684\n",
      "Step 701 | Sliding Loss Window : 1.0492601402744\n",
      "Step 751 | Sliding Loss Window : 0.9138623081704114\n",
      "Step 801 | Sliding Loss Window : 1.136734391401038\n",
      "Step 851 | Sliding Loss Window : 1.0344473291611676\n",
      "Step 901 | Sliding Loss Window : 1.135384349409444\n",
      "Step 951 | Sliding Loss Window : 0.945653272392272\n",
      "Step 1001 | Sliding Loss Window : 1.1068334317393715\n",
      "Step 1051 | Sliding Loss Window : 1.0268305254076067\n",
      "Step 1101 | Sliding Loss Window : 1.0446718454477923\n",
      "Step 1151 | Sliding Loss Window : 1.0269947880809098\n",
      "Step 1201 | Sliding Loss Window : 1.0641054645933508\n",
      "Step 1251 | Sliding Loss Window : 1.002487191535254\n",
      "Step 1301 | Sliding Loss Window : 0.9864348209854376\n",
      "Step 1351 | Sliding Loss Window : 1.0858322252686858\n",
      "Step 1401 | Sliding Loss Window : 0.9811923545253827\n",
      "Step 1451 | Sliding Loss Window : 0.9804456890980988\n",
      "Step 1501 | Sliding Loss Window : 1.0088031098000547\n",
      "Step 1551 | Sliding Loss Window : 0.9937822648187161\n",
      "Step 1601 | Sliding Loss Window : 0.9349378165126097\n",
      "Step 1651 | Sliding Loss Window : 1.0092985618784949\n",
      "Step 1701 | Sliding Loss Window : 0.9670790952915729\n",
      "Step 1751 | Sliding Loss Window : 1.157748713448674\n",
      "Step 1801 | Sliding Loss Window : 1.023000873344934\n",
      "Step 1851 | Sliding Loss Window : 0.8913073210412216\n",
      "Step 1901 | Sliding Loss Window : 1.1389185674169833\n",
      "Step 1951 | Sliding Loss Window : 1.0441130437177528\n",
      "Step 2001 | Sliding Loss Window : 1.1425971893699565\n",
      "Step 2051 | Sliding Loss Window : 0.9130482349071432\n",
      "Step 2101 | Sliding Loss Window : 1.0882390032042586\n",
      "Step 2151 | Sliding Loss Window : 1.0171887182113912\n",
      "Step 2201 | Sliding Loss Window : 1.0466174345719341\n",
      "Step 2251 | Sliding Loss Window : 0.9999019274570782\n",
      "Step 2301 | Sliding Loss Window : 1.0851959193916343\n",
      "Step 2351 | Sliding Loss Window : 0.9531206455234088\n",
      "Step 2401 | Sliding Loss Window : 0.9883705702005656\n",
      "Step 2451 | Sliding Loss Window : 1.0859315370865266\n",
      "Step 2501 | Sliding Loss Window : 0.9786781673038368\n",
      "Step 2551 | Sliding Loss Window : 0.963453219262403\n",
      "Step 2601 | Sliding Loss Window : 1.0224295203600315\n",
      "Step 2651 | Sliding Loss Window : 1.0261604697938471\n",
      "Step 2701 | Sliding Loss Window : 0.907115546426783\n",
      "Step 2751 | Sliding Loss Window : 1.004740729358599\n",
      "Step 2801 | Sliding Loss Window : 0.9893933763283684\n",
      "Step 2851 | Sliding Loss Window : 1.1298746210769515\n",
      "Step 2901 | Sliding Loss Window : 1.0418706735595487\n",
      "Step 2951 | Sliding Loss Window : 0.9040976264145677\n",
      "Step 3001 | Sliding Loss Window : 1.1279180690773558\n",
      "Step 3051 | Sliding Loss Window : 1.0416721518150884\n",
      "Step 3101 | Sliding Loss Window : 1.1412369662857167\n",
      "Step 3151 | Sliding Loss Window : 0.9372526686804707\n",
      "Step 3201 | Sliding Loss Window : 1.098765041122812\n",
      "Step 3251 | Sliding Loss Window : 0.9840594630425216\n",
      "Step 3301 | Sliding Loss Window : 1.0320922524818557\n",
      "Step 3351 | Sliding Loss Window : 1.019723436973769\n",
      "Step 3401 | Sliding Loss Window : 1.1256845866911833\n",
      "Step 3451 | Sliding Loss Window : 0.8969353821327379\n",
      "Step 3501 | Sliding Loss Window : 0.9734391595338611\n",
      "Step 3551 | Sliding Loss Window : 1.0776598857946444\n",
      "Step 3601 | Sliding Loss Window : 0.9897944769424583\n",
      "Step 3651 | Sliding Loss Window : 1.029949295207029\n",
      "Step 3701 | Sliding Loss Window : 0.9446407546468473\n",
      "Step 3751 | Sliding Loss Window : 1.0459747534623818\n",
      "Step 3801 | Sliding Loss Window : 0.8805918440323016\n",
      "Step 3851 | Sliding Loss Window : 0.9974296248915533\n",
      "Step 3901 | Sliding Loss Window : 1.0277993212694942\n",
      "Step 3951 | Sliding Loss Window : 1.122415896866647\n",
      "Step 4001 | Sliding Loss Window : 1.0330205889244972\n",
      "Step 4051 | Sliding Loss Window : 0.9218166976440273\n",
      "Step 4101 | Sliding Loss Window : 1.1069339124946367\n",
      "Step 4151 | Sliding Loss Window : 1.0433152248730737\n",
      "Step 4201 | Sliding Loss Window : 1.141600291291324\n",
      "Step 4251 | Sliding Loss Window : 0.9685098638517681\n",
      "Step 4301 | Sliding Loss Window : 1.0530634225216697\n",
      "Step 4351 | Sliding Loss Window : 0.9970113834450435\n",
      "Step 4401 | Sliding Loss Window : 1.0278064596209493\n",
      "Step 4451 | Sliding Loss Window : 1.0222734443088768\n",
      "Step 4501 | Sliding Loss Window : 1.1327534100931593\n",
      "Step 4551 | Sliding Loss Window : 0.9026135047755538\n",
      "Step 4601 | Sliding Loss Window : 0.9642306458928583\n",
      "Step 4651 | Sliding Loss Window : 1.0845715574132329\n",
      "Step 4701 | Sliding Loss Window : 0.9739234412530976\n",
      "Step 4751 | Sliding Loss Window : 1.0350038377003983\n",
      "Step 4801 | Sliding Loss Window : 0.9317919312388195\n",
      "Step 4851 | Sliding Loss Window : 1.067016680144629\n",
      "Step 4901 | Sliding Loss Window : 0.875612918051962\n",
      "Step 4951 | Sliding Loss Window : 1.030285279585865\n",
      "Step 5001 | Sliding Loss Window : 1.0355445286563953\n",
      "Step 5051 | Sliding Loss Window : 1.0836675742507857\n",
      "Step 5101 | Sliding Loss Window : 1.0411527284357263\n",
      "Step 5151 | Sliding Loss Window : 0.8867574665678323\n",
      "Step 5201 | Sliding Loss Window : 1.1291823920299153\n",
      "Step 5251 | Sliding Loss Window : 1.0380739630837776\n",
      "Step 5301 | Sliding Loss Window : 1.139786178852431\n",
      "Step 5351 | Sliding Loss Window : 0.9971667436671126\n",
      "Step 5401 | Sliding Loss Window : 1.0560894680969148\n",
      "Step 5451 | Sliding Loss Window : 0.9778197849825093\n",
      "[ 125  860 2266 2134 1254 2414 1710 2002  994 1053  426 1431  748  602\n",
      "  753  301  930 1028 1732 1449 1953 1255 1439 2012 1221 2162  131 1876\n",
      " 1106  950  837  477  422 2178  310 2236 1570 1318  158 1571  829 2057\n",
      "  351 1475  969 1076 1373 1967 1114  239 2411  811  429  386 1902  484\n",
      " 1247 1172 2096  962  655 1306 2339  303 2340 2297 2492  784 2437  216\n",
      " 1189 2039 1037 2084 1620  838 2225  946  494 2138 1055 2190 1328 2182\n",
      " 2177 2087 1904 2406 1764 2251 1057  964  189 2127  612  184 2184 2028\n",
      " 2232 1234] [0.52122948 0.55261002 0.39653707 0.46843004 0.60751536 0.46592355\n",
      " 0.42689168 0.51208155 0.44623908 0.5815409  0.4541418  0.48623881\n",
      " 0.57699518 0.60984157 0.57131115 0.54890285 0.59048758 0.62539444\n",
      " 0.45051157 0.43367386 0.45690805 0.54923221 0.58600334 0.51393094\n",
      " 0.49724563 0.54893809 0.61685605 0.58278488 0.54501221 0.48840684\n",
      " 0.47410846 0.59359727 0.56238054 0.55527986 0.43798596 0.41145593\n",
      " 0.45493355 0.59556883 0.5895101  0.50982789 0.46040475 0.42645872\n",
      " 0.44309604 0.62397145 0.57056323 0.42420101 0.46362269 0.54351497\n",
      " 0.55041028 0.57101745 0.58584711 0.50148198 0.47061718 0.58181765\n",
      " 0.46647462 0.61341597 0.45194626 0.61655535 0.50494623 0.42176819\n",
      " 0.43935439 0.59602326 0.44696203 0.60136012 0.44608593 0.57172753\n",
      " 0.53184307 0.59278474 0.43722484 0.58739452 0.43488854 0.494771\n",
      " 0.5718143  0.4627749  0.63670558 0.61420426 0.49297273 0.52762701\n",
      " 0.62441238 0.51996052 0.45916677 0.59128355 0.58010146 0.46720424\n",
      " 0.59705973 0.51114949 0.50554907 0.42214298 0.50929356 0.43885422\n",
      " 0.56982101 0.54881055 0.66497142 0.54650994 0.57832867 0.46538782\n",
      " 0.57064874 0.59259576 0.58475767 0.60500939]\n",
      "189 0.6649714198429137\n",
      "0.9261016845703125 0.7180328369140625\n",
      "Step 1 | Sliding Loss Window : 0.19538140252925054\n",
      "Step 51 | Sliding Loss Window : 1.0300133404551206\n",
      "Step 101 | Sliding Loss Window : 0.5203307175513349\n",
      "Step 151 | Sliding Loss Window : 0.614739774573114\n",
      "Step 201 | Sliding Loss Window : 0.7670283749224077\n",
      "Step 251 | Sliding Loss Window : 0.624542751603472\n",
      "Step 301 | Sliding Loss Window : 0.5974703392667388\n",
      "Step 351 | Sliding Loss Window : 0.6561784411465925\n",
      "Step 401 | Sliding Loss Window : 0.6247182890081964\n",
      "Step 451 | Sliding Loss Window : 0.5595973650640356\n",
      "Step 501 | Sliding Loss Window : 0.7632047243764106\n",
      "Step 551 | Sliding Loss Window : 0.5884275249115372\n",
      "Step 601 | Sliding Loss Window : 0.7041462462726504\n",
      "Step 651 | Sliding Loss Window : 0.7046020119760108\n",
      "Step 701 | Sliding Loss Window : 0.6162169864501221\n",
      "Step 751 | Sliding Loss Window : 0.7556698302556292\n",
      "Step 801 | Sliding Loss Window : 0.7069380421604992\n",
      "Step 851 | Sliding Loss Window : 0.5426344510521587\n",
      "Step 901 | Sliding Loss Window : 0.5258224228222383\n",
      "Step 951 | Sliding Loss Window : 0.6245148357814693\n",
      "Step 1001 | Sliding Loss Window : 0.6740542980724017\n",
      "Step 1051 | Sliding Loss Window : 0.5918080873322227\n",
      "Step 1101 | Sliding Loss Window : 0.5501457081966685\n",
      "Step 1151 | Sliding Loss Window : 0.5861528820473516\n",
      "Step 1201 | Sliding Loss Window : 0.4781162650478345\n",
      "Step 1251 | Sliding Loss Window : 0.6749620706029014\n",
      "Step 1301 | Sliding Loss Window : 0.7147593220134947\n",
      "Step 1351 | Sliding Loss Window : 0.6496638928985824\n",
      "Step 1401 | Sliding Loss Window : 0.5905114373100812\n",
      "Step 1451 | Sliding Loss Window : 0.6367915227269623\n",
      "Step 1501 | Sliding Loss Window : 0.6180685450925127\n",
      "Step 1551 | Sliding Loss Window : 0.5707898464378653\n",
      "Step 1601 | Sliding Loss Window : 0.7871233768173869\n",
      "Step 1651 | Sliding Loss Window : 0.6099667987226426\n",
      "Step 1701 | Sliding Loss Window : 0.689869665494672\n",
      "Step 1751 | Sliding Loss Window : 0.6902245170679907\n",
      "Step 1801 | Sliding Loss Window : 0.6764844716831963\n",
      "Step 1851 | Sliding Loss Window : 0.7006721930353779\n",
      "Step 1901 | Sliding Loss Window : 0.678415575915637\n",
      "Step 1951 | Sliding Loss Window : 0.536707836065227\n",
      "Step 2001 | Sliding Loss Window : 0.5535180790385819\n",
      "Step 2051 | Sliding Loss Window : 0.6163969027163324\n",
      "Step 2101 | Sliding Loss Window : 0.6664985206357866\n",
      "Step 2151 | Sliding Loss Window : 0.5871930504322374\n",
      "Step 2201 | Sliding Loss Window : 0.5587328018324755\n",
      "Step 2251 | Sliding Loss Window : 0.5771787879677763\n",
      "Step 2301 | Sliding Loss Window : 0.5068056741470006\n",
      "Step 2351 | Sliding Loss Window : 0.6702869156307529\n",
      "Step 2401 | Sliding Loss Window : 0.6831444981479919\n",
      "Step 2451 | Sliding Loss Window : 0.6647868986705422\n",
      "Step 2501 | Sliding Loss Window : 0.5980891700923214\n",
      "Step 2551 | Sliding Loss Window : 0.6189421000697436\n",
      "Step 2601 | Sliding Loss Window : 0.6236797121929469\n",
      "Step 2651 | Sliding Loss Window : 0.5822969048183466\n",
      "Step 2701 | Sliding Loss Window : 0.7762919299743423\n",
      "Step 2751 | Sliding Loss Window : 0.6250153146674957\n",
      "Step 2801 | Sliding Loss Window : 0.7226353601352092\n",
      "Step 2851 | Sliding Loss Window : 0.6465646218856733\n",
      "Step 2901 | Sliding Loss Window : 0.6750627155045404\n",
      "Step 2951 | Sliding Loss Window : 0.7003599955062334\n",
      "Step 3001 | Sliding Loss Window : 0.679522380215797\n",
      "Step 3051 | Sliding Loss Window : 0.5744371995752889\n",
      "Step 3101 | Sliding Loss Window : 0.5227962872507828\n",
      "Step 3151 | Sliding Loss Window : 0.6127960116804072\n",
      "Step 3201 | Sliding Loss Window : 0.651530859446624\n",
      "Step 3251 | Sliding Loss Window : 0.613714802154374\n",
      "Step 3301 | Sliding Loss Window : 0.552408136978065\n",
      "Step 3351 | Sliding Loss Window : 0.571092437958505\n",
      "Step 3401 | Sliding Loss Window : 0.5322778599442649\n",
      "Step 3451 | Sliding Loss Window : 0.6479039481878536\n",
      "Step 3501 | Sliding Loss Window : 0.6917428484922507\n",
      "Step 3551 | Sliding Loss Window : 0.6441944549913053\n",
      "Step 3601 | Sliding Loss Window : 0.5984615168947685\n",
      "Step 3651 | Sliding Loss Window : 0.6377239589524429\n",
      "Step 3701 | Sliding Loss Window : 0.603818482144477\n",
      "Step 3751 | Sliding Loss Window : 0.5856528076168279\n",
      "Step 3801 | Sliding Loss Window : 0.7940147321409008\n",
      "Step 3851 | Sliding Loss Window : 0.6070600260853268\n",
      "Step 3901 | Sliding Loss Window : 0.7370918776626589\n",
      "Step 3951 | Sliding Loss Window : 0.6219631441378659\n",
      "Step 4001 | Sliding Loss Window : 0.722180724171848\n",
      "Step 4051 | Sliding Loss Window : 0.6561800795809094\n",
      "Step 4101 | Sliding Loss Window : 0.6931219135972898\n",
      "Step 4151 | Sliding Loss Window : 0.5576696739194982\n",
      "Step 4201 | Sliding Loss Window : 0.5679919220828458\n",
      "Step 4251 | Sliding Loss Window : 0.5918028412677178\n",
      "Step 4301 | Sliding Loss Window : 0.6436190712617302\n",
      "Step 4351 | Sliding Loss Window : 0.6578830353177639\n",
      "Step 4401 | Sliding Loss Window : 0.5005423634209454\n",
      "Step 4451 | Sliding Loss Window : 0.5966665100244593\n",
      "Step 4501 | Sliding Loss Window : 0.537700185653788\n",
      "Step 4551 | Sliding Loss Window : 0.6325327281207966\n",
      "Step 4601 | Sliding Loss Window : 0.6839981062099949\n",
      "Step 4651 | Sliding Loss Window : 0.6513058273927257\n",
      "Step 4701 | Sliding Loss Window : 0.5963515310330777\n",
      "Step 4751 | Sliding Loss Window : 0.6282308190652149\n",
      "Step 4801 | Sliding Loss Window : 0.6131698509638273\n",
      "Step 4851 | Sliding Loss Window : 0.6253414328144102\n",
      "Step 4901 | Sliding Loss Window : 0.8048586492386199\n",
      "Step 4951 | Sliding Loss Window : 0.5562058907225509\n",
      "Step 5001 | Sliding Loss Window : 0.7375766386560432\n",
      "Step 5051 | Sliding Loss Window : 0.6462327405476538\n",
      "Step 5101 | Sliding Loss Window : 0.7029168548504231\n",
      "Step 5151 | Sliding Loss Window : 0.6792782567930391\n",
      "Step 5201 | Sliding Loss Window : 0.6952049922450898\n",
      "Step 5251 | Sliding Loss Window : 0.5200921801448708\n",
      "Step 5301 | Sliding Loss Window : 0.5972148101542948\n",
      "Step 5351 | Sliding Loss Window : 0.5505786379764124\n",
      "Step 5401 | Sliding Loss Window : 0.6586908990244307\n",
      "Step 5451 | Sliding Loss Window : 0.6531138080321435\n",
      "Step 1 | Sliding Loss Window : 0.3687121347916363\n",
      "Step 51 | Sliding Loss Window : 1.1446186594392527\n",
      "Step 101 | Sliding Loss Window : 0.991031115212445\n",
      "Step 151 | Sliding Loss Window : 0.7715622831620622\n",
      "Step 201 | Sliding Loss Window : 1.040487423675024\n",
      "Step 251 | Sliding Loss Window : 0.7561700138209475\n",
      "Step 301 | Sliding Loss Window : 0.8185866569996822\n",
      "Step 351 | Sliding Loss Window : 0.8606193267839796\n",
      "Step 401 | Sliding Loss Window : 0.8643100278597742\n",
      "Step 451 | Sliding Loss Window : 0.8240562348832955\n",
      "Step 501 | Sliding Loss Window : 0.6903559966510565\n",
      "Step 551 | Sliding Loss Window : 0.6782399897246683\n",
      "Step 601 | Sliding Loss Window : 0.6449493263306798\n",
      "Step 651 | Sliding Loss Window : 0.7921740373312756\n",
      "Step 701 | Sliding Loss Window : 0.6509905831812033\n",
      "Step 751 | Sliding Loss Window : 0.5489275758681049\n",
      "Step 801 | Sliding Loss Window : 0.6659091324531254\n",
      "Step 851 | Sliding Loss Window : 0.6981984612906581\n",
      "Step 901 | Sliding Loss Window : 0.9355350343213421\n",
      "Step 951 | Sliding Loss Window : 0.6152552664186178\n",
      "Step 1001 | Sliding Loss Window : 0.6153447306116762\n",
      "Step 1051 | Sliding Loss Window : 0.8785089144730053\n",
      "Step 1101 | Sliding Loss Window : 0.6886456623353315\n",
      "Step 1151 | Sliding Loss Window : 0.7002975784098392\n",
      "Step 1201 | Sliding Loss Window : 0.7824408246246564\n",
      "Step 1251 | Sliding Loss Window : 0.8267178652936525\n",
      "Step 1301 | Sliding Loss Window : 0.7537735320891423\n",
      "Step 1351 | Sliding Loss Window : 0.6946274047353246\n",
      "Step 1401 | Sliding Loss Window : 0.7699911799213127\n",
      "Step 1451 | Sliding Loss Window : 0.8076005284301678\n",
      "Step 1501 | Sliding Loss Window : 0.8077575276296867\n",
      "Step 1551 | Sliding Loss Window : 0.8034809807004933\n",
      "Step 1601 | Sliding Loss Window : 0.6932631887786175\n",
      "Step 1651 | Sliding Loss Window : 0.6425888517530772\n",
      "Step 1701 | Sliding Loss Window : 0.6780481233878697\n",
      "Step 1751 | Sliding Loss Window : 0.7576273656888184\n",
      "Step 1801 | Sliding Loss Window : 0.6653357264734217\n",
      "Step 1851 | Sliding Loss Window : 0.5598109217551819\n",
      "Step 1901 | Sliding Loss Window : 0.6726946341222717\n",
      "Step 1951 | Sliding Loss Window : 0.6857900946043755\n",
      "Step 2001 | Sliding Loss Window : 0.9479546929957857\n",
      "Step 2051 | Sliding Loss Window : 0.6074263595755034\n",
      "Step 2101 | Sliding Loss Window : 0.6003357951717131\n",
      "Step 2151 | Sliding Loss Window : 0.9144382633754278\n",
      "Step 2201 | Sliding Loss Window : 0.6457532278676607\n",
      "Step 2251 | Sliding Loss Window : 0.7456829372504618\n",
      "Step 2301 | Sliding Loss Window : 0.7556142468884112\n",
      "Step 2351 | Sliding Loss Window : 0.85013384123827\n",
      "Step 2401 | Sliding Loss Window : 0.7196702044868033\n",
      "Step 2451 | Sliding Loss Window : 0.698031963127266\n",
      "Step 2501 | Sliding Loss Window : 0.7931937499096577\n",
      "Step 2551 | Sliding Loss Window : 0.7820472997934913\n",
      "Step 2601 | Sliding Loss Window : 0.8012502042117186\n",
      "Step 2651 | Sliding Loss Window : 0.7985848941312291\n",
      "Step 2701 | Sliding Loss Window : 0.7123563272811279\n",
      "Step 2751 | Sliding Loss Window : 0.6808049704537451\n",
      "Step 2801 | Sliding Loss Window : 0.6443355071141792\n",
      "Step 2851 | Sliding Loss Window : 0.7527517239555377\n",
      "Step 2901 | Sliding Loss Window : 0.6770174456552194\n",
      "Step 2951 | Sliding Loss Window : 0.5558250462033055\n",
      "Step 3001 | Sliding Loss Window : 0.6741137237793623\n",
      "Step 3051 | Sliding Loss Window : 0.7046567256279006\n",
      "Step 3101 | Sliding Loss Window : 0.917375762027408\n",
      "Step 3151 | Sliding Loss Window : 0.6179976186742078\n",
      "Step 3201 | Sliding Loss Window : 0.5964671070575208\n",
      "Step 3251 | Sliding Loss Window : 0.8932470397459844\n",
      "Step 3301 | Sliding Loss Window : 0.6716861360952977\n",
      "Step 3351 | Sliding Loss Window : 0.7578696378279409\n",
      "Step 3401 | Sliding Loss Window : 0.7248592482434977\n",
      "Step 3451 | Sliding Loss Window : 0.8626132072180843\n",
      "Step 3501 | Sliding Loss Window : 0.7144088938611267\n",
      "Step 3551 | Sliding Loss Window : 0.7009579012875905\n",
      "Step 3601 | Sliding Loss Window : 0.8094552477387044\n",
      "Step 3651 | Sliding Loss Window : 0.858450640325165\n",
      "Step 3701 | Sliding Loss Window : 0.7133557528509985\n",
      "Step 3751 | Sliding Loss Window : 0.8279947897714738\n",
      "Step 3801 | Sliding Loss Window : 0.659483808352085\n",
      "Step 3851 | Sliding Loss Window : 0.6886049567996421\n",
      "Step 3901 | Sliding Loss Window : 0.677079173485644\n",
      "Step 3951 | Sliding Loss Window : 0.7470486743619768\n",
      "Step 4001 | Sliding Loss Window : 0.6426404113615096\n",
      "Step 4051 | Sliding Loss Window : 0.5914588817212791\n",
      "Step 4101 | Sliding Loss Window : 0.6511133453389629\n",
      "Step 4151 | Sliding Loss Window : 0.7092771900104101\n",
      "Step 4201 | Sliding Loss Window : 0.9180028261518771\n",
      "Step 4251 | Sliding Loss Window : 0.6063155805278072\n",
      "Step 4301 | Sliding Loss Window : 0.6277507200855225\n",
      "Step 4351 | Sliding Loss Window : 0.873381577131962\n",
      "Step 4401 | Sliding Loss Window : 0.6570876768695599\n",
      "Step 4451 | Sliding Loss Window : 0.7788328654855076\n",
      "Step 4501 | Sliding Loss Window : 0.7175186679530181\n",
      "Step 4551 | Sliding Loss Window : 0.8612323000365255\n",
      "Step 4601 | Sliding Loss Window : 0.724357827836537\n",
      "Step 4651 | Sliding Loss Window : 0.6797545342024588\n",
      "Step 4701 | Sliding Loss Window : 0.837894623881761\n",
      "Step 4751 | Sliding Loss Window : 0.8448580990799666\n",
      "Step 4801 | Sliding Loss Window : 0.7045600764693518\n",
      "Step 4851 | Sliding Loss Window : 0.8143384166472228\n",
      "Step 4901 | Sliding Loss Window : 0.6954694230875772\n",
      "Step 4951 | Sliding Loss Window : 0.6522691765414957\n",
      "Step 5001 | Sliding Loss Window : 0.6862924555747156\n",
      "Step 5051 | Sliding Loss Window : 0.7502105671767347\n",
      "Step 5101 | Sliding Loss Window : 0.651981094975458\n",
      "Step 5151 | Sliding Loss Window : 0.5745021432105585\n",
      "Step 5201 | Sliding Loss Window : 0.6593910402505467\n",
      "Step 5251 | Sliding Loss Window : 0.7639560889914863\n",
      "Step 5301 | Sliding Loss Window : 0.8629746495115365\n",
      "Step 5351 | Sliding Loss Window : 0.6567660977109111\n",
      "Step 5401 | Sliding Loss Window : 0.6019850257896304\n",
      "Step 5451 | Sliding Loss Window : 0.8750828142628363\n",
      "Step 1 | Sliding Loss Window : 1.385861611513575\n",
      "Step 51 | Sliding Loss Window : 0.8703596949218645\n",
      "Step 101 | Sliding Loss Window : 0.9042474036144305\n",
      "Step 151 | Sliding Loss Window : 0.9357434880653224\n",
      "Step 201 | Sliding Loss Window : 1.0184586475951694\n",
      "Step 251 | Sliding Loss Window : 0.8584665738707493\n",
      "Step 301 | Sliding Loss Window : 1.0478281529746756\n",
      "Step 351 | Sliding Loss Window : 0.7054466514060773\n",
      "Step 401 | Sliding Loss Window : 0.6858718839103958\n",
      "Step 451 | Sliding Loss Window : 0.8977626626831209\n",
      "Step 501 | Sliding Loss Window : 0.8366300776700814\n",
      "Step 551 | Sliding Loss Window : 0.6657713200879911\n",
      "Step 601 | Sliding Loss Window : 0.9209283573891416\n",
      "Step 651 | Sliding Loss Window : 0.8166042947143555\n",
      "Step 701 | Sliding Loss Window : 0.7460241963705094\n",
      "Step 751 | Sliding Loss Window : 0.7536400845305857\n",
      "Step 801 | Sliding Loss Window : 0.7648598382189787\n",
      "Step 851 | Sliding Loss Window : 0.68465379400693\n",
      "Step 901 | Sliding Loss Window : 0.6132282760958406\n",
      "Step 951 | Sliding Loss Window : 0.6622413496627765\n",
      "Step 1001 | Sliding Loss Window : 0.6589745766113831\n",
      "Step 1051 | Sliding Loss Window : 0.651335783219357\n",
      "Step 1101 | Sliding Loss Window : 0.6920453836450293\n",
      "Step 1151 | Sliding Loss Window : 0.6831512631195625\n",
      "Step 1201 | Sliding Loss Window : 0.7255298059135601\n",
      "Step 1251 | Sliding Loss Window : 0.6809446624320417\n",
      "Step 1301 | Sliding Loss Window : 0.6863498892525796\n",
      "Step 1351 | Sliding Loss Window : 0.7548935198444887\n",
      "Step 1401 | Sliding Loss Window : 0.7162127153195713\n",
      "Step 1451 | Sliding Loss Window : 0.7410600969541705\n",
      "Step 1501 | Sliding Loss Window : 0.7296061975323842\n",
      "Step 1551 | Sliding Loss Window : 0.7089905172137034\n",
      "Step 1601 | Sliding Loss Window : 0.7284990546421954\n",
      "Step 1651 | Sliding Loss Window : 0.6403040036273394\n",
      "Step 1701 | Sliding Loss Window : 0.9100610262424743\n",
      "Step 1751 | Sliding Loss Window : 0.7452625966435885\n",
      "Step 1801 | Sliding Loss Window : 0.6926492894484766\n",
      "Step 1851 | Sliding Loss Window : 0.7419510998074438\n",
      "Step 1901 | Sliding Loss Window : 0.7412673099258572\n",
      "Step 1951 | Sliding Loss Window : 0.6707097284767467\n",
      "Step 2001 | Sliding Loss Window : 0.6061745380018785\n",
      "Step 2051 | Sliding Loss Window : 0.6886884144938944\n",
      "Step 2101 | Sliding Loss Window : 0.6762029472759933\n",
      "Step 2151 | Sliding Loss Window : 0.6408932918884628\n",
      "Step 2201 | Sliding Loss Window : 0.6688415439726139\n",
      "Step 2251 | Sliding Loss Window : 0.672166686411714\n",
      "Step 2301 | Sliding Loss Window : 0.7278460699717848\n",
      "Step 2351 | Sliding Loss Window : 0.6833211415395631\n",
      "Step 2401 | Sliding Loss Window : 0.697745315324221\n",
      "Step 2451 | Sliding Loss Window : 0.7588307329086201\n",
      "Step 2501 | Sliding Loss Window : 0.7485815260699933\n",
      "Step 2551 | Sliding Loss Window : 0.6906289392855719\n",
      "Step 2601 | Sliding Loss Window : 0.731009055305001\n",
      "Step 2651 | Sliding Loss Window : 0.7261212989657043\n",
      "Step 2701 | Sliding Loss Window : 0.7320289454839356\n",
      "Step 2751 | Sliding Loss Window : 0.6295821704735178\n",
      "Step 2801 | Sliding Loss Window : 0.9377860248335995\n",
      "Step 2851 | Sliding Loss Window : 0.7064128246460719\n",
      "Step 2901 | Sliding Loss Window : 0.738281702855002\n",
      "Step 2951 | Sliding Loss Window : 0.7110285593072269\n",
      "Step 3001 | Sliding Loss Window : 0.7289452598874712\n",
      "Step 3051 | Sliding Loss Window : 0.6862791582576464\n",
      "Step 3101 | Sliding Loss Window : 0.5954795573946418\n",
      "Step 3151 | Sliding Loss Window : 0.6856888466063148\n",
      "Step 3201 | Sliding Loss Window : 0.6848299766695058\n",
      "Step 3251 | Sliding Loss Window : 0.6322021216358945\n",
      "Step 3301 | Sliding Loss Window : 0.6670088585440552\n",
      "Step 3351 | Sliding Loss Window : 0.7442559875617496\n",
      "Step 3401 | Sliding Loss Window : 0.6855055069460382\n",
      "Step 3451 | Sliding Loss Window : 0.6694119391121177\n",
      "Step 3501 | Sliding Loss Window : 0.7223247068918078\n",
      "Step 3551 | Sliding Loss Window : 0.7256343463050621\n",
      "Step 3601 | Sliding Loss Window : 0.7831468992048727\n",
      "Step 3651 | Sliding Loss Window : 0.6512317226040736\n",
      "Step 3701 | Sliding Loss Window : 0.7600373186409933\n",
      "Step 3751 | Sliding Loss Window : 0.7354782162588508\n",
      "Step 3801 | Sliding Loss Window : 0.7200503975807849\n",
      "Step 3851 | Sliding Loss Window : 0.6346554754839872\n",
      "Step 3901 | Sliding Loss Window : 0.9110664350143748\n",
      "Step 3951 | Sliding Loss Window : 0.7023413761963415\n",
      "Step 4001 | Sliding Loss Window : 0.7394499083393917\n",
      "Step 4051 | Sliding Loss Window : 0.7210982699599867\n",
      "Step 4101 | Sliding Loss Window : 0.723403210002377\n",
      "Step 4151 | Sliding Loss Window : 0.6745167992532919\n",
      "Step 4201 | Sliding Loss Window : 0.6054225376734654\n",
      "Step 4251 | Sliding Loss Window : 0.6778896186983961\n",
      "Step 4301 | Sliding Loss Window : 0.6956860630243731\n",
      "Step 4351 | Sliding Loss Window : 0.6644660082318734\n",
      "Step 4401 | Sliding Loss Window : 0.6397320648700462\n",
      "Step 4451 | Sliding Loss Window : 0.7305281071204925\n",
      "Step 4501 | Sliding Loss Window : 0.7179201782454997\n",
      "Step 4551 | Sliding Loss Window : 0.6878606340152664\n",
      "Step 4601 | Sliding Loss Window : 0.6973429936824455\n",
      "Step 4651 | Sliding Loss Window : 0.7383877827082178\n",
      "Step 4701 | Sliding Loss Window : 0.747575855935456\n",
      "Step 4751 | Sliding Loss Window : 0.677924473959161\n",
      "Step 4801 | Sliding Loss Window : 0.7604376784206768\n",
      "Step 4851 | Sliding Loss Window : 0.7146475108307506\n",
      "Step 4901 | Sliding Loss Window : 0.7354916287952582\n",
      "Step 4951 | Sliding Loss Window : 0.6216005676871644\n",
      "Step 5001 | Sliding Loss Window : 0.9282735228217528\n",
      "Step 5051 | Sliding Loss Window : 0.7161684587252094\n",
      "Step 5101 | Sliding Loss Window : 0.7342140765487092\n",
      "Step 5151 | Sliding Loss Window : 0.7109152660149591\n",
      "Step 5201 | Sliding Loss Window : 0.695064562102075\n",
      "Step 5251 | Sliding Loss Window : 0.7159925532211489\n",
      "Step 5301 | Sliding Loss Window : 0.5627324928102734\n",
      "Step 5351 | Sliding Loss Window : 0.6843060470131135\n",
      "Step 5401 | Sliding Loss Window : 0.7014311204918188\n",
      "Step 5451 | Sliding Loss Window : 0.7056606855663458\n",
      "Step 1 | Sliding Loss Window : 1.1243553774186168\n",
      "Step 51 | Sliding Loss Window : 1.2223652225999782\n",
      "Step 101 | Sliding Loss Window : 1.0931573180930132\n",
      "Step 151 | Sliding Loss Window : 0.7575913169304581\n",
      "Step 201 | Sliding Loss Window : 0.7260694254186366\n",
      "Step 251 | Sliding Loss Window : 0.8706637336109304\n",
      "Step 301 | Sliding Loss Window : 0.4911675149475582\n",
      "Step 351 | Sliding Loss Window : 0.7109633442434529\n",
      "Step 401 | Sliding Loss Window : 0.6403568964738686\n",
      "Step 451 | Sliding Loss Window : 0.7346913373931456\n",
      "Step 501 | Sliding Loss Window : 0.5951212867392784\n",
      "Step 551 | Sliding Loss Window : 0.7090346604488286\n",
      "Step 601 | Sliding Loss Window : 0.5997279767248983\n",
      "Step 651 | Sliding Loss Window : 0.7634543199847503\n",
      "Step 701 | Sliding Loss Window : 0.8566936129906675\n",
      "Step 751 | Sliding Loss Window : 0.7364075722787697\n",
      "Step 801 | Sliding Loss Window : 0.6975886713681182\n",
      "Step 851 | Sliding Loss Window : 0.7296206945678533\n",
      "Step 901 | Sliding Loss Window : 0.7068590416643485\n",
      "Step 951 | Sliding Loss Window : 0.6477921771419792\n",
      "Step 1001 | Sliding Loss Window : 0.7130437501900702\n",
      "Step 1051 | Sliding Loss Window : 0.7200815974369434\n",
      "Step 1101 | Sliding Loss Window : 0.735958938612114\n",
      "Step 1151 | Sliding Loss Window : 0.6840485592013762\n",
      "Step 1201 | Sliding Loss Window : 0.8414003049096915\n",
      "Step 1251 | Sliding Loss Window : 0.7255212366611681\n",
      "Step 1301 | Sliding Loss Window : 0.6488464498829417\n",
      "Step 1351 | Sliding Loss Window : 0.8027658835964081\n",
      "Step 1401 | Sliding Loss Window : 0.4799542710230578\n",
      "Step 1451 | Sliding Loss Window : 0.7020806944099333\n",
      "Step 1501 | Sliding Loss Window : 0.6704991242668575\n",
      "Step 1551 | Sliding Loss Window : 0.7038587670665661\n",
      "Step 1601 | Sliding Loss Window : 0.6244413321735902\n",
      "Step 1651 | Sliding Loss Window : 0.691044227175067\n",
      "Step 1701 | Sliding Loss Window : 0.6307117266367231\n",
      "Step 1751 | Sliding Loss Window : 0.7574330523821723\n",
      "Step 1801 | Sliding Loss Window : 0.8247127529724888\n",
      "Step 1851 | Sliding Loss Window : 0.7454658855290728\n",
      "Step 1901 | Sliding Loss Window : 0.6814364744277893\n",
      "Step 1951 | Sliding Loss Window : 0.8103797666158081\n",
      "Step 2001 | Sliding Loss Window : 0.6304725902110458\n",
      "Step 2051 | Sliding Loss Window : 0.6449847774568606\n",
      "Step 2101 | Sliding Loss Window : 0.7251641214337823\n",
      "Step 2151 | Sliding Loss Window : 0.7104903573036444\n",
      "Step 2201 | Sliding Loss Window : 0.752636247371419\n",
      "Step 2251 | Sliding Loss Window : 0.6636283069331923\n",
      "Step 2301 | Sliding Loss Window : 0.8537635697922429\n",
      "Step 2351 | Sliding Loss Window : 0.7139772776717073\n",
      "Step 2401 | Sliding Loss Window : 0.6713448796925394\n",
      "Step 2451 | Sliding Loss Window : 0.782217150302705\n",
      "Step 2501 | Sliding Loss Window : 0.5283969478507314\n",
      "Step 2551 | Sliding Loss Window : 0.6669330906550841\n",
      "Step 2601 | Sliding Loss Window : 0.6897903490194239\n",
      "Step 2651 | Sliding Loss Window : 0.6835889770077217\n",
      "Step 2701 | Sliding Loss Window : 0.622504183660172\n",
      "Step 2751 | Sliding Loss Window : 0.7040143555955926\n",
      "Step 2801 | Sliding Loss Window : 0.6539077249818445\n",
      "Step 2851 | Sliding Loss Window : 0.7280986519639298\n",
      "Step 2901 | Sliding Loss Window : 0.8307641687165557\n",
      "Step 2951 | Sliding Loss Window : 0.7204375821061556\n",
      "Step 3001 | Sliding Loss Window : 0.728501408157092\n",
      "Step 3051 | Sliding Loss Window : 0.7665506262126697\n",
      "Step 3101 | Sliding Loss Window : 0.6670809607607963\n",
      "Step 3151 | Sliding Loss Window : 0.6178142800033155\n",
      "Step 3201 | Sliding Loss Window : 0.7495719522744101\n",
      "Step 3251 | Sliding Loss Window : 0.6965112135232413\n",
      "Step 3301 | Sliding Loss Window : 0.7532268544974251\n",
      "Step 3351 | Sliding Loss Window : 0.6655429852487968\n",
      "Step 3401 | Sliding Loss Window : 0.8567950957371281\n",
      "Step 3451 | Sliding Loss Window : 0.6973365117756414\n",
      "Step 3501 | Sliding Loss Window : 0.6982629421712286\n",
      "Step 3551 | Sliding Loss Window : 0.7583877481008908\n",
      "Step 3601 | Sliding Loss Window : 0.5562605612131807\n",
      "Step 3651 | Sliding Loss Window : 0.6405694840706044\n",
      "Step 3701 | Sliding Loss Window : 0.6938199729045103\n",
      "Step 3751 | Sliding Loss Window : 0.6760069737681225\n",
      "Step 3801 | Sliding Loss Window : 0.6266954615085064\n",
      "Step 3851 | Sliding Loss Window : 0.694019762442862\n",
      "Step 3901 | Sliding Loss Window : 0.6475807103700333\n",
      "Step 3951 | Sliding Loss Window : 0.7391451738522438\n",
      "Step 4001 | Sliding Loss Window : 0.8192859011468103\n",
      "Step 4051 | Sliding Loss Window : 0.7301038570379675\n",
      "Step 4101 | Sliding Loss Window : 0.7865389688511041\n",
      "Step 4151 | Sliding Loss Window : 0.7025598954396053\n",
      "Step 4201 | Sliding Loss Window : 0.7011595869163132\n",
      "Step 4251 | Sliding Loss Window : 0.6017052782256966\n",
      "Step 4301 | Sliding Loss Window : 0.738199876547691\n",
      "Step 4351 | Sliding Loss Window : 0.7277954009296154\n",
      "Step 4401 | Sliding Loss Window : 0.7493221023229013\n",
      "Step 4451 | Sliding Loss Window : 0.6429591717082966\n",
      "Step 4501 | Sliding Loss Window : 0.8915845008850302\n",
      "Step 4551 | Sliding Loss Window : 0.6498375579936843\n",
      "Step 4601 | Sliding Loss Window : 0.7120766328918492\n",
      "Step 4651 | Sliding Loss Window : 0.798817913348811\n",
      "Step 4701 | Sliding Loss Window : 0.49526852458463844\n",
      "Step 4751 | Sliding Loss Window : 0.6858551265813151\n",
      "Step 4801 | Sliding Loss Window : 0.6627981927529278\n",
      "Step 4851 | Sliding Loss Window : 0.6804630647189938\n",
      "Step 4901 | Sliding Loss Window : 0.6245060627174622\n",
      "Step 4951 | Sliding Loss Window : 0.683228864579943\n",
      "Step 5001 | Sliding Loss Window : 0.6769522021356422\n",
      "Step 5051 | Sliding Loss Window : 0.7504070780008054\n",
      "Step 5101 | Sliding Loss Window : 0.794337019689454\n",
      "Step 5151 | Sliding Loss Window : 0.761718210877479\n",
      "Step 5201 | Sliding Loss Window : 0.7495506967578222\n",
      "Step 5251 | Sliding Loss Window : 0.7321628284438729\n",
      "Step 5301 | Sliding Loss Window : 0.6657393124164069\n",
      "Step 5351 | Sliding Loss Window : 0.6130839925364543\n",
      "Step 5401 | Sliding Loss Window : 0.755325548804694\n",
      "Step 5451 | Sliding Loss Window : 0.7166289071114446\n",
      "Step 1 | Sliding Loss Window : 0.9642634202143127\n",
      "Step 51 | Sliding Loss Window : 1.3067312328887735\n",
      "Step 101 | Sliding Loss Window : 1.1367362355842416\n",
      "Step 151 | Sliding Loss Window : 0.8087036016539519\n",
      "Step 201 | Sliding Loss Window : 0.5238950997024538\n",
      "Step 251 | Sliding Loss Window : 0.6006652079003838\n",
      "Step 301 | Sliding Loss Window : 0.5804931593640825\n",
      "Step 351 | Sliding Loss Window : 0.7409254817105659\n",
      "Step 401 | Sliding Loss Window : 0.6458872958533347\n",
      "Step 451 | Sliding Loss Window : 0.509692749107211\n",
      "Step 501 | Sliding Loss Window : 0.5715600756081644\n",
      "Step 551 | Sliding Loss Window : 0.7309780609387048\n",
      "Step 601 | Sliding Loss Window : 0.6879374728924487\n",
      "Step 651 | Sliding Loss Window : 0.7293811318047798\n",
      "Step 701 | Sliding Loss Window : 0.5426405843147967\n",
      "Step 751 | Sliding Loss Window : 0.5961191972994797\n",
      "Step 801 | Sliding Loss Window : 0.737789734114488\n",
      "Step 851 | Sliding Loss Window : 0.5605401021313778\n",
      "Step 901 | Sliding Loss Window : 0.7224954922375327\n",
      "Step 951 | Sliding Loss Window : 0.6635410853487903\n",
      "Step 1001 | Sliding Loss Window : 0.7062886318743195\n",
      "Step 1051 | Sliding Loss Window : 0.6311535012652305\n",
      "Step 1101 | Sliding Loss Window : 0.46592872268205016\n",
      "Step 1151 | Sliding Loss Window : 0.6862636547844798\n",
      "Step 1201 | Sliding Loss Window : 0.5931749459681229\n",
      "Step 1251 | Sliding Loss Window : 0.6496578467290675\n",
      "Step 1301 | Sliding Loss Window : 0.4779202601913625\n",
      "Step 1351 | Sliding Loss Window : 0.6264839154968559\n",
      "Step 1401 | Sliding Loss Window : 0.5663985989070643\n",
      "Step 1451 | Sliding Loss Window : 0.7269385647042952\n",
      "Step 1501 | Sliding Loss Window : 0.6415440948638268\n",
      "Step 1551 | Sliding Loss Window : 0.5344159411416785\n",
      "Step 1601 | Sliding Loss Window : 0.568961995121454\n",
      "Step 1651 | Sliding Loss Window : 0.7347149178874861\n",
      "Step 1701 | Sliding Loss Window : 0.6672522532524877\n",
      "Step 1751 | Sliding Loss Window : 0.7370451432675245\n",
      "Step 1801 | Sliding Loss Window : 0.5440020748698202\n",
      "Step 1851 | Sliding Loss Window : 0.5720729512939041\n",
      "Step 1901 | Sliding Loss Window : 0.7399943899876814\n",
      "Step 1951 | Sliding Loss Window : 0.5970139520747336\n",
      "Step 2001 | Sliding Loss Window : 0.6818307261421138\n",
      "Step 2051 | Sliding Loss Window : 0.6958608021833074\n",
      "Step 2101 | Sliding Loss Window : 0.6955990777102855\n",
      "Step 2151 | Sliding Loss Window : 0.6113014945629298\n",
      "Step 2201 | Sliding Loss Window : 0.4873790456999872\n",
      "Step 2251 | Sliding Loss Window : 0.6738012862986548\n",
      "Step 2301 | Sliding Loss Window : 0.5926234667902517\n",
      "Step 2351 | Sliding Loss Window : 0.6615670992434524\n"
     ]
    }
   ],
   "source": [
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import get_circ_params, create_gate_circ\n",
    "from train_circ import train_qnn, mse_loss\n",
    "from create_noise_models import noisy_dev_from_backend\n",
    "\n",
    "search_nums = [100]\n",
    "param_nums = [4, 8, 12, 16, 20]\n",
    "\n",
    "dataset = 'bank'\n",
    "num_reps = 2\n",
    "\n",
    "num_qubits = 4\n",
    "dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "meas_qubits = [0]\n",
    "mat_size = 1024\n",
    "\n",
    "device_name = 'ibm_nairobi'\n",
    "noisy_dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "ours_dir = './ours/bank/'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', num_reps)\n",
    "\n",
    "for search_num in search_nums:       \n",
    "    for param_num in param_nums:\n",
    "        noise_scores = []\n",
    "        \n",
    "        for i in range(2500):\n",
    "            noise_scores.append(np.genfromtxt(ours_dir + '/{}_params/circ_{}/noise_metric/{}/metric_tvd_score.txt'.format(param_num, i + 1, device_name)))\n",
    "            \n",
    "        noise_scores = np.array(noise_scores)\n",
    "        \n",
    "        mean_t_mat_scores = np.genfromtxt(ours_dir + '{}_params/d2_mean_t_mat_scores.txt'.format(param_num))\n",
    "        perf_scores = (mat_size - mean_t_mat_scores) / mat_size\n",
    "        combined_scores = np.multiply(perf_scores, noise_scores)\n",
    "        \n",
    "        circ_inds = np.random.permutation(2500)\n",
    "        \n",
    "        curr_dir = ours_dir + '/{}_params/search_{}_{}'.format(param_num, search_num, device_name)\n",
    "        \n",
    "        if not os.path.exists(curr_dir):\n",
    "            os.mkdir(curr_dir)\n",
    "\n",
    "        for j in range(25):\n",
    "            curr_trial_dir = curr_dir + '/trial_{}'.format(j + 1)\n",
    "\n",
    "            if not os.path.exists(curr_trial_dir):\n",
    "                os.mkdir(curr_trial_dir)\n",
    "        \n",
    "            sel_inds = circ_inds[range(j * 100, j * 100 + 100)]\n",
    "            sel_ind = sel_inds[np.argmax(combined_scores[sel_inds])]\n",
    "\n",
    "            np.savetxt(curr_trial_dir + '/searched_circuit_inds.txt', sel_inds)\n",
    "            np.savetxt(curr_trial_dir + '/searched_circuit_scores.txt', combined_scores[sel_inds])\n",
    "            np.savetxt(curr_trial_dir + '/sel_circuit_ind.txt', [sel_ind])\n",
    "            np.savetxt(curr_trial_dir + '/sel_circuit_score.txt', [combined_scores[sel_ind]])\n",
    "            \n",
    "            print(sel_inds, combined_scores[sel_inds])\n",
    "            print(sel_ind, combined_scores[sel_ind])\n",
    "            print(noise_scores[sel_ind], perf_scores[sel_ind])\n",
    "\n",
    "            circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(ours_dir + '{}_params/circ_{}'.format(param_num, sel_ind + 1))\n",
    "\n",
    "            circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'exp', 'adjoint')\n",
    "            \n",
    "            noisy_circ = create_gate_circ(noisy_dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'exp', None)\n",
    "        \n",
    "            losses_list = []\n",
    "            accs_list = []\n",
    "            noisy_losses_list = []\n",
    "            noisy_accs_list = []\n",
    "\n",
    "            for j in range(5):\n",
    "                curr_train_dir = curr_trial_dir + '/run_{}'.format(j + 1)\n",
    "\n",
    "                if os.path.exists(curr_train_dir):\n",
    "                    pass\n",
    "                else:\n",
    "                    os.mkdir(curr_train_dir)\n",
    "\n",
    "\n",
    "                info = train_qnn(circ, x_train, y_train, x_test, y_test, [weights_bounds[-1]], 5490, 0.05, 1, mse_loss, verbosity=17300, \n",
    "                                                                                                loss_window=50, init_params=None, \n",
    "                                                                                                acc_thres=1.1, shuffle=True, print_loss=50)\n",
    "\n",
    "                val_exps = [circ(x_test[i], info[-1][-1]) for i in range(len(x_test))]\n",
    "                val_loss = np.array([mse_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "                \n",
    "                noisy_val_exps = [noisy_circ(x_test[i], info[-1][-1]) for i in range(len(x_test))]\n",
    "                noisy_val_loss = np.array([mse_loss(y_test[k], noisy_val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "                acc = np.mean(val_loss < 1)\n",
    "                noisy_acc = np.mean(noisy_val_loss < 1)\n",
    "#                 acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "\n",
    "                np.savetxt(curr_train_dir + '/params_{}.txt'.format(j + 1), info[-1])\n",
    "                np.savetxt(curr_train_dir + '/losses_{}.txt'.format(j + 1), info[0])             \n",
    "\n",
    "                losses_list.append(val_loss)\n",
    "                accs_list.append(acc)\n",
    "                \n",
    "                noisy_losses_list.append(noisy_val_loss)\n",
    "                noisy_accs_list.append(noisy_acc)\n",
    "\n",
    "            np.savetxt(curr_trial_dir + '/accs.txt', accs_list)\n",
    "            np.savetxt(curr_trial_dir + '/val_losses.txt', losses_list)          \n",
    "            np.savetxt(curr_trial_dir + '/noisy_accs.txt', noisy_accs_list)\n",
    "            np.savetxt(curr_trial_dir + '/noisy_val_losses.txt', noisy_losses_list) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlaation circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from create_noise_models import noisy_dev_from_backend\n",
    "from datasets_nt import load_dataset\n",
    "from create_gate_circs import create_gate_circ, get_circ_params\n",
    "from train_circ import train_qnn, mse_loss\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset = 'mnist_2'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', 2)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 32\n",
    "num_params = 12\n",
    "\n",
    "device_name = 'ibm_nairobi'\n",
    "\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "for i in range(600, 800):\n",
    "    curr_dir = './experiment_data/{}/trained_circuits/circ_{}'.format(dataset, i + 1)\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                    weights_bounds, [0], 'exp')\n",
    "    \n",
    "    \n",
    "    noiseless_losses = np.genfromtxt(curr_dir + '/val_losses.txt')\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    curr_dev_dir = curr_dir + '/' + device_name\n",
    "\n",
    "#     if not os.path.exists(curr_dev_dir + '/accs_inference_only.txt'):\n",
    "    if True:\n",
    "        if not os.path.exists(curr_dev_dir):\n",
    "            os.mkdir(curr_dev_dir)\n",
    "\n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "            curr_params = np.genfromtxt(curr_train_dir + '/params_{}.txt'.format(j + 1))[-1]\n",
    "\n",
    "            val_exps = [circ(x_test[i], curr_params) for i in range(len(x_test))]\n",
    "            val_loss = np.array([mse_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "#             acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "            acc = np.mean(val_loss < 1)\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        print(np.mean(noiseless_losses), np.mean(losses_list), i + 1)\n",
    "\n",
    "        np.save(curr_dev_dir + '/val_losses_inference_only.npy', losses_list)\n",
    "        np.savetxt(curr_dev_dir + '/accs_inference_only.txt', accs_list)\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'train_circ' from '/root/train_circ.py'>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import datasets\n",
    "import create_gate_circs\n",
    "import train_circ\n",
    "\n",
    "reload(datasets)\n",
    "reload(create_gate_circs)\n",
    "reload(train_circ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1 | Loss: 1.3369333744049072\n",
      "Step 101 | Loss: 1.0034929513931274\n",
      "Step 201 | Loss: 0.9121675491333008\n",
      "Step 301 | Loss: 0.8650904893875122\n",
      "Step 401 | Loss: 0.8072453141212463\n",
      "Step 501 | Loss: 0.7626935839653015\n",
      "Step 601 | Loss: 0.6002445220947266\n",
      "Step 701 | Loss: 0.631428599357605\n",
      "Step 801 | Loss: 0.6582334041595459\n",
      "Step 901 | Loss: 0.523783266544342\n",
      "Step 1001 | Loss: 0.6060664653778076\n",
      "Step 1101 | Loss: 0.6223019957542419\n",
      "Step 1201 | Loss: 0.534239649772644\n",
      "Step 1301 | Loss: 0.553539514541626\n",
      "Step 1401 | Loss: 0.5586219429969788\n",
      "Step 1501 | Loss: 0.5730311274528503\n",
      "Step 1601 | Loss: 0.5850419998168945\n",
      "Step 1701 | Loss: 0.6638661623001099\n",
      "Step 1801 | Loss: 0.5224451422691345\n",
      "Step 1901 | Loss: 0.5741580128669739\n",
      "Step 2001 | Loss: 0.5830962061882019\n",
      "Step 2101 | Loss: 0.6019691824913025\n",
      "Step 2201 | Loss: 0.6239913702011108\n",
      "Step 2301 | Loss: 0.46938976645469666\n",
      "Step 2401 | Loss: 0.520551323890686\n",
      "0.5545499068688317 0.8775\n",
      "Step 1 | Loss: 1.051339864730835\n",
      "Step 101 | Loss: 0.7728914022445679\n",
      "Step 201 | Loss: 0.6347917318344116\n",
      "Step 301 | Loss: 0.4762631952762604\n",
      "Step 401 | Loss: 0.5540812015533447\n",
      "Step 501 | Loss: 0.5751224756240845\n",
      "Step 601 | Loss: 0.5866957902908325\n",
      "Step 701 | Loss: 0.7116332650184631\n",
      "Step 801 | Loss: 0.5717669725418091\n",
      "Step 901 | Loss: 0.57993084192276\n",
      "Step 1001 | Loss: 0.6800470948219299\n",
      "Step 1101 | Loss: 0.5698449015617371\n",
      "Step 1201 | Loss: 0.6035484671592712\n",
      "Step 1301 | Loss: 0.5003944635391235\n",
      "Step 1401 | Loss: 0.6173164248466492\n",
      "Step 1501 | Loss: 0.4955793023109436\n",
      "Step 1601 | Loss: 0.48336657881736755\n",
      "Step 1701 | Loss: 0.6191418766975403\n",
      "Step 1801 | Loss: 0.6216937899589539\n",
      "Step 1901 | Loss: 0.5080574154853821\n",
      "Step 2001 | Loss: 0.4549310505390167\n",
      "Step 2101 | Loss: 0.6014037132263184\n",
      "Step 2201 | Loss: 0.6374714970588684\n",
      "Step 2301 | Loss: 0.7286048531532288\n",
      "Step 2401 | Loss: 0.44510236382484436\n",
      "0.5570816203116155 0.88\n",
      "Step 1 | Loss: 1.0078468322753906\n",
      "Step 101 | Loss: 0.9886297583580017\n",
      "Step 201 | Loss: 1.0123803615570068\n",
      "Step 301 | Loss: 1.0169965028762817\n",
      "Step 401 | Loss: 0.9112582802772522\n",
      "Step 501 | Loss: 0.8024723529815674\n",
      "Step 601 | Loss: 0.7257962822914124\n",
      "Step 701 | Loss: 0.5659833550453186\n",
      "Step 801 | Loss: 0.5698229670524597\n",
      "Step 901 | Loss: 0.5920994877815247\n",
      "Step 1001 | Loss: 0.542510449886322\n",
      "Step 1101 | Loss: 0.6231762766838074\n",
      "Step 1201 | Loss: 0.5177948474884033\n",
      "Step 1301 | Loss: 0.6216886043548584\n",
      "Step 1401 | Loss: 0.7398176789283752\n",
      "Step 1501 | Loss: 0.5350654125213623\n",
      "Step 1601 | Loss: 0.57172691822052\n",
      "Step 1701 | Loss: 0.5850667953491211\n",
      "Step 1801 | Loss: 0.6571263074874878\n",
      "Step 1901 | Loss: 0.6697075366973877\n",
      "Step 2001 | Loss: 0.6262916922569275\n",
      "Step 2101 | Loss: 0.6683232188224792\n",
      "Step 2201 | Loss: 0.5763922333717346\n",
      "Step 2301 | Loss: 0.6405418515205383\n",
      "Step 2401 | Loss: 0.5626382231712341\n",
      "0.5535052660118791 0.86\n",
      "Step 1 | Loss: 1.1617140769958496\n",
      "Step 101 | Loss: 0.9497016072273254\n",
      "Step 201 | Loss: 0.7089212536811829\n",
      "Step 301 | Loss: 0.7183951735496521\n",
      "Step 401 | Loss: 0.6038069725036621\n",
      "Step 501 | Loss: 0.6519628763198853\n",
      "Step 601 | Loss: 0.6993595957756042\n",
      "Step 701 | Loss: 0.8932799100875854\n",
      "Step 801 | Loss: 0.5998873114585876\n",
      "Step 901 | Loss: 0.7926788926124573\n",
      "Step 1001 | Loss: 0.7666558027267456\n",
      "Step 1101 | Loss: 0.7109348773956299\n",
      "Step 1201 | Loss: 0.7395095825195312\n",
      "Step 1301 | Loss: 0.633736252784729\n",
      "Step 1401 | Loss: 0.5724519491195679\n",
      "Step 1501 | Loss: 0.5718323588371277\n",
      "Step 1601 | Loss: 0.649239182472229\n",
      "Step 1701 | Loss: 0.5995621681213379\n",
      "Step 1801 | Loss: 0.5436698794364929\n",
      "Step 1901 | Loss: 0.6704171299934387\n",
      "Step 2001 | Loss: 0.7174072861671448\n",
      "Step 2101 | Loss: 0.6289587616920471\n",
      "Step 2201 | Loss: 0.6304600834846497\n",
      "Step 2301 | Loss: 0.558628499507904\n",
      "Step 2401 | Loss: 0.5988832116127014\n",
      "0.5580753095957693 0.8825\n",
      "Step 1 | Loss: 1.0228614807128906\n",
      "Step 101 | Loss: 0.9266790151596069\n",
      "Step 201 | Loss: 0.7566965222358704\n",
      "Step 301 | Loss: 0.5892778635025024\n",
      "Step 401 | Loss: 0.7109623551368713\n",
      "Step 501 | Loss: 0.7261298894882202\n",
      "Step 601 | Loss: 0.6267738342285156\n",
      "Step 701 | Loss: 0.6979367733001709\n",
      "Step 801 | Loss: 0.6045659184455872\n",
      "Step 901 | Loss: 0.619959831237793\n",
      "Step 1001 | Loss: 0.5953486561775208\n",
      "Step 1101 | Loss: 0.5880926847457886\n",
      "Step 1201 | Loss: 0.7155978679656982\n",
      "Step 1301 | Loss: 0.5791336297988892\n",
      "Step 1401 | Loss: 0.6776599287986755\n",
      "Step 1501 | Loss: 0.6282029747962952\n",
      "Step 1601 | Loss: 0.6371597647666931\n",
      "Step 1701 | Loss: 0.743783175945282\n",
      "Step 1801 | Loss: 0.7398783564567566\n",
      "Step 1901 | Loss: 0.7679299712181091\n",
      "Step 2001 | Loss: 0.6243188381195068\n",
      "Step 2101 | Loss: 0.7029943466186523\n",
      "Step 2201 | Loss: 0.6789177656173706\n",
      "Step 2301 | Loss: 0.6381210088729858\n",
      "Step 2401 | Loss: 0.8072166442871094\n",
      "0.6705133796206806 0.8525\n",
      "Step 1 | Loss: 1.862720012664795\n",
      "Step 101 | Loss: 1.529201865196228\n",
      "Step 201 | Loss: 1.1569715738296509\n",
      "Step 301 | Loss: 1.5202312469482422\n",
      "Step 401 | Loss: 1.48331618309021\n",
      "Step 501 | Loss: 1.435148000717163\n",
      "Step 601 | Loss: 1.6689627170562744\n",
      "Step 701 | Loss: 1.4960594177246094\n",
      "Step 801 | Loss: 1.2145110368728638\n",
      "Step 901 | Loss: 1.419546365737915\n",
      "Step 1001 | Loss: 1.730812668800354\n",
      "Step 1101 | Loss: 1.3512654304504395\n",
      "Step 1201 | Loss: 1.2327947616577148\n",
      "Step 1301 | Loss: 1.1811350584030151\n",
      "Step 1401 | Loss: 1.178511619567871\n",
      "Step 1501 | Loss: 1.2000395059585571\n",
      "Step 1601 | Loss: 1.3200896978378296\n",
      "Step 1701 | Loss: 1.4646224975585938\n",
      "Step 1801 | Loss: 1.3373191356658936\n",
      "Step 1901 | Loss: 1.2745730876922607\n",
      "Step 2001 | Loss: 1.0469672679901123\n",
      "Step 2101 | Loss: 1.2296185493469238\n",
      "Step 2201 | Loss: 1.9233405590057373\n",
      "Step 2301 | Loss: 1.5990287065505981\n",
      "Step 2401 | Loss: 1.424373745918274\n",
      "1.4324219663590145 0.5275\n",
      "Step 1 | Loss: 1.6322028636932373\n",
      "Step 101 | Loss: 1.0280529260635376\n",
      "Step 201 | Loss: 1.409956455230713\n",
      "Step 301 | Loss: 1.3508732318878174\n",
      "Step 401 | Loss: 0.8752949833869934\n",
      "Step 501 | Loss: 1.609228491783142\n",
      "Step 601 | Loss: 1.515506386756897\n",
      "Step 701 | Loss: 1.8018114566802979\n",
      "Step 801 | Loss: 1.654435157775879\n",
      "Step 901 | Loss: 1.5592154264450073\n",
      "Step 1001 | Loss: 1.458618402481079\n",
      "Step 1101 | Loss: 1.23170006275177\n",
      "Step 1201 | Loss: 1.3010910749435425\n",
      "Step 1301 | Loss: 1.1836148500442505\n",
      "Step 1401 | Loss: 1.3786405324935913\n",
      "Step 1501 | Loss: 1.6698880195617676\n",
      "Step 1601 | Loss: 1.2656593322753906\n",
      "Step 1701 | Loss: 0.9760211110115051\n",
      "Step 1801 | Loss: 1.4032328128814697\n",
      "Step 1901 | Loss: 1.2533559799194336\n",
      "Step 2001 | Loss: 1.5949499607086182\n",
      "Step 2101 | Loss: 1.2235263586044312\n",
      "Step 2201 | Loss: 1.8763999938964844\n",
      "Step 2301 | Loss: 1.3719691038131714\n",
      "Step 2401 | Loss: 1.6541086435317993\n",
      "1.432421924959532 0.5275\n",
      "Step 1 | Loss: 1.4379093647003174\n",
      "Step 101 | Loss: 1.228252649307251\n",
      "Step 201 | Loss: 1.3637899160385132\n",
      "Step 301 | Loss: 1.2089585065841675\n",
      "Step 401 | Loss: 1.058077335357666\n",
      "Step 501 | Loss: 1.434027910232544\n",
      "Step 601 | Loss: 1.615185260772705\n",
      "Step 701 | Loss: 1.2859935760498047\n",
      "Step 801 | Loss: 1.4876935482025146\n",
      "Step 901 | Loss: 1.4247663021087646\n",
      "Step 1001 | Loss: 1.5312256813049316\n",
      "Step 1101 | Loss: 1.1339248418807983\n",
      "Step 1201 | Loss: 1.7485471963882446\n",
      "Step 1301 | Loss: 1.3292385339736938\n",
      "Step 1401 | Loss: 1.1917734146118164\n",
      "Step 1501 | Loss: 1.6605933904647827\n",
      "Step 1601 | Loss: 1.7401072978973389\n",
      "Step 1701 | Loss: 1.5288023948669434\n",
      "Step 1801 | Loss: 0.9074118733406067\n",
      "Step 1901 | Loss: 1.5043538808822632\n",
      "Step 2001 | Loss: 2.0652573108673096\n",
      "Step 2101 | Loss: 1.7897670269012451\n",
      "Step 2201 | Loss: 1.9266185760498047\n",
      "Step 2301 | Loss: 1.1605234146118164\n",
      "Step 2401 | Loss: 1.6713213920593262\n",
      "1.4324219474854407 0.5275\n",
      "Step 1 | Loss: 1.7220724821090698\n",
      "Step 101 | Loss: 1.6831294298171997\n",
      "Step 201 | Loss: 1.2113617658615112\n",
      "Step 301 | Loss: 1.0170459747314453\n",
      "Step 401 | Loss: 1.0034863948822021\n",
      "Step 501 | Loss: 1.4122616052627563\n",
      "Step 601 | Loss: 1.7788519859313965\n",
      "Step 701 | Loss: 1.636941909790039\n",
      "Step 801 | Loss: 1.4007569551467896\n",
      "Step 901 | Loss: 1.0171072483062744\n",
      "Step 1001 | Loss: 1.8603363037109375\n",
      "Step 1101 | Loss: 1.3984510898590088\n",
      "Step 1201 | Loss: 2.2510125637054443\n",
      "Step 1301 | Loss: 1.4265601634979248\n",
      "Step 1401 | Loss: 1.2673701047897339\n",
      "Step 1501 | Loss: 1.0921179056167603\n",
      "Step 1601 | Loss: 1.0549077987670898\n",
      "Step 1701 | Loss: 1.0105600357055664\n",
      "Step 1801 | Loss: 1.2746011018753052\n",
      "Step 1901 | Loss: 1.2807066440582275\n",
      "Step 2001 | Loss: 1.2481536865234375\n",
      "Step 2101 | Loss: 1.143320918083191\n",
      "Step 2201 | Loss: 1.3620259761810303\n",
      "Step 2301 | Loss: 1.1723859310150146\n",
      "Step 2401 | Loss: 1.1310811042785645\n",
      "1.4324219318802291 0.5275\n",
      "Step 1 | Loss: 1.5116753578186035\n",
      "Step 101 | Loss: 1.496452808380127\n",
      "Step 201 | Loss: 1.3085097074508667\n",
      "Step 301 | Loss: 1.097312569618225\n",
      "Step 401 | Loss: 1.4927403926849365\n",
      "Step 501 | Loss: 0.8933862447738647\n",
      "Step 601 | Loss: 1.0158953666687012\n",
      "Step 701 | Loss: 1.646787166595459\n",
      "Step 801 | Loss: 1.4269757270812988\n",
      "Step 901 | Loss: 1.2437944412231445\n",
      "Step 1001 | Loss: 0.6976811289787292\n",
      "Step 1101 | Loss: 1.6877429485321045\n",
      "Step 1201 | Loss: 1.5158007144927979\n",
      "Step 1301 | Loss: 1.7645624876022339\n",
      "Step 1401 | Loss: 1.5502393245697021\n",
      "Step 1501 | Loss: 1.1118961572647095\n",
      "Step 1601 | Loss: 1.2452303171157837\n",
      "Step 1701 | Loss: 1.2305015325546265\n",
      "Step 1801 | Loss: 1.3912608623504639\n",
      "Step 1901 | Loss: 1.7558794021606445\n",
      "Step 2001 | Loss: 1.2214348316192627\n",
      "Step 2101 | Loss: 1.2990790605545044\n",
      "Step 2201 | Loss: 1.7278547286987305\n",
      "Step 2301 | Loss: 0.9945318698883057\n",
      "Step 2401 | Loss: 1.8436747789382935\n",
      "1.4324218986743165 0.5275\n",
      "Step 1 | Loss: 1.2289901971817017\n",
      "Step 101 | Loss: 0.18313972651958466\n",
      "Step 201 | Loss: 0.07038185745477676\n",
      "Step 301 | Loss: 0.1541343629360199\n",
      "Step 401 | Loss: 0.25697481632232666\n",
      "Step 501 | Loss: 0.1262698620557785\n",
      "Step 601 | Loss: 0.23126809298992157\n",
      "Step 701 | Loss: 0.25504279136657715\n",
      "Step 801 | Loss: 0.18297472596168518\n",
      "Step 901 | Loss: 0.2536374032497406\n",
      "Step 1001 | Loss: 0.18356865644454956\n",
      "Step 1101 | Loss: 0.2173268049955368\n",
      "Step 1201 | Loss: 0.06312984228134155\n",
      "Step 1301 | Loss: 0.2257121205329895\n",
      "Step 1401 | Loss: 0.2843163311481476\n",
      "Step 1501 | Loss: 0.11270033568143845\n",
      "Step 1601 | Loss: 0.10718060284852982\n",
      "Step 1701 | Loss: 0.2878419756889343\n",
      "Step 1801 | Loss: 0.09703028947114944\n",
      "Step 1901 | Loss: 0.13961632549762726\n",
      "Step 2001 | Loss: 0.30163517594337463\n",
      "Step 2101 | Loss: 0.04706102982163429\n",
      "Step 2201 | Loss: 0.10545730590820312\n",
      "Step 2301 | Loss: 0.09872287511825562\n",
      "Step 2401 | Loss: 0.06153782457113266\n",
      "0.13162579648486655 0.955\n",
      "Step 1 | Loss: 0.8734861016273499\n",
      "Step 101 | Loss: 0.15938261151313782\n",
      "Step 201 | Loss: 0.24055664241313934\n",
      "Step 301 | Loss: 0.14741963148117065\n",
      "Step 401 | Loss: 0.19575348496437073\n",
      "Step 501 | Loss: 0.270663321018219\n",
      "Step 601 | Loss: 0.16389121115207672\n",
      "Step 701 | Loss: 0.10905352234840393\n",
      "Step 801 | Loss: 0.10550455749034882\n",
      "Step 901 | Loss: 0.12105582654476166\n",
      "Step 1001 | Loss: 0.2045583873987198\n",
      "Step 1101 | Loss: 0.12295334786176682\n",
      "Step 1201 | Loss: 0.06214702129364014\n",
      "Step 1301 | Loss: 0.18158894777297974\n",
      "Step 1401 | Loss: 0.1771051287651062\n",
      "Step 1501 | Loss: 0.08417537063360214\n",
      "Step 1601 | Loss: 0.06797011196613312\n",
      "Step 1701 | Loss: 0.17184562981128693\n",
      "Step 1801 | Loss: 0.10840131342411041\n",
      "Step 1901 | Loss: 0.1290060132741928\n",
      "Step 2001 | Loss: 0.12495970726013184\n",
      "Step 2101 | Loss: 0.11195842176675797\n",
      "Step 2201 | Loss: 0.1676914393901825\n",
      "Step 2301 | Loss: 0.06098327785730362\n",
      "Step 2401 | Loss: 0.08774858713150024\n",
      "0.13182366846622445 0.955\n",
      "Step 1 | Loss: 1.6109024286270142\n",
      "Step 101 | Loss: 0.04019040986895561\n",
      "Step 201 | Loss: 0.29344290494918823\n",
      "Step 301 | Loss: 0.048120878636837006\n",
      "Step 401 | Loss: 0.23059475421905518\n",
      "Step 501 | Loss: 0.21881385147571564\n",
      "Step 601 | Loss: 0.23287293314933777\n",
      "Step 701 | Loss: 0.1815100759267807\n",
      "Step 801 | Loss: 0.0479300394654274\n",
      "Step 901 | Loss: 0.0704643726348877\n",
      "Step 1001 | Loss: 0.2512156665325165\n",
      "Step 1101 | Loss: 0.13795053958892822\n",
      "Step 1201 | Loss: 0.17950521409511566\n",
      "Step 1301 | Loss: 0.035764437168836594\n",
      "Step 1401 | Loss: 0.17207060754299164\n",
      "Step 1501 | Loss: 0.13014037907123566\n",
      "Step 1601 | Loss: 0.06708189845085144\n",
      "Step 1701 | Loss: 0.07140878587961197\n",
      "Step 1801 | Loss: 0.050627000629901886\n",
      "Step 1901 | Loss: 0.22278334200382233\n",
      "Step 2001 | Loss: 0.14165349304676056\n",
      "Step 2101 | Loss: 0.100418820977211\n",
      "Step 2201 | Loss: 0.10562217980623245\n",
      "Step 2301 | Loss: 0.33781298995018005\n",
      "Step 2401 | Loss: 0.12452998012304306\n",
      "0.1313110428046747 0.9575\n",
      "Step 1 | Loss: 1.1955983638763428\n",
      "Step 101 | Loss: 0.20769891142845154\n",
      "Step 201 | Loss: 0.17392365634441376\n",
      "Step 301 | Loss: 0.12818698585033417\n",
      "Step 401 | Loss: 0.24545583128929138\n",
      "Step 501 | Loss: 0.07319317013025284\n",
      "Step 601 | Loss: 0.1249026209115982\n",
      "Step 701 | Loss: 0.20279747247695923\n",
      "Step 801 | Loss: 0.1886604130268097\n",
      "Step 901 | Loss: 0.08328930288553238\n",
      "Step 1001 | Loss: 0.16940201818943024\n",
      "Step 1101 | Loss: 0.11969465017318726\n",
      "Step 1201 | Loss: 0.07665193825960159\n",
      "Step 1301 | Loss: 0.28464168310165405\n",
      "Step 1401 | Loss: 0.19856564700603485\n",
      "Step 1501 | Loss: 0.1479288935661316\n",
      "Step 1601 | Loss: 0.12589877843856812\n",
      "Step 1701 | Loss: 0.14151789247989655\n",
      "Step 1801 | Loss: 0.12264584749937057\n",
      "Step 1901 | Loss: 0.12315504252910614\n",
      "Step 2001 | Loss: 0.15756626427173615\n",
      "Step 2101 | Loss: 0.2201552391052246\n",
      "Step 2201 | Loss: 0.29892581701278687\n",
      "Step 2301 | Loss: 0.04894924536347389\n",
      "Step 2401 | Loss: 0.07871002703905106\n",
      "0.1317679078716973 0.955\n",
      "Step 1 | Loss: 0.47071313858032227\n",
      "Step 101 | Loss: 0.2694016695022583\n",
      "Step 201 | Loss: 0.1263236701488495\n",
      "Step 301 | Loss: 0.12450650334358215\n",
      "Step 401 | Loss: 0.21080511808395386\n",
      "Step 501 | Loss: 0.1297057867050171\n",
      "Step 601 | Loss: 0.17099666595458984\n",
      "Step 701 | Loss: 0.1894686222076416\n",
      "Step 801 | Loss: 0.11938031762838364\n",
      "Step 901 | Loss: 0.07773319631814957\n",
      "Step 1001 | Loss: 0.09772463887929916\n",
      "Step 1101 | Loss: 0.09535182267427444\n",
      "Step 1201 | Loss: 0.12493278086185455\n",
      "Step 1301 | Loss: 0.19162769615650177\n",
      "Step 1401 | Loss: 0.1370869278907776\n",
      "Step 1501 | Loss: 0.15288856625556946\n",
      "Step 1601 | Loss: 0.18459540605545044\n",
      "Step 1701 | Loss: 0.08177759498357773\n",
      "Step 1801 | Loss: 0.05204079672694206\n",
      "Step 1901 | Loss: 0.12095388025045395\n",
      "Step 2001 | Loss: 0.06147637963294983\n",
      "Step 2101 | Loss: 0.07115212082862854\n",
      "Step 2201 | Loss: 0.13965989649295807\n",
      "Step 2301 | Loss: 0.15095224976539612\n",
      "Step 2401 | Loss: 0.07161235809326172\n",
      "0.1321344476703543 0.955\n",
      "Step 1 | Loss: 2.588622570037842\n",
      "Step 101 | Loss: 0.7357228398323059\n",
      "Step 201 | Loss: 0.7365537285804749\n",
      "Step 301 | Loss: 0.48390525579452515\n",
      "Step 401 | Loss: 0.5085743069648743\n",
      "Step 501 | Loss: 0.33528780937194824\n",
      "Step 601 | Loss: 0.328525185585022\n",
      "Step 701 | Loss: 0.6346760392189026\n",
      "Step 801 | Loss: 0.2416159212589264\n",
      "Step 901 | Loss: 0.6003259420394897\n",
      "Step 1001 | Loss: 0.8610967397689819\n",
      "Step 1101 | Loss: 0.2913781702518463\n",
      "Step 1201 | Loss: 0.5161325931549072\n",
      "Step 1301 | Loss: 0.378229558467865\n",
      "Step 1401 | Loss: 0.712083637714386\n",
      "Step 1501 | Loss: 0.4613041281700134\n",
      "Step 1601 | Loss: 0.555103600025177\n",
      "Step 1701 | Loss: 0.45193803310394287\n",
      "Step 1801 | Loss: 0.28269922733306885\n",
      "Step 1901 | Loss: 0.6698188185691833\n",
      "Step 2001 | Loss: 0.6333732008934021\n",
      "Step 2101 | Loss: 0.5913754105567932\n",
      "Step 2201 | Loss: 0.5357488393783569\n",
      "Step 2301 | Loss: 0.4642844796180725\n",
      "Step 2401 | Loss: 0.5454144477844238\n",
      "0.4576873717882591 0.8625\n",
      "Step 1 | Loss: 0.8476690649986267\n",
      "Step 101 | Loss: 0.44937044382095337\n",
      "Step 201 | Loss: 0.7667366862297058\n",
      "Step 301 | Loss: 0.5858779549598694\n",
      "Step 401 | Loss: 0.40383416414260864\n",
      "Step 501 | Loss: 0.42068055272102356\n",
      "Step 601 | Loss: 0.5156803131103516\n",
      "Step 701 | Loss: 0.28029492497444153\n",
      "Step 801 | Loss: 0.4477011263370514\n",
      "Step 901 | Loss: 0.6547034978866577\n",
      "Step 1001 | Loss: 0.2592940330505371\n",
      "Step 1101 | Loss: 0.5115578174591064\n",
      "Step 1201 | Loss: 0.3392036557197571\n",
      "Step 1301 | Loss: 0.425542414188385\n",
      "Step 1401 | Loss: 0.37242257595062256\n",
      "Step 1501 | Loss: 0.4273837208747864\n",
      "Step 1601 | Loss: 0.452043354511261\n",
      "Step 1701 | Loss: 0.5578889846801758\n",
      "Step 1801 | Loss: 0.4354168176651001\n",
      "Step 1901 | Loss: 0.5167978405952454\n",
      "Step 2001 | Loss: 0.7121348977088928\n",
      "Step 2101 | Loss: 0.44303083419799805\n",
      "Step 2201 | Loss: 0.4258993864059448\n",
      "Step 2301 | Loss: 0.42214930057525635\n",
      "Step 2401 | Loss: 0.4876377284526825\n",
      "0.4563316324160851 0.8575\n",
      "Step 1 | Loss: 1.463409423828125\n",
      "Step 101 | Loss: 0.40613996982574463\n",
      "Step 201 | Loss: 0.581923246383667\n",
      "Step 301 | Loss: 0.32009997963905334\n",
      "Step 401 | Loss: 0.42021259665489197\n",
      "Step 501 | Loss: 0.8039369583129883\n",
      "Step 601 | Loss: 0.6353213787078857\n",
      "Step 701 | Loss: 0.3518816828727722\n",
      "Step 801 | Loss: 0.5893348455429077\n",
      "Step 901 | Loss: 0.44507265090942383\n",
      "Step 1001 | Loss: 0.6403543949127197\n",
      "Step 1101 | Loss: 0.7217990756034851\n",
      "Step 1201 | Loss: 0.3139157295227051\n",
      "Step 1301 | Loss: 0.1971459984779358\n",
      "Step 1401 | Loss: 0.5179551243782043\n",
      "Step 1501 | Loss: 0.4129464328289032\n",
      "Step 1601 | Loss: 0.3055627942085266\n",
      "Step 1701 | Loss: 0.3249130845069885\n",
      "Step 1801 | Loss: 0.39343172311782837\n",
      "Step 1901 | Loss: 0.48909351229667664\n",
      "Step 2001 | Loss: 0.2622267007827759\n",
      "Step 2101 | Loss: 0.4964215159416199\n",
      "Step 2201 | Loss: 0.5505263209342957\n",
      "Step 2301 | Loss: 0.37315911054611206\n",
      "Step 2401 | Loss: 0.32662832736968994\n",
      "0.45654804441562774 0.86\n",
      "Step 1 | Loss: 0.7192968726158142\n",
      "Step 101 | Loss: 0.23070093989372253\n",
      "Step 201 | Loss: 0.5993618965148926\n",
      "Step 301 | Loss: 0.31338056921958923\n",
      "Step 401 | Loss: 0.416148841381073\n",
      "Step 501 | Loss: 0.38673198223114014\n",
      "Step 601 | Loss: 0.420625239610672\n",
      "Step 701 | Loss: 0.521648108959198\n",
      "Step 801 | Loss: 0.3354402482509613\n",
      "Step 901 | Loss: 0.6183587312698364\n",
      "Step 1001 | Loss: 0.2977195084095001\n",
      "Step 1101 | Loss: 0.3386242687702179\n",
      "Step 1201 | Loss: 0.7744556069374084\n",
      "Step 1301 | Loss: 0.5173466205596924\n",
      "Step 1401 | Loss: 0.746258556842804\n",
      "Step 1501 | Loss: 0.37683185935020447\n",
      "Step 1601 | Loss: 0.48777925968170166\n",
      "Step 1701 | Loss: 0.5089077949523926\n",
      "Step 1801 | Loss: 0.4320433735847473\n",
      "Step 1901 | Loss: 0.2635667324066162\n",
      "Step 2001 | Loss: 0.3148600161075592\n",
      "Step 2101 | Loss: 0.3799807131290436\n",
      "Step 2201 | Loss: 0.44331252574920654\n",
      "Step 2301 | Loss: 0.297451376914978\n",
      "Step 2401 | Loss: 0.4366615414619446\n",
      "0.4563435321080597 0.8575\n",
      "Step 1 | Loss: 2.0575642585754395\n",
      "Step 101 | Loss: 0.27870500087738037\n",
      "Step 201 | Loss: 0.5153084397315979\n",
      "Step 301 | Loss: 0.5179810523986816\n",
      "Step 401 | Loss: 0.38884761929512024\n",
      "Step 501 | Loss: 0.6430286765098572\n",
      "Step 601 | Loss: 0.4583929479122162\n",
      "Step 701 | Loss: 0.4615090489387512\n",
      "Step 801 | Loss: 0.5494683384895325\n",
      "Step 901 | Loss: 0.32127341628074646\n",
      "Step 1001 | Loss: 0.5227766036987305\n",
      "Step 1101 | Loss: 0.3492431342601776\n",
      "Step 1201 | Loss: 0.558215856552124\n",
      "Step 1301 | Loss: 0.40973031520843506\n",
      "Step 1401 | Loss: 0.4987453818321228\n",
      "Step 1501 | Loss: 0.48383474349975586\n",
      "Step 1601 | Loss: 0.4246186316013336\n",
      "Step 1701 | Loss: 0.7679157257080078\n",
      "Step 1801 | Loss: 0.44776010513305664\n",
      "Step 1901 | Loss: 0.6979894638061523\n",
      "Step 2001 | Loss: 0.375002920627594\n",
      "Step 2101 | Loss: 0.5802947878837585\n",
      "Step 2201 | Loss: 0.4263562858104706\n",
      "Step 2301 | Loss: 0.37085187435150146\n",
      "Step 2401 | Loss: 0.4500409662723541\n",
      "0.45611229265918724 0.8575\n",
      "Step 1 | Loss: 1.606293797492981\n",
      "Step 101 | Loss: 0.9599516987800598\n",
      "Step 201 | Loss: 0.8419036865234375\n",
      "Step 301 | Loss: 0.6326377391815186\n",
      "Step 401 | Loss: 0.5151656866073608\n",
      "Step 501 | Loss: 0.4259396493434906\n",
      "Step 601 | Loss: 0.3900972306728363\n",
      "Step 701 | Loss: 0.24267472326755524\n",
      "Step 801 | Loss: 0.21589885652065277\n",
      "Step 901 | Loss: 0.19414615631103516\n",
      "Step 1001 | Loss: 0.34434375166893005\n",
      "Step 1101 | Loss: 0.196242555975914\n",
      "Step 1201 | Loss: 0.1321679949760437\n",
      "Step 1301 | Loss: 0.28526294231414795\n",
      "Step 1401 | Loss: 0.20583705604076385\n",
      "Step 1501 | Loss: 0.1741953343153\n",
      "Step 1601 | Loss: 0.19312123954296112\n",
      "Step 1701 | Loss: 0.12843656539916992\n",
      "Step 1801 | Loss: 0.19497156143188477\n",
      "Step 1901 | Loss: 0.18885213136672974\n",
      "Step 2001 | Loss: 0.09068439900875092\n",
      "Step 2101 | Loss: 0.1495298445224762\n",
      "Step 2201 | Loss: 0.21411295235157013\n",
      "Step 2301 | Loss: 0.17614784836769104\n",
      "Step 2401 | Loss: 0.19963809847831726\n",
      "0.18660436720157406 0.975\n",
      "Step 1 | Loss: 0.40731263160705566\n",
      "Step 101 | Loss: 0.2581377625465393\n",
      "Step 201 | Loss: 0.16671638190746307\n",
      "Step 301 | Loss: 0.1543165147304535\n",
      "Step 401 | Loss: 0.15415745973587036\n",
      "Step 501 | Loss: 0.278119295835495\n",
      "Step 601 | Loss: 0.29853522777557373\n",
      "Step 701 | Loss: 0.1723558008670807\n",
      "Step 801 | Loss: 0.20847932994365692\n",
      "Step 901 | Loss: 0.31574323773384094\n",
      "Step 1001 | Loss: 0.1827124059200287\n",
      "Step 1101 | Loss: 0.1802750527858734\n",
      "Step 1201 | Loss: 0.1600411981344223\n",
      "Step 1301 | Loss: 0.09720677882432938\n",
      "Step 1401 | Loss: 0.20034727454185486\n",
      "Step 1501 | Loss: 0.12149588018655777\n",
      "Step 1601 | Loss: 0.23903992772102356\n",
      "Step 1701 | Loss: 0.160680890083313\n",
      "Step 1801 | Loss: 0.15459184348583221\n",
      "Step 1901 | Loss: 0.25970396399497986\n",
      "Step 2001 | Loss: 0.18866553902626038\n",
      "Step 2101 | Loss: 0.1655954271554947\n",
      "Step 2201 | Loss: 0.13554054498672485\n",
      "Step 2301 | Loss: 0.10250803828239441\n",
      "Step 2401 | Loss: 0.15908341109752655\n",
      "0.17658574117595863 0.9625\n",
      "Step 1 | Loss: 0.6328611373901367\n",
      "Step 101 | Loss: 0.4380452036857605\n",
      "Step 201 | Loss: 0.5462135672569275\n",
      "Step 301 | Loss: 0.5828473567962646\n",
      "Step 401 | Loss: 0.39475950598716736\n",
      "Step 501 | Loss: 0.3272910416126251\n",
      "Step 601 | Loss: 0.23110808432102203\n",
      "Step 701 | Loss: 0.2587438225746155\n",
      "Step 801 | Loss: 0.3939427137374878\n",
      "Step 901 | Loss: 0.3061370253562927\n",
      "Step 1001 | Loss: 0.16407662630081177\n",
      "Step 1101 | Loss: 0.2929306626319885\n",
      "Step 1201 | Loss: 0.1907361000776291\n",
      "Step 1301 | Loss: 0.22514565289020538\n",
      "Step 1401 | Loss: 0.2563382685184479\n",
      "Step 1501 | Loss: 0.23998533189296722\n",
      "Step 1601 | Loss: 0.3080172538757324\n",
      "Step 1701 | Loss: 0.3022189736366272\n",
      "Step 1801 | Loss: 0.29120227694511414\n",
      "Step 1901 | Loss: 0.3537914454936981\n",
      "Step 2001 | Loss: 0.22533811628818512\n",
      "Step 2101 | Loss: 0.22248399257659912\n",
      "Step 2201 | Loss: 0.19339220225811005\n",
      "Step 2301 | Loss: 0.1846727728843689\n",
      "Step 2401 | Loss: 0.2554820477962494\n",
      "0.2210966957616495 0.95\n",
      "Step 1 | Loss: 2.0993871688842773\n",
      "Step 101 | Loss: 0.4215753972530365\n",
      "Step 201 | Loss: 0.2832573354244232\n",
      "Step 301 | Loss: 0.22360019385814667\n",
      "Step 401 | Loss: 0.22905470430850983\n",
      "Step 501 | Loss: 0.18817028403282166\n",
      "Step 601 | Loss: 0.25680264830589294\n",
      "Step 701 | Loss: 0.28868430852890015\n",
      "Step 801 | Loss: 0.31470566987991333\n",
      "Step 901 | Loss: 0.16613294184207916\n",
      "Step 1001 | Loss: 0.18858473002910614\n",
      "Step 1101 | Loss: 0.142775297164917\n",
      "Step 1201 | Loss: 0.3286893367767334\n",
      "Step 1301 | Loss: 0.1770775318145752\n",
      "Step 1401 | Loss: 0.2714139521121979\n",
      "Step 1501 | Loss: 0.23072336614131927\n",
      "Step 1601 | Loss: 0.13683679699897766\n",
      "Step 1701 | Loss: 0.2005414366722107\n",
      "Step 1801 | Loss: 0.2248438447713852\n",
      "Step 1901 | Loss: 0.27045905590057373\n",
      "Step 2001 | Loss: 0.1041976660490036\n",
      "Step 2101 | Loss: 0.13461533188819885\n",
      "Step 2201 | Loss: 0.42029690742492676\n",
      "Step 2301 | Loss: 0.3018796443939209\n",
      "Step 2401 | Loss: 0.22984416782855988\n",
      "0.18524431797369426 0.97\n",
      "Step 1 | Loss: 1.0774024724960327\n",
      "Step 101 | Loss: 1.1060385704040527\n",
      "Step 201 | Loss: 0.856326699256897\n",
      "Step 301 | Loss: 0.5588394999504089\n",
      "Step 401 | Loss: 0.3814304769039154\n",
      "Step 501 | Loss: 0.27385759353637695\n",
      "Step 601 | Loss: 0.31679338216781616\n",
      "Step 701 | Loss: 0.2268751710653305\n",
      "Step 801 | Loss: 0.35667023062705994\n",
      "Step 901 | Loss: 0.3480353057384491\n",
      "Step 1001 | Loss: 0.21924836933612823\n",
      "Step 1101 | Loss: 0.257313072681427\n",
      "Step 1201 | Loss: 0.15309898555278778\n",
      "Step 1301 | Loss: 0.20875288546085358\n",
      "Step 1401 | Loss: 0.21929427981376648\n",
      "Step 1501 | Loss: 0.1701587289571762\n",
      "Step 1601 | Loss: 0.15570805966854095\n",
      "Step 1701 | Loss: 0.29408302903175354\n",
      "Step 1801 | Loss: 0.19094707071781158\n",
      "Step 1901 | Loss: 0.17105039954185486\n",
      "Step 2001 | Loss: 0.1700737178325653\n",
      "Step 2101 | Loss: 0.16947738826274872\n",
      "Step 2201 | Loss: 0.21958127617835999\n",
      "Step 2301 | Loss: 0.23911790549755096\n",
      "Step 2401 | Loss: 0.22563259303569794\n",
      "0.18616217963888604 0.9725\n",
      "Step 1 | Loss: 0.8655610084533691\n",
      "Step 101 | Loss: 0.640056312084198\n",
      "Step 201 | Loss: 0.8513897061347961\n",
      "Step 301 | Loss: 0.6850968599319458\n",
      "Step 401 | Loss: 0.5636513233184814\n",
      "Step 501 | Loss: 0.7206287384033203\n",
      "Step 601 | Loss: 0.7809712886810303\n",
      "Step 701 | Loss: 0.8109782934188843\n",
      "Step 801 | Loss: 0.41151681542396545\n",
      "Step 901 | Loss: 0.6881471276283264\n",
      "Step 1001 | Loss: 0.6756485104560852\n",
      "Step 1101 | Loss: 0.7013804912567139\n",
      "Step 1201 | Loss: 0.5725173950195312\n",
      "Step 1301 | Loss: 0.5372660756111145\n",
      "Step 1401 | Loss: 0.6681218147277832\n",
      "Step 1501 | Loss: 0.5241799354553223\n",
      "Step 1601 | Loss: 0.5400397777557373\n",
      "Step 1701 | Loss: 0.5405794382095337\n",
      "Step 1801 | Loss: 0.5631450414657593\n",
      "Step 1901 | Loss: 0.5569166541099548\n",
      "Step 2001 | Loss: 0.47588419914245605\n",
      "Step 2101 | Loss: 0.7932435274124146\n",
      "Step 2201 | Loss: 0.7747575044631958\n",
      "Step 2301 | Loss: 0.5145632028579712\n",
      "Step 2401 | Loss: 0.5358240008354187\n",
      "0.6361608622724937 0.8\n",
      "Step 1 | Loss: 0.9744073152542114\n",
      "Step 101 | Loss: 0.8582537174224854\n",
      "Step 201 | Loss: 0.6045745611190796\n",
      "Step 301 | Loss: 0.6763278245925903\n",
      "Step 401 | Loss: 0.5489937663078308\n",
      "Step 501 | Loss: 0.5019313097000122\n",
      "Step 601 | Loss: 0.6327540278434753\n",
      "Step 701 | Loss: 0.5364137291908264\n",
      "Step 801 | Loss: 0.5940561890602112\n",
      "Step 901 | Loss: 0.5432619452476501\n",
      "Step 1001 | Loss: 0.6633816361427307\n",
      "Step 1101 | Loss: 0.6180539727210999\n",
      "Step 1201 | Loss: 0.5303679704666138\n",
      "Step 1301 | Loss: 0.6678038835525513\n",
      "Step 1401 | Loss: 0.4653424918651581\n",
      "Step 1501 | Loss: 0.5753610730171204\n",
      "Step 1601 | Loss: 0.7740702033042908\n",
      "Step 1701 | Loss: 0.6240639686584473\n",
      "Step 1801 | Loss: 0.44368070363998413\n",
      "Step 1901 | Loss: 0.8638618588447571\n",
      "Step 2001 | Loss: 0.6451057195663452\n",
      "Step 2101 | Loss: 0.6676221489906311\n",
      "Step 2201 | Loss: 0.517238199710846\n",
      "Step 2301 | Loss: 0.6089625358581543\n",
      "Step 2401 | Loss: 0.5872021913528442\n",
      "0.6285818837206624 0.8025\n",
      "Step 1 | Loss: 1.1122047901153564\n",
      "Step 101 | Loss: 0.6479457020759583\n",
      "Step 201 | Loss: 0.6184455752372742\n",
      "Step 301 | Loss: 0.7738777995109558\n",
      "Step 401 | Loss: 0.6162324547767639\n",
      "Step 501 | Loss: 0.6592776775360107\n",
      "Step 601 | Loss: 0.5413683652877808\n",
      "Step 701 | Loss: 0.5702441334724426\n",
      "Step 801 | Loss: 0.5909606218338013\n",
      "Step 901 | Loss: 0.5975875854492188\n",
      "Step 1001 | Loss: 0.6228904128074646\n",
      "Step 1101 | Loss: 0.6528028845787048\n",
      "Step 1201 | Loss: 0.6773549318313599\n",
      "Step 1301 | Loss: 0.6171478033065796\n",
      "Step 1401 | Loss: 0.33084383606910706\n",
      "Step 1501 | Loss: 0.5716683864593506\n",
      "Step 1601 | Loss: 0.4686550498008728\n",
      "Step 1701 | Loss: 0.6291506290435791\n",
      "Step 1801 | Loss: 0.6334494948387146\n",
      "Step 1901 | Loss: 0.5674727559089661\n",
      "Step 2001 | Loss: 0.7840689420700073\n",
      "Step 2101 | Loss: 0.5572826862335205\n",
      "Step 2201 | Loss: 0.5214091539382935\n",
      "Step 2301 | Loss: 0.49232515692710876\n",
      "Step 2401 | Loss: 0.5786722898483276\n",
      "0.5969114437412345 0.8225\n",
      "Step 1 | Loss: 1.2667068243026733\n",
      "Step 101 | Loss: 0.6700204610824585\n",
      "Step 201 | Loss: 0.8489458560943604\n",
      "Step 301 | Loss: 0.45155051350593567\n",
      "Step 401 | Loss: 0.7122492790222168\n",
      "Step 501 | Loss: 1.1386250257492065\n",
      "Step 601 | Loss: 0.7274199724197388\n",
      "Step 701 | Loss: 0.5288030505180359\n",
      "Step 801 | Loss: 0.7747655510902405\n",
      "Step 901 | Loss: 0.6174769401550293\n",
      "Step 1001 | Loss: 0.4133680760860443\n",
      "Step 1101 | Loss: 0.6829611659049988\n",
      "Step 1201 | Loss: 0.5772411823272705\n",
      "Step 1301 | Loss: 0.5377832651138306\n",
      "Step 1401 | Loss: 0.5662233829498291\n",
      "Step 1501 | Loss: 0.5870584845542908\n",
      "Step 1601 | Loss: 0.6621390581130981\n",
      "Step 1701 | Loss: 0.6397982239723206\n",
      "Step 1801 | Loss: 0.8049657344818115\n",
      "Step 1901 | Loss: 0.7490045428276062\n",
      "Step 2001 | Loss: 0.6933608055114746\n",
      "Step 2101 | Loss: 0.6112013459205627\n",
      "Step 2201 | Loss: 0.5300514698028564\n",
      "Step 2301 | Loss: 1.0388792753219604\n",
      "Step 2401 | Loss: 0.6478774547576904\n",
      "0.6380967035807804 0.7975\n",
      "Step 1 | Loss: 1.013595700263977\n",
      "Step 101 | Loss: 0.9884345531463623\n",
      "Step 201 | Loss: 0.8312370777130127\n",
      "Step 301 | Loss: 0.610992431640625\n",
      "Step 401 | Loss: 0.6936367750167847\n",
      "Step 501 | Loss: 0.5615687966346741\n",
      "Step 601 | Loss: 0.6856245398521423\n",
      "Step 701 | Loss: 0.581147313117981\n",
      "Step 801 | Loss: 0.7609241604804993\n",
      "Step 901 | Loss: 0.6771655082702637\n",
      "Step 1001 | Loss: 0.7405922412872314\n",
      "Step 1101 | Loss: 0.7044974565505981\n",
      "Step 1201 | Loss: 0.7100354433059692\n",
      "Step 1301 | Loss: 0.8259516954421997\n",
      "Step 1401 | Loss: 0.520838737487793\n",
      "Step 1501 | Loss: 0.5028458833694458\n",
      "Step 1601 | Loss: 0.5816108584403992\n",
      "Step 1701 | Loss: 0.6450902223587036\n",
      "Step 1801 | Loss: 0.6436509490013123\n",
      "Step 1901 | Loss: 0.5868086218833923\n",
      "Step 2001 | Loss: 0.6289113163948059\n",
      "Step 2101 | Loss: 0.4205518662929535\n",
      "Step 2201 | Loss: 0.7487561702728271\n",
      "Step 2301 | Loss: 0.72850102186203\n",
      "Step 2401 | Loss: 0.64018315076828\n",
      "0.6338454908959439 0.795\n",
      "Step 1 | Loss: 1.1049168109893799\n",
      "Step 101 | Loss: 0.859004020690918\n",
      "Step 201 | Loss: 0.656224250793457\n",
      "Step 301 | Loss: 0.7925655841827393\n",
      "Step 401 | Loss: 0.7153134346008301\n",
      "Step 501 | Loss: 0.6391693353652954\n",
      "Step 601 | Loss: 0.7460184693336487\n",
      "Step 701 | Loss: 0.5355121493339539\n",
      "Step 801 | Loss: 0.567577064037323\n",
      "Step 901 | Loss: 0.6272116303443909\n",
      "Step 1001 | Loss: 0.5966467261314392\n",
      "Step 1101 | Loss: 0.5904892683029175\n",
      "Step 1201 | Loss: 0.7005652785301208\n",
      "Step 1301 | Loss: 0.5932692289352417\n",
      "Step 1401 | Loss: 0.697980523109436\n",
      "Step 1501 | Loss: 0.6187729239463806\n",
      "Step 1601 | Loss: 0.715619683265686\n",
      "Step 1701 | Loss: 0.5979870557785034\n",
      "Step 1801 | Loss: 0.6484091877937317\n",
      "Step 1901 | Loss: 0.6556878685951233\n",
      "Step 2001 | Loss: 0.6601066589355469\n",
      "Step 2101 | Loss: 0.5554689168930054\n",
      "Step 2201 | Loss: 0.7039437294006348\n",
      "Step 2301 | Loss: 0.7657836079597473\n",
      "Step 2401 | Loss: 0.5481821298599243\n",
      "0.6569419169978048 0.785\n",
      "Step 1 | Loss: 0.9950385689735413\n",
      "Step 101 | Loss: 0.8331581950187683\n",
      "Step 201 | Loss: 0.8764117956161499\n",
      "Step 301 | Loss: 0.8188776969909668\n",
      "Step 401 | Loss: 0.8296892642974854\n",
      "Step 501 | Loss: 0.8624446988105774\n",
      "Step 601 | Loss: 0.8140736818313599\n",
      "Step 701 | Loss: 0.8169600963592529\n",
      "Step 801 | Loss: 0.8740414381027222\n",
      "Step 901 | Loss: 0.8481045961380005\n",
      "Step 1001 | Loss: 0.8191643953323364\n",
      "Step 1101 | Loss: 0.6988617181777954\n",
      "Step 1201 | Loss: 0.7065428495407104\n",
      "Step 1301 | Loss: 0.7196170091629028\n",
      "Step 1401 | Loss: 0.7854753732681274\n",
      "Step 1501 | Loss: 0.7819472551345825\n",
      "Step 1601 | Loss: 0.8013412952423096\n",
      "Step 1701 | Loss: 0.7123464345932007\n",
      "Step 1801 | Loss: 0.6635739803314209\n",
      "Step 1901 | Loss: 0.6973316073417664\n",
      "Step 2001 | Loss: 0.7873874306678772\n",
      "Step 2101 | Loss: 0.7942198514938354\n",
      "Step 2201 | Loss: 0.7795497179031372\n",
      "Step 2301 | Loss: 0.6628772020339966\n",
      "Step 2401 | Loss: 0.7011573910713196\n",
      "0.7327542003964214 0.7975\n",
      "Step 1 | Loss: 1.0909514427185059\n",
      "Step 101 | Loss: 1.0088849067687988\n",
      "Step 201 | Loss: 0.9014058113098145\n",
      "Step 301 | Loss: 0.8516879081726074\n",
      "Step 401 | Loss: 0.8001456260681152\n",
      "Step 501 | Loss: 0.7426049113273621\n",
      "Step 601 | Loss: 0.8512576818466187\n",
      "Step 701 | Loss: 0.7528207898139954\n",
      "Step 801 | Loss: 0.8501260280609131\n",
      "Step 901 | Loss: 0.8142784833908081\n",
      "Step 1001 | Loss: 0.697650671005249\n",
      "Step 1101 | Loss: 0.7521858215332031\n",
      "Step 1201 | Loss: 0.8262046575546265\n",
      "Step 1301 | Loss: 0.758433997631073\n",
      "Step 1401 | Loss: 0.7065612077713013\n",
      "Step 1501 | Loss: 0.7263943552970886\n",
      "Step 1601 | Loss: 0.7110135555267334\n",
      "Step 1701 | Loss: 0.6902818083763123\n",
      "Step 1801 | Loss: 0.7343228459358215\n",
      "Step 1901 | Loss: 0.7223698496818542\n",
      "Step 2001 | Loss: 0.6540185809135437\n",
      "Step 2101 | Loss: 0.6850968599319458\n",
      "Step 2201 | Loss: 0.63004469871521\n",
      "Step 2301 | Loss: 0.712850034236908\n",
      "Step 2401 | Loss: 0.6778706312179565\n",
      "0.7229305640063848 0.78\n",
      "Step 1 | Loss: 0.8951200246810913\n",
      "Step 101 | Loss: 0.7270132303237915\n",
      "Step 201 | Loss: 0.786098301410675\n",
      "Step 301 | Loss: 0.7690984606742859\n",
      "Step 401 | Loss: 0.7815434336662292\n",
      "Step 501 | Loss: 0.930052638053894\n",
      "Step 601 | Loss: 0.6429005265235901\n",
      "Step 701 | Loss: 0.6887102723121643\n",
      "Step 801 | Loss: 0.6051006317138672\n",
      "Step 901 | Loss: 0.7093045115470886\n",
      "Step 1001 | Loss: 0.5439488887786865\n",
      "Step 1101 | Loss: 0.674450159072876\n",
      "Step 1201 | Loss: 0.6525940299034119\n",
      "Step 1301 | Loss: 0.5197237730026245\n",
      "Step 1401 | Loss: 0.6525136232376099\n",
      "Step 1501 | Loss: 0.70564866065979\n",
      "Step 1601 | Loss: 0.5829014182090759\n",
      "Step 1701 | Loss: 0.7120692133903503\n",
      "Step 1801 | Loss: 0.627068817615509\n",
      "Step 1901 | Loss: 0.6787070631980896\n",
      "Step 2001 | Loss: 0.6454441547393799\n",
      "Step 2101 | Loss: 0.5222270488739014\n",
      "Step 2201 | Loss: 0.56031334400177\n",
      "Step 2301 | Loss: 0.48668384552001953\n",
      "Step 2401 | Loss: 0.6974160671234131\n",
      "0.6427975284729925 0.78\n",
      "Step 1 | Loss: 1.0884373188018799\n",
      "Step 101 | Loss: 0.9034467935562134\n",
      "Step 201 | Loss: 0.8829890489578247\n",
      "Step 301 | Loss: 0.8178644180297852\n",
      "Step 401 | Loss: 0.587959349155426\n",
      "Step 501 | Loss: 0.7315492630004883\n",
      "Step 601 | Loss: 0.7428709268569946\n",
      "Step 701 | Loss: 0.7232853770256042\n",
      "Step 801 | Loss: 0.7462080717086792\n",
      "Step 901 | Loss: 0.6024878025054932\n",
      "Step 1001 | Loss: 0.6866171360015869\n",
      "Step 1101 | Loss: 0.7719904184341431\n",
      "Step 1201 | Loss: 0.7655849456787109\n",
      "Step 1301 | Loss: 0.6656683683395386\n",
      "Step 1401 | Loss: 0.6326999068260193\n",
      "Step 1501 | Loss: 0.7496094107627869\n",
      "Step 1601 | Loss: 0.7210560441017151\n",
      "Step 1701 | Loss: 0.678783118724823\n",
      "Step 1801 | Loss: 0.7209492325782776\n",
      "Step 1901 | Loss: 0.7246650457382202\n",
      "Step 2001 | Loss: 0.7602600455284119\n",
      "Step 2101 | Loss: 0.6836862564086914\n",
      "Step 2201 | Loss: 0.6736745834350586\n",
      "Step 2301 | Loss: 0.7423385977745056\n",
      "Step 2401 | Loss: 0.7381908893585205\n",
      "0.677896873004892 0.8275\n",
      "Step 1 | Loss: 1.0238361358642578\n",
      "Step 101 | Loss: 0.3649136424064636\n",
      "Step 201 | Loss: 0.17890320718288422\n",
      "Step 301 | Loss: 0.3039866089820862\n",
      "Step 401 | Loss: 0.2637634873390198\n",
      "Step 501 | Loss: 0.29503026604652405\n",
      "Step 601 | Loss: 0.2827235758304596\n",
      "Step 701 | Loss: 0.21552038192749023\n",
      "Step 801 | Loss: 0.25988519191741943\n",
      "Step 901 | Loss: 0.1747698187828064\n",
      "Step 1001 | Loss: 0.20155920088291168\n",
      "Step 1101 | Loss: 0.23094218969345093\n",
      "Step 1201 | Loss: 0.33803480863571167\n",
      "Step 1301 | Loss: 0.20721451938152313\n",
      "Step 1401 | Loss: 0.2416086196899414\n",
      "Step 1501 | Loss: 0.2029973417520523\n",
      "Step 1601 | Loss: 0.22673936188220978\n",
      "Step 1701 | Loss: 0.18602843582630157\n",
      "Step 1801 | Loss: 0.2546285390853882\n",
      "Step 1901 | Loss: 0.21970754861831665\n",
      "Step 2001 | Loss: 0.1527436077594757\n",
      "Step 2101 | Loss: 0.1211058497428894\n",
      "Step 2201 | Loss: 0.14089751243591309\n",
      "Step 2301 | Loss: 0.1824803650379181\n",
      "Step 2401 | Loss: 0.15808741748332977\n",
      "0.16731796732043078 0.975\n",
      "Step 1 | Loss: 0.8474889993667603\n",
      "Step 101 | Loss: 0.44577381014823914\n",
      "Step 201 | Loss: 0.40333884954452515\n",
      "Step 301 | Loss: 0.4785017967224121\n",
      "Step 401 | Loss: 0.176376074552536\n",
      "Step 501 | Loss: 0.27787870168685913\n",
      "Step 601 | Loss: 0.17474453151226044\n",
      "Step 701 | Loss: 0.21613909304141998\n",
      "Step 801 | Loss: 0.17352482676506042\n",
      "Step 901 | Loss: 0.19254349172115326\n",
      "Step 1001 | Loss: 0.23836642503738403\n",
      "Step 1101 | Loss: 0.14832273125648499\n",
      "Step 1201 | Loss: 0.21928705275058746\n",
      "Step 1301 | Loss: 0.27338114380836487\n",
      "Step 1401 | Loss: 0.20900779962539673\n",
      "Step 1501 | Loss: 0.20695289969444275\n",
      "Step 1601 | Loss: 0.18793171644210815\n",
      "Step 1701 | Loss: 0.15518972277641296\n",
      "Step 1801 | Loss: 0.2758311331272125\n",
      "Step 1901 | Loss: 0.19911926984786987\n",
      "Step 2001 | Loss: 0.15731152892112732\n",
      "Step 2101 | Loss: 0.2126169502735138\n",
      "Step 2201 | Loss: 0.20116111636161804\n",
      "Step 2301 | Loss: 0.2221856415271759\n",
      "Step 2401 | Loss: 0.2166290432214737\n",
      "0.18706629283896492 0.9725\n",
      "Step 1 | Loss: 1.0747334957122803\n",
      "Step 101 | Loss: 0.2120581567287445\n",
      "Step 201 | Loss: 0.2841082513332367\n",
      "Step 301 | Loss: 0.38618022203445435\n",
      "Step 401 | Loss: 0.29628321528434753\n",
      "Step 501 | Loss: 0.2922400236129761\n",
      "Step 601 | Loss: 0.20541894435882568\n",
      "Step 701 | Loss: 0.24638012051582336\n",
      "Step 801 | Loss: 0.34887880086898804\n",
      "Step 901 | Loss: 0.2877005338668823\n",
      "Step 1001 | Loss: 0.2335205078125\n",
      "Step 1101 | Loss: 0.3674164116382599\n",
      "Step 1201 | Loss: 0.37974706292152405\n",
      "Step 1301 | Loss: 0.19266581535339355\n",
      "Step 1401 | Loss: 0.35991930961608887\n",
      "Step 1501 | Loss: 0.25957369804382324\n",
      "Step 1601 | Loss: 0.16954147815704346\n",
      "Step 1701 | Loss: 0.13894400000572205\n",
      "Step 1801 | Loss: 0.16557243466377258\n",
      "Step 1901 | Loss: 0.3367849886417389\n",
      "Step 2001 | Loss: 0.0952499657869339\n",
      "Step 2101 | Loss: 0.2537151575088501\n",
      "Step 2201 | Loss: 0.16217225790023804\n",
      "Step 2301 | Loss: 0.21029138565063477\n",
      "Step 2401 | Loss: 0.09079353511333466\n",
      "0.16648080230648965 0.9775\n",
      "Step 1 | Loss: 1.172573447227478\n",
      "Step 101 | Loss: 0.29029735922813416\n",
      "Step 201 | Loss: 0.15202583372592926\n",
      "Step 301 | Loss: 0.28208988904953003\n",
      "Step 401 | Loss: 0.2685571312904358\n",
      "Step 501 | Loss: 0.13857102394104004\n",
      "Step 601 | Loss: 0.3164246082305908\n",
      "Step 701 | Loss: 0.15171194076538086\n",
      "Step 801 | Loss: 0.16458414494991302\n",
      "Step 901 | Loss: 0.1668272167444229\n",
      "Step 1001 | Loss: 0.15419116616249084\n",
      "Step 1101 | Loss: 0.14139169454574585\n",
      "Step 1201 | Loss: 0.2053246647119522\n",
      "Step 1301 | Loss: 0.18104659020900726\n",
      "Step 1401 | Loss: 0.20282164216041565\n",
      "Step 1501 | Loss: 0.2788039743900299\n",
      "Step 1601 | Loss: 0.230479434132576\n",
      "Step 1701 | Loss: 0.24480679631233215\n",
      "Step 1801 | Loss: 0.19181066751480103\n",
      "Step 1901 | Loss: 0.12046296149492264\n",
      "Step 2001 | Loss: 0.14774784445762634\n",
      "Step 2101 | Loss: 0.14625008404254913\n",
      "Step 2201 | Loss: 0.07689701020717621\n",
      "Step 2301 | Loss: 0.16059157252311707\n",
      "Step 2401 | Loss: 0.16689950227737427\n",
      "0.16198442721659237 0.9775\n",
      "Step 1 | Loss: 0.47761550545692444\n",
      "Step 101 | Loss: 0.6334704756736755\n",
      "Step 201 | Loss: 0.19477279484272003\n",
      "Step 301 | Loss: 0.23587343096733093\n",
      "Step 401 | Loss: 0.22827962040901184\n",
      "Step 501 | Loss: 0.1972421258687973\n",
      "Step 601 | Loss: 0.14463436603546143\n",
      "Step 701 | Loss: 0.21262820065021515\n",
      "Step 801 | Loss: 0.11501602828502655\n",
      "Step 901 | Loss: 0.2736617922782898\n",
      "Step 1001 | Loss: 0.14226801693439484\n",
      "Step 1101 | Loss: 0.11444802582263947\n",
      "Step 1201 | Loss: 0.15092414617538452\n",
      "Step 1301 | Loss: 0.1863749921321869\n",
      "Step 1401 | Loss: 0.19088099896907806\n",
      "Step 1501 | Loss: 0.1569632887840271\n",
      "Step 1601 | Loss: 0.14486460387706757\n",
      "Step 1701 | Loss: 0.1501273214817047\n",
      "Step 1801 | Loss: 0.10264445096254349\n",
      "Step 1901 | Loss: 0.1321362406015396\n",
      "Step 2001 | Loss: 0.15105780959129333\n",
      "Step 2101 | Loss: 0.18089811503887177\n",
      "Step 2201 | Loss: 0.11447171121835709\n",
      "Step 2301 | Loss: 0.21728965640068054\n",
      "Step 2401 | Loss: 0.21945950388908386\n",
      "0.16145859417366826 0.98\n",
      "Step 1 | Loss: 0.411196231842041\n",
      "Step 101 | Loss: 0.1763014793395996\n",
      "Step 201 | Loss: 0.3286689519882202\n",
      "Step 301 | Loss: 0.1447596400976181\n",
      "Step 401 | Loss: 0.3473813235759735\n",
      "Step 501 | Loss: 0.17687176167964935\n",
      "Step 601 | Loss: 0.2143973708152771\n",
      "Step 701 | Loss: 0.2790721356868744\n",
      "Step 801 | Loss: 0.17741720378398895\n",
      "Step 901 | Loss: 0.20742066204547882\n",
      "Step 1001 | Loss: 0.3367517590522766\n",
      "Step 1101 | Loss: 0.30622008442878723\n",
      "Step 1201 | Loss: 0.3515542149543762\n",
      "Step 1301 | Loss: 0.20198273658752441\n",
      "Step 1401 | Loss: 0.3599747121334076\n",
      "Step 1501 | Loss: 0.21776200830936432\n",
      "Step 1601 | Loss: 0.35583001375198364\n",
      "Step 1701 | Loss: 0.20534813404083252\n",
      "Step 1801 | Loss: 0.21032015979290009\n",
      "Step 1901 | Loss: 0.31568071246147156\n",
      "Step 2001 | Loss: 0.22951239347457886\n",
      "Step 2101 | Loss: 0.19064827263355255\n",
      "Step 2201 | Loss: 0.2769760489463806\n",
      "Step 2301 | Loss: 0.17645448446273804\n",
      "Step 2401 | Loss: 0.2723650336265564\n",
      "0.20449405903019047 0.9675\n",
      "Step 1 | Loss: 0.8217768669128418\n",
      "Step 101 | Loss: 0.15413904190063477\n",
      "Step 201 | Loss: 0.361528217792511\n",
      "Step 301 | Loss: 0.29066458344459534\n",
      "Step 401 | Loss: 0.4755555987358093\n",
      "Step 501 | Loss: 0.41068556904792786\n",
      "Step 601 | Loss: 0.2946040630340576\n",
      "Step 701 | Loss: 0.26261189579963684\n",
      "Step 801 | Loss: 0.2841992676258087\n",
      "Step 901 | Loss: 0.36932793259620667\n",
      "Step 1001 | Loss: 0.30991241335868835\n",
      "Step 1101 | Loss: 0.22837723791599274\n",
      "Step 1201 | Loss: 0.2855624258518219\n",
      "Step 1301 | Loss: 0.2554876506328583\n",
      "Step 1401 | Loss: 0.357034295797348\n",
      "Step 1501 | Loss: 0.23144130408763885\n",
      "Step 1601 | Loss: 0.19945108890533447\n",
      "Step 1701 | Loss: 0.08714254200458527\n",
      "Step 1801 | Loss: 0.23280923068523407\n",
      "Step 1901 | Loss: 0.23506155610084534\n",
      "Step 2001 | Loss: 0.24756111204624176\n",
      "Step 2101 | Loss: 0.21440501511096954\n",
      "Step 2201 | Loss: 0.2458200305700302\n",
      "Step 2301 | Loss: 0.2202834188938141\n",
      "Step 2401 | Loss: 0.28665903210639954\n",
      "0.20467980837386643 0.9675\n",
      "Step 1 | Loss: 1.3642640113830566\n",
      "Step 101 | Loss: 1.0020015239715576\n",
      "Step 201 | Loss: 0.9549890756607056\n",
      "Step 301 | Loss: 0.8017380833625793\n",
      "Step 401 | Loss: 0.8452557325363159\n",
      "Step 501 | Loss: 0.6446422338485718\n",
      "Step 601 | Loss: 0.7411529421806335\n",
      "Step 701 | Loss: 0.8220064043998718\n",
      "Step 801 | Loss: 0.6091793775558472\n",
      "Step 901 | Loss: 0.3444870710372925\n",
      "Step 1001 | Loss: 0.4350336790084839\n",
      "Step 1101 | Loss: 0.3049834072589874\n",
      "Step 1201 | Loss: 0.39031627774238586\n",
      "Step 1301 | Loss: 0.22902031242847443\n",
      "Step 1401 | Loss: 0.24694985151290894\n",
      "Step 1501 | Loss: 0.4810419976711273\n",
      "Step 1601 | Loss: 0.301053524017334\n",
      "Step 1701 | Loss: 0.2284003496170044\n",
      "Step 1801 | Loss: 0.2754165232181549\n",
      "Step 1901 | Loss: 0.25483238697052\n",
      "Step 2001 | Loss: 0.21665959060192108\n",
      "Step 2101 | Loss: 0.1462555229663849\n",
      "Step 2201 | Loss: 0.24617993831634521\n",
      "Step 2301 | Loss: 0.2186570167541504\n",
      "Step 2401 | Loss: 0.3883781135082245\n",
      "0.2632119236255412 0.9325\n",
      "Step 1 | Loss: 1.0114924907684326\n",
      "Step 101 | Loss: 0.3792155385017395\n",
      "Step 201 | Loss: 0.25921130180358887\n",
      "Step 301 | Loss: 0.23889406025409698\n",
      "Step 401 | Loss: 0.1421608328819275\n",
      "Step 501 | Loss: 0.24741825461387634\n",
      "Step 601 | Loss: 0.1994026154279709\n",
      "Step 701 | Loss: 0.12886494398117065\n",
      "Step 801 | Loss: 0.15954506397247314\n",
      "Step 901 | Loss: 0.3035004734992981\n",
      "Step 1001 | Loss: 0.3518461585044861\n",
      "Step 1101 | Loss: 0.2074280083179474\n",
      "Step 1201 | Loss: 0.19996583461761475\n",
      "Step 1301 | Loss: 0.18683859705924988\n",
      "Step 1401 | Loss: 0.2811957001686096\n",
      "Step 1501 | Loss: 0.3222852349281311\n",
      "Step 1601 | Loss: 0.23280473053455353\n",
      "Step 1701 | Loss: 0.1677531749010086\n",
      "Step 1801 | Loss: 0.22709587216377258\n",
      "Step 1901 | Loss: 0.3511369824409485\n",
      "Step 2001 | Loss: 0.14101789891719818\n",
      "Step 2101 | Loss: 0.2632918357849121\n",
      "Step 2201 | Loss: 0.3532556891441345\n",
      "Step 2301 | Loss: 0.2368897646665573\n",
      "Step 2401 | Loss: 0.15150752663612366\n",
      "0.20478528923963635 0.9675\n",
      "Step 1 | Loss: 1.3377526998519897\n",
      "Step 101 | Loss: 0.8894039392471313\n",
      "Step 201 | Loss: 0.9514654874801636\n",
      "Step 301 | Loss: 0.7801249027252197\n",
      "Step 401 | Loss: 0.6453444361686707\n",
      "Step 501 | Loss: 0.8646001219749451\n",
      "Step 601 | Loss: 0.4589008092880249\n",
      "Step 701 | Loss: 0.28195226192474365\n",
      "Step 801 | Loss: 0.23499131202697754\n",
      "Step 901 | Loss: 0.2604871392250061\n",
      "Step 1001 | Loss: 0.31184688210487366\n",
      "Step 1101 | Loss: 0.2883588373661041\n",
      "Step 1201 | Loss: 0.2879059314727783\n",
      "Step 1301 | Loss: 0.13286051154136658\n",
      "Step 1401 | Loss: 0.44268375635147095\n",
      "Step 1501 | Loss: 0.2676718831062317\n",
      "Step 1601 | Loss: 0.3741057813167572\n",
      "Step 1701 | Loss: 0.25891751050949097\n",
      "Step 1801 | Loss: 0.3545653820037842\n",
      "Step 1901 | Loss: 0.4293524920940399\n",
      "Step 2001 | Loss: 0.32927244901657104\n",
      "Step 2101 | Loss: 0.3762315511703491\n",
      "Step 2201 | Loss: 0.2875711917877197\n",
      "Step 2301 | Loss: 0.2954826056957245\n",
      "Step 2401 | Loss: 0.22331592440605164\n",
      "0.2629759169696265 0.9375\n",
      "Step 1 | Loss: 1.3719236850738525\n",
      "Step 101 | Loss: 0.563877522945404\n",
      "Step 201 | Loss: 0.48710161447525024\n",
      "Step 301 | Loss: 0.42277273535728455\n",
      "Step 401 | Loss: 0.5000223517417908\n",
      "Step 501 | Loss: 0.40195590257644653\n",
      "Step 601 | Loss: 0.2679632008075714\n",
      "Step 701 | Loss: 0.5238486528396606\n",
      "Step 801 | Loss: 0.35917383432388306\n",
      "Step 901 | Loss: 0.4693027138710022\n",
      "Step 1001 | Loss: 0.5767018795013428\n",
      "Step 1101 | Loss: 0.5537304282188416\n",
      "Step 1201 | Loss: 0.3390061855316162\n",
      "Step 1301 | Loss: 0.38600677251815796\n",
      "Step 1401 | Loss: 0.547560453414917\n",
      "Step 1501 | Loss: 0.4899168610572815\n",
      "Step 1601 | Loss: 0.4099460542201996\n",
      "Step 1701 | Loss: 0.3146649897098541\n",
      "Step 1801 | Loss: 0.4521366357803345\n",
      "Step 1901 | Loss: 0.48651057481765747\n",
      "Step 2001 | Loss: 0.36833545565605164\n",
      "Step 2101 | Loss: 0.3321917653083801\n",
      "Step 2201 | Loss: 0.3845920264720917\n",
      "Step 2301 | Loss: 0.4204038679599762\n",
      "Step 2401 | Loss: 0.3655321002006531\n",
      "0.4177065307159956 0.8975\n",
      "Step 1 | Loss: 2.216223955154419\n",
      "Step 101 | Loss: 0.6702229976654053\n",
      "Step 201 | Loss: 0.40087369084358215\n",
      "Step 301 | Loss: 0.3950897753238678\n",
      "Step 401 | Loss: 0.4576990306377411\n",
      "Step 501 | Loss: 0.3403419554233551\n",
      "Step 601 | Loss: 0.35779905319213867\n",
      "Step 701 | Loss: 0.35283994674682617\n",
      "Step 801 | Loss: 0.3003285229206085\n",
      "Step 901 | Loss: 0.5232922434806824\n",
      "Step 1001 | Loss: 0.4143076539039612\n",
      "Step 1101 | Loss: 0.488120436668396\n",
      "Step 1201 | Loss: 0.42046406865119934\n",
      "Step 1301 | Loss: 0.451480895280838\n",
      "Step 1401 | Loss: 0.4727870225906372\n",
      "Step 1501 | Loss: 0.5381940603256226\n",
      "Step 1601 | Loss: 0.45245838165283203\n",
      "Step 1701 | Loss: 0.4766911268234253\n",
      "Step 1801 | Loss: 0.6449772715568542\n",
      "Step 1901 | Loss: 0.4771832823753357\n",
      "Step 2001 | Loss: 0.37119579315185547\n",
      "Step 2101 | Loss: 0.40645653009414673\n",
      "Step 2201 | Loss: 0.3961632251739502\n",
      "Step 2301 | Loss: 0.32224228978157043\n",
      "Step 2401 | Loss: 0.5634837746620178\n",
      "0.41688808619812934 0.895\n",
      "Step 1 | Loss: 1.6252164840698242\n",
      "Step 101 | Loss: 0.8953825235366821\n",
      "Step 201 | Loss: 0.6777138710021973\n",
      "Step 301 | Loss: 0.7077857851982117\n",
      "Step 401 | Loss: 0.5500636100769043\n",
      "Step 501 | Loss: 0.6801382899284363\n",
      "Step 601 | Loss: 0.7310764193534851\n",
      "Step 701 | Loss: 0.7141104936599731\n",
      "Step 801 | Loss: 0.7086873054504395\n",
      "Step 901 | Loss: 0.7329519987106323\n",
      "Step 1001 | Loss: 0.6373805999755859\n",
      "Step 1101 | Loss: 0.5766524076461792\n",
      "Step 1201 | Loss: 0.49678128957748413\n",
      "Step 1301 | Loss: 0.47726914286613464\n",
      "Step 1401 | Loss: 0.40373560786247253\n",
      "Step 1501 | Loss: 0.566699206829071\n",
      "Step 1601 | Loss: 0.39945366978645325\n",
      "Step 1701 | Loss: 0.3694693446159363\n",
      "Step 1801 | Loss: 0.3582758605480194\n",
      "Step 1901 | Loss: 0.36853155493736267\n",
      "Step 2001 | Loss: 0.46654221415519714\n",
      "Step 2101 | Loss: 0.5299360752105713\n",
      "Step 2201 | Loss: 0.34731271862983704\n",
      "Step 2301 | Loss: 0.4467150866985321\n",
      "Step 2401 | Loss: 0.46472495794296265\n",
      "0.41751881366718196 0.88\n",
      "Step 1 | Loss: 0.5596802234649658\n",
      "Step 101 | Loss: 0.34829267859458923\n",
      "Step 201 | Loss: 0.3954783082008362\n",
      "Step 301 | Loss: 0.4213240146636963\n",
      "Step 401 | Loss: 0.3008338212966919\n",
      "Step 501 | Loss: 0.3401440382003784\n",
      "Step 601 | Loss: 0.4808019995689392\n",
      "Step 701 | Loss: 0.3661371171474457\n",
      "Step 801 | Loss: 0.4044540524482727\n",
      "Step 901 | Loss: 0.43253007531166077\n",
      "Step 1001 | Loss: 0.27620217204093933\n",
      "Step 1101 | Loss: 0.3517114520072937\n",
      "Step 1201 | Loss: 0.3692779541015625\n",
      "Step 1301 | Loss: 0.4564223289489746\n",
      "Step 1401 | Loss: 0.2812023460865021\n",
      "Step 1501 | Loss: 0.5172398686408997\n",
      "Step 1601 | Loss: 0.3786637783050537\n",
      "Step 1701 | Loss: 0.435667484998703\n",
      "Step 1801 | Loss: 0.5049135684967041\n",
      "Step 1901 | Loss: 0.4900372326374054\n",
      "Step 2001 | Loss: 0.6793811321258545\n",
      "Step 2101 | Loss: 0.5665654540061951\n",
      "Step 2201 | Loss: 0.4810563027858734\n",
      "Step 2301 | Loss: 0.501920223236084\n",
      "Step 2401 | Loss: 0.4086180329322815\n",
      "0.41618218882692815 0.88\n",
      "Step 1 | Loss: 1.6638333797454834\n",
      "Step 101 | Loss: 0.4338887929916382\n",
      "Step 201 | Loss: 0.3335708677768707\n",
      "Step 301 | Loss: 0.3008976876735687\n",
      "Step 401 | Loss: 0.3059774935245514\n",
      "Step 501 | Loss: 0.40145888924598694\n",
      "Step 601 | Loss: 0.32462596893310547\n",
      "Step 701 | Loss: 0.5539919137954712\n",
      "Step 801 | Loss: 0.42592403292655945\n",
      "Step 901 | Loss: 0.4536672532558441\n",
      "Step 1001 | Loss: 0.4855491518974304\n",
      "Step 1101 | Loss: 0.3964708745479584\n",
      "Step 1201 | Loss: 0.40275758504867554\n",
      "Step 1301 | Loss: 0.3524325489997864\n",
      "Step 1401 | Loss: 0.4641900360584259\n",
      "Step 1501 | Loss: 0.3244994282722473\n",
      "Step 1601 | Loss: 0.3969476521015167\n",
      "Step 1701 | Loss: 0.38063764572143555\n",
      "Step 1801 | Loss: 0.3911478817462921\n",
      "Step 1901 | Loss: 0.401086688041687\n",
      "Step 2001 | Loss: 0.4483073353767395\n",
      "Step 2101 | Loss: 0.47965168952941895\n",
      "Step 2201 | Loss: 0.5354962348937988\n",
      "Step 2301 | Loss: 0.2694545388221741\n",
      "Step 2401 | Loss: 0.35578620433807373\n",
      "0.4157102234268102 0.895\n",
      "Step 1 | Loss: 0.5547811388969421\n",
      "Step 101 | Loss: 0.6811432242393494\n",
      "Step 201 | Loss: 0.4670667052268982\n",
      "Step 301 | Loss: 0.5674289464950562\n",
      "Step 401 | Loss: 0.4347449243068695\n",
      "Step 501 | Loss: 0.4776771664619446\n",
      "Step 601 | Loss: 0.45925775170326233\n",
      "Step 701 | Loss: 0.31367117166519165\n",
      "Step 801 | Loss: 0.4106251299381256\n",
      "Step 901 | Loss: 0.35927721858024597\n",
      "Step 1001 | Loss: 0.5438104867935181\n",
      "Step 1101 | Loss: 0.2756275236606598\n",
      "Step 1201 | Loss: 0.23976047337055206\n",
      "Step 1301 | Loss: 0.2462448626756668\n",
      "Step 1401 | Loss: 0.44503718614578247\n",
      "Step 1501 | Loss: 0.3577432334423065\n",
      "Step 1601 | Loss: 0.5683376789093018\n",
      "Step 1701 | Loss: 0.5196665525436401\n",
      "Step 1801 | Loss: 0.3282548487186432\n",
      "Step 1901 | Loss: 0.2757767140865326\n",
      "Step 2001 | Loss: 0.28405457735061646\n",
      "Step 2101 | Loss: 0.5150623321533203\n",
      "Step 2201 | Loss: 0.32767215371131897\n",
      "Step 2301 | Loss: 0.5211493372917175\n",
      "Step 2401 | Loss: 0.4838750958442688\n",
      "0.36127959425113837 0.8775\n",
      "Step 1 | Loss: 1.6768102645874023\n",
      "Step 101 | Loss: 0.33100080490112305\n",
      "Step 201 | Loss: 0.3716353476047516\n",
      "Step 301 | Loss: 0.3444298803806305\n",
      "Step 401 | Loss: 0.15121802687644958\n",
      "Step 501 | Loss: 0.32687056064605713\n",
      "Step 601 | Loss: 0.2673073410987854\n",
      "Step 701 | Loss: 0.397533655166626\n",
      "Step 801 | Loss: 0.4717295467853546\n",
      "Step 901 | Loss: 0.39286473393440247\n",
      "Step 1001 | Loss: 0.5444104671478271\n",
      "Step 1101 | Loss: 0.21944047510623932\n",
      "Step 1201 | Loss: 0.16295543313026428\n",
      "Step 1301 | Loss: 0.3544856309890747\n",
      "Step 1401 | Loss: 0.4261772334575653\n",
      "Step 1501 | Loss: 0.3820593059062958\n",
      "Step 1601 | Loss: 0.46563956141471863\n",
      "Step 1701 | Loss: 0.44162148237228394\n",
      "Step 1801 | Loss: 0.2644311487674713\n",
      "Step 1901 | Loss: 0.370572954416275\n",
      "Step 2001 | Loss: 0.17354387044906616\n",
      "Step 2101 | Loss: 0.2509501576423645\n",
      "Step 2201 | Loss: 0.33803677558898926\n",
      "Step 2301 | Loss: 0.3933808207511902\n",
      "Step 2401 | Loss: 0.37038013339042664\n",
      "0.3415268563536752 0.91\n",
      "Step 1 | Loss: 1.9125499725341797\n",
      "Step 101 | Loss: 0.44555938243865967\n",
      "Step 201 | Loss: 0.27792567014694214\n",
      "Step 301 | Loss: 0.5225120782852173\n",
      "Step 401 | Loss: 0.5412312746047974\n",
      "Step 501 | Loss: 0.3415415585041046\n",
      "Step 601 | Loss: 0.16192789375782013\n",
      "Step 701 | Loss: 0.342492938041687\n",
      "Step 801 | Loss: 0.4883987307548523\n",
      "Step 901 | Loss: 0.39354920387268066\n",
      "Step 1001 | Loss: 0.3748060464859009\n",
      "Step 1101 | Loss: 0.17585551738739014\n",
      "Step 1201 | Loss: 0.31336429715156555\n",
      "Step 1301 | Loss: 0.4425297975540161\n",
      "Step 1401 | Loss: 0.22989335656166077\n",
      "Step 1501 | Loss: 0.24857878684997559\n",
      "Step 1601 | Loss: 0.30982470512390137\n",
      "Step 1701 | Loss: 0.26001474261283875\n",
      "Step 1801 | Loss: 0.3762253224849701\n",
      "Step 1901 | Loss: 0.265278160572052\n",
      "Step 2001 | Loss: 0.2406797856092453\n",
      "Step 2101 | Loss: 0.18138772249221802\n",
      "Step 2201 | Loss: 0.3217662572860718\n",
      "Step 2301 | Loss: 0.4632350206375122\n",
      "Step 2401 | Loss: 0.44489386677742004\n",
      "0.361072905743115 0.8725\n",
      "Step 1 | Loss: 2.7330052852630615\n",
      "Step 101 | Loss: 0.3340534269809723\n",
      "Step 201 | Loss: 0.3809778690338135\n",
      "Step 301 | Loss: 0.2798166275024414\n",
      "Step 401 | Loss: 0.45677468180656433\n",
      "Step 501 | Loss: 0.198582261800766\n",
      "Step 601 | Loss: 0.367708295583725\n",
      "Step 701 | Loss: 0.38543063402175903\n",
      "Step 801 | Loss: 0.34443357586860657\n",
      "Step 901 | Loss: 0.3678276538848877\n",
      "Step 1001 | Loss: 0.320205420255661\n",
      "Step 1101 | Loss: 0.520050585269928\n",
      "Step 1201 | Loss: 0.5590416193008423\n",
      "Step 1301 | Loss: 0.2579496502876282\n",
      "Step 1401 | Loss: 0.5862981677055359\n",
      "Step 1501 | Loss: 0.4227622449398041\n",
      "Step 1601 | Loss: 0.249685138463974\n",
      "Step 1701 | Loss: 0.3897174894809723\n",
      "Step 1801 | Loss: 0.1489017903804779\n",
      "Step 1901 | Loss: 0.5338611006736755\n",
      "Step 2001 | Loss: 0.2656511664390564\n",
      "Step 2101 | Loss: 0.4534021019935608\n",
      "Step 2201 | Loss: 0.20897620916366577\n",
      "Step 2301 | Loss: 0.25280046463012695\n",
      "Step 2401 | Loss: 0.4460858404636383\n",
      "0.3471357679344126 0.9025\n",
      "Step 1 | Loss: 2.111156940460205\n",
      "Step 101 | Loss: 0.5345689654350281\n",
      "Step 201 | Loss: 0.36728349328041077\n",
      "Step 301 | Loss: 0.6457651853561401\n",
      "Step 401 | Loss: 0.3893722891807556\n",
      "Step 501 | Loss: 0.38637498021125793\n",
      "Step 601 | Loss: 0.4471571445465088\n",
      "Step 701 | Loss: 0.21849407255649567\n",
      "Step 801 | Loss: 0.5216710567474365\n",
      "Step 901 | Loss: 0.2586507797241211\n",
      "Step 1001 | Loss: 0.12522569298744202\n",
      "Step 1101 | Loss: 0.3228328824043274\n",
      "Step 1201 | Loss: 0.3142375349998474\n",
      "Step 1301 | Loss: 0.23385311663150787\n",
      "Step 1401 | Loss: 0.4012351334095001\n",
      "Step 1501 | Loss: 0.3132512867450714\n",
      "Step 1601 | Loss: 0.5104367136955261\n",
      "Step 1701 | Loss: 0.3354559540748596\n",
      "Step 1801 | Loss: 0.5420397520065308\n",
      "Step 1901 | Loss: 0.26704663038253784\n",
      "Step 2001 | Loss: 0.2737153172492981\n",
      "Step 2101 | Loss: 0.2980051636695862\n",
      "Step 2201 | Loss: 0.23297372460365295\n",
      "Step 2301 | Loss: 0.3580993115901947\n",
      "Step 2401 | Loss: 0.4670643210411072\n",
      "0.3423396121159437 0.9125\n",
      "Step 1 | Loss: 1.5056030750274658\n",
      "Step 101 | Loss: 0.5474940538406372\n",
      "Step 201 | Loss: 0.6573110222816467\n",
      "Step 301 | Loss: 0.6300122737884521\n",
      "Step 401 | Loss: 0.5350515842437744\n",
      "Step 501 | Loss: 0.33186566829681396\n",
      "Step 601 | Loss: 0.44995003938674927\n",
      "Step 701 | Loss: 0.37801700830459595\n",
      "Step 801 | Loss: 0.360867977142334\n",
      "Step 901 | Loss: 0.48995283246040344\n",
      "Step 1001 | Loss: 0.49517083168029785\n",
      "Step 1101 | Loss: 0.3998437821865082\n",
      "Step 1201 | Loss: 0.4973278045654297\n",
      "Step 1301 | Loss: 0.27688437700271606\n",
      "Step 1401 | Loss: 0.3325037956237793\n",
      "Step 1501 | Loss: 0.43214574456214905\n",
      "Step 1601 | Loss: 0.41935843229293823\n",
      "Step 1701 | Loss: 0.42789849638938904\n",
      "Step 1801 | Loss: 0.6351110935211182\n",
      "Step 1901 | Loss: 0.3883659839630127\n",
      "Step 2001 | Loss: 0.374269962310791\n",
      "Step 2101 | Loss: 0.40068507194519043\n",
      "Step 2201 | Loss: 0.40141457319259644\n",
      "Step 2301 | Loss: 0.33719801902770996\n",
      "Step 2401 | Loss: 0.4771597683429718\n",
      "0.4432179401505551 0.87\n",
      "Step 1 | Loss: 0.9342922568321228\n",
      "Step 101 | Loss: 0.818422257900238\n",
      "Step 201 | Loss: 0.39344996213912964\n",
      "Step 301 | Loss: 0.39409032464027405\n",
      "Step 401 | Loss: 0.5710054636001587\n",
      "Step 501 | Loss: 0.5296249389648438\n",
      "Step 601 | Loss: 0.5988304018974304\n",
      "Step 701 | Loss: 0.5728345513343811\n",
      "Step 801 | Loss: 0.4351998567581177\n",
      "Step 901 | Loss: 0.35963156819343567\n",
      "Step 1001 | Loss: 0.47141727805137634\n",
      "Step 1101 | Loss: 0.6671140193939209\n",
      "Step 1201 | Loss: 0.3470151424407959\n",
      "Step 1301 | Loss: 0.34594279527664185\n",
      "Step 1401 | Loss: 0.3886936604976654\n",
      "Step 1501 | Loss: 0.5006105899810791\n",
      "Step 1601 | Loss: 0.2554805278778076\n",
      "Step 1701 | Loss: 0.4106023907661438\n",
      "Step 1801 | Loss: 0.3536924421787262\n",
      "Step 1901 | Loss: 0.4547826051712036\n",
      "Step 2001 | Loss: 0.40099596977233887\n",
      "Step 2101 | Loss: 0.2721869647502899\n",
      "Step 2201 | Loss: 0.34633249044418335\n",
      "Step 2301 | Loss: 0.28675377368927\n",
      "Step 2401 | Loss: 0.3345107138156891\n",
      "0.3700287636997143 0.9125\n",
      "Step 1 | Loss: 0.979529619216919\n",
      "Step 101 | Loss: 0.6552706956863403\n",
      "Step 201 | Loss: 0.7660062909126282\n",
      "Step 301 | Loss: 0.6428675651550293\n",
      "Step 401 | Loss: 0.6724108457565308\n",
      "Step 501 | Loss: 0.6837329864501953\n",
      "Step 601 | Loss: 0.6130710244178772\n",
      "Step 701 | Loss: 0.5903947353363037\n",
      "Step 801 | Loss: 0.5707190036773682\n",
      "Step 901 | Loss: 0.5367758274078369\n",
      "Step 1001 | Loss: 0.49141910672187805\n",
      "Step 1101 | Loss: 0.4890363812446594\n",
      "Step 1201 | Loss: 0.4223271310329437\n",
      "Step 1301 | Loss: 0.3413510322570801\n",
      "Step 1401 | Loss: 0.7100266218185425\n",
      "Step 1501 | Loss: 0.606848955154419\n",
      "Step 1601 | Loss: 0.4870055913925171\n",
      "Step 1701 | Loss: 0.4254046380519867\n",
      "Step 1801 | Loss: 0.5904896855354309\n",
      "Step 1901 | Loss: 0.5951350331306458\n",
      "Step 2001 | Loss: 0.3212745785713196\n",
      "Step 2101 | Loss: 0.3737002909183502\n",
      "Step 2201 | Loss: 0.6308119297027588\n",
      "Step 2301 | Loss: 0.5419216156005859\n",
      "Step 2401 | Loss: 0.4577004015445709\n",
      "0.4918320442490456 0.8425\n",
      "Step 1 | Loss: 1.0247983932495117\n",
      "Step 101 | Loss: 0.7891231775283813\n",
      "Step 201 | Loss: 0.644329309463501\n",
      "Step 301 | Loss: 0.4632698893547058\n",
      "Step 401 | Loss: 0.46507546305656433\n",
      "Step 501 | Loss: 0.4487185776233673\n",
      "Step 601 | Loss: 0.5784430503845215\n",
      "Step 701 | Loss: 0.4605347514152527\n",
      "Step 801 | Loss: 0.4777063727378845\n",
      "Step 901 | Loss: 0.3892425298690796\n",
      "Step 1001 | Loss: 0.5205895900726318\n",
      "Step 1101 | Loss: 0.4933604300022125\n",
      "Step 1201 | Loss: 0.4166508615016937\n",
      "Step 1301 | Loss: 0.44738054275512695\n",
      "Step 1401 | Loss: 0.3521307706832886\n",
      "Step 1501 | Loss: 0.5228912234306335\n",
      "Step 1601 | Loss: 0.32489272952079773\n",
      "Step 1701 | Loss: 0.5394702553749084\n",
      "Step 1801 | Loss: 0.5070938467979431\n",
      "Step 1901 | Loss: 0.43197956681251526\n",
      "Step 2001 | Loss: 0.39905494451522827\n",
      "Step 2101 | Loss: 0.4159678518772125\n",
      "Step 2201 | Loss: 0.4527086615562439\n",
      "Step 2301 | Loss: 0.5179011821746826\n",
      "Step 2401 | Loss: 0.3477292060852051\n",
      "0.4626020534364895 0.89\n",
      "Step 1 | Loss: 1.9394967555999756\n",
      "Step 101 | Loss: 0.9072960019111633\n",
      "Step 201 | Loss: 0.8507472276687622\n",
      "Step 301 | Loss: 0.6358581185340881\n",
      "Step 401 | Loss: 0.7588357925415039\n",
      "Step 501 | Loss: 0.5452930331230164\n",
      "Step 601 | Loss: 0.4359973967075348\n",
      "Step 701 | Loss: 0.4668125808238983\n",
      "Step 801 | Loss: 0.6811081171035767\n",
      "Step 901 | Loss: 0.5799276828765869\n",
      "Step 1001 | Loss: 0.3586718738079071\n",
      "Step 1101 | Loss: 0.7196487188339233\n",
      "Step 1201 | Loss: 0.5818377733230591\n",
      "Step 1301 | Loss: 0.5617204308509827\n",
      "Step 1401 | Loss: 0.6200401782989502\n",
      "Step 1501 | Loss: 0.4200570583343506\n",
      "Step 1601 | Loss: 0.37207406759262085\n",
      "Step 1701 | Loss: 0.46373283863067627\n",
      "Step 1801 | Loss: 0.3075081408023834\n",
      "Step 1901 | Loss: 0.20428642630577087\n",
      "Step 2001 | Loss: 0.34272438287734985\n",
      "Step 2101 | Loss: 0.4133296310901642\n",
      "Step 2201 | Loss: 0.3560646176338196\n",
      "Step 2301 | Loss: 0.5796358585357666\n",
      "Step 2401 | Loss: 0.4685869812965393\n",
      "0.4634040238510505 0.865\n",
      "Step 1 | Loss: 0.9114386439323425\n",
      "Step 101 | Loss: 1.0557955503463745\n",
      "Step 201 | Loss: 1.0543906688690186\n",
      "Step 301 | Loss: 0.8388044238090515\n",
      "Step 401 | Loss: 0.770609974861145\n",
      "Step 501 | Loss: 0.7746118903160095\n",
      "Step 601 | Loss: 0.8075598478317261\n",
      "Step 701 | Loss: 0.8245565295219421\n",
      "Step 801 | Loss: 0.7688925862312317\n",
      "Step 901 | Loss: 0.7797642350196838\n",
      "Step 1001 | Loss: 0.8557162284851074\n",
      "Step 1101 | Loss: 0.8292543888092041\n",
      "Step 1201 | Loss: 0.7757406234741211\n",
      "Step 1301 | Loss: 0.855673611164093\n",
      "Step 1401 | Loss: 0.8149981498718262\n",
      "Step 1501 | Loss: 0.6909047365188599\n",
      "Step 1601 | Loss: 0.8108679056167603\n",
      "Step 1701 | Loss: 0.7449576258659363\n",
      "Step 1801 | Loss: 0.7474757432937622\n",
      "Step 1901 | Loss: 0.7420026659965515\n",
      "Step 2001 | Loss: 0.7038904428482056\n",
      "Step 2101 | Loss: 0.7601378560066223\n",
      "Step 2201 | Loss: 0.7003727555274963\n",
      "Step 2301 | Loss: 0.8459107875823975\n",
      "Step 2401 | Loss: 0.7381213903427124\n",
      "0.7761411939294701 0.6675\n",
      "Step 1 | Loss: 1.4163844585418701\n",
      "Step 101 | Loss: 0.9132195711135864\n",
      "Step 201 | Loss: 0.9778561592102051\n",
      "Step 301 | Loss: 0.7827104330062866\n",
      "Step 401 | Loss: 0.8170871734619141\n",
      "Step 501 | Loss: 0.7162460088729858\n",
      "Step 601 | Loss: 0.8094456195831299\n",
      "Step 701 | Loss: 0.826817512512207\n",
      "Step 801 | Loss: 0.9544945955276489\n",
      "Step 901 | Loss: 0.8620719909667969\n",
      "Step 1001 | Loss: 0.8024266958236694\n",
      "Step 1101 | Loss: 0.8705639243125916\n",
      "Step 1201 | Loss: 0.8105432987213135\n",
      "Step 1301 | Loss: 0.7875480651855469\n",
      "Step 1401 | Loss: 0.9103350639343262\n",
      "Step 1501 | Loss: 0.7402933239936829\n",
      "Step 1601 | Loss: 0.8502898812294006\n",
      "Step 1701 | Loss: 0.6931763291358948\n",
      "Step 1801 | Loss: 0.8135997653007507\n",
      "Step 1901 | Loss: 0.8399701118469238\n",
      "Step 2001 | Loss: 0.8777750134468079\n",
      "Step 2101 | Loss: 0.8567870855331421\n",
      "Step 2201 | Loss: 0.7511274218559265\n",
      "Step 2301 | Loss: 0.7998481392860413\n",
      "Step 2401 | Loss: 0.7864612340927124\n",
      "0.843485883772401 0.6225\n",
      "Step 1 | Loss: 1.2883161306381226\n",
      "Step 101 | Loss: 1.1213808059692383\n",
      "Step 201 | Loss: 1.0311251878738403\n",
      "Step 301 | Loss: 0.8632621765136719\n",
      "Step 401 | Loss: 0.7591339349746704\n",
      "Step 501 | Loss: 0.6411022543907166\n",
      "Step 601 | Loss: 0.8417442440986633\n",
      "Step 701 | Loss: 0.7654886841773987\n",
      "Step 801 | Loss: 0.6782940626144409\n",
      "Step 901 | Loss: 0.7282217741012573\n",
      "Step 1001 | Loss: 0.7225757241249084\n",
      "Step 1101 | Loss: 0.6884664297103882\n",
      "Step 1201 | Loss: 0.8044450283050537\n",
      "Step 1301 | Loss: 0.7696810960769653\n",
      "Step 1401 | Loss: 0.8805210590362549\n",
      "Step 1501 | Loss: 0.6932908892631531\n",
      "Step 1601 | Loss: 0.7305980324745178\n",
      "Step 1701 | Loss: 0.7366898655891418\n",
      "Step 1801 | Loss: 0.6620616316795349\n",
      "Step 1901 | Loss: 0.6631325483322144\n",
      "Step 2001 | Loss: 0.7527044415473938\n",
      "Step 2101 | Loss: 0.7976849675178528\n",
      "Step 2201 | Loss: 0.7225149273872375\n",
      "Step 2301 | Loss: 0.7636430263519287\n",
      "Step 2401 | Loss: 0.6864916682243347\n",
      "0.7520336354370768 0.77\n",
      "Step 1 | Loss: 1.0542786121368408\n",
      "Step 101 | Loss: 0.9310004711151123\n",
      "Step 201 | Loss: 0.9790154695510864\n",
      "Step 301 | Loss: 0.9806381464004517\n",
      "Step 401 | Loss: 0.7917689085006714\n",
      "Step 501 | Loss: 0.8973121047019958\n",
      "Step 601 | Loss: 0.7340214252471924\n",
      "Step 701 | Loss: 0.7764283418655396\n",
      "Step 801 | Loss: 0.8016833662986755\n",
      "Step 901 | Loss: 0.8665738105773926\n",
      "Step 1001 | Loss: 0.7259505391120911\n",
      "Step 1101 | Loss: 0.738899827003479\n",
      "Step 1201 | Loss: 0.7703981399536133\n",
      "Step 1301 | Loss: 0.8339782953262329\n",
      "Step 1401 | Loss: 0.9013239741325378\n",
      "Step 1501 | Loss: 0.8452416658401489\n",
      "Step 1601 | Loss: 0.7888745069503784\n",
      "Step 1701 | Loss: 0.7999022006988525\n",
      "Step 1801 | Loss: 0.9047215580940247\n",
      "Step 1901 | Loss: 0.7821225523948669\n",
      "Step 2001 | Loss: 0.8693287968635559\n",
      "Step 2101 | Loss: 0.873565137386322\n",
      "Step 2201 | Loss: 0.859136164188385\n",
      "Step 2301 | Loss: 0.8150308132171631\n",
      "Step 2401 | Loss: 0.8007324934005737\n",
      "0.8454630202545597 0.61\n",
      "Step 1 | Loss: 1.0724347829818726\n",
      "Step 101 | Loss: 0.8481231331825256\n",
      "Step 201 | Loss: 0.8027587532997131\n",
      "Step 301 | Loss: 0.7332449555397034\n",
      "Step 401 | Loss: 0.7192666530609131\n",
      "Step 501 | Loss: 0.7624598741531372\n",
      "Step 601 | Loss: 0.8117672801017761\n",
      "Step 701 | Loss: 0.7989713549613953\n",
      "Step 801 | Loss: 0.5478082895278931\n",
      "Step 901 | Loss: 0.841112494468689\n",
      "Step 1001 | Loss: 0.7365924119949341\n",
      "Step 1101 | Loss: 0.7642135620117188\n",
      "Step 1201 | Loss: 0.8307569622993469\n",
      "Step 1301 | Loss: 0.8110774755477905\n",
      "Step 1401 | Loss: 0.5969252586364746\n",
      "Step 1501 | Loss: 0.7799215912818909\n",
      "Step 1601 | Loss: 0.6679517030715942\n",
      "Step 1701 | Loss: 0.7394319772720337\n",
      "Step 1801 | Loss: 0.725855827331543\n",
      "Step 1901 | Loss: 0.8378453850746155\n",
      "Step 2001 | Loss: 0.7958479523658752\n",
      "Step 2101 | Loss: 0.7547820806503296\n",
      "Step 2201 | Loss: 0.6357669234275818\n",
      "Step 2301 | Loss: 0.8062601685523987\n",
      "Step 2401 | Loss: 0.7967087626457214\n",
      "0.7537840614123932 0.76\n",
      "Step 1 | Loss: 1.555291771888733\n",
      "Step 101 | Loss: 1.0028553009033203\n",
      "Step 201 | Loss: 0.9819523096084595\n",
      "Step 301 | Loss: 0.9978106021881104\n",
      "Step 401 | Loss: 1.024929165840149\n",
      "Step 501 | Loss: 0.9894809126853943\n",
      "Step 601 | Loss: 1.0006272792816162\n",
      "Step 701 | Loss: 0.9964710474014282\n",
      "Step 801 | Loss: 1.0099945068359375\n",
      "Step 901 | Loss: 1.0031702518463135\n",
      "Step 1001 | Loss: 0.9971993565559387\n",
      "Step 1101 | Loss: 1.0185208320617676\n",
      "Step 1201 | Loss: 1.0094943046569824\n",
      "Step 1301 | Loss: 1.012044906616211\n",
      "Step 1401 | Loss: 1.004345417022705\n",
      "Step 1501 | Loss: 0.9805566668510437\n",
      "Step 1601 | Loss: 0.9984037280082703\n",
      "Step 1701 | Loss: 0.9992189407348633\n",
      "Step 1801 | Loss: 1.0000810623168945\n",
      "Step 1901 | Loss: 1.0013718605041504\n",
      "Step 2001 | Loss: 1.0126807689666748\n",
      "Step 2101 | Loss: 1.0019515752792358\n",
      "Step 2201 | Loss: 0.9952750205993652\n",
      "Step 2301 | Loss: 1.0053281784057617\n",
      "Step 2401 | Loss: 1.002497911453247\n",
      "1.0046166634463278 0.5\n",
      "Step 1 | Loss: 1.5890734195709229\n",
      "Step 101 | Loss: 1.0131072998046875\n",
      "Step 201 | Loss: 0.9993621706962585\n",
      "Step 301 | Loss: 1.003398060798645\n",
      "Step 401 | Loss: 0.9909971952438354\n",
      "Step 501 | Loss: 1.0022871494293213\n",
      "Step 601 | Loss: 1.0012104511260986\n",
      "Step 701 | Loss: 1.000044345855713\n",
      "Step 801 | Loss: 0.9843188524246216\n",
      "Step 901 | Loss: 0.999866247177124\n",
      "Step 1001 | Loss: 1.0347211360931396\n",
      "Step 1101 | Loss: 1.01700758934021\n",
      "Step 1201 | Loss: 0.9939219951629639\n",
      "Step 1301 | Loss: 1.0473272800445557\n",
      "Step 1401 | Loss: 1.0044209957122803\n",
      "Step 1501 | Loss: 1.0000710487365723\n",
      "Step 1601 | Loss: 1.0015188455581665\n",
      "Step 1701 | Loss: 1.0186060667037964\n",
      "Step 1801 | Loss: 1.0089625120162964\n",
      "Step 1901 | Loss: 0.9961724281311035\n",
      "Step 2001 | Loss: 0.9991881251335144\n",
      "Step 2101 | Loss: 0.9972838163375854\n",
      "Step 2201 | Loss: 0.9979801774024963\n",
      "Step 2301 | Loss: 1.0005015134811401\n",
      "Step 2401 | Loss: 1.0038859844207764\n",
      "1.0006581674127935 0.5\n",
      "Step 1 | Loss: 1.5933573246002197\n",
      "Step 101 | Loss: 1.01310133934021\n",
      "Step 201 | Loss: 1.0005732774734497\n",
      "Step 301 | Loss: 1.0022478103637695\n",
      "Step 401 | Loss: 0.9875401258468628\n",
      "Step 501 | Loss: 1.0052878856658936\n",
      "Step 601 | Loss: 0.985149621963501\n",
      "Step 701 | Loss: 0.9998719692230225\n",
      "Step 801 | Loss: 1.0044615268707275\n",
      "Step 901 | Loss: 1.004335880279541\n",
      "Step 1001 | Loss: 1.012900471687317\n",
      "Step 1101 | Loss: 1.0185643434524536\n",
      "Step 1201 | Loss: 0.9839625358581543\n",
      "Step 1301 | Loss: 0.9995422959327698\n",
      "Step 1401 | Loss: 1.0047836303710938\n",
      "Step 1501 | Loss: 0.9925506114959717\n",
      "Step 1601 | Loss: 1.0232881307601929\n",
      "Step 1701 | Loss: 1.0281511545181274\n",
      "Step 1801 | Loss: 1.0227051973342896\n",
      "Step 1901 | Loss: 0.9799287915229797\n",
      "Step 2001 | Loss: 1.0050511360168457\n",
      "Step 2101 | Loss: 0.9966410994529724\n",
      "Step 2201 | Loss: 1.0020049810409546\n",
      "Step 2301 | Loss: 1.0025967359542847\n",
      "Step 2401 | Loss: 0.9933916330337524\n",
      "1.0001521609776751 0.5\n",
      "Step 1 | Loss: 1.8749537467956543\n",
      "Step 101 | Loss: 0.9901909828186035\n",
      "Step 201 | Loss: 1.0162490606307983\n",
      "Step 301 | Loss: 0.9813318252563477\n",
      "Step 401 | Loss: 0.9945030212402344\n",
      "Step 501 | Loss: 1.01313316822052\n",
      "Step 601 | Loss: 0.9919998049736023\n",
      "Step 701 | Loss: 1.0062930583953857\n",
      "Step 801 | Loss: 1.0350884199142456\n",
      "Step 901 | Loss: 1.0014556646347046\n",
      "Step 1001 | Loss: 0.9975121021270752\n",
      "Step 1101 | Loss: 1.0161011219024658\n",
      "Step 1201 | Loss: 1.000280737876892\n",
      "Step 1301 | Loss: 1.0000115633010864\n",
      "Step 1401 | Loss: 1.0013582706451416\n",
      "Step 1501 | Loss: 1.002607822418213\n",
      "Step 1601 | Loss: 0.9935548901557922\n",
      "Step 1701 | Loss: 1.016014575958252\n",
      "Step 1801 | Loss: 1.0013868808746338\n",
      "Step 1901 | Loss: 0.9961986541748047\n",
      "Step 2001 | Loss: 0.990845799446106\n",
      "Step 2101 | Loss: 1.0031185150146484\n",
      "Step 2201 | Loss: 1.045074462890625\n",
      "Step 2301 | Loss: 1.0011650323867798\n",
      "Step 2401 | Loss: 1.0014286041259766\n",
      "1.000173356236767 0.5\n",
      "Step 1 | Loss: 1.1270511150360107\n",
      "Step 101 | Loss: 0.9993163347244263\n",
      "Step 201 | Loss: 0.9989822506904602\n",
      "Step 301 | Loss: 1.0015852451324463\n",
      "Step 401 | Loss: 1.0004082918167114\n",
      "Step 501 | Loss: 1.0262393951416016\n",
      "Step 601 | Loss: 1.0006141662597656\n",
      "Step 701 | Loss: 0.9977891445159912\n",
      "Step 801 | Loss: 0.9586578607559204\n",
      "Step 901 | Loss: 1.014487624168396\n",
      "Step 1001 | Loss: 0.9963498115539551\n",
      "Step 1101 | Loss: 0.99162757396698\n",
      "Step 1201 | Loss: 1.002652645111084\n",
      "Step 1301 | Loss: 1.00001060962677\n",
      "Step 1401 | Loss: 1.0520217418670654\n",
      "Step 1501 | Loss: 1.0019530057907104\n",
      "Step 1601 | Loss: 1.011314868927002\n",
      "Step 1701 | Loss: 0.9608050584793091\n",
      "Step 1801 | Loss: 1.0080254077911377\n",
      "Step 1901 | Loss: 0.9919231534004211\n",
      "Step 2001 | Loss: 0.9713622331619263\n",
      "Step 2101 | Loss: 0.9987116456031799\n",
      "Step 2201 | Loss: 0.9993752241134644\n",
      "Step 2301 | Loss: 1.0074454545974731\n",
      "Step 2401 | Loss: 1.007022738456726\n",
      "1.0026799921777039 0.5\n",
      "Step 1 | Loss: 0.9421324729919434\n",
      "Step 101 | Loss: 0.774189293384552\n",
      "Step 201 | Loss: 0.6886727213859558\n",
      "Step 301 | Loss: 0.6075935959815979\n",
      "Step 401 | Loss: 0.6838751435279846\n",
      "Step 501 | Loss: 0.6643977165222168\n",
      "Step 601 | Loss: 0.5533449053764343\n",
      "Step 701 | Loss: 0.6390775442123413\n",
      "Step 801 | Loss: 0.4948028326034546\n",
      "Step 901 | Loss: 0.4655300974845886\n",
      "Step 1001 | Loss: 0.6496233940124512\n",
      "Step 1101 | Loss: 0.5523377656936646\n",
      "Step 1201 | Loss: 0.47330525517463684\n",
      "Step 1301 | Loss: 0.4348450303077698\n",
      "Step 1401 | Loss: 0.4804806709289551\n",
      "Step 1501 | Loss: 0.7541643977165222\n",
      "Step 1601 | Loss: 0.537759006023407\n",
      "Step 1701 | Loss: 0.5664461255073547\n",
      "Step 1801 | Loss: 0.5470722317695618\n",
      "Step 1901 | Loss: 0.5408836603164673\n",
      "Step 2001 | Loss: 0.5523188710212708\n",
      "Step 2101 | Loss: 0.48350781202316284\n",
      "Step 2201 | Loss: 0.47955700755119324\n",
      "Step 2301 | Loss: 0.5532799959182739\n",
      "Step 2401 | Loss: 0.5675393342971802\n",
      "0.5575984956933839 0.8825\n",
      "Step 1 | Loss: 1.0706384181976318\n",
      "Step 101 | Loss: 0.7087062001228333\n",
      "Step 201 | Loss: 0.6065860986709595\n",
      "Step 301 | Loss: 0.5701649785041809\n",
      "Step 401 | Loss: 0.654846727848053\n",
      "Step 501 | Loss: 0.4462827444076538\n",
      "Step 601 | Loss: 0.6449230313301086\n",
      "Step 701 | Loss: 0.6695178151130676\n",
      "Step 801 | Loss: 0.5246536731719971\n",
      "Step 901 | Loss: 0.611320972442627\n",
      "Step 1001 | Loss: 0.540948212146759\n",
      "Step 1101 | Loss: 0.5301425457000732\n",
      "Step 1201 | Loss: 0.6819918751716614\n",
      "Step 1301 | Loss: 0.5853704810142517\n",
      "Step 1401 | Loss: 0.5524580478668213\n",
      "Step 1501 | Loss: 0.5649534463882446\n",
      "Step 1601 | Loss: 0.507804811000824\n",
      "Step 1701 | Loss: 0.6088699102401733\n",
      "Step 1801 | Loss: 0.3572503328323364\n",
      "Step 1901 | Loss: 0.3906727433204651\n",
      "Step 2001 | Loss: 0.5676875114440918\n",
      "Step 2101 | Loss: 0.4954543113708496\n",
      "Step 2201 | Loss: 0.5771098136901855\n",
      "Step 2301 | Loss: 0.5110406279563904\n",
      "Step 2401 | Loss: 0.4577814042568207\n",
      "0.541170540947552 0.8775\n",
      "Step 1 | Loss: 1.2232190370559692\n",
      "Step 101 | Loss: 0.9535089731216431\n",
      "Step 201 | Loss: 0.9586682319641113\n",
      "Step 301 | Loss: 0.9116830229759216\n",
      "Step 401 | Loss: 0.6821736693382263\n",
      "Step 501 | Loss: 0.6953327059745789\n",
      "Step 601 | Loss: 0.6625287532806396\n",
      "Step 701 | Loss: 0.648622989654541\n",
      "Step 801 | Loss: 0.4725845158100128\n",
      "Step 901 | Loss: 0.573851466178894\n",
      "Step 1001 | Loss: 0.5766990184783936\n",
      "Step 1101 | Loss: 0.5726791620254517\n",
      "Step 1201 | Loss: 0.6643534898757935\n",
      "Step 1301 | Loss: 0.6726787090301514\n",
      "Step 1401 | Loss: 0.6555023193359375\n",
      "Step 1501 | Loss: 0.545024037361145\n",
      "Step 1601 | Loss: 0.6246666312217712\n",
      "Step 1701 | Loss: 0.5879004001617432\n",
      "Step 1801 | Loss: 0.5049724578857422\n",
      "Step 1901 | Loss: 0.5504190325737\n",
      "Step 2001 | Loss: 0.4273345470428467\n",
      "Step 2101 | Loss: 0.5471921563148499\n",
      "Step 2201 | Loss: 0.6765623092651367\n",
      "Step 2301 | Loss: 0.5025471448898315\n",
      "Step 2401 | Loss: 0.629917562007904\n",
      "0.5701136914360603 0.8275\n",
      "Step 1 | Loss: 0.8773378133773804\n",
      "Step 101 | Loss: 0.6015517115592957\n",
      "Step 201 | Loss: 0.6034041047096252\n",
      "Step 301 | Loss: 0.47700077295303345\n",
      "Step 401 | Loss: 0.5776040554046631\n",
      "Step 501 | Loss: 0.6053351759910583\n",
      "Step 601 | Loss: 0.5657922625541687\n",
      "Step 701 | Loss: 0.4725913405418396\n",
      "Step 801 | Loss: 0.517491340637207\n",
      "Step 901 | Loss: 0.458660364151001\n",
      "Step 1001 | Loss: 0.41108953952789307\n",
      "Step 1101 | Loss: 0.5007433295249939\n",
      "Step 1201 | Loss: 0.4379393458366394\n",
      "Step 1301 | Loss: 0.4940758943557739\n",
      "Step 1401 | Loss: 0.49852484464645386\n",
      "Step 1501 | Loss: 0.6320531964302063\n",
      "Step 1601 | Loss: 0.4121626615524292\n",
      "Step 1701 | Loss: 0.6542495489120483\n",
      "Step 1801 | Loss: 0.3836228549480438\n",
      "Step 1901 | Loss: 0.43854883313179016\n",
      "Step 2001 | Loss: 0.5233825445175171\n",
      "Step 2101 | Loss: 0.4396432340145111\n",
      "Step 2201 | Loss: 0.47073015570640564\n",
      "Step 2301 | Loss: 0.5590919256210327\n",
      "Step 2401 | Loss: 0.4351974129676819\n",
      "0.4734585690457803 0.9175\n",
      "Step 1 | Loss: 1.0708866119384766\n",
      "Step 101 | Loss: 0.76571124792099\n",
      "Step 201 | Loss: 0.5975282192230225\n",
      "Step 301 | Loss: 0.6357209086418152\n",
      "Step 401 | Loss: 0.45237669348716736\n",
      "Step 501 | Loss: 0.6291148066520691\n",
      "Step 601 | Loss: 0.4412299394607544\n",
      "Step 701 | Loss: 0.5020379424095154\n",
      "Step 801 | Loss: 0.5424515604972839\n",
      "Step 901 | Loss: 0.5607202053070068\n",
      "Step 1001 | Loss: 0.5189785361289978\n",
      "Step 1101 | Loss: 0.5348133444786072\n",
      "Step 1201 | Loss: 0.4559231698513031\n",
      "Step 1301 | Loss: 0.4845612645149231\n",
      "Step 1401 | Loss: 0.4549708962440491\n",
      "Step 1501 | Loss: 0.44113942980766296\n",
      "Step 1601 | Loss: 0.5295052528381348\n",
      "Step 1701 | Loss: 0.49647536873817444\n",
      "Step 1801 | Loss: 0.4397363066673279\n",
      "Step 1901 | Loss: 0.5247840881347656\n",
      "Step 2001 | Loss: 0.4905061721801758\n",
      "Step 2101 | Loss: 0.39868009090423584\n",
      "Step 2201 | Loss: 0.4660084843635559\n",
      "Step 2301 | Loss: 0.5951719284057617\n",
      "Step 2401 | Loss: 0.3847521245479584\n",
      "0.49889242111589777 0.88\n",
      "Step 1 | Loss: 0.422194242477417\n",
      "Step 101 | Loss: 0.4496774971485138\n",
      "Step 201 | Loss: 0.2812372148036957\n",
      "Step 301 | Loss: 0.3061544895172119\n",
      "Step 401 | Loss: 0.23363476991653442\n",
      "Step 501 | Loss: 0.23496823012828827\n",
      "Step 601 | Loss: 0.2694031298160553\n",
      "Step 701 | Loss: 0.26677629351615906\n",
      "Step 801 | Loss: 0.2307695746421814\n",
      "Step 901 | Loss: 0.34717172384262085\n",
      "Step 1001 | Loss: 0.2779281735420227\n",
      "Step 1101 | Loss: 0.1836232841014862\n",
      "Step 1201 | Loss: 0.18968847393989563\n",
      "Step 1301 | Loss: 0.30495768785476685\n",
      "Step 1401 | Loss: 0.25254401564598083\n",
      "Step 1501 | Loss: 0.16460128128528595\n",
      "Step 1601 | Loss: 0.24168439209461212\n",
      "Step 1701 | Loss: 0.17990365624427795\n",
      "Step 1801 | Loss: 0.31829720735549927\n",
      "Step 1901 | Loss: 0.2771390676498413\n",
      "Step 2001 | Loss: 0.21195125579833984\n",
      "Step 2101 | Loss: 0.2428106814622879\n",
      "Step 2201 | Loss: 0.2772923409938812\n",
      "Step 2301 | Loss: 0.28124386072158813\n",
      "Step 2401 | Loss: 0.26598140597343445\n",
      "0.24221161927505716 0.9425\n",
      "Step 1 | Loss: 1.3673474788665771\n",
      "Step 101 | Loss: 0.29351112246513367\n",
      "Step 201 | Loss: 0.2917230725288391\n",
      "Step 301 | Loss: 0.3351821005344391\n",
      "Step 401 | Loss: 0.2558896243572235\n",
      "Step 501 | Loss: 0.28528326749801636\n",
      "Step 601 | Loss: 0.3686794340610504\n",
      "Step 701 | Loss: 0.3475266098976135\n",
      "Step 801 | Loss: 0.319060355424881\n",
      "Step 901 | Loss: 0.2477344423532486\n",
      "Step 1001 | Loss: 0.39127305150032043\n",
      "Step 1101 | Loss: 0.3830946683883667\n",
      "Step 1201 | Loss: 0.2683190107345581\n",
      "Step 1301 | Loss: 0.2697189152240753\n",
      "Step 1401 | Loss: 0.3032166659832001\n",
      "Step 1501 | Loss: 0.2761059105396271\n",
      "Step 1601 | Loss: 0.27697092294692993\n",
      "Step 1701 | Loss: 0.2842434048652649\n",
      "Step 1801 | Loss: 0.1547689586877823\n",
      "Step 1901 | Loss: 0.3075402081012726\n",
      "Step 2001 | Loss: 0.23850522935390472\n",
      "Step 2101 | Loss: 0.2837379276752472\n",
      "Step 2201 | Loss: 0.31189072132110596\n",
      "Step 2301 | Loss: 0.3215019106864929\n",
      "Step 2401 | Loss: 0.3806076943874359\n",
      "0.29230630820032333 0.97\n",
      "Step 1 | Loss: 1.3305037021636963\n",
      "Step 101 | Loss: 0.7608041763305664\n",
      "Step 201 | Loss: 0.509013831615448\n",
      "Step 301 | Loss: 0.7036153078079224\n",
      "Step 401 | Loss: 0.43983525037765503\n",
      "Step 501 | Loss: 0.4493378698825836\n",
      "Step 601 | Loss: 0.4753083288669586\n",
      "Step 701 | Loss: 0.3024386167526245\n",
      "Step 801 | Loss: 0.3013930320739746\n",
      "Step 901 | Loss: 0.43012288212776184\n",
      "Step 1001 | Loss: 0.37850964069366455\n",
      "Step 1101 | Loss: 0.25308477878570557\n",
      "Step 1201 | Loss: 0.42858457565307617\n",
      "Step 1301 | Loss: 0.36802607774734497\n",
      "Step 1401 | Loss: 0.3508412539958954\n",
      "Step 1501 | Loss: 0.27059251070022583\n",
      "Step 1601 | Loss: 0.3327617645263672\n",
      "Step 1701 | Loss: 0.33657634258270264\n",
      "Step 1801 | Loss: 0.2827000319957733\n",
      "Step 1901 | Loss: 0.25273051857948303\n",
      "Step 2001 | Loss: 0.272373765707016\n",
      "Step 2101 | Loss: 0.2227502465248108\n",
      "Step 2201 | Loss: 0.27269184589385986\n",
      "Step 2301 | Loss: 0.24925194680690765\n",
      "Step 2401 | Loss: 0.26462817192077637\n",
      "0.2911437291019485 0.97\n",
      "Step 1 | Loss: 0.6838687658309937\n",
      "Step 101 | Loss: 0.3937755525112152\n",
      "Step 201 | Loss: 0.2436543107032776\n",
      "Step 301 | Loss: 0.2017025351524353\n",
      "Step 401 | Loss: 0.20996063947677612\n",
      "Step 501 | Loss: 0.35612404346466064\n",
      "Step 601 | Loss: 0.29010337591171265\n",
      "Step 701 | Loss: 0.21332348883152008\n",
      "Step 801 | Loss: 0.3261948525905609\n",
      "Step 901 | Loss: 0.2994409501552582\n",
      "Step 1001 | Loss: 0.1599949449300766\n",
      "Step 1101 | Loss: 0.2999435365200043\n",
      "Step 1201 | Loss: 0.22707682847976685\n",
      "Step 1301 | Loss: 0.2781389355659485\n",
      "Step 1401 | Loss: 0.32034191489219666\n",
      "Step 1501 | Loss: 0.24779540300369263\n",
      "Step 1601 | Loss: 0.20958846807479858\n",
      "Step 1701 | Loss: 0.3107168674468994\n",
      "Step 1801 | Loss: 0.2705010175704956\n",
      "Step 1901 | Loss: 0.2960027754306793\n",
      "Step 2001 | Loss: 0.15658777952194214\n",
      "Step 2101 | Loss: 0.20098404586315155\n",
      "Step 2201 | Loss: 0.33596381545066833\n",
      "Step 2301 | Loss: 0.293697714805603\n",
      "Step 2401 | Loss: 0.25836968421936035\n",
      "0.24226416848484256 0.94\n",
      "Step 1 | Loss: 1.2350491285324097\n",
      "Step 101 | Loss: 0.9079006314277649\n",
      "Step 201 | Loss: 0.3090190291404724\n",
      "Step 301 | Loss: 0.2877736985683441\n",
      "Step 401 | Loss: 0.1327764391899109\n",
      "Step 501 | Loss: 0.27471333742141724\n",
      "Step 601 | Loss: 0.1417796015739441\n",
      "Step 701 | Loss: 0.1798902153968811\n",
      "Step 801 | Loss: 0.26963841915130615\n",
      "Step 901 | Loss: 0.24908281862735748\n",
      "Step 1001 | Loss: 0.14570525288581848\n",
      "Step 1101 | Loss: 0.34021344780921936\n",
      "Step 1201 | Loss: 0.2760668992996216\n",
      "Step 1301 | Loss: 0.316164493560791\n",
      "Step 1401 | Loss: 0.32683390378952026\n",
      "Step 1501 | Loss: 0.24057018756866455\n",
      "Step 1601 | Loss: 0.22492890059947968\n",
      "Step 1701 | Loss: 0.2871474325656891\n",
      "Step 1801 | Loss: 0.17581243813037872\n",
      "Step 1901 | Loss: 0.21009209752082825\n",
      "Step 2001 | Loss: 0.19996580481529236\n",
      "Step 2101 | Loss: 0.38441428542137146\n",
      "Step 2201 | Loss: 0.22369341552257538\n",
      "Step 2301 | Loss: 0.22549545764923096\n",
      "Step 2401 | Loss: 0.17002534866333008\n",
      "0.24163541330285424 0.94\n",
      "Step 1 | Loss: 1.783034086227417\n",
      "Step 101 | Loss: 0.5681840181350708\n",
      "Step 201 | Loss: 0.5685316920280457\n",
      "Step 301 | Loss: 0.4096360504627228\n",
      "Step 401 | Loss: 0.5752505660057068\n",
      "Step 501 | Loss: 0.5129762291908264\n",
      "Step 601 | Loss: 0.7159194946289062\n",
      "Step 701 | Loss: 0.46226221323013306\n",
      "Step 801 | Loss: 0.5653479099273682\n",
      "Step 901 | Loss: 0.2724052369594574\n",
      "Step 1001 | Loss: 0.4849468469619751\n",
      "Step 1101 | Loss: 0.5566472411155701\n",
      "Step 1201 | Loss: 0.3413916826248169\n",
      "Step 1301 | Loss: 0.3469056487083435\n",
      "Step 1401 | Loss: 0.48696035146713257\n",
      "Step 1501 | Loss: 0.41612735390663147\n",
      "Step 1601 | Loss: 0.5002105832099915\n",
      "Step 1701 | Loss: 0.5533307194709778\n",
      "Step 1801 | Loss: 0.42064154148101807\n",
      "Step 1901 | Loss: 0.5660619735717773\n",
      "Step 2001 | Loss: 0.5767374634742737\n",
      "Step 2101 | Loss: 0.49283406138420105\n",
      "Step 2201 | Loss: 0.5320340991020203\n",
      "Step 2301 | Loss: 0.34299689531326294\n",
      "Step 2401 | Loss: 0.5832943916320801\n",
      "0.45119452327653153 0.8925\n",
      "Step 1 | Loss: 1.2824666500091553\n",
      "Step 101 | Loss: 0.8604786992073059\n",
      "Step 201 | Loss: 0.651496410369873\n",
      "Step 301 | Loss: 0.6992636919021606\n",
      "Step 401 | Loss: 0.7045164108276367\n",
      "Step 501 | Loss: 0.750797688961029\n",
      "Step 601 | Loss: 0.7088137269020081\n",
      "Step 701 | Loss: 0.6930347084999084\n",
      "Step 801 | Loss: 0.5643056035041809\n",
      "Step 901 | Loss: 0.6309823393821716\n",
      "Step 1001 | Loss: 0.6479229927062988\n",
      "Step 1101 | Loss: 0.5207433700561523\n",
      "Step 1201 | Loss: 0.5631147623062134\n",
      "Step 1301 | Loss: 0.44153285026550293\n",
      "Step 1401 | Loss: 0.509594738483429\n",
      "Step 1501 | Loss: 0.4247516095638275\n",
      "Step 1601 | Loss: 0.45931556820869446\n",
      "Step 1701 | Loss: 0.496333509683609\n",
      "Step 1801 | Loss: 0.6250476241111755\n",
      "Step 1901 | Loss: 0.4178886413574219\n",
      "Step 2001 | Loss: 0.38945016264915466\n",
      "Step 2101 | Loss: 0.4144577383995056\n",
      "Step 2201 | Loss: 0.4691656231880188\n",
      "Step 2301 | Loss: 0.4768170714378357\n",
      "Step 2401 | Loss: 0.5523136258125305\n",
      "0.4933894014644944 0.8775\n",
      "Step 1 | Loss: 1.373917818069458\n",
      "Step 101 | Loss: 0.8657322525978088\n",
      "Step 201 | Loss: 0.773291289806366\n",
      "Step 301 | Loss: 0.7812859416007996\n",
      "Step 401 | Loss: 1.0609774589538574\n",
      "Step 501 | Loss: 0.9352158308029175\n",
      "Step 601 | Loss: 0.9007366299629211\n",
      "Step 701 | Loss: 1.0586724281311035\n",
      "Step 801 | Loss: 0.8022481203079224\n",
      "Step 901 | Loss: 0.9178996086120605\n",
      "Step 1001 | Loss: 0.8247109651565552\n",
      "Step 1101 | Loss: 0.9919732213020325\n",
      "Step 1201 | Loss: 0.7939902544021606\n",
      "Step 1301 | Loss: 0.9389691352844238\n",
      "Step 1401 | Loss: 1.0035958290100098\n",
      "Step 1501 | Loss: 0.8364243507385254\n",
      "Step 1601 | Loss: 0.9322612881660461\n",
      "Step 1701 | Loss: 1.2523902654647827\n",
      "Step 1801 | Loss: 0.9635701179504395\n",
      "Step 1901 | Loss: 0.996576189994812\n",
      "Step 2001 | Loss: 0.8291241526603699\n",
      "Step 2101 | Loss: 0.643247127532959\n",
      "Step 2201 | Loss: 1.1687359809875488\n",
      "Step 2301 | Loss: 1.1479288339614868\n",
      "Step 2401 | Loss: 0.9159697890281677\n",
      "0.8600473216771148 0.685\n",
      "Step 1 | Loss: 1.0762159824371338\n",
      "Step 101 | Loss: 0.8994211554527283\n",
      "Step 201 | Loss: 0.6746246218681335\n",
      "Step 301 | Loss: 0.39769718050956726\n",
      "Step 401 | Loss: 0.6913948059082031\n",
      "Step 501 | Loss: 0.5461481213569641\n",
      "Step 601 | Loss: 0.5418490767478943\n",
      "Step 701 | Loss: 0.5051393508911133\n",
      "Step 801 | Loss: 0.5738078355789185\n",
      "Step 901 | Loss: 0.4413308799266815\n",
      "Step 1001 | Loss: 0.4099394679069519\n",
      "Step 1101 | Loss: 0.593988299369812\n",
      "Step 1201 | Loss: 0.5364260673522949\n",
      "Step 1301 | Loss: 0.4150278568267822\n",
      "Step 1401 | Loss: 0.46775200963020325\n",
      "Step 1501 | Loss: 0.3399609923362732\n",
      "Step 1601 | Loss: 0.39761680364608765\n",
      "Step 1701 | Loss: 0.44162070751190186\n",
      "Step 1801 | Loss: 0.4148575961589813\n",
      "Step 1901 | Loss: 0.26486533880233765\n",
      "Step 2001 | Loss: 0.4328029751777649\n",
      "Step 2101 | Loss: 0.39556819200515747\n",
      "Step 2201 | Loss: 0.3899422883987427\n",
      "Step 2301 | Loss: 0.40844759345054626\n",
      "Step 2401 | Loss: 0.45063623785972595\n",
      "0.45366041911743593 0.9025\n",
      "Step 1 | Loss: 0.9965412616729736\n",
      "Step 101 | Loss: 0.80949866771698\n",
      "Step 201 | Loss: 0.6358077526092529\n",
      "Step 301 | Loss: 0.48477810621261597\n",
      "Step 401 | Loss: 0.7615925669670105\n",
      "Step 501 | Loss: 0.3792416453361511\n",
      "Step 601 | Loss: 0.5230610966682434\n",
      "Step 701 | Loss: 0.4957733750343323\n",
      "Step 801 | Loss: 0.4970080852508545\n",
      "Step 901 | Loss: 0.6259307265281677\n",
      "Step 1001 | Loss: 0.5164531469345093\n",
      "Step 1101 | Loss: 0.39925143122673035\n",
      "Step 1201 | Loss: 0.4727402627468109\n",
      "Step 1301 | Loss: 0.5665103197097778\n",
      "Step 1401 | Loss: 0.49644705653190613\n",
      "Step 1501 | Loss: 0.4728466868400574\n",
      "Step 1601 | Loss: 0.326698362827301\n",
      "Step 1701 | Loss: 0.3646365702152252\n",
      "Step 1801 | Loss: 0.4416491389274597\n",
      "Step 1901 | Loss: 0.6774512529373169\n",
      "Step 2001 | Loss: 0.4356324076652527\n",
      "Step 2101 | Loss: 0.7047699093818665\n",
      "Step 2201 | Loss: 0.41228944063186646\n",
      "Step 2301 | Loss: 0.44222292304039\n",
      "Step 2401 | Loss: 0.4780631363391876\n",
      "0.4537941704701456 0.8975\n",
      "Step 1 | Loss: 1.4656635522842407\n",
      "Step 101 | Loss: 0.6239893436431885\n",
      "Step 201 | Loss: 0.8213388919830322\n",
      "Step 301 | Loss: 0.8059943914413452\n",
      "Step 401 | Loss: 0.36734622716903687\n",
      "Step 501 | Loss: 0.7257019281387329\n",
      "Step 601 | Loss: 0.7972469925880432\n",
      "Step 701 | Loss: 0.48346731066703796\n",
      "Step 801 | Loss: 0.5298819541931152\n",
      "Step 901 | Loss: 0.4985679090023041\n",
      "Step 1001 | Loss: 0.5724173784255981\n",
      "Step 1101 | Loss: 0.6326114535331726\n",
      "Step 1201 | Loss: 0.7424122095108032\n",
      "Step 1301 | Loss: 0.9635118246078491\n",
      "Step 1401 | Loss: 0.6731016039848328\n",
      "Step 1501 | Loss: 0.6441616415977478\n",
      "Step 1601 | Loss: 0.5595372319221497\n",
      "Step 1701 | Loss: 0.7466334104537964\n",
      "Step 1801 | Loss: 0.6658472418785095\n",
      "Step 1901 | Loss: 0.9491758942604065\n",
      "Step 2001 | Loss: 0.5995944738388062\n",
      "Step 2101 | Loss: 0.5016031265258789\n",
      "Step 2201 | Loss: 0.7976166009902954\n",
      "Step 2301 | Loss: 0.5126466155052185\n",
      "Step 2401 | Loss: 0.6971139907836914\n",
      "0.630728217889701 0.78\n",
      "Step 1 | Loss: 1.553755760192871\n",
      "Step 101 | Loss: 0.7494726181030273\n",
      "Step 201 | Loss: 0.606675386428833\n",
      "Step 301 | Loss: 0.6488849520683289\n",
      "Step 401 | Loss: 0.8823884725570679\n",
      "Step 501 | Loss: 0.828172504901886\n",
      "Step 601 | Loss: 0.6199220418930054\n",
      "Step 701 | Loss: 0.8689633011817932\n",
      "Step 801 | Loss: 0.4802083373069763\n",
      "Step 901 | Loss: 0.8083431720733643\n",
      "Step 1001 | Loss: 0.7721565365791321\n",
      "Step 1101 | Loss: 0.6544402837753296\n",
      "Step 1201 | Loss: 0.6254062652587891\n",
      "Step 1301 | Loss: 0.8244502544403076\n",
      "Step 1401 | Loss: 0.833522617816925\n",
      "Step 1501 | Loss: 0.5085793733596802\n",
      "Step 1601 | Loss: 0.7437801957130432\n",
      "Step 1701 | Loss: 0.48864221572875977\n",
      "Step 1801 | Loss: 0.6494390964508057\n",
      "Step 1901 | Loss: 0.6363007426261902\n",
      "Step 2001 | Loss: 0.44344985485076904\n",
      "Step 2101 | Loss: 0.7310761213302612\n",
      "Step 2201 | Loss: 0.664705753326416\n",
      "Step 2301 | Loss: 0.6084871888160706\n",
      "Step 2401 | Loss: 0.6404616832733154\n",
      "0.6314742611780771 0.7775\n",
      "Step 1 | Loss: 1.2232022285461426\n",
      "Step 101 | Loss: 0.5053560137748718\n",
      "Step 201 | Loss: 0.5765979886054993\n",
      "Step 301 | Loss: 0.8814349174499512\n",
      "Step 401 | Loss: 0.63545823097229\n",
      "Step 501 | Loss: 0.6841945648193359\n",
      "Step 601 | Loss: 0.7832003831863403\n",
      "Step 701 | Loss: 0.7459971308708191\n",
      "Step 801 | Loss: 0.5379359722137451\n",
      "Step 901 | Loss: 0.7384713292121887\n",
      "Step 1001 | Loss: 0.9902942776679993\n",
      "Step 1101 | Loss: 0.8362557888031006\n",
      "Step 1201 | Loss: 0.4707084596157074\n",
      "Step 1301 | Loss: 0.5627618432044983\n",
      "Step 1401 | Loss: 0.645071268081665\n",
      "Step 1501 | Loss: 0.740268886089325\n",
      "Step 1601 | Loss: 0.7921341061592102\n",
      "Step 1701 | Loss: 0.7738801836967468\n",
      "Step 1801 | Loss: 0.5187903642654419\n",
      "Step 1901 | Loss: 0.8877348303794861\n",
      "Step 2001 | Loss: 0.6587581634521484\n",
      "Step 2101 | Loss: 0.5778449177742004\n",
      "Step 2201 | Loss: 0.6110901832580566\n",
      "Step 2301 | Loss: 0.5235668420791626\n",
      "Step 2401 | Loss: 0.8273056745529175\n",
      "0.7447715421776135 0.7675\n",
      "Step 1 | Loss: 1.178057074546814\n",
      "Step 101 | Loss: 0.6163284778594971\n",
      "Step 201 | Loss: 0.5049791932106018\n",
      "Step 301 | Loss: 0.7531975507736206\n",
      "Step 401 | Loss: 0.5701184272766113\n",
      "Step 501 | Loss: 0.4427466094493866\n",
      "Step 601 | Loss: 0.7620536088943481\n",
      "Step 701 | Loss: 0.42551302909851074\n",
      "Step 801 | Loss: 0.48295149207115173\n",
      "Step 901 | Loss: 0.7176389694213867\n",
      "Step 1001 | Loss: 0.5235409736633301\n",
      "Step 1101 | Loss: 0.6721055507659912\n",
      "Step 1201 | Loss: 0.7131047248840332\n",
      "Step 1301 | Loss: 0.6602296829223633\n",
      "Step 1401 | Loss: 0.8257619738578796\n",
      "Step 1501 | Loss: 0.7469114661216736\n",
      "Step 1601 | Loss: 0.6134003400802612\n",
      "Step 1701 | Loss: 0.5344355702400208\n",
      "Step 1801 | Loss: 0.8153409361839294\n",
      "Step 1901 | Loss: 0.8405888080596924\n",
      "Step 2001 | Loss: 0.8045294880867004\n",
      "Step 2101 | Loss: 0.5508515238761902\n",
      "Step 2201 | Loss: 0.7776175141334534\n",
      "Step 2301 | Loss: 0.6054253578186035\n",
      "Step 2401 | Loss: 0.7014825344085693\n",
      "0.6328886793429037 0.7925\n",
      "Step 1 | Loss: 1.1675796508789062\n",
      "Step 101 | Loss: 0.849129855632782\n",
      "Step 201 | Loss: 0.4669298827648163\n",
      "Step 301 | Loss: 0.6764057874679565\n",
      "Step 401 | Loss: 0.6815672516822815\n",
      "Step 501 | Loss: 1.0094914436340332\n",
      "Step 601 | Loss: 0.5550119280815125\n",
      "Step 701 | Loss: 0.6255995035171509\n",
      "Step 801 | Loss: 0.7357038259506226\n",
      "Step 901 | Loss: 0.43102025985717773\n",
      "Step 1001 | Loss: 0.6082462072372437\n",
      "Step 1101 | Loss: 0.7017073631286621\n",
      "Step 1201 | Loss: 0.8795199990272522\n",
      "Step 1301 | Loss: 0.6202816963195801\n",
      "Step 1401 | Loss: 0.6951612234115601\n",
      "Step 1501 | Loss: 0.5607063174247742\n",
      "Step 1601 | Loss: 0.6900423765182495\n",
      "Step 1701 | Loss: 0.5424690842628479\n",
      "Step 1801 | Loss: 0.9557139277458191\n",
      "Step 1901 | Loss: 0.7198683619499207\n",
      "Step 2001 | Loss: 0.710094690322876\n",
      "Step 2101 | Loss: 0.8404290080070496\n",
      "Step 2201 | Loss: 0.7671210169792175\n",
      "Step 2301 | Loss: 0.6745846271514893\n",
      "Step 2401 | Loss: 0.6311016082763672\n",
      "0.7433056705751018 0.77\n",
      "Step 1 | Loss: 1.7248046398162842\n",
      "Step 101 | Loss: 0.38307854533195496\n",
      "Step 201 | Loss: 0.3396264910697937\n",
      "Step 301 | Loss: 0.4102400541305542\n",
      "Step 401 | Loss: 0.3883211314678192\n",
      "Step 501 | Loss: 0.3152875602245331\n",
      "Step 601 | Loss: 0.322308212518692\n",
      "Step 701 | Loss: 0.4827329218387604\n",
      "Step 801 | Loss: 0.26491525769233704\n",
      "Step 901 | Loss: 0.27005600929260254\n",
      "Step 1001 | Loss: 0.26435431838035583\n",
      "Step 1101 | Loss: 0.5407155752182007\n",
      "Step 1201 | Loss: 0.3735128939151764\n",
      "Step 1301 | Loss: 0.26367321610450745\n",
      "Step 1401 | Loss: 0.5151669383049011\n",
      "Step 1501 | Loss: 0.18881940841674805\n",
      "Step 1601 | Loss: 0.2324429452419281\n",
      "Step 1701 | Loss: 0.2690030038356781\n",
      "Step 1801 | Loss: 0.32938849925994873\n",
      "Step 1901 | Loss: 0.35527825355529785\n",
      "Step 2001 | Loss: 0.4713476598262787\n",
      "Step 2101 | Loss: 0.36526229977607727\n",
      "Step 2201 | Loss: 0.49354416131973267\n",
      "Step 2301 | Loss: 0.38383737206459045\n",
      "Step 2401 | Loss: 0.42483970522880554\n",
      "0.37306386825355065 0.8875\n",
      "Step 1 | Loss: 0.6179003119468689\n",
      "Step 101 | Loss: 0.18754000961780548\n",
      "Step 201 | Loss: 0.3105791509151459\n",
      "Step 301 | Loss: 0.15526419878005981\n",
      "Step 401 | Loss: 0.2556908428668976\n",
      "Step 501 | Loss: 0.15973074734210968\n",
      "Step 601 | Loss: 0.28583553433418274\n",
      "Step 701 | Loss: 0.23009756207466125\n",
      "Step 801 | Loss: 0.2295498549938202\n",
      "Step 901 | Loss: 0.2697529196739197\n",
      "Step 1001 | Loss: 0.166002556681633\n",
      "Step 1101 | Loss: 0.5093873143196106\n",
      "Step 1201 | Loss: 0.3615192472934723\n",
      "Step 1301 | Loss: 0.2105882167816162\n",
      "Step 1401 | Loss: 0.4040050804615021\n",
      "Step 1501 | Loss: 0.18633495271205902\n",
      "Step 1601 | Loss: 0.1671850085258484\n",
      "Step 1701 | Loss: 0.20539525151252747\n",
      "Step 1801 | Loss: 0.2535461485385895\n",
      "Step 1901 | Loss: 0.2092236578464508\n",
      "Step 2001 | Loss: 0.3331732153892517\n",
      "Step 2101 | Loss: 0.23185120522975922\n",
      "Step 2201 | Loss: 0.22828055918216705\n",
      "Step 2301 | Loss: 0.13345015048980713\n",
      "Step 2401 | Loss: 0.24892941117286682\n",
      "0.2885300801377163 0.9125\n",
      "Step 1 | Loss: 1.7562217712402344\n",
      "Step 101 | Loss: 0.3766101002693176\n",
      "Step 201 | Loss: 0.12086401134729385\n",
      "Step 301 | Loss: 0.4136677384376526\n",
      "Step 401 | Loss: 0.35729750990867615\n",
      "Step 501 | Loss: 0.28867048025131226\n",
      "Step 601 | Loss: 0.2050929218530655\n",
      "Step 701 | Loss: 0.37149327993392944\n",
      "Step 801 | Loss: 0.21310241520404816\n",
      "Step 901 | Loss: 0.10654992610216141\n",
      "Step 1001 | Loss: 0.21480071544647217\n",
      "Step 1101 | Loss: 0.22388406097888947\n",
      "Step 1201 | Loss: 0.367481529712677\n",
      "Step 1301 | Loss: 0.284975528717041\n",
      "Step 1401 | Loss: 0.30482205748558044\n",
      "Step 1501 | Loss: 0.25757092237472534\n",
      "Step 1601 | Loss: 0.2630913555622101\n",
      "Step 1701 | Loss: 0.19767460227012634\n",
      "Step 1801 | Loss: 0.16607604920864105\n",
      "Step 1901 | Loss: 0.2805451452732086\n",
      "Step 2001 | Loss: 0.15855509042739868\n",
      "Step 2101 | Loss: 0.23673301935195923\n",
      "Step 2201 | Loss: 0.3609743118286133\n",
      "Step 2301 | Loss: 0.23423662781715393\n",
      "Step 2401 | Loss: 0.2547552287578583\n",
      "0.2866668043485967 0.9175\n",
      "Step 1 | Loss: 2.1860897541046143\n",
      "Step 101 | Loss: 0.42392823100090027\n",
      "Step 201 | Loss: 0.43576598167419434\n",
      "Step 301 | Loss: 0.11268836259841919\n",
      "Step 401 | Loss: 0.3022719621658325\n",
      "Step 501 | Loss: 0.25944069027900696\n",
      "Step 601 | Loss: 0.37266167998313904\n",
      "Step 701 | Loss: 0.26744404435157776\n",
      "Step 801 | Loss: 0.41759005188941956\n",
      "Step 901 | Loss: 0.3212622106075287\n",
      "Step 1001 | Loss: 0.47900721430778503\n",
      "Step 1101 | Loss: 0.3446650505065918\n",
      "Step 1201 | Loss: 0.32964590191841125\n",
      "Step 1301 | Loss: 0.36617511510849\n",
      "Step 1401 | Loss: 0.16058480739593506\n",
      "Step 1501 | Loss: 0.3500014543533325\n",
      "Step 1601 | Loss: 0.42406588792800903\n",
      "Step 1701 | Loss: 0.32250452041625977\n",
      "Step 1801 | Loss: 0.44146403670310974\n",
      "Step 1901 | Loss: 0.24056477844715118\n",
      "Step 2001 | Loss: 0.26904022693634033\n",
      "Step 2101 | Loss: 0.37085607647895813\n",
      "Step 2201 | Loss: 0.3439539074897766\n",
      "Step 2301 | Loss: 0.3059380054473877\n",
      "Step 2401 | Loss: 0.30100077390670776\n",
      "0.3719398705193899 0.89\n",
      "Step 1 | Loss: 1.5654401779174805\n",
      "Step 101 | Loss: 0.2265569567680359\n",
      "Step 201 | Loss: 0.23931118845939636\n",
      "Step 301 | Loss: 0.3366253972053528\n",
      "Step 401 | Loss: 0.2637004554271698\n",
      "Step 501 | Loss: 0.29166436195373535\n",
      "Step 601 | Loss: 0.3599627614021301\n",
      "Step 701 | Loss: 0.2961612939834595\n",
      "Step 801 | Loss: 0.37040504813194275\n",
      "Step 901 | Loss: 0.38165879249572754\n",
      "Step 1001 | Loss: 0.3703975975513458\n",
      "Step 1101 | Loss: 0.48837602138519287\n",
      "Step 1201 | Loss: 0.25375157594680786\n",
      "Step 1301 | Loss: 0.25459763407707214\n",
      "Step 1401 | Loss: 0.3111928701400757\n",
      "Step 1501 | Loss: 0.26942679286003113\n",
      "Step 1601 | Loss: 0.31566375494003296\n",
      "Step 1701 | Loss: 0.5753660798072815\n",
      "Step 1801 | Loss: 0.30717143416404724\n",
      "Step 1901 | Loss: 0.5591285824775696\n",
      "Step 2001 | Loss: 0.5422311425209045\n",
      "Step 2101 | Loss: 0.4982975125312805\n",
      "Step 2201 | Loss: 0.30437713861465454\n",
      "Step 2301 | Loss: 0.3751525282859802\n",
      "Step 2401 | Loss: 0.32017138600349426\n",
      "0.37118810557661214 0.8875\n",
      "Step 1 | Loss: 1.0324797630310059\n",
      "Step 101 | Loss: 0.9526158571243286\n",
      "Step 201 | Loss: 1.0509614944458008\n",
      "Step 301 | Loss: 0.926579475402832\n",
      "Step 401 | Loss: 0.9578835368156433\n",
      "Step 501 | Loss: 1.2734628915786743\n",
      "Step 601 | Loss: 0.976308286190033\n",
      "Step 701 | Loss: 1.1250072717666626\n",
      "Step 801 | Loss: 1.2100526094436646\n",
      "Step 901 | Loss: 1.0746986865997314\n",
      "Step 1001 | Loss: 1.1116563081741333\n",
      "Step 1101 | Loss: 1.0142439603805542\n",
      "Step 1201 | Loss: 1.0309641361236572\n",
      "Step 1301 | Loss: 1.0670874118804932\n",
      "Step 1401 | Loss: 0.9661648273468018\n",
      "Step 1501 | Loss: 1.0168935060501099\n",
      "Step 1601 | Loss: 1.0541173219680786\n",
      "Step 1701 | Loss: 1.0725114345550537\n",
      "Step 1801 | Loss: 1.0337258577346802\n",
      "Step 1901 | Loss: 1.0501569509506226\n",
      "Step 2001 | Loss: 1.1171702146530151\n",
      "Step 2101 | Loss: 1.0750513076782227\n",
      "Step 2201 | Loss: 1.000070571899414\n",
      "Step 2301 | Loss: 1.1960774660110474\n",
      "Step 2401 | Loss: 1.1447702646255493\n",
      "0.9853520492074439 0.4625\n",
      "Step 1 | Loss: 1.6869028806686401\n",
      "Step 101 | Loss: 1.1397532224655151\n",
      "Step 201 | Loss: 0.98646479845047\n",
      "Step 301 | Loss: 1.0699148178100586\n",
      "Step 401 | Loss: 0.8443817496299744\n",
      "Step 501 | Loss: 1.0321276187896729\n",
      "Step 601 | Loss: 0.9435951113700867\n",
      "Step 701 | Loss: 1.0028148889541626\n",
      "Step 801 | Loss: 1.001281499862671\n",
      "Step 901 | Loss: 1.076371192932129\n",
      "Step 1001 | Loss: 0.9790891408920288\n",
      "Step 1101 | Loss: 1.1749374866485596\n",
      "Step 1201 | Loss: 0.9871614575386047\n",
      "Step 1301 | Loss: 1.0462782382965088\n",
      "Step 1401 | Loss: 1.070218801498413\n",
      "Step 1501 | Loss: 1.0477817058563232\n",
      "Step 1601 | Loss: 1.1637293100357056\n",
      "Step 1701 | Loss: 0.8742313385009766\n",
      "Step 1801 | Loss: 1.2607113122940063\n",
      "Step 1901 | Loss: 1.0848643779754639\n",
      "Step 2001 | Loss: 0.906804084777832\n",
      "Step 2101 | Loss: 0.9159795045852661\n",
      "Step 2201 | Loss: 1.1051241159439087\n",
      "Step 2301 | Loss: 1.0054974555969238\n",
      "Step 2401 | Loss: 0.7717345952987671\n",
      "0.9390061495741413 0.5175\n",
      "Step 1 | Loss: 1.4325484037399292\n",
      "Step 101 | Loss: 1.052085041999817\n",
      "Step 201 | Loss: 1.0558972358703613\n",
      "Step 301 | Loss: 1.0429329872131348\n",
      "Step 401 | Loss: 1.1900566816329956\n",
      "Step 501 | Loss: 0.9315912127494812\n",
      "Step 601 | Loss: 0.9255584478378296\n",
      "Step 701 | Loss: 0.9971457719802856\n",
      "Step 801 | Loss: 1.0396732091903687\n",
      "Step 901 | Loss: 1.1858289241790771\n",
      "Step 1001 | Loss: 1.09283447265625\n",
      "Step 1101 | Loss: 0.961987316608429\n",
      "Step 1201 | Loss: 1.013916015625\n",
      "Step 1301 | Loss: 1.3147392272949219\n",
      "Step 1401 | Loss: 0.9854444861412048\n",
      "Step 1501 | Loss: 1.0289584398269653\n",
      "Step 1601 | Loss: 1.1817220449447632\n",
      "Step 1701 | Loss: 1.0086467266082764\n",
      "Step 1801 | Loss: 0.9977983236312866\n",
      "Step 1901 | Loss: 0.9158447980880737\n",
      "Step 2001 | Loss: 1.021059513092041\n",
      "Step 2101 | Loss: 0.8631437420845032\n",
      "Step 2201 | Loss: 1.024064540863037\n",
      "Step 2301 | Loss: 0.9026032090187073\n",
      "Step 2401 | Loss: 1.0913928747177124\n",
      "0.9480855391054441 0.515\n",
      "Step 1 | Loss: 1.156562089920044\n",
      "Step 101 | Loss: 1.0848255157470703\n",
      "Step 201 | Loss: 1.1113934516906738\n",
      "Step 301 | Loss: 1.011620044708252\n",
      "Step 401 | Loss: 1.1384937763214111\n",
      "Step 501 | Loss: 1.0285515785217285\n",
      "Step 601 | Loss: 1.0485680103302002\n",
      "Step 701 | Loss: 1.0977615118026733\n",
      "Step 801 | Loss: 1.0535931587219238\n",
      "Step 901 | Loss: 1.0514997243881226\n",
      "Step 1001 | Loss: 0.9147632718086243\n",
      "Step 1101 | Loss: 1.034779667854309\n",
      "Step 1201 | Loss: 1.128226637840271\n",
      "Step 1301 | Loss: 0.9985566735267639\n",
      "Step 1401 | Loss: 0.9214356541633606\n",
      "Step 1501 | Loss: 1.2189832925796509\n",
      "Step 1601 | Loss: 0.995468258857727\n",
      "Step 1701 | Loss: 1.0667011737823486\n",
      "Step 1801 | Loss: 1.041279911994934\n",
      "Step 1901 | Loss: 0.8765788674354553\n",
      "Step 2001 | Loss: 1.1262099742889404\n",
      "Step 2101 | Loss: 1.1789336204528809\n",
      "Step 2201 | Loss: 1.0393205881118774\n",
      "Step 2301 | Loss: 1.0074021816253662\n",
      "Step 2401 | Loss: 1.1035469770431519\n",
      "0.9776092585718232 0.5475\n",
      "Step 1 | Loss: 1.5534850358963013\n",
      "Step 101 | Loss: 0.9699649810791016\n",
      "Step 201 | Loss: 0.9974169731140137\n",
      "Step 301 | Loss: 1.0688928365707397\n",
      "Step 401 | Loss: 0.8283920288085938\n",
      "Step 501 | Loss: 1.0968594551086426\n",
      "Step 601 | Loss: 1.0694537162780762\n",
      "Step 701 | Loss: 0.9298214912414551\n",
      "Step 801 | Loss: 0.9105709195137024\n",
      "Step 901 | Loss: 0.9907134175300598\n",
      "Step 1001 | Loss: 0.922601044178009\n",
      "Step 1101 | Loss: 0.8944511413574219\n",
      "Step 1201 | Loss: 0.9700854420661926\n",
      "Step 1301 | Loss: 1.091162919998169\n",
      "Step 1401 | Loss: 0.9522031545639038\n",
      "Step 1501 | Loss: 0.993786633014679\n",
      "Step 1601 | Loss: 1.0511870384216309\n",
      "Step 1701 | Loss: 1.1300454139709473\n",
      "Step 1801 | Loss: 1.091019868850708\n",
      "Step 1901 | Loss: 0.9916048645973206\n",
      "Step 2001 | Loss: 0.93965083360672\n",
      "Step 2101 | Loss: 0.8322044014930725\n",
      "Step 2201 | Loss: 0.8323478102684021\n",
      "Step 2301 | Loss: 0.9600241184234619\n",
      "Step 2401 | Loss: 0.9987949132919312\n",
      "0.9396149114714928 0.585\n",
      "Step 1 | Loss: 1.0772978067398071\n",
      "Step 101 | Loss: 0.8118202090263367\n",
      "Step 201 | Loss: 0.8489769697189331\n",
      "Step 301 | Loss: 0.6341658234596252\n",
      "Step 401 | Loss: 0.5685443878173828\n",
      "Step 501 | Loss: 0.5736169815063477\n",
      "Step 601 | Loss: 0.5972209572792053\n",
      "Step 701 | Loss: 0.522477388381958\n",
      "Step 801 | Loss: 0.5749456882476807\n",
      "Step 901 | Loss: 0.6265143156051636\n",
      "Step 1001 | Loss: 0.48866158723831177\n",
      "Step 1101 | Loss: 0.408936470746994\n",
      "Step 1201 | Loss: 0.41687023639678955\n",
      "Step 1301 | Loss: 0.5619850754737854\n",
      "Step 1401 | Loss: 0.5726618766784668\n",
      "Step 1501 | Loss: 0.6869246959686279\n",
      "Step 1601 | Loss: 0.35479533672332764\n",
      "Step 1701 | Loss: 0.40886786580085754\n",
      "Step 1801 | Loss: 0.43185263872146606\n",
      "Step 1901 | Loss: 0.5373353362083435\n",
      "Step 2001 | Loss: 0.4266471862792969\n",
      "Step 2101 | Loss: 0.44428977370262146\n",
      "Step 2201 | Loss: 0.6465663909912109\n",
      "Step 2301 | Loss: 0.5260698795318604\n",
      "Step 2401 | Loss: 0.44915950298309326\n",
      "0.46766161143221663 0.9025\n",
      "Step 1 | Loss: 0.83281409740448\n",
      "Step 101 | Loss: 0.6010151505470276\n",
      "Step 201 | Loss: 0.6809760928153992\n",
      "Step 301 | Loss: 0.4945482313632965\n",
      "Step 401 | Loss: 0.4994787275791168\n",
      "Step 501 | Loss: 0.5440444350242615\n",
      "Step 601 | Loss: 0.5606870055198669\n",
      "Step 701 | Loss: 0.6502246856689453\n",
      "Step 801 | Loss: 0.5118511915206909\n",
      "Step 901 | Loss: 0.5336894989013672\n",
      "Step 1001 | Loss: 0.6819216012954712\n",
      "Step 1101 | Loss: 0.5846946239471436\n",
      "Step 1201 | Loss: 0.380606085062027\n",
      "Step 1301 | Loss: 0.5457842350006104\n",
      "Step 1401 | Loss: 0.5711309313774109\n",
      "Step 1501 | Loss: 0.4454389214515686\n",
      "Step 1601 | Loss: 0.5597630143165588\n",
      "Step 1701 | Loss: 0.4151361584663391\n",
      "Step 1801 | Loss: 0.5645041465759277\n",
      "Step 1901 | Loss: 0.5139979124069214\n",
      "Step 2001 | Loss: 0.5564228892326355\n",
      "Step 2101 | Loss: 0.5124184489250183\n",
      "Step 2201 | Loss: 0.6314495801925659\n",
      "Step 2301 | Loss: 0.27477312088012695\n",
      "Step 2401 | Loss: 0.5143933296203613\n",
      "0.46695790222753025 0.905\n",
      "Step 1 | Loss: 0.9847903847694397\n",
      "Step 101 | Loss: 0.753756582736969\n",
      "Step 201 | Loss: 0.5532304048538208\n",
      "Step 301 | Loss: 0.5089867115020752\n",
      "Step 401 | Loss: 0.5421338081359863\n",
      "Step 501 | Loss: 0.7823445796966553\n",
      "Step 601 | Loss: 0.5977643728256226\n",
      "Step 701 | Loss: 0.6703428030014038\n",
      "Step 801 | Loss: 0.5079209208488464\n",
      "Step 901 | Loss: 0.5551512241363525\n",
      "Step 1001 | Loss: 0.5287112593650818\n",
      "Step 1101 | Loss: 0.4480794072151184\n",
      "Step 1201 | Loss: 0.5242924094200134\n",
      "Step 1301 | Loss: 0.6810221672058105\n",
      "Step 1401 | Loss: 0.4775753617286682\n",
      "Step 1501 | Loss: 0.46631908416748047\n",
      "Step 1601 | Loss: 0.5807530283927917\n",
      "Step 1701 | Loss: 0.6255508065223694\n",
      "Step 1801 | Loss: 0.6757433414459229\n",
      "Step 1901 | Loss: 0.44571661949157715\n",
      "Step 2001 | Loss: 0.5142989158630371\n",
      "Step 2101 | Loss: 0.7752425670623779\n",
      "Step 2201 | Loss: 0.37774524092674255\n",
      "Step 2301 | Loss: 0.4782152473926544\n",
      "Step 2401 | Loss: 0.668745756149292\n",
      "0.46707092276577156 0.9075\n",
      "Step 1 | Loss: 1.0955135822296143\n",
      "Step 101 | Loss: 0.9082491397857666\n",
      "Step 201 | Loss: 0.8360804319381714\n",
      "Step 301 | Loss: 0.8394525051116943\n",
      "Step 401 | Loss: 0.8698370456695557\n",
      "Step 501 | Loss: 0.6630700826644897\n",
      "Step 601 | Loss: 0.7607408761978149\n",
      "Step 701 | Loss: 0.749395489692688\n",
      "Step 801 | Loss: 0.7662519216537476\n",
      "Step 901 | Loss: 0.6452816128730774\n",
      "Step 1001 | Loss: 0.7094476222991943\n",
      "Step 1101 | Loss: 0.7509326934814453\n",
      "Step 1201 | Loss: 0.8225951194763184\n",
      "Step 1301 | Loss: 0.607794463634491\n",
      "Step 1401 | Loss: 0.679050624370575\n",
      "Step 1501 | Loss: 0.752700686454773\n",
      "Step 1601 | Loss: 0.6749284267425537\n",
      "Step 1701 | Loss: 0.733241617679596\n",
      "Step 1801 | Loss: 0.8077362775802612\n",
      "Step 1901 | Loss: 0.6848236918449402\n",
      "Step 2001 | Loss: 0.6590126752853394\n",
      "Step 2101 | Loss: 0.5684483051300049\n",
      "Step 2201 | Loss: 0.7430360317230225\n",
      "Step 2301 | Loss: 0.7627127170562744\n",
      "Step 2401 | Loss: 0.6702041625976562\n",
      "0.7041207435207404 0.74\n",
      "Step 1 | Loss: 0.9085097908973694\n",
      "Step 101 | Loss: 0.7225037813186646\n",
      "Step 201 | Loss: 0.6292355060577393\n",
      "Step 301 | Loss: 0.6693688631057739\n",
      "Step 401 | Loss: 0.5389606356620789\n",
      "Step 501 | Loss: 0.6484805345535278\n",
      "Step 601 | Loss: 0.5281348824501038\n",
      "Step 701 | Loss: 0.48741838335990906\n",
      "Step 801 | Loss: 0.49464336037635803\n",
      "Step 901 | Loss: 0.6122698783874512\n",
      "Step 1001 | Loss: 0.6058320999145508\n",
      "Step 1101 | Loss: 0.6253819465637207\n",
      "Step 1201 | Loss: 0.6199867129325867\n",
      "Step 1301 | Loss: 0.6175322532653809\n",
      "Step 1401 | Loss: 0.4752050042152405\n",
      "Step 1501 | Loss: 0.5693219900131226\n",
      "Step 1601 | Loss: 0.5129092931747437\n",
      "Step 1701 | Loss: 0.5362253785133362\n",
      "Step 1801 | Loss: 0.6658186912536621\n",
      "Step 1901 | Loss: 0.6690731644630432\n",
      "Step 2001 | Loss: 0.5877642631530762\n",
      "Step 2101 | Loss: 0.6324719786643982\n",
      "Step 2201 | Loss: 0.5632173418998718\n",
      "Step 2301 | Loss: 0.5591883063316345\n",
      "Step 2401 | Loss: 0.5584740042686462\n",
      "0.535729940273391 0.9125\n",
      "Step 1 | Loss: 0.789192259311676\n",
      "Step 101 | Loss: 0.3146003186702728\n",
      "Step 201 | Loss: 0.409304678440094\n",
      "Step 301 | Loss: 0.5158363580703735\n",
      "Step 401 | Loss: 0.4126456677913666\n",
      "Step 501 | Loss: 0.217685267329216\n",
      "Step 601 | Loss: 0.2825871706008911\n",
      "Step 701 | Loss: 0.35504505038261414\n",
      "Step 801 | Loss: 0.40270307660102844\n",
      "Step 901 | Loss: 0.39065441489219666\n",
      "Step 1001 | Loss: 0.22300079464912415\n",
      "Step 1101 | Loss: 0.22334840893745422\n",
      "Step 1201 | Loss: 0.3146054744720459\n",
      "Step 1301 | Loss: 0.294837087392807\n",
      "Step 1401 | Loss: 0.3023761510848999\n",
      "Step 1501 | Loss: 0.2767614424228668\n",
      "Step 1601 | Loss: 0.2359503209590912\n",
      "Step 1701 | Loss: 0.5532589554786682\n",
      "Step 1801 | Loss: 0.2502671480178833\n",
      "Step 1901 | Loss: 0.20790159702301025\n",
      "Step 2001 | Loss: 0.2568810284137726\n",
      "Step 2101 | Loss: 0.4217508137226105\n",
      "Step 2201 | Loss: 0.23799340426921844\n",
      "Step 2301 | Loss: 0.3434317708015442\n",
      "Step 2401 | Loss: 0.27213820815086365\n",
      "0.2798636023511938 0.945\n",
      "Step 1 | Loss: 0.25899237394332886\n",
      "Step 101 | Loss: 0.3005048334598541\n",
      "Step 201 | Loss: 0.21022458374500275\n",
      "Step 301 | Loss: 0.11251567304134369\n",
      "Step 401 | Loss: 0.13167518377304077\n",
      "Step 501 | Loss: 0.12282256782054901\n",
      "Step 601 | Loss: 0.28142672777175903\n",
      "Step 701 | Loss: 0.09650905430316925\n",
      "Step 801 | Loss: 0.2095433622598648\n",
      "Step 901 | Loss: 0.2051268219947815\n",
      "Step 1001 | Loss: 0.17148415744304657\n",
      "Step 1101 | Loss: 0.127309188246727\n",
      "Step 1201 | Loss: 0.13756178319454193\n",
      "Step 1301 | Loss: 0.1785038411617279\n",
      "Step 1401 | Loss: 0.19991743564605713\n",
      "Step 1501 | Loss: 0.15624043345451355\n",
      "Step 1601 | Loss: 0.18524712324142456\n",
      "Step 1701 | Loss: 0.2151464968919754\n",
      "Step 1801 | Loss: 0.23983097076416016\n",
      "Step 1901 | Loss: 0.16513755917549133\n",
      "Step 2001 | Loss: 0.15187479555606842\n",
      "Step 2101 | Loss: 0.14200598001480103\n",
      "Step 2201 | Loss: 0.23697824776172638\n",
      "Step 2301 | Loss: 0.18259233236312866\n",
      "Step 2401 | Loss: 0.11761168390512466\n",
      "0.1632198108812738 0.9875\n",
      "Step 1 | Loss: 0.5979591012001038\n",
      "Step 101 | Loss: 0.34185370802879333\n",
      "Step 201 | Loss: 0.23326192796230316\n",
      "Step 301 | Loss: 0.4250345230102539\n",
      "Step 401 | Loss: 0.19589322805404663\n",
      "Step 501 | Loss: 0.18522430956363678\n",
      "Step 601 | Loss: 0.19070206582546234\n",
      "Step 701 | Loss: 0.161445751786232\n",
      "Step 801 | Loss: 0.3113664388656616\n",
      "Step 901 | Loss: 0.3566718101501465\n",
      "Step 1001 | Loss: 0.19080032408237457\n",
      "Step 1101 | Loss: 0.284176766872406\n",
      "Step 1201 | Loss: 0.2930734157562256\n",
      "Step 1301 | Loss: 0.3290711045265198\n",
      "Step 1401 | Loss: 0.28647661209106445\n",
      "Step 1501 | Loss: 0.17096740007400513\n",
      "Step 1601 | Loss: 0.1717863529920578\n",
      "Step 1701 | Loss: 0.205557182431221\n",
      "Step 1801 | Loss: 0.3168448507785797\n",
      "Step 1901 | Loss: 0.48220813274383545\n",
      "Step 2001 | Loss: 0.30989062786102295\n",
      "Step 2101 | Loss: 0.16267424821853638\n",
      "Step 2201 | Loss: 0.2169100046157837\n",
      "Step 2301 | Loss: 0.3798738121986389\n",
      "Step 2401 | Loss: 0.17124494910240173\n",
      "0.22011028153488763 0.96\n",
      "Step 1 | Loss: 1.157910704612732\n",
      "Step 101 | Loss: 0.46497392654418945\n",
      "Step 201 | Loss: 0.3451867699623108\n",
      "Step 301 | Loss: 0.32858219742774963\n",
      "Step 401 | Loss: 0.5082870721817017\n",
      "Step 501 | Loss: 0.30882763862609863\n",
      "Step 601 | Loss: 0.4732290208339691\n",
      "Step 701 | Loss: 0.23948639631271362\n",
      "Step 801 | Loss: 0.37606552243232727\n",
      "Step 901 | Loss: 0.4335379898548126\n",
      "Step 1001 | Loss: 0.4839269518852234\n",
      "Step 1101 | Loss: 0.24712079763412476\n",
      "Step 1201 | Loss: 0.2788081765174866\n",
      "Step 1301 | Loss: 0.5525280237197876\n",
      "Step 1401 | Loss: 0.3272414803504944\n",
      "Step 1501 | Loss: 0.35225358605384827\n",
      "Step 1601 | Loss: 0.24419410526752472\n",
      "Step 1701 | Loss: 0.2966623604297638\n",
      "Step 1801 | Loss: 0.3205344080924988\n",
      "Step 1901 | Loss: 0.3634899854660034\n",
      "Step 2001 | Loss: 0.24601206183433533\n",
      "Step 2101 | Loss: 0.2756129503250122\n",
      "Step 2201 | Loss: 0.2953970432281494\n",
      "Step 2301 | Loss: 0.23972998559474945\n",
      "Step 2401 | Loss: 0.2929752469062805\n",
      "0.2595082816045298 0.97\n",
      "Step 1 | Loss: 0.5442250370979309\n",
      "Step 101 | Loss: 0.4406270682811737\n",
      "Step 201 | Loss: 0.18515345454216003\n",
      "Step 301 | Loss: 0.472440242767334\n",
      "Step 401 | Loss: 0.3282388746738434\n",
      "Step 501 | Loss: 0.34091293811798096\n",
      "Step 601 | Loss: 0.3037821054458618\n",
      "Step 701 | Loss: 0.2229095995426178\n",
      "Step 801 | Loss: 0.24019700288772583\n",
      "Step 901 | Loss: 0.2852714955806732\n",
      "Step 1001 | Loss: 0.19081395864486694\n",
      "Step 1101 | Loss: 0.26480841636657715\n",
      "Step 1201 | Loss: 0.17753024399280548\n",
      "Step 1301 | Loss: 0.310813844203949\n",
      "Step 1401 | Loss: 0.30435481667518616\n",
      "Step 1501 | Loss: 0.2612941563129425\n",
      "Step 1601 | Loss: 0.26031258702278137\n",
      "Step 1701 | Loss: 0.2303723692893982\n",
      "Step 1801 | Loss: 0.18546339869499207\n",
      "Step 1901 | Loss: 0.29113444685935974\n",
      "Step 2001 | Loss: 0.19110439717769623\n",
      "Step 2101 | Loss: 0.3644288182258606\n",
      "Step 2201 | Loss: 0.35119977593421936\n",
      "Step 2301 | Loss: 0.3883426785469055\n",
      "Step 2401 | Loss: 0.22004073858261108\n",
      "0.24001227302890035 0.9725\n",
      "Step 1 | Loss: 1.560225009918213\n",
      "Step 101 | Loss: 1.4636454582214355\n",
      "Step 201 | Loss: 1.4029074907302856\n",
      "Step 301 | Loss: 1.591683030128479\n",
      "Step 401 | Loss: 0.8975670337677002\n",
      "Step 501 | Loss: 0.9219830632209778\n",
      "Step 601 | Loss: 1.449041485786438\n",
      "Step 701 | Loss: 1.3451642990112305\n",
      "Step 801 | Loss: 1.377598762512207\n",
      "Step 901 | Loss: 1.309569239616394\n",
      "Step 1001 | Loss: 1.3537653684616089\n",
      "Step 1101 | Loss: 0.8061375617980957\n",
      "Step 1201 | Loss: 0.7717773914337158\n",
      "Step 1301 | Loss: 1.050190806388855\n",
      "Step 1401 | Loss: 1.0853090286254883\n",
      "Step 1501 | Loss: 1.3831143379211426\n",
      "Step 1601 | Loss: 1.3528813123703003\n",
      "Step 1701 | Loss: 1.2546956539154053\n",
      "Step 1801 | Loss: 1.1509531736373901\n",
      "Step 1901 | Loss: 1.099226951599121\n",
      "Step 2001 | Loss: 1.7378848791122437\n",
      "Step 2101 | Loss: 1.1059123277664185\n",
      "Step 2201 | Loss: 0.7574723958969116\n",
      "Step 2301 | Loss: 1.3893966674804688\n",
      "Step 2401 | Loss: 1.286119818687439\n",
      "1.1814750371761624 0.65\n",
      "Step 1 | Loss: 1.3581695556640625\n",
      "Step 101 | Loss: 1.5694245100021362\n",
      "Step 201 | Loss: 1.8803846836090088\n",
      "Step 301 | Loss: 1.1227754354476929\n",
      "Step 401 | Loss: 1.3305764198303223\n",
      "Step 501 | Loss: 1.0850404500961304\n",
      "Step 601 | Loss: 1.0623224973678589\n",
      "Step 701 | Loss: 1.580659031867981\n",
      "Step 801 | Loss: 0.9792079329490662\n",
      "Step 901 | Loss: 1.1278345584869385\n",
      "Step 1001 | Loss: 0.9633856415748596\n",
      "Step 1101 | Loss: 1.5113024711608887\n",
      "Step 1201 | Loss: 1.213249683380127\n",
      "Step 1301 | Loss: 1.6134703159332275\n",
      "Step 1401 | Loss: 1.2654478549957275\n",
      "Step 1501 | Loss: 1.0816446542739868\n",
      "Step 1601 | Loss: 1.3343886137008667\n",
      "Step 1701 | Loss: 1.4977113008499146\n",
      "Step 1801 | Loss: 1.2956397533416748\n",
      "Step 1901 | Loss: 1.4889686107635498\n",
      "Step 2001 | Loss: 0.9253470301628113\n",
      "Step 2101 | Loss: 0.9519343376159668\n",
      "Step 2201 | Loss: 1.3340023756027222\n",
      "Step 2301 | Loss: 1.5497502088546753\n",
      "Step 2401 | Loss: 0.8839901089668274\n",
      "1.2388664147219914 0.635\n",
      "Step 1 | Loss: 1.3521342277526855\n",
      "Step 101 | Loss: 1.7707324028015137\n",
      "Step 201 | Loss: 1.387708067893982\n",
      "Step 301 | Loss: 1.354371428489685\n",
      "Step 401 | Loss: 1.615728735923767\n",
      "Step 501 | Loss: 1.2808161973953247\n",
      "Step 601 | Loss: 1.029146671295166\n",
      "Step 701 | Loss: 1.638032078742981\n",
      "Step 801 | Loss: 1.2842535972595215\n",
      "Step 901 | Loss: 1.131854772567749\n",
      "Step 1001 | Loss: 1.0740563869476318\n",
      "Step 1101 | Loss: 1.3906745910644531\n",
      "Step 1201 | Loss: 0.914255678653717\n",
      "Step 1301 | Loss: 1.1638200283050537\n",
      "Step 1401 | Loss: 1.2027146816253662\n",
      "Step 1501 | Loss: 1.2086844444274902\n",
      "Step 1601 | Loss: 1.4018447399139404\n",
      "Step 1701 | Loss: 0.9398614168167114\n",
      "Step 1801 | Loss: 1.1615030765533447\n",
      "Step 1901 | Loss: 1.3544834852218628\n",
      "Step 2001 | Loss: 1.210580825805664\n",
      "Step 2101 | Loss: 1.1658779382705688\n",
      "Step 2201 | Loss: 0.8428411483764648\n",
      "Step 2301 | Loss: 1.3465523719787598\n",
      "Step 2401 | Loss: 1.1826566457748413\n",
      "1.2988194734643308 0.61\n",
      "Step 1 | Loss: 1.3870797157287598\n",
      "Step 101 | Loss: 1.641855239868164\n",
      "Step 201 | Loss: 1.4919891357421875\n",
      "Step 301 | Loss: 1.1965030431747437\n",
      "Step 401 | Loss: 1.2642329931259155\n",
      "Step 501 | Loss: 1.0658107995986938\n",
      "Step 601 | Loss: 1.0509364604949951\n",
      "Step 701 | Loss: 0.9161168932914734\n",
      "Step 801 | Loss: 0.9533947706222534\n",
      "Step 901 | Loss: 1.0097661018371582\n",
      "Step 1001 | Loss: 0.8260582089424133\n",
      "Step 1101 | Loss: 1.2807867527008057\n",
      "Step 1201 | Loss: 1.286921501159668\n",
      "Step 1301 | Loss: 1.2146083116531372\n",
      "Step 1401 | Loss: 1.1476151943206787\n",
      "Step 1501 | Loss: 1.468534231185913\n",
      "Step 1601 | Loss: 1.2841691970825195\n",
      "Step 1701 | Loss: 1.2895532846450806\n",
      "Step 1801 | Loss: 1.4553868770599365\n",
      "Step 1901 | Loss: 1.2940306663513184\n",
      "Step 2001 | Loss: 1.1845518350601196\n",
      "Step 2101 | Loss: 1.2088580131530762\n",
      "Step 2201 | Loss: 1.3227274417877197\n",
      "Step 2301 | Loss: 0.6923589706420898\n",
      "Step 2401 | Loss: 1.25956392288208\n",
      "1.1514926321587504 0.6575\n",
      "Step 1 | Loss: 2.3069396018981934\n",
      "Step 101 | Loss: 1.2431449890136719\n",
      "Step 201 | Loss: 1.5720934867858887\n",
      "Step 301 | Loss: 1.323035717010498\n",
      "Step 401 | Loss: 1.2286946773529053\n",
      "Step 501 | Loss: 1.1712785959243774\n",
      "Step 601 | Loss: 1.5717332363128662\n",
      "Step 701 | Loss: 1.374902367591858\n",
      "Step 801 | Loss: 1.6819415092468262\n",
      "Step 901 | Loss: 1.9237436056137085\n",
      "Step 1001 | Loss: 1.6670900583267212\n",
      "Step 1101 | Loss: 1.3658006191253662\n",
      "Step 1201 | Loss: 1.4355659484863281\n",
      "Step 1301 | Loss: 1.2843730449676514\n",
      "Step 1401 | Loss: 1.2967925071716309\n",
      "Step 1501 | Loss: 1.30177640914917\n",
      "Step 1601 | Loss: 1.5282284021377563\n",
      "Step 1701 | Loss: 1.039891242980957\n",
      "Step 1801 | Loss: 1.6109334230422974\n",
      "Step 1901 | Loss: 0.9933443665504456\n",
      "Step 2001 | Loss: 1.2939625978469849\n",
      "Step 2101 | Loss: 1.3918225765228271\n",
      "Step 2201 | Loss: 1.35465669631958\n",
      "Step 2301 | Loss: 1.2746763229370117\n",
      "Step 2401 | Loss: 1.2315235137939453\n",
      "1.2383653035082318 0.625\n",
      "Step 1 | Loss: 1.3526333570480347\n",
      "Step 101 | Loss: 0.3481580317020416\n",
      "Step 201 | Loss: 0.14914916455745697\n",
      "Step 301 | Loss: 0.41401901841163635\n",
      "Step 401 | Loss: 0.3052824139595032\n",
      "Step 501 | Loss: 0.3007785677909851\n",
      "Step 601 | Loss: 0.22364574670791626\n",
      "Step 701 | Loss: 0.20864585041999817\n",
      "Step 801 | Loss: 0.40868717432022095\n",
      "Step 901 | Loss: 0.30310681462287903\n",
      "Step 1001 | Loss: 0.19736886024475098\n",
      "Step 1101 | Loss: 0.23659120500087738\n",
      "Step 1201 | Loss: 0.2248910367488861\n",
      "Step 1301 | Loss: 0.19876445829868317\n",
      "Step 1401 | Loss: 0.15474030375480652\n",
      "Step 1501 | Loss: 0.17238420248031616\n",
      "Step 1601 | Loss: 0.19858846068382263\n",
      "Step 1701 | Loss: 0.276245653629303\n",
      "Step 1801 | Loss: 0.26921436190605164\n",
      "Step 1901 | Loss: 0.168570414185524\n",
      "Step 2001 | Loss: 0.3279725909233093\n",
      "Step 2101 | Loss: 0.2200499176979065\n",
      "Step 2201 | Loss: 0.20342834293842316\n",
      "Step 2301 | Loss: 0.38250789046287537\n",
      "Step 2401 | Loss: 0.30615752935409546\n",
      "0.22649163297751282 0.95\n",
      "Step 1 | Loss: 1.2006772756576538\n",
      "Step 101 | Loss: 0.7228121757507324\n",
      "Step 201 | Loss: 0.31700244545936584\n",
      "Step 301 | Loss: 0.30638229846954346\n",
      "Step 401 | Loss: 0.24779468774795532\n",
      "Step 501 | Loss: 0.25601714849472046\n",
      "Step 601 | Loss: 0.3238444924354553\n",
      "Step 701 | Loss: 0.31912192702293396\n",
      "Step 801 | Loss: 0.21240219473838806\n",
      "Step 901 | Loss: 0.36004525423049927\n",
      "Step 1001 | Loss: 0.4181881546974182\n",
      "Step 1101 | Loss: 0.23501762747764587\n",
      "Step 1201 | Loss: 0.24716520309448242\n",
      "Step 1301 | Loss: 0.17536580562591553\n",
      "Step 1401 | Loss: 0.36083805561065674\n",
      "Step 1501 | Loss: 0.1760351061820984\n",
      "Step 1601 | Loss: 0.2559064030647278\n",
      "Step 1701 | Loss: 0.31888118386268616\n",
      "Step 1801 | Loss: 0.21006649732589722\n",
      "Step 1901 | Loss: 0.20950564742088318\n",
      "Step 2001 | Loss: 0.19452394545078278\n",
      "Step 2101 | Loss: 0.32160067558288574\n",
      "Step 2201 | Loss: 0.37279459834098816\n",
      "Step 2301 | Loss: 0.2753918766975403\n",
      "Step 2401 | Loss: 0.2069711983203888\n",
      "0.24487729052396792 0.9475\n",
      "Step 1 | Loss: 2.4424421787261963\n",
      "Step 101 | Loss: 0.6381895542144775\n",
      "Step 201 | Loss: 0.23618291318416595\n",
      "Step 301 | Loss: 0.45693832635879517\n",
      "Step 401 | Loss: 0.3526221811771393\n",
      "Step 501 | Loss: 0.3018704950809479\n",
      "Step 601 | Loss: 0.2351306676864624\n",
      "Step 701 | Loss: 0.23763267695903778\n",
      "Step 801 | Loss: 0.28005602955818176\n",
      "Step 901 | Loss: 0.3352378308773041\n",
      "Step 1001 | Loss: 0.20207270979881287\n",
      "Step 1101 | Loss: 0.15130601823329926\n",
      "Step 1201 | Loss: 0.2608443796634674\n",
      "Step 1301 | Loss: 0.23364318907260895\n",
      "Step 1401 | Loss: 0.16886906325817108\n",
      "Step 1501 | Loss: 0.1950373500585556\n",
      "Step 1601 | Loss: 0.21522396802902222\n",
      "Step 1701 | Loss: 0.3673912286758423\n",
      "Step 1801 | Loss: 0.2381467968225479\n",
      "Step 1901 | Loss: 0.25211843848228455\n",
      "Step 2001 | Loss: 0.1774226576089859\n",
      "Step 2101 | Loss: 0.32375913858413696\n",
      "Step 2201 | Loss: 0.2011621743440628\n",
      "Step 2301 | Loss: 0.21888533234596252\n",
      "Step 2401 | Loss: 0.4100499153137207\n",
      "0.24339564402593128 0.9475\n",
      "Step 1 | Loss: 0.812134325504303\n",
      "Step 101 | Loss: 0.5276585221290588\n",
      "Step 201 | Loss: 0.31578218936920166\n",
      "Step 301 | Loss: 0.309932142496109\n",
      "Step 401 | Loss: 0.3496975004673004\n",
      "Step 501 | Loss: 0.4157632291316986\n",
      "Step 601 | Loss: 0.24510261416435242\n",
      "Step 701 | Loss: 0.22920070588588715\n",
      "Step 801 | Loss: 0.2813454866409302\n",
      "Step 901 | Loss: 0.32911911606788635\n",
      "Step 1001 | Loss: 0.2866854667663574\n",
      "Step 1101 | Loss: 0.12226073443889618\n",
      "Step 1201 | Loss: 0.20539531111717224\n",
      "Step 1301 | Loss: 0.2971833348274231\n",
      "Step 1401 | Loss: 0.32502511143684387\n",
      "Step 1501 | Loss: 0.18016771972179413\n",
      "Step 1601 | Loss: 0.3337058424949646\n",
      "Step 1701 | Loss: 0.21423770487308502\n",
      "Step 1801 | Loss: 0.18985305726528168\n",
      "Step 1901 | Loss: 0.2790493965148926\n",
      "Step 2001 | Loss: 0.31045016646385193\n",
      "Step 2101 | Loss: 0.3325906991958618\n",
      "Step 2201 | Loss: 0.2473018765449524\n",
      "Step 2301 | Loss: 0.23871465027332306\n",
      "Step 2401 | Loss: 0.27173569798469543\n",
      "0.2408141508238328 0.945\n",
      "Step 1 | Loss: 0.3612831234931946\n",
      "Step 101 | Loss: 0.3064131736755371\n",
      "Step 201 | Loss: 0.20930424332618713\n",
      "Step 301 | Loss: 0.20495489239692688\n",
      "Step 401 | Loss: 0.28401440382003784\n",
      "Step 501 | Loss: 0.24343477189540863\n",
      "Step 601 | Loss: 0.20879296958446503\n",
      "Step 701 | Loss: 0.38095903396606445\n",
      "Step 801 | Loss: 0.2753864526748657\n",
      "Step 901 | Loss: 0.2613547146320343\n",
      "Step 1001 | Loss: 0.5270852446556091\n",
      "Step 1101 | Loss: 0.3059987425804138\n",
      "Step 1201 | Loss: 0.3507903814315796\n",
      "Step 1301 | Loss: 0.2848469018936157\n",
      "Step 1401 | Loss: 0.30650994181632996\n",
      "Step 1501 | Loss: 0.19288939237594604\n",
      "Step 1601 | Loss: 0.28051817417144775\n",
      "Step 1701 | Loss: 0.2651054263114929\n",
      "Step 1801 | Loss: 0.24032384157180786\n",
      "Step 1901 | Loss: 0.32435086369514465\n",
      "Step 2001 | Loss: 0.34120020270347595\n",
      "Step 2101 | Loss: 0.3118533790111542\n",
      "Step 2201 | Loss: 0.2601001262664795\n",
      "Step 2301 | Loss: 0.27736443281173706\n",
      "Step 2401 | Loss: 0.3817911744117737\n",
      "0.24945921274854874 0.9475\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 2.625\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.375\n",
      "Step 401 | Loss: 2.25\n",
      "Step 501 | Loss: 2.375\n",
      "Step 601 | Loss: 2.0\n",
      "Step 701 | Loss: 2.125\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 2.5\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 1.625\n",
      "Step 1201 | Loss: 1.75\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 2.5\n",
      "Step 1501 | Loss: 2.125\n",
      "Step 1601 | Loss: 1.5\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 2.375\n",
      "Step 1901 | Loss: 1.5\n",
      "Step 2001 | Loss: 1.75\n",
      "Step 2101 | Loss: 1.75\n",
      "Step 2201 | Loss: 2.125\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 1.625\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.25\n",
      "Step 101 | Loss: 1.875\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 1.75\n",
      "Step 401 | Loss: 2.125\n",
      "Step 501 | Loss: 1.25\n",
      "Step 601 | Loss: 2.25\n",
      "Step 701 | Loss: 1.25\n",
      "Step 801 | Loss: 1.625\n",
      "Step 901 | Loss: 2.375\n",
      "Step 1001 | Loss: 2.25\n",
      "Step 1101 | Loss: 1.875\n",
      "Step 1201 | Loss: 1.625\n",
      "Step 1301 | Loss: 1.5\n",
      "Step 1401 | Loss: 1.75\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 2.0\n",
      "Step 1801 | Loss: 1.75\n",
      "Step 1901 | Loss: 1.875\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 1.625\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 2.625\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.5\n",
      "Step 101 | Loss: 1.625\n",
      "Step 201 | Loss: 2.875\n",
      "Step 301 | Loss: 1.625\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 1.875\n",
      "Step 701 | Loss: 2.375\n",
      "Step 801 | Loss: 1.5\n",
      "Step 901 | Loss: 2.125\n",
      "Step 1001 | Loss: 2.25\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 2.0\n",
      "Step 1401 | Loss: 2.0\n",
      "Step 1501 | Loss: 1.25\n",
      "Step 1601 | Loss: 1.75\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 2.0\n",
      "Step 2001 | Loss: 1.625\n",
      "Step 2101 | Loss: 2.0\n",
      "Step 2201 | Loss: 1.5\n",
      "Step 2301 | Loss: 1.625\n",
      "Step 2401 | Loss: 2.125\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.5\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.25\n",
      "Step 501 | Loss: 2.375\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 1.375\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 2.5\n",
      "Step 1301 | Loss: 1.5\n",
      "Step 1401 | Loss: 2.25\n",
      "Step 1501 | Loss: 2.5\n",
      "Step 1601 | Loss: 2.125\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 2.5\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 1.5\n",
      "Step 2201 | Loss: 2.0\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 1.75\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.5\n",
      "Step 201 | Loss: 1.625\n",
      "Step 301 | Loss: 1.375\n",
      "Step 401 | Loss: 1.625\n",
      "Step 501 | Loss: 2.125\n",
      "Step 601 | Loss: 1.5\n",
      "Step 701 | Loss: 2.25\n",
      "Step 801 | Loss: 2.125\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 1.625\n",
      "Step 1101 | Loss: 2.75\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 1.5\n",
      "Step 1401 | Loss: 2.375\n",
      "Step 1501 | Loss: 2.375\n",
      "Step 1601 | Loss: 1.375\n",
      "Step 1701 | Loss: 1.75\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 2.25\n",
      "Step 2001 | Loss: 1.75\n",
      "Step 2101 | Loss: 2.0\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.375\n",
      "Step 2401 | Loss: 2.125\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 1.110832929611206\n",
      "Step 101 | Loss: 0.5472934246063232\n",
      "Step 201 | Loss: 0.4011460542678833\n",
      "Step 301 | Loss: 0.4841673970222473\n",
      "Step 401 | Loss: 0.5426002740859985\n",
      "Step 501 | Loss: 0.39339587092399597\n",
      "Step 601 | Loss: 0.3793848156929016\n",
      "Step 701 | Loss: 0.2810189723968506\n",
      "Step 801 | Loss: 0.4000515043735504\n",
      "Step 901 | Loss: 0.4493171274662018\n",
      "Step 1001 | Loss: 0.36669695377349854\n",
      "Step 1101 | Loss: 0.36872202157974243\n",
      "Step 1201 | Loss: 0.3817351758480072\n",
      "Step 1301 | Loss: 0.27915528416633606\n",
      "Step 1401 | Loss: 0.337435245513916\n",
      "Step 1501 | Loss: 0.22072753310203552\n",
      "Step 1601 | Loss: 0.28387653827667236\n",
      "Step 1701 | Loss: 0.4868890047073364\n",
      "Step 1801 | Loss: 0.4147683084011078\n",
      "Step 1901 | Loss: 0.18775959312915802\n",
      "Step 2001 | Loss: 0.541653573513031\n",
      "Step 2101 | Loss: 0.5212520360946655\n",
      "Step 2201 | Loss: 0.31351128220558167\n",
      "Step 2301 | Loss: 0.4039553105831146\n",
      "Step 2401 | Loss: 0.5104941725730896\n",
      "0.2972250459049471 0.9375\n",
      "Step 1 | Loss: 1.2282220125198364\n",
      "Step 101 | Loss: 0.8033509850502014\n",
      "Step 201 | Loss: 0.8988219499588013\n",
      "Step 301 | Loss: 0.7826873064041138\n",
      "Step 401 | Loss: 0.5827544927597046\n",
      "Step 501 | Loss: 0.47820764780044556\n",
      "Step 601 | Loss: 0.4067649841308594\n",
      "Step 701 | Loss: 0.4546811282634735\n",
      "Step 801 | Loss: 0.46223387122154236\n",
      "Step 901 | Loss: 0.4040648937225342\n",
      "Step 1001 | Loss: 0.4305102527141571\n",
      "Step 1101 | Loss: 0.3940674960613251\n",
      "Step 1201 | Loss: 0.4407527446746826\n",
      "Step 1301 | Loss: 0.4744020998477936\n",
      "Step 1401 | Loss: 0.44190913438796997\n",
      "Step 1501 | Loss: 0.39917320013046265\n",
      "Step 1601 | Loss: 0.46919000148773193\n",
      "Step 1701 | Loss: 0.4198298752307892\n",
      "Step 1801 | Loss: 0.37837719917297363\n",
      "Step 1901 | Loss: 0.38489651679992676\n",
      "Step 2001 | Loss: 0.3928101658821106\n",
      "Step 2101 | Loss: 0.45463240146636963\n",
      "Step 2201 | Loss: 0.35871434211730957\n",
      "Step 2301 | Loss: 0.454092800617218\n",
      "Step 2401 | Loss: 0.41999584436416626\n",
      "0.44261067589377556 0.965\n",
      "Step 1 | Loss: 1.0190415382385254\n",
      "Step 101 | Loss: 0.854968249797821\n",
      "Step 201 | Loss: 0.5290871262550354\n",
      "Step 301 | Loss: 0.7408397197723389\n",
      "Step 401 | Loss: 0.4978152811527252\n",
      "Step 501 | Loss: 0.48424088954925537\n",
      "Step 601 | Loss: 0.4648176431655884\n",
      "Step 701 | Loss: 0.38209742307662964\n",
      "Step 801 | Loss: 0.3660816550254822\n",
      "Step 901 | Loss: 0.44032347202301025\n",
      "Step 1001 | Loss: 0.46166691184043884\n",
      "Step 1101 | Loss: 0.35648488998413086\n",
      "Step 1201 | Loss: 0.3829455077648163\n",
      "Step 1301 | Loss: 0.5421226620674133\n",
      "Step 1401 | Loss: 0.4587628245353699\n",
      "Step 1501 | Loss: 0.35244226455688477\n",
      "Step 1601 | Loss: 0.4320608079433441\n",
      "Step 1701 | Loss: 0.41872429847717285\n",
      "Step 1801 | Loss: 0.4543111026287079\n",
      "Step 1901 | Loss: 0.4157412052154541\n",
      "Step 2001 | Loss: 0.3764840364456177\n",
      "Step 2101 | Loss: 0.43713077902793884\n",
      "Step 2201 | Loss: 0.4587039053440094\n",
      "Step 2301 | Loss: 0.38037240505218506\n",
      "Step 2401 | Loss: 0.34145456552505493\n",
      "0.3479879539016457 0.9625\n",
      "Step 1 | Loss: 1.2283915281295776\n",
      "Step 101 | Loss: 0.6435290575027466\n",
      "Step 201 | Loss: 0.7056721448898315\n",
      "Step 301 | Loss: 0.673038899898529\n",
      "Step 401 | Loss: 0.4972032904624939\n",
      "Step 501 | Loss: 0.6139119863510132\n",
      "Step 601 | Loss: 0.42288511991500854\n",
      "Step 701 | Loss: 0.47062355279922485\n",
      "Step 801 | Loss: 0.4057821035385132\n",
      "Step 901 | Loss: 0.5018563866615295\n",
      "Step 1001 | Loss: 0.4855785667896271\n",
      "Step 1101 | Loss: 0.5981300473213196\n",
      "Step 1201 | Loss: 0.46565884351730347\n",
      "Step 1301 | Loss: 0.5078115463256836\n",
      "Step 1401 | Loss: 0.4207865297794342\n",
      "Step 1501 | Loss: 0.35196205973625183\n",
      "Step 1601 | Loss: 0.3033214211463928\n",
      "Step 1701 | Loss: 0.45762160420417786\n",
      "Step 1801 | Loss: 0.33161798119544983\n",
      "Step 1901 | Loss: 0.3872261047363281\n",
      "Step 2001 | Loss: 0.34475207328796387\n",
      "Step 2101 | Loss: 0.3484773635864258\n",
      "Step 2201 | Loss: 0.2467925250530243\n",
      "Step 2301 | Loss: 0.3171882927417755\n",
      "Step 2401 | Loss: 0.23298132419586182\n",
      "0.3272513295935185 0.9775\n",
      "Step 1 | Loss: 1.4390859603881836\n",
      "Step 101 | Loss: 0.8440905809402466\n",
      "Step 201 | Loss: 0.8440446853637695\n",
      "Step 301 | Loss: 0.6623831391334534\n",
      "Step 401 | Loss: 0.5654142498970032\n",
      "Step 501 | Loss: 0.5377105474472046\n",
      "Step 601 | Loss: 0.5280526876449585\n",
      "Step 701 | Loss: 0.41502848267555237\n",
      "Step 801 | Loss: 0.4758143424987793\n",
      "Step 901 | Loss: 0.4311597943305969\n",
      "Step 1001 | Loss: 0.5217945575714111\n",
      "Step 1101 | Loss: 0.4347619116306305\n",
      "Step 1201 | Loss: 0.46488288044929504\n",
      "Step 1301 | Loss: 0.39472776651382446\n",
      "Step 1401 | Loss: 0.5708823800086975\n",
      "Step 1501 | Loss: 0.492205947637558\n",
      "Step 1601 | Loss: 0.49102017283439636\n",
      "Step 1701 | Loss: 0.607900857925415\n",
      "Step 1801 | Loss: 0.3551653027534485\n",
      "Step 1901 | Loss: 0.46396932005882263\n",
      "Step 2001 | Loss: 0.3553289771080017\n",
      "Step 2101 | Loss: 0.4384280741214752\n",
      "Step 2201 | Loss: 0.39111602306365967\n",
      "Step 2301 | Loss: 0.508830726146698\n",
      "Step 2401 | Loss: 0.3976980745792389\n",
      "0.41064293505710975 0.93\n",
      "Step 1 | Loss: 2.0730929374694824\n",
      "Step 101 | Loss: 0.2402135580778122\n",
      "Step 201 | Loss: 0.26760417222976685\n",
      "Step 301 | Loss: 0.12787416577339172\n",
      "Step 401 | Loss: 0.21962492167949677\n",
      "Step 501 | Loss: 0.22237269580364227\n",
      "Step 601 | Loss: 0.17227765917778015\n",
      "Step 701 | Loss: 0.14798061549663544\n",
      "Step 801 | Loss: 0.14082291722297668\n",
      "Step 901 | Loss: 0.2761339247226715\n",
      "Step 1001 | Loss: 0.27548718452453613\n",
      "Step 1101 | Loss: 0.2124025672674179\n",
      "Step 1201 | Loss: 0.24557900428771973\n",
      "Step 1301 | Loss: 0.22970861196517944\n",
      "Step 1401 | Loss: 0.1937413215637207\n",
      "Step 1501 | Loss: 0.24394041299819946\n",
      "Step 1601 | Loss: 0.22742831707000732\n",
      "Step 1701 | Loss: 0.17954714596271515\n",
      "Step 1801 | Loss: 0.24598604440689087\n",
      "Step 1901 | Loss: 0.1675097644329071\n",
      "Step 2001 | Loss: 0.1897716373205185\n",
      "Step 2101 | Loss: 0.17733542621135712\n",
      "Step 2201 | Loss: 0.2622160017490387\n",
      "Step 2301 | Loss: 0.25044363737106323\n",
      "Step 2401 | Loss: 0.20453564822673798\n",
      "0.20981462282673494 0.9725\n",
      "Step 1 | Loss: 0.7922286987304688\n",
      "Step 101 | Loss: 0.508514404296875\n",
      "Step 201 | Loss: 0.35465410351753235\n",
      "Step 301 | Loss: 0.40949133038520813\n",
      "Step 401 | Loss: 0.3728649914264679\n",
      "Step 501 | Loss: 0.30007070302963257\n",
      "Step 601 | Loss: 0.4335680603981018\n",
      "Step 701 | Loss: 0.34605517983436584\n",
      "Step 801 | Loss: 0.23784634470939636\n",
      "Step 901 | Loss: 0.35850298404693604\n",
      "Step 1001 | Loss: 0.35189589858055115\n",
      "Step 1101 | Loss: 0.3070000410079956\n",
      "Step 1201 | Loss: 0.35165825486183167\n",
      "Step 1301 | Loss: 0.29875025153160095\n",
      "Step 1401 | Loss: 0.2956410050392151\n",
      "Step 1501 | Loss: 0.3461979627609253\n",
      "Step 1601 | Loss: 0.3235882818698883\n",
      "Step 1701 | Loss: 0.3027934432029724\n",
      "Step 1801 | Loss: 0.2991766333580017\n",
      "Step 1901 | Loss: 0.23847967386245728\n",
      "Step 2001 | Loss: 0.36972513794898987\n",
      "Step 2101 | Loss: 0.33910977840423584\n",
      "Step 2201 | Loss: 0.4932914674282074\n",
      "Step 2301 | Loss: 0.3898750841617584\n",
      "Step 2401 | Loss: 0.43408656120300293\n",
      "0.30968151112280096 0.935\n",
      "Step 1 | Loss: 0.7706077098846436\n",
      "Step 101 | Loss: 0.36223092675209045\n",
      "Step 201 | Loss: 0.28246989846229553\n",
      "Step 301 | Loss: 0.25773483514785767\n",
      "Step 401 | Loss: 0.2955693006515503\n",
      "Step 501 | Loss: 0.38847485184669495\n",
      "Step 601 | Loss: 0.24964351952075958\n",
      "Step 701 | Loss: 0.13787731528282166\n",
      "Step 801 | Loss: 0.15957105159759521\n",
      "Step 901 | Loss: 0.23333267867565155\n",
      "Step 1001 | Loss: 0.2283858060836792\n",
      "Step 1101 | Loss: 0.2559189200401306\n",
      "Step 1201 | Loss: 0.22318008542060852\n",
      "Step 1301 | Loss: 0.1758524626493454\n",
      "Step 1401 | Loss: 0.15024231374263763\n",
      "Step 1501 | Loss: 0.2479965090751648\n",
      "Step 1601 | Loss: 0.190199077129364\n",
      "Step 1701 | Loss: 0.20611314475536346\n",
      "Step 1801 | Loss: 0.16832616925239563\n",
      "Step 1901 | Loss: 0.2189144790172577\n",
      "Step 2001 | Loss: 0.17859026789665222\n",
      "Step 2101 | Loss: 0.24528895318508148\n",
      "Step 2201 | Loss: 0.15526479482650757\n",
      "Step 2301 | Loss: 0.25863853096961975\n",
      "Step 2401 | Loss: 0.3026530146598816\n",
      "0.18716922545393103 0.9825\n",
      "Step 1 | Loss: 0.8018288612365723\n",
      "Step 101 | Loss: 0.4047248661518097\n",
      "Step 201 | Loss: 0.23440393805503845\n",
      "Step 301 | Loss: 0.31649619340896606\n",
      "Step 401 | Loss: 0.24539609253406525\n",
      "Step 501 | Loss: 0.5753651857376099\n",
      "Step 601 | Loss: 0.318441778421402\n",
      "Step 701 | Loss: 0.21693629026412964\n",
      "Step 801 | Loss: 0.4075130522251129\n",
      "Step 901 | Loss: 0.2835146188735962\n",
      "Step 1001 | Loss: 0.3256855010986328\n",
      "Step 1101 | Loss: 0.48145896196365356\n",
      "Step 1201 | Loss: 0.42898839712142944\n",
      "Step 1301 | Loss: 0.3742840886116028\n",
      "Step 1401 | Loss: 0.3497433066368103\n",
      "Step 1501 | Loss: 0.22031337022781372\n",
      "Step 1601 | Loss: 0.3373778462409973\n",
      "Step 1701 | Loss: 0.33193832635879517\n",
      "Step 1801 | Loss: 0.2959643602371216\n",
      "Step 1901 | Loss: 0.2557554841041565\n",
      "Step 2001 | Loss: 0.36549845337867737\n",
      "Step 2101 | Loss: 0.24645200371742249\n",
      "Step 2201 | Loss: 0.40843239426612854\n",
      "Step 2301 | Loss: 0.30893874168395996\n",
      "Step 2401 | Loss: 0.27468493580818176\n",
      "0.3052377253528104 0.9325\n",
      "Step 1 | Loss: 0.6757896542549133\n",
      "Step 101 | Loss: 0.4282873868942261\n",
      "Step 201 | Loss: 0.28544408082962036\n",
      "Step 301 | Loss: 0.5060738325119019\n",
      "Step 401 | Loss: 0.3218918442726135\n",
      "Step 501 | Loss: 0.33430665731430054\n",
      "Step 601 | Loss: 0.2874412536621094\n",
      "Step 701 | Loss: 0.5056299567222595\n",
      "Step 801 | Loss: 0.27289295196533203\n",
      "Step 901 | Loss: 0.3205554485321045\n",
      "Step 1001 | Loss: 0.23314335942268372\n",
      "Step 1101 | Loss: 0.44716718792915344\n",
      "Step 1201 | Loss: 0.265129953622818\n",
      "Step 1301 | Loss: 0.47866466641426086\n",
      "Step 1401 | Loss: 0.3627515733242035\n",
      "Step 1501 | Loss: 0.355108380317688\n",
      "Step 1601 | Loss: 0.34733957052230835\n",
      "Step 1701 | Loss: 0.24839968979358673\n",
      "Step 1801 | Loss: 0.40307968854904175\n",
      "Step 1901 | Loss: 0.41752514243125916\n",
      "Step 2001 | Loss: 0.2717457413673401\n",
      "Step 2101 | Loss: 0.37058699131011963\n",
      "Step 2201 | Loss: 0.4355555772781372\n",
      "Step 2301 | Loss: 0.31359145045280457\n",
      "Step 2401 | Loss: 0.39013203978538513\n",
      "0.3049693926586038 0.93\n",
      "Step 1 | Loss: 1.6848160028457642\n",
      "Step 101 | Loss: 1.7618921995162964\n",
      "Step 201 | Loss: 0.5902625322341919\n",
      "Step 301 | Loss: 0.6051537394523621\n",
      "Step 401 | Loss: 0.5061014890670776\n",
      "Step 501 | Loss: 0.5065265893936157\n",
      "Step 601 | Loss: 0.4145735502243042\n",
      "Step 701 | Loss: 0.7349129915237427\n",
      "Step 801 | Loss: 0.5584891438484192\n",
      "Step 901 | Loss: 0.5673603415489197\n",
      "Step 1001 | Loss: 0.698305606842041\n",
      "Step 1101 | Loss: 0.4675091505050659\n",
      "Step 1201 | Loss: 0.5953686237335205\n",
      "Step 1301 | Loss: 0.5368944406509399\n",
      "Step 1401 | Loss: 0.5917356014251709\n",
      "Step 1501 | Loss: 0.533182680606842\n",
      "Step 1601 | Loss: 0.4715389311313629\n",
      "Step 1701 | Loss: 0.5723721981048584\n",
      "Step 1801 | Loss: 0.440779447555542\n",
      "Step 1901 | Loss: 0.6059349179267883\n",
      "Step 2001 | Loss: 0.45079442858695984\n",
      "Step 2101 | Loss: 0.4334580898284912\n",
      "Step 2201 | Loss: 0.5398614406585693\n",
      "Step 2301 | Loss: 0.4684257209300995\n",
      "Step 2401 | Loss: 0.42956727743148804\n",
      "0.5203223409411506 0.8975\n",
      "Step 1 | Loss: 1.2853100299835205\n",
      "Step 101 | Loss: 0.7591150403022766\n",
      "Step 201 | Loss: 0.5934486389160156\n",
      "Step 301 | Loss: 0.6411626935005188\n",
      "Step 401 | Loss: 0.5650188326835632\n",
      "Step 501 | Loss: 0.6326603889465332\n",
      "Step 601 | Loss: 0.6102211475372314\n",
      "Step 701 | Loss: 0.6357851028442383\n",
      "Step 801 | Loss: 0.5296807289123535\n",
      "Step 901 | Loss: 0.5710263252258301\n",
      "Step 1001 | Loss: 0.6011888384819031\n",
      "Step 1101 | Loss: 0.5653972029685974\n",
      "Step 1201 | Loss: 0.6863163709640503\n",
      "Step 1301 | Loss: 0.585856556892395\n",
      "Step 1401 | Loss: 0.5628862977027893\n",
      "Step 1501 | Loss: 0.5633554458618164\n",
      "Step 1601 | Loss: 0.487771600484848\n",
      "Step 1701 | Loss: 0.5283399224281311\n",
      "Step 1801 | Loss: 0.537520170211792\n",
      "Step 1901 | Loss: 0.47896310687065125\n",
      "Step 2001 | Loss: 0.47497445344924927\n",
      "Step 2101 | Loss: 0.6353363394737244\n",
      "Step 2201 | Loss: 0.5026395320892334\n",
      "Step 2301 | Loss: 0.5452532172203064\n",
      "Step 2401 | Loss: 0.5022907257080078\n",
      "0.5415367537133537 0.9175\n",
      "Step 1 | Loss: 1.762194275856018\n",
      "Step 101 | Loss: 1.1980797052383423\n",
      "Step 201 | Loss: 0.9615945816040039\n",
      "Step 301 | Loss: 0.6648930311203003\n",
      "Step 401 | Loss: 0.47387200593948364\n",
      "Step 501 | Loss: 0.5304487347602844\n",
      "Step 601 | Loss: 0.5361321568489075\n",
      "Step 701 | Loss: 0.4908234179019928\n",
      "Step 801 | Loss: 0.48054397106170654\n",
      "Step 901 | Loss: 0.4798906147480011\n",
      "Step 1001 | Loss: 0.4706645607948303\n",
      "Step 1101 | Loss: 0.5449168086051941\n",
      "Step 1201 | Loss: 0.475298136472702\n",
      "Step 1301 | Loss: 0.42198798060417175\n",
      "Step 1401 | Loss: 0.43224218487739563\n",
      "Step 1501 | Loss: 0.4023055136203766\n",
      "Step 1601 | Loss: 0.5067310333251953\n",
      "Step 1701 | Loss: 0.42467644810676575\n",
      "Step 1801 | Loss: 0.49690717458724976\n",
      "Step 1901 | Loss: 0.408527672290802\n",
      "Step 2001 | Loss: 0.4470055401325226\n",
      "Step 2101 | Loss: 0.3972001075744629\n",
      "Step 2201 | Loss: 0.45365381240844727\n",
      "Step 2301 | Loss: 0.42702028155326843\n",
      "Step 2401 | Loss: 0.4685056805610657\n",
      "0.4480026244001781 0.96\n",
      "Step 1 | Loss: 1.2020299434661865\n",
      "Step 101 | Loss: 1.3503353595733643\n",
      "Step 201 | Loss: 0.9504144191741943\n",
      "Step 301 | Loss: 0.7789506316184998\n",
      "Step 401 | Loss: 0.6265153884887695\n",
      "Step 501 | Loss: 0.5760520696640015\n",
      "Step 601 | Loss: 0.6298997402191162\n",
      "Step 701 | Loss: 0.469740092754364\n",
      "Step 801 | Loss: 0.5852596759796143\n",
      "Step 901 | Loss: 0.5664164423942566\n",
      "Step 1001 | Loss: 0.4918840825557709\n",
      "Step 1101 | Loss: 0.5861099362373352\n",
      "Step 1201 | Loss: 0.4525788128376007\n",
      "Step 1301 | Loss: 0.49984684586524963\n",
      "Step 1401 | Loss: 0.5730159282684326\n",
      "Step 1501 | Loss: 0.5276793837547302\n",
      "Step 1601 | Loss: 0.6345548033714294\n",
      "Step 1701 | Loss: 0.6050041913986206\n",
      "Step 1801 | Loss: 0.5514217615127563\n",
      "Step 1901 | Loss: 0.560425341129303\n",
      "Step 2001 | Loss: 0.5512862205505371\n",
      "Step 2101 | Loss: 0.5777936577796936\n",
      "Step 2201 | Loss: 0.5659645795822144\n",
      "Step 2301 | Loss: 0.5524815917015076\n",
      "Step 2401 | Loss: 0.5252774953842163\n",
      "0.5733023525078281 0.9\n",
      "Step 1 | Loss: 1.1411551237106323\n",
      "Step 101 | Loss: 1.8502652645111084\n",
      "Step 201 | Loss: 0.8638052940368652\n",
      "Step 301 | Loss: 0.5256531238555908\n",
      "Step 401 | Loss: 0.4601801931858063\n",
      "Step 501 | Loss: 0.3399679660797119\n",
      "Step 601 | Loss: 0.3236285150051117\n",
      "Step 701 | Loss: 0.454143762588501\n",
      "Step 801 | Loss: 0.40737760066986084\n",
      "Step 901 | Loss: 0.424324095249176\n",
      "Step 1001 | Loss: 0.3995237350463867\n",
      "Step 1101 | Loss: 0.3649051785469055\n",
      "Step 1201 | Loss: 0.4458805322647095\n",
      "Step 1301 | Loss: 0.3758007884025574\n",
      "Step 1401 | Loss: 0.34884583950042725\n",
      "Step 1501 | Loss: 0.4359455108642578\n",
      "Step 1601 | Loss: 0.3993711471557617\n",
      "Step 1701 | Loss: 0.36094212532043457\n",
      "Step 1801 | Loss: 0.3263757824897766\n",
      "Step 1901 | Loss: 0.3097185492515564\n",
      "Step 2001 | Loss: 0.38985759019851685\n",
      "Step 2101 | Loss: 0.445599228143692\n",
      "Step 2201 | Loss: 0.33641287684440613\n",
      "Step 2301 | Loss: 0.440017968416214\n",
      "Step 2401 | Loss: 0.47920727729797363\n",
      "0.4064355715856029 0.9475\n",
      "Step 1 | Loss: 0.8582230806350708\n",
      "Step 101 | Loss: 0.9113602042198181\n",
      "Step 201 | Loss: 0.3977065086364746\n",
      "Step 301 | Loss: 0.33397176861763\n",
      "Step 401 | Loss: 0.4424825608730316\n",
      "Step 501 | Loss: 0.39966458082199097\n",
      "Step 601 | Loss: 0.3428090214729309\n",
      "Step 701 | Loss: 0.39645594358444214\n",
      "Step 801 | Loss: 0.2749750316143036\n",
      "Step 901 | Loss: 0.20655307173728943\n",
      "Step 1001 | Loss: 0.3139723241329193\n",
      "Step 1101 | Loss: 0.4067978262901306\n",
      "Step 1201 | Loss: 0.44301366806030273\n",
      "Step 1301 | Loss: 0.30335962772369385\n",
      "Step 1401 | Loss: 0.2592756152153015\n",
      "Step 1501 | Loss: 0.3161921501159668\n",
      "Step 1601 | Loss: 0.3043174743652344\n",
      "Step 1701 | Loss: 0.2822832763195038\n",
      "Step 1801 | Loss: 0.3030604422092438\n",
      "Step 1901 | Loss: 0.27830350399017334\n",
      "Step 2001 | Loss: 0.38164129853248596\n",
      "Step 2101 | Loss: 0.4761239290237427\n",
      "Step 2201 | Loss: 0.263830304145813\n",
      "Step 2301 | Loss: 0.3129655420780182\n",
      "Step 2401 | Loss: 0.3639427125453949\n",
      "0.30677967931500694 0.94\n",
      "Step 1 | Loss: 1.2992620468139648\n",
      "Step 101 | Loss: 0.6470769643783569\n",
      "Step 201 | Loss: 0.6065622568130493\n",
      "Step 301 | Loss: 0.47863253951072693\n",
      "Step 401 | Loss: 0.48743337392807007\n",
      "Step 501 | Loss: 0.3755239248275757\n",
      "Step 601 | Loss: 0.355920672416687\n",
      "Step 701 | Loss: 0.3890911340713501\n",
      "Step 801 | Loss: 0.2160424143075943\n",
      "Step 901 | Loss: 0.17057359218597412\n",
      "Step 1001 | Loss: 0.15614593029022217\n",
      "Step 1101 | Loss: 0.1726822555065155\n",
      "Step 1201 | Loss: 0.2140890508890152\n",
      "Step 1301 | Loss: 0.24366512894630432\n",
      "Step 1401 | Loss: 0.1917424499988556\n",
      "Step 1501 | Loss: 0.20329634845256805\n",
      "Step 1601 | Loss: 0.25980204343795776\n",
      "Step 1701 | Loss: 0.11682659387588501\n",
      "Step 1801 | Loss: 0.2065955251455307\n",
      "Step 1901 | Loss: 0.2764222323894501\n",
      "Step 2001 | Loss: 0.16120290756225586\n",
      "Step 2101 | Loss: 0.23507249355316162\n",
      "Step 2201 | Loss: 0.2500406503677368\n",
      "Step 2301 | Loss: 0.23678749799728394\n",
      "Step 2401 | Loss: 0.15725256502628326\n",
      "0.1767578114962457 0.9825\n",
      "Step 1 | Loss: 0.8386495113372803\n",
      "Step 101 | Loss: 0.4376395642757416\n",
      "Step 201 | Loss: 0.32639312744140625\n",
      "Step 301 | Loss: 0.4079279601573944\n",
      "Step 401 | Loss: 0.3258984386920929\n",
      "Step 501 | Loss: 0.465263307094574\n",
      "Step 601 | Loss: 0.31928732991218567\n",
      "Step 701 | Loss: 0.28974077105522156\n",
      "Step 801 | Loss: 0.22062379121780396\n",
      "Step 901 | Loss: 0.30563637614250183\n",
      "Step 1001 | Loss: 0.37259161472320557\n",
      "Step 1101 | Loss: 0.18891006708145142\n",
      "Step 1201 | Loss: 0.31844261288642883\n",
      "Step 1301 | Loss: 0.27760088443756104\n",
      "Step 1401 | Loss: 0.271818608045578\n",
      "Step 1501 | Loss: 0.32013973593711853\n",
      "Step 1601 | Loss: 0.26735448837280273\n",
      "Step 1701 | Loss: 0.24619990587234497\n",
      "Step 1801 | Loss: 0.34923338890075684\n",
      "Step 1901 | Loss: 0.24742341041564941\n",
      "Step 2001 | Loss: 0.25417542457580566\n",
      "Step 2101 | Loss: 0.28483232855796814\n",
      "Step 2201 | Loss: 0.26584991812705994\n",
      "Step 2301 | Loss: 0.3010854423046112\n",
      "Step 2401 | Loss: 0.30095088481903076\n",
      "0.26684094354837096 0.985\n",
      "Step 1 | Loss: 0.7804133296012878\n",
      "Step 101 | Loss: 0.512081503868103\n",
      "Step 201 | Loss: 0.4314524829387665\n",
      "Step 301 | Loss: 0.39337316155433655\n",
      "Step 401 | Loss: 0.30880576372146606\n",
      "Step 501 | Loss: 0.38801154494285583\n",
      "Step 601 | Loss: 0.3045468032360077\n",
      "Step 701 | Loss: 0.32135578989982605\n",
      "Step 801 | Loss: 0.29601433873176575\n",
      "Step 901 | Loss: 0.3683489263057709\n",
      "Step 1001 | Loss: 0.36633044481277466\n",
      "Step 1101 | Loss: 0.27819472551345825\n",
      "Step 1201 | Loss: 0.19372081756591797\n",
      "Step 1301 | Loss: 0.27882951498031616\n",
      "Step 1401 | Loss: 0.32205861806869507\n",
      "Step 1501 | Loss: 0.18662914633750916\n",
      "Step 1601 | Loss: 0.2862360179424286\n",
      "Step 1701 | Loss: 0.24690763652324677\n",
      "Step 1801 | Loss: 0.3402499258518219\n",
      "Step 1901 | Loss: 0.26020246744155884\n",
      "Step 2001 | Loss: 0.26864317059516907\n",
      "Step 2101 | Loss: 0.34045225381851196\n",
      "Step 2201 | Loss: 0.23684753477573395\n",
      "Step 2301 | Loss: 0.23067748546600342\n",
      "Step 2401 | Loss: 0.32873886823654175\n",
      "0.23987995539646934 0.97\n",
      "Step 1 | Loss: 0.6974962949752808\n",
      "Step 101 | Loss: 0.3138759136199951\n",
      "Step 201 | Loss: 0.3372402787208557\n",
      "Step 301 | Loss: 0.26095902919769287\n",
      "Step 401 | Loss: 0.3217064440250397\n",
      "Step 501 | Loss: 0.29263168573379517\n",
      "Step 601 | Loss: 0.2719508409500122\n",
      "Step 701 | Loss: 0.32210636138916016\n",
      "Step 801 | Loss: 0.3024172782897949\n",
      "Step 901 | Loss: 0.25418365001678467\n",
      "Step 1001 | Loss: 0.20463299751281738\n",
      "Step 1101 | Loss: 0.29696306586265564\n",
      "Step 1201 | Loss: 0.2017492949962616\n",
      "Step 1301 | Loss: 0.2859137952327728\n",
      "Step 1401 | Loss: 0.3575759530067444\n",
      "Step 1501 | Loss: 0.2926230728626251\n",
      "Step 1601 | Loss: 0.16221316158771515\n",
      "Step 1701 | Loss: 0.30861759185791016\n",
      "Step 1801 | Loss: 0.20596179366111755\n",
      "Step 1901 | Loss: 0.24256153404712677\n",
      "Step 2001 | Loss: 0.2721880078315735\n",
      "Step 2101 | Loss: 0.2858981490135193\n",
      "Step 2201 | Loss: 0.27312156558036804\n",
      "Step 2301 | Loss: 0.2509517967700958\n",
      "Step 2401 | Loss: 0.3712179660797119\n",
      "0.21821244412786645 0.96\n",
      "Step 1 | Loss: 2.5\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 2.125\n",
      "Step 401 | Loss: 1.75\n",
      "Step 501 | Loss: 1.375\n",
      "Step 601 | Loss: 1.75\n",
      "Step 701 | Loss: 1.625\n",
      "Step 801 | Loss: 2.0\n",
      "Step 901 | Loss: 1.5\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 2.125\n",
      "Step 1301 | Loss: 1.375\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 2.0\n",
      "Step 1601 | Loss: 2.375\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 1.75\n",
      "Step 1901 | Loss: 1.625\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 1.25\n",
      "Step 2301 | Loss: 1.25\n",
      "Step 2401 | Loss: 2.875\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 1.875\n",
      "Step 101 | Loss: 1.875\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 1.875\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 2.125\n",
      "Step 901 | Loss: 2.125\n",
      "Step 1001 | Loss: 2.0\n",
      "Step 1101 | Loss: 1.75\n",
      "Step 1201 | Loss: 1.75\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 2.875\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 1.375\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 1.875\n",
      "Step 2101 | Loss: 1.75\n",
      "Step 2201 | Loss: 1.625\n",
      "Step 2301 | Loss: 1.625\n",
      "Step 2401 | Loss: 1.75\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 1.875\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 2.125\n",
      "Step 401 | Loss: 2.125\n",
      "Step 501 | Loss: 2.375\n",
      "Step 601 | Loss: 1.25\n",
      "Step 701 | Loss: 2.5\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 1.75\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 1.875\n",
      "Step 1201 | Loss: 2.625\n",
      "Step 1301 | Loss: 1.75\n",
      "Step 1401 | Loss: 1.5\n",
      "Step 1501 | Loss: 1.5\n",
      "Step 1601 | Loss: 2.0\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 2.5\n",
      "Step 1901 | Loss: 1.125\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 1.875\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 1.625\n",
      "Step 301 | Loss: 2.5\n",
      "Step 401 | Loss: 2.5\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 1.875\n",
      "Step 901 | Loss: 1.625\n",
      "Step 1001 | Loss: 1.625\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 2.625\n",
      "Step 1401 | Loss: 2.25\n",
      "Step 1501 | Loss: 1.875\n",
      "Step 1601 | Loss: 1.75\n",
      "Step 1701 | Loss: 2.0\n",
      "Step 1801 | Loss: 1.125\n",
      "Step 1901 | Loss: 2.375\n",
      "Step 2001 | Loss: 1.625\n",
      "Step 2101 | Loss: 2.0\n",
      "Step 2201 | Loss: 2.0\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 2.0\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 1.75\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 1.375\n",
      "Step 401 | Loss: 2.375\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.625\n",
      "Step 801 | Loss: 1.625\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 1.875\n",
      "Step 1101 | Loss: 2.875\n",
      "Step 1201 | Loss: 2.75\n",
      "Step 1301 | Loss: 2.0\n",
      "Step 1401 | Loss: 2.0\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 2.0\n",
      "Step 1901 | Loss: 2.5\n",
      "Step 2001 | Loss: 2.5\n",
      "Step 2101 | Loss: 2.125\n",
      "Step 2201 | Loss: 1.375\n",
      "Step 2301 | Loss: 1.75\n",
      "Step 2401 | Loss: 2.125\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.0373854637145996\n",
      "Step 101 | Loss: 0.9270020723342896\n",
      "Step 201 | Loss: 0.8161019682884216\n",
      "Step 301 | Loss: 0.8818008899688721\n",
      "Step 401 | Loss: 0.8317078351974487\n",
      "Step 501 | Loss: 0.7624083757400513\n",
      "Step 601 | Loss: 0.8008354902267456\n",
      "Step 701 | Loss: 0.8805855512619019\n",
      "Step 801 | Loss: 0.8055359125137329\n",
      "Step 901 | Loss: 0.8644179105758667\n",
      "Step 1001 | Loss: 0.9104199409484863\n",
      "Step 1101 | Loss: 0.7512632012367249\n",
      "Step 1201 | Loss: 0.8467495441436768\n",
      "Step 1301 | Loss: 0.9185527563095093\n",
      "Step 1401 | Loss: 0.827897310256958\n",
      "Step 1501 | Loss: 0.8670001029968262\n",
      "Step 1601 | Loss: 0.7584439516067505\n",
      "Step 1701 | Loss: 0.8100796341896057\n",
      "Step 1801 | Loss: 0.8813650012016296\n",
      "Step 1901 | Loss: 0.8506794571876526\n",
      "Step 2001 | Loss: 0.8285653591156006\n",
      "Step 2101 | Loss: 0.8201533555984497\n",
      "Step 2201 | Loss: 0.7517209053039551\n",
      "Step 2301 | Loss: 0.9173120856285095\n",
      "Step 2401 | Loss: 0.7554287314414978\n",
      "0.8216001820432118 0.7475\n",
      "Step 1 | Loss: 1.6306501626968384\n",
      "Step 101 | Loss: 0.9580581188201904\n",
      "Step 201 | Loss: 0.896781861782074\n",
      "Step 301 | Loss: 0.7664747834205627\n",
      "Step 401 | Loss: 0.8550129532814026\n",
      "Step 501 | Loss: 0.7732493281364441\n",
      "Step 601 | Loss: 0.7936406135559082\n",
      "Step 701 | Loss: 0.8292860984802246\n",
      "Step 801 | Loss: 0.8690290451049805\n",
      "Step 901 | Loss: 0.8349717855453491\n",
      "Step 1001 | Loss: 0.8930152654647827\n",
      "Step 1101 | Loss: 0.8497476577758789\n",
      "Step 1201 | Loss: 0.8443818092346191\n",
      "Step 1301 | Loss: 0.8053286075592041\n",
      "Step 1401 | Loss: 0.7970278859138489\n",
      "Step 1501 | Loss: 0.8672400116920471\n",
      "Step 1601 | Loss: 0.8744967579841614\n",
      "Step 1701 | Loss: 0.8072508573532104\n",
      "Step 1801 | Loss: 0.865105390548706\n",
      "Step 1901 | Loss: 0.7562441229820251\n",
      "Step 2001 | Loss: 0.827118992805481\n",
      "Step 2101 | Loss: 0.8600946068763733\n",
      "Step 2201 | Loss: 0.8786038756370544\n",
      "Step 2301 | Loss: 0.7654696106910706\n",
      "Step 2401 | Loss: 0.8412470817565918\n",
      "0.8204327876488792 0.715\n",
      "Step 1 | Loss: 1.858866572380066\n",
      "Step 101 | Loss: 0.7976135015487671\n",
      "Step 201 | Loss: 0.8044760823249817\n",
      "Step 301 | Loss: 0.8006580471992493\n",
      "Step 401 | Loss: 0.7722288370132446\n",
      "Step 501 | Loss: 0.9183759689331055\n",
      "Step 601 | Loss: 0.7521781325340271\n",
      "Step 701 | Loss: 0.8743764162063599\n",
      "Step 801 | Loss: 0.9634265899658203\n",
      "Step 901 | Loss: 0.8498106002807617\n",
      "Step 1001 | Loss: 0.841097354888916\n",
      "Step 1101 | Loss: 0.8205877542495728\n",
      "Step 1201 | Loss: 0.7458149790763855\n",
      "Step 1301 | Loss: 0.7703400254249573\n",
      "Step 1401 | Loss: 0.8034266233444214\n",
      "Step 1501 | Loss: 0.830376148223877\n",
      "Step 1601 | Loss: 0.8638020753860474\n",
      "Step 1701 | Loss: 0.9071200489997864\n",
      "Step 1801 | Loss: 0.7648642063140869\n",
      "Step 1901 | Loss: 0.7998895049095154\n",
      "Step 2001 | Loss: 0.7822605967521667\n",
      "Step 2101 | Loss: 0.8572757244110107\n",
      "Step 2201 | Loss: 0.8990027904510498\n",
      "Step 2301 | Loss: 0.8423991203308105\n",
      "Step 2401 | Loss: 0.8649569153785706\n",
      "0.8241325514635507 0.7625\n",
      "Step 1 | Loss: 1.3151068687438965\n",
      "Step 101 | Loss: 0.9400876760482788\n",
      "Step 201 | Loss: 0.9084073901176453\n",
      "Step 301 | Loss: 0.8105063438415527\n",
      "Step 401 | Loss: 0.8781211972236633\n",
      "Step 501 | Loss: 0.7968271374702454\n",
      "Step 601 | Loss: 0.8903594613075256\n",
      "Step 701 | Loss: 0.8580350875854492\n",
      "Step 801 | Loss: 0.8634122610092163\n",
      "Step 901 | Loss: 0.8671061992645264\n",
      "Step 1001 | Loss: 0.8471481800079346\n",
      "Step 1101 | Loss: 0.8840527534484863\n",
      "Step 1201 | Loss: 0.8342803716659546\n",
      "Step 1301 | Loss: 0.8500162363052368\n",
      "Step 1401 | Loss: 0.7799373269081116\n",
      "Step 1501 | Loss: 0.8318754434585571\n",
      "Step 1601 | Loss: 0.8152273297309875\n",
      "Step 1701 | Loss: 0.9171258211135864\n",
      "Step 1801 | Loss: 0.8451740741729736\n",
      "Step 1901 | Loss: 0.8414973616600037\n",
      "Step 2001 | Loss: 0.8462920188903809\n",
      "Step 2101 | Loss: 0.888481616973877\n",
      "Step 2201 | Loss: 0.8579439520835876\n",
      "Step 2301 | Loss: 0.840093731880188\n",
      "Step 2401 | Loss: 0.8800937533378601\n",
      "0.8206178854878302 0.735\n",
      "Step 1 | Loss: 1.2895444631576538\n",
      "Step 101 | Loss: 0.9656874537467957\n",
      "Step 201 | Loss: 0.9611942768096924\n",
      "Step 301 | Loss: 0.9013755917549133\n",
      "Step 401 | Loss: 0.9262052178382874\n",
      "Step 501 | Loss: 0.8241610527038574\n",
      "Step 601 | Loss: 0.8428211212158203\n",
      "Step 701 | Loss: 0.7984839677810669\n",
      "Step 801 | Loss: 0.8110893368721008\n",
      "Step 901 | Loss: 0.8069464564323425\n",
      "Step 1001 | Loss: 0.7393782734870911\n",
      "Step 1101 | Loss: 0.7880005240440369\n",
      "Step 1201 | Loss: 0.8113597631454468\n",
      "Step 1301 | Loss: 0.9206326007843018\n",
      "Step 1401 | Loss: 0.7230309247970581\n",
      "Step 1501 | Loss: 0.8667300939559937\n",
      "Step 1601 | Loss: 0.8536719679832458\n",
      "Step 1701 | Loss: 0.8301419019699097\n",
      "Step 1801 | Loss: 0.7678600549697876\n",
      "Step 1901 | Loss: 0.8260540962219238\n",
      "Step 2001 | Loss: 0.8111852407455444\n",
      "Step 2101 | Loss: 0.8294644951820374\n",
      "Step 2201 | Loss: 0.8433806896209717\n",
      "Step 2301 | Loss: 0.8115803599357605\n",
      "Step 2401 | Loss: 0.8367288112640381\n",
      "0.8211023030714467 0.7425\n",
      "Step 1 | Loss: 1.1180733442306519\n",
      "Step 101 | Loss: 0.7186523079872131\n",
      "Step 201 | Loss: 0.9580651521682739\n",
      "Step 301 | Loss: 0.9497147798538208\n",
      "Step 401 | Loss: 1.0233720541000366\n",
      "Step 501 | Loss: 0.7900345325469971\n",
      "Step 601 | Loss: 0.8514387607574463\n",
      "Step 701 | Loss: 0.9717403054237366\n",
      "Step 801 | Loss: 0.8641258478164673\n",
      "Step 901 | Loss: 0.9151667356491089\n",
      "Step 1001 | Loss: 0.9434539079666138\n",
      "Step 1101 | Loss: 1.0887311697006226\n",
      "Step 1201 | Loss: 0.7751109004020691\n",
      "Step 1301 | Loss: 0.7482690811157227\n",
      "Step 1401 | Loss: 1.0134222507476807\n",
      "Step 1501 | Loss: 1.036519169807434\n",
      "Step 1601 | Loss: 0.7481083273887634\n",
      "Step 1701 | Loss: 1.0240368843078613\n",
      "Step 1801 | Loss: 0.823631763458252\n",
      "Step 1901 | Loss: 0.8462815284729004\n",
      "Step 2001 | Loss: 0.9272081255912781\n",
      "Step 2101 | Loss: 0.9821768999099731\n",
      "Step 2201 | Loss: 0.8728340864181519\n",
      "Step 2301 | Loss: 0.854211151599884\n",
      "Step 2401 | Loss: 0.9293107390403748\n",
      "0.8578679278696574 0.725\n",
      "Step 1 | Loss: 1.1918108463287354\n",
      "Step 101 | Loss: 0.9383047223091125\n",
      "Step 201 | Loss: 0.735360324382782\n",
      "Step 301 | Loss: 0.8622928261756897\n",
      "Step 401 | Loss: 0.7239565849304199\n",
      "Step 501 | Loss: 1.0551652908325195\n",
      "Step 601 | Loss: 0.6008142828941345\n",
      "Step 701 | Loss: 0.908111572265625\n",
      "Step 801 | Loss: 1.0204397439956665\n",
      "Step 901 | Loss: 1.0137999057769775\n",
      "Step 1001 | Loss: 1.0313472747802734\n",
      "Step 1101 | Loss: 0.9690839648246765\n",
      "Step 1201 | Loss: 0.9544739723205566\n",
      "Step 1301 | Loss: 0.8583159446716309\n",
      "Step 1401 | Loss: 0.977264404296875\n",
      "Step 1501 | Loss: 0.9702169299125671\n",
      "Step 1601 | Loss: 0.8830336928367615\n",
      "Step 1701 | Loss: 0.7920798063278198\n",
      "Step 1801 | Loss: 0.9531970620155334\n",
      "Step 1901 | Loss: 1.0124249458312988\n",
      "Step 2001 | Loss: 0.8573784828186035\n",
      "Step 2101 | Loss: 0.955917239189148\n",
      "Step 2201 | Loss: 0.8348188996315002\n",
      "Step 2301 | Loss: 1.1304000616073608\n",
      "Step 2401 | Loss: 0.7686790823936462\n",
      "0.8615694850946538 0.7275\n",
      "Step 1 | Loss: 0.9547744393348694\n",
      "Step 101 | Loss: 1.079161286354065\n",
      "Step 201 | Loss: 1.2571666240692139\n",
      "Step 301 | Loss: 0.7998000383377075\n",
      "Step 401 | Loss: 0.9320574402809143\n",
      "Step 501 | Loss: 0.9534732699394226\n",
      "Step 601 | Loss: 0.9394038915634155\n",
      "Step 701 | Loss: 0.7070326805114746\n",
      "Step 801 | Loss: 0.8004023432731628\n",
      "Step 901 | Loss: 0.8482703566551208\n",
      "Step 1001 | Loss: 0.8144588470458984\n",
      "Step 1101 | Loss: 1.1951013803482056\n",
      "Step 1201 | Loss: 0.7146398425102234\n",
      "Step 1301 | Loss: 1.050898551940918\n",
      "Step 1401 | Loss: 0.8796882033348083\n",
      "Step 1501 | Loss: 0.7940711975097656\n",
      "Step 1601 | Loss: 0.7553836703300476\n",
      "Step 1701 | Loss: 1.0276999473571777\n",
      "Step 1801 | Loss: 0.971175491809845\n",
      "Step 1901 | Loss: 0.948703408241272\n",
      "Step 2001 | Loss: 1.093949794769287\n",
      "Step 2101 | Loss: 0.8893154263496399\n",
      "Step 2201 | Loss: 0.9454586505889893\n",
      "Step 2301 | Loss: 0.8859875202178955\n",
      "Step 2401 | Loss: 0.7528225183486938\n",
      "0.8622386914757953 0.7275\n",
      "Step 1 | Loss: 1.952998161315918\n",
      "Step 101 | Loss: 0.8628494739532471\n",
      "Step 201 | Loss: 0.968045175075531\n",
      "Step 301 | Loss: 0.9637546539306641\n",
      "Step 401 | Loss: 0.7555079460144043\n",
      "Step 501 | Loss: 0.742457389831543\n",
      "Step 601 | Loss: 0.8822375535964966\n",
      "Step 701 | Loss: 0.9551337957382202\n",
      "Step 801 | Loss: 0.9424049258232117\n",
      "Step 901 | Loss: 0.9890314340591431\n",
      "Step 1001 | Loss: 0.8161309957504272\n",
      "Step 1101 | Loss: 0.8509303331375122\n",
      "Step 1201 | Loss: 0.6407513618469238\n",
      "Step 1301 | Loss: 0.8954910039901733\n",
      "Step 1401 | Loss: 0.9423542022705078\n",
      "Step 1501 | Loss: 1.011800765991211\n",
      "Step 1601 | Loss: 0.8808537721633911\n",
      "Step 1701 | Loss: 0.9838725328445435\n",
      "Step 1801 | Loss: 1.0739874839782715\n",
      "Step 1901 | Loss: 0.8325682282447815\n",
      "Step 2001 | Loss: 0.907300591468811\n",
      "Step 2101 | Loss: 1.0632562637329102\n",
      "Step 2201 | Loss: 0.8890811204910278\n",
      "Step 2301 | Loss: 0.9335225820541382\n",
      "Step 2401 | Loss: 0.9663239121437073\n",
      "0.8812651402074297 0.7225\n",
      "Step 1 | Loss: 1.074756383895874\n",
      "Step 101 | Loss: 1.0345282554626465\n",
      "Step 201 | Loss: 0.8759418725967407\n",
      "Step 301 | Loss: 0.9174716472625732\n",
      "Step 401 | Loss: 0.7935730218887329\n",
      "Step 501 | Loss: 0.9451804161071777\n",
      "Step 601 | Loss: 0.9150320291519165\n",
      "Step 701 | Loss: 0.9373462200164795\n",
      "Step 801 | Loss: 0.8618354201316833\n",
      "Step 901 | Loss: 0.7896793484687805\n",
      "Step 1001 | Loss: 0.9013048410415649\n",
      "Step 1101 | Loss: 0.9395734071731567\n",
      "Step 1201 | Loss: 0.8948432207107544\n",
      "Step 1301 | Loss: 0.9601104259490967\n",
      "Step 1401 | Loss: 0.8325116038322449\n",
      "Step 1501 | Loss: 0.7909507155418396\n",
      "Step 1601 | Loss: 0.8616634011268616\n",
      "Step 1701 | Loss: 0.7829550504684448\n",
      "Step 1801 | Loss: 0.8984584808349609\n",
      "Step 1901 | Loss: 0.7514394521713257\n",
      "Step 2001 | Loss: 0.7655973434448242\n",
      "Step 2101 | Loss: 0.9333673715591431\n",
      "Step 2201 | Loss: 0.8753980994224548\n",
      "Step 2301 | Loss: 0.9504892826080322\n",
      "Step 2401 | Loss: 1.1091160774230957\n",
      "0.8634676475058545 0.72\n",
      "Step 1 | Loss: 1.3313112258911133\n",
      "Step 101 | Loss: 0.39591696858406067\n",
      "Step 201 | Loss: 0.37303805351257324\n",
      "Step 301 | Loss: 0.40218067169189453\n",
      "Step 401 | Loss: 0.5841884613037109\n",
      "Step 501 | Loss: 0.5612870454788208\n",
      "Step 601 | Loss: 0.5645213723182678\n",
      "Step 701 | Loss: 0.43626126646995544\n",
      "Step 801 | Loss: 0.4178202152252197\n",
      "Step 901 | Loss: 0.4532904624938965\n",
      "Step 1001 | Loss: 0.5577906370162964\n",
      "Step 1101 | Loss: 0.37120598554611206\n",
      "Step 1201 | Loss: 0.3062622547149658\n",
      "Step 1301 | Loss: 0.4250786304473877\n",
      "Step 1401 | Loss: 0.35071876645088196\n",
      "Step 1501 | Loss: 0.5564912557601929\n",
      "Step 1601 | Loss: 0.3376207649707794\n",
      "Step 1701 | Loss: 0.42292720079421997\n",
      "Step 1801 | Loss: 0.47363024950027466\n",
      "Step 1901 | Loss: 0.4292905628681183\n",
      "Step 2001 | Loss: 0.4818950891494751\n",
      "Step 2101 | Loss: 0.3900837302207947\n",
      "Step 2201 | Loss: 0.4747154712677002\n",
      "Step 2301 | Loss: 0.5594173669815063\n",
      "Step 2401 | Loss: 0.36463284492492676\n",
      "0.4429419414612876 0.91\n",
      "Step 1 | Loss: 1.1821085214614868\n",
      "Step 101 | Loss: 0.9569714665412903\n",
      "Step 201 | Loss: 0.8042420744895935\n",
      "Step 301 | Loss: 0.5569466948509216\n",
      "Step 401 | Loss: 0.421652615070343\n",
      "Step 501 | Loss: 0.422029048204422\n",
      "Step 601 | Loss: 0.39894384145736694\n",
      "Step 701 | Loss: 0.4808160066604614\n",
      "Step 801 | Loss: 0.37864118814468384\n",
      "Step 901 | Loss: 0.3152247667312622\n",
      "Step 1001 | Loss: 0.32450973987579346\n",
      "Step 1101 | Loss: 0.2921859323978424\n",
      "Step 1201 | Loss: 0.37373578548431396\n",
      "Step 1301 | Loss: 0.4847060739994049\n",
      "Step 1401 | Loss: 0.3142699897289276\n",
      "Step 1501 | Loss: 0.37172824144363403\n",
      "Step 1601 | Loss: 0.28176161646842957\n",
      "Step 1701 | Loss: 0.568611204624176\n",
      "Step 1801 | Loss: 0.3342728614807129\n",
      "Step 1901 | Loss: 0.2637827396392822\n",
      "Step 2001 | Loss: 0.3323242664337158\n",
      "Step 2101 | Loss: 0.35821297764778137\n",
      "Step 2201 | Loss: 0.5006194114685059\n",
      "Step 2301 | Loss: 0.535336971282959\n",
      "Step 2401 | Loss: 0.3109778165817261\n",
      "0.39074743822922736 0.9175\n",
      "Step 1 | Loss: 1.1299415826797485\n",
      "Step 101 | Loss: 0.9208962917327881\n",
      "Step 201 | Loss: 0.5942009687423706\n",
      "Step 301 | Loss: 0.613039493560791\n",
      "Step 401 | Loss: 0.6638522744178772\n",
      "Step 501 | Loss: 0.45758429169654846\n",
      "Step 601 | Loss: 0.589683473110199\n",
      "Step 701 | Loss: 0.5764665007591248\n",
      "Step 801 | Loss: 0.5689369440078735\n",
      "Step 901 | Loss: 0.38574448227882385\n",
      "Step 1001 | Loss: 0.35713621973991394\n",
      "Step 1101 | Loss: 0.44755685329437256\n",
      "Step 1201 | Loss: 0.4225817918777466\n",
      "Step 1301 | Loss: 0.4523158669471741\n",
      "Step 1401 | Loss: 0.4137362539768219\n",
      "Step 1501 | Loss: 0.4127817749977112\n",
      "Step 1601 | Loss: 0.38565486669540405\n",
      "Step 1701 | Loss: 0.30401045083999634\n",
      "Step 1801 | Loss: 0.4137392044067383\n",
      "Step 1901 | Loss: 0.311339408159256\n",
      "Step 2001 | Loss: 0.4152815341949463\n",
      "Step 2101 | Loss: 0.37904486060142517\n",
      "Step 2201 | Loss: 0.3713153600692749\n",
      "Step 2301 | Loss: 0.4201905429363251\n",
      "Step 2401 | Loss: 0.5504932403564453\n",
      "0.44166527365242475 0.915\n",
      "Step 1 | Loss: 1.0320316553115845\n",
      "Step 101 | Loss: 0.742260217666626\n",
      "Step 201 | Loss: 0.44745543599128723\n",
      "Step 301 | Loss: 0.35642170906066895\n",
      "Step 401 | Loss: 0.3937934637069702\n",
      "Step 501 | Loss: 0.65599524974823\n",
      "Step 601 | Loss: 0.46169281005859375\n",
      "Step 701 | Loss: 0.27023351192474365\n",
      "Step 801 | Loss: 0.453884482383728\n",
      "Step 901 | Loss: 0.4739152491092682\n",
      "Step 1001 | Loss: 0.5271733403205872\n",
      "Step 1101 | Loss: 0.4036244750022888\n",
      "Step 1201 | Loss: 0.35810497403144836\n",
      "Step 1301 | Loss: 0.4697766602039337\n",
      "Step 1401 | Loss: 0.39000755548477173\n",
      "Step 1501 | Loss: 0.4553298056125641\n",
      "Step 1601 | Loss: 0.3797246217727661\n",
      "Step 1701 | Loss: 0.44441118836402893\n",
      "Step 1801 | Loss: 0.39685943722724915\n",
      "Step 1901 | Loss: 0.49432575702667236\n",
      "Step 2001 | Loss: 0.5055366158485413\n",
      "Step 2101 | Loss: 0.40247151255607605\n",
      "Step 2201 | Loss: 0.3788279891014099\n",
      "Step 2301 | Loss: 0.44154295325279236\n",
      "Step 2401 | Loss: 0.43813732266426086\n",
      "0.45888824926202354 0.895\n",
      "Step 1 | Loss: 1.712802767753601\n",
      "Step 101 | Loss: 0.5288099646568298\n",
      "Step 201 | Loss: 0.3811262845993042\n",
      "Step 301 | Loss: 0.3656732439994812\n",
      "Step 401 | Loss: 0.6154118180274963\n",
      "Step 501 | Loss: 0.5322980880737305\n",
      "Step 601 | Loss: 0.4028845429420471\n",
      "Step 701 | Loss: 0.3126975893974304\n",
      "Step 801 | Loss: 0.488074392080307\n",
      "Step 901 | Loss: 0.36762991547584534\n",
      "Step 1001 | Loss: 0.43525755405426025\n",
      "Step 1101 | Loss: 0.49791789054870605\n",
      "Step 1201 | Loss: 0.37074190378189087\n",
      "Step 1301 | Loss: 0.3820701837539673\n",
      "Step 1401 | Loss: 0.37517306208610535\n",
      "Step 1501 | Loss: 0.39237380027770996\n",
      "Step 1601 | Loss: 0.4138908386230469\n",
      "Step 1701 | Loss: 0.38021257519721985\n",
      "Step 1801 | Loss: 0.32414186000823975\n",
      "Step 1901 | Loss: 0.39075392484664917\n",
      "Step 2001 | Loss: 0.25122496485710144\n",
      "Step 2101 | Loss: 0.5007650256156921\n",
      "Step 2201 | Loss: 0.3389546275138855\n",
      "Step 2301 | Loss: 0.41016900539398193\n",
      "Step 2401 | Loss: 0.4912432134151459\n",
      "0.39722525823931576 0.91\n",
      "Step 1 | Loss: 0.8736721277236938\n",
      "Step 101 | Loss: 0.7275804281234741\n",
      "Step 201 | Loss: 0.7672626376152039\n",
      "Step 301 | Loss: 0.6001672744750977\n",
      "Step 401 | Loss: 0.3294684886932373\n",
      "Step 501 | Loss: 0.3800349533557892\n",
      "Step 601 | Loss: 0.32729774713516235\n",
      "Step 701 | Loss: 0.25337648391723633\n",
      "Step 801 | Loss: 0.31487026810646057\n",
      "Step 901 | Loss: 0.3394147753715515\n",
      "Step 1001 | Loss: 0.3051894009113312\n",
      "Step 1101 | Loss: 0.4207153916358948\n",
      "Step 1201 | Loss: 0.29723647236824036\n",
      "Step 1301 | Loss: 0.35693931579589844\n",
      "Step 1401 | Loss: 0.35811954736709595\n",
      "Step 1501 | Loss: 0.3678407669067383\n",
      "Step 1601 | Loss: 0.34928855299949646\n",
      "Step 1701 | Loss: 0.2903962731361389\n",
      "Step 1801 | Loss: 0.24106581509113312\n",
      "Step 1901 | Loss: 0.2597256600856781\n",
      "Step 2001 | Loss: 0.20406118035316467\n",
      "Step 2101 | Loss: 0.36845579743385315\n",
      "Step 2201 | Loss: 0.32207635045051575\n",
      "Step 2301 | Loss: 0.35768723487854004\n",
      "Step 2401 | Loss: 0.2801283597946167\n",
      "0.28363991326832666 0.9475\n",
      "Step 1 | Loss: 1.5530219078063965\n",
      "Step 101 | Loss: 0.5975556969642639\n",
      "Step 201 | Loss: 0.3671014904975891\n",
      "Step 301 | Loss: 0.419944703578949\n",
      "Step 401 | Loss: 0.4405388832092285\n",
      "Step 501 | Loss: 0.554629385471344\n",
      "Step 601 | Loss: 0.4893174171447754\n",
      "Step 701 | Loss: 0.5654434561729431\n",
      "Step 801 | Loss: 0.344565749168396\n",
      "Step 901 | Loss: 0.412476122379303\n",
      "Step 1001 | Loss: 0.36520230770111084\n",
      "Step 1101 | Loss: 0.6157540082931519\n",
      "Step 1201 | Loss: 0.36563408374786377\n",
      "Step 1301 | Loss: 0.43223556876182556\n",
      "Step 1401 | Loss: 0.4277022182941437\n",
      "Step 1501 | Loss: 0.4046745002269745\n",
      "Step 1601 | Loss: 0.3563278913497925\n",
      "Step 1701 | Loss: 0.326860249042511\n",
      "Step 1801 | Loss: 0.19403991103172302\n",
      "Step 1901 | Loss: 0.31488126516342163\n",
      "Step 2001 | Loss: 0.19306425750255585\n",
      "Step 2101 | Loss: 0.27754223346710205\n",
      "Step 2201 | Loss: 0.28206947445869446\n",
      "Step 2301 | Loss: 0.21952587366104126\n",
      "Step 2401 | Loss: 0.17380878329277039\n",
      "0.23106543634214663 0.985\n",
      "Step 1 | Loss: 0.9188205003738403\n",
      "Step 101 | Loss: 0.6330519914627075\n",
      "Step 201 | Loss: 0.4204147756099701\n",
      "Step 301 | Loss: 0.5458264350891113\n",
      "Step 401 | Loss: 0.44000327587127686\n",
      "Step 501 | Loss: 0.44399040937423706\n",
      "Step 601 | Loss: 0.4205482602119446\n",
      "Step 701 | Loss: 0.3398566246032715\n",
      "Step 801 | Loss: 0.33580851554870605\n",
      "Step 901 | Loss: 0.3167223334312439\n",
      "Step 1001 | Loss: 0.3803845942020416\n",
      "Step 1101 | Loss: 0.2654463052749634\n",
      "Step 1201 | Loss: 0.28099942207336426\n",
      "Step 1301 | Loss: 0.225632444024086\n",
      "Step 1401 | Loss: 0.35200029611587524\n",
      "Step 1501 | Loss: 0.3365907371044159\n",
      "Step 1601 | Loss: 0.33342623710632324\n",
      "Step 1701 | Loss: 0.2691979706287384\n",
      "Step 1801 | Loss: 0.2747865617275238\n",
      "Step 1901 | Loss: 0.3417017161846161\n",
      "Step 2001 | Loss: 0.3018738031387329\n",
      "Step 2101 | Loss: 0.2657409608364105\n",
      "Step 2201 | Loss: 0.3614853322505951\n",
      "Step 2301 | Loss: 0.2738282084465027\n",
      "Step 2401 | Loss: 0.330586314201355\n",
      "0.28617222671285625 0.97\n",
      "Step 1 | Loss: 1.4652559757232666\n",
      "Step 101 | Loss: 0.3796486258506775\n",
      "Step 201 | Loss: 0.32371577620506287\n",
      "Step 301 | Loss: 0.38791003823280334\n",
      "Step 401 | Loss: 0.31309375166893005\n",
      "Step 501 | Loss: 0.2501135766506195\n",
      "Step 601 | Loss: 0.3181508779525757\n",
      "Step 701 | Loss: 0.2600330114364624\n",
      "Step 801 | Loss: 0.31085479259490967\n",
      "Step 901 | Loss: 0.2097998857498169\n",
      "Step 1001 | Loss: 0.2736276388168335\n",
      "Step 1101 | Loss: 0.24086913466453552\n",
      "Step 1201 | Loss: 0.3257785737514496\n",
      "Step 1301 | Loss: 0.25461429357528687\n",
      "Step 1401 | Loss: 0.24894015491008759\n",
      "Step 1501 | Loss: 0.26394379138946533\n",
      "Step 1601 | Loss: 0.25046247243881226\n",
      "Step 1701 | Loss: 0.21720373630523682\n",
      "Step 1801 | Loss: 0.30358996987342834\n",
      "Step 1901 | Loss: 0.26810187101364136\n",
      "Step 2001 | Loss: 0.30219605565071106\n",
      "Step 2101 | Loss: 0.227407306432724\n",
      "Step 2201 | Loss: 0.22007429599761963\n",
      "Step 2301 | Loss: 0.17553365230560303\n",
      "Step 2401 | Loss: 0.19553320109844208\n",
      "0.2095403513624738 0.9625\n",
      "Step 1 | Loss: 0.9674425721168518\n",
      "Step 101 | Loss: 0.5842743515968323\n",
      "Step 201 | Loss: 0.3894663453102112\n",
      "Step 301 | Loss: 0.43295419216156006\n",
      "Step 401 | Loss: 0.37286442518234253\n",
      "Step 501 | Loss: 0.296712189912796\n",
      "Step 601 | Loss: 0.47884973883628845\n",
      "Step 701 | Loss: 0.44584086537361145\n",
      "Step 801 | Loss: 0.3847706913948059\n",
      "Step 901 | Loss: 0.5096560120582581\n",
      "Step 1001 | Loss: 0.4110000729560852\n",
      "Step 1101 | Loss: 0.3819786608219147\n",
      "Step 1201 | Loss: 0.39429256319999695\n",
      "Step 1301 | Loss: 0.3868471384048462\n",
      "Step 1401 | Loss: 0.4370129108428955\n",
      "Step 1501 | Loss: 0.36475497484207153\n",
      "Step 1601 | Loss: 0.4336983561515808\n",
      "Step 1701 | Loss: 0.3473399877548218\n",
      "Step 1801 | Loss: 0.31699246168136597\n",
      "Step 1901 | Loss: 0.3267112076282501\n",
      "Step 2001 | Loss: 0.3345526158809662\n",
      "Step 2101 | Loss: 0.28108349442481995\n",
      "Step 2201 | Loss: 0.43738606572151184\n",
      "Step 2301 | Loss: 0.3662499785423279\n",
      "Step 2401 | Loss: 0.46616026759147644\n",
      "0.2924895834232614 0.965\n",
      "Step 1 | Loss: 1.9556193351745605\n",
      "Step 101 | Loss: 0.5351961851119995\n",
      "Step 201 | Loss: 0.14081154763698578\n",
      "Step 301 | Loss: 0.3350450396537781\n",
      "Step 401 | Loss: 0.3233047425746918\n",
      "Step 501 | Loss: 0.2252790629863739\n",
      "Step 601 | Loss: 0.17111347615718842\n",
      "Step 701 | Loss: 0.09037227183580399\n",
      "Step 801 | Loss: 0.27331778407096863\n",
      "Step 901 | Loss: 0.22383704781532288\n",
      "Step 1001 | Loss: 0.34788158535957336\n",
      "Step 1101 | Loss: 0.39925238490104675\n",
      "Step 1201 | Loss: 0.6217849254608154\n",
      "Step 1301 | Loss: 0.20400872826576233\n",
      "Step 1401 | Loss: 0.1030765026807785\n",
      "Step 1501 | Loss: 0.17084217071533203\n",
      "Step 1601 | Loss: 0.22708800435066223\n",
      "Step 1701 | Loss: 0.18663212656974792\n",
      "Step 1801 | Loss: 0.12578536570072174\n",
      "Step 1901 | Loss: 0.15201643109321594\n",
      "Step 2001 | Loss: 0.13930146396160126\n",
      "Step 2101 | Loss: 0.2988117039203644\n",
      "Step 2201 | Loss: 0.30560898780822754\n",
      "Step 2301 | Loss: 0.17487110197544098\n",
      "Step 2401 | Loss: 0.3359542787075043\n",
      "0.29044620032169954 0.89\n",
      "Step 1 | Loss: 0.3612408936023712\n",
      "Step 101 | Loss: 0.14324429631233215\n",
      "Step 201 | Loss: 0.13538232445716858\n",
      "Step 301 | Loss: 0.23613974452018738\n",
      "Step 401 | Loss: 0.366279661655426\n",
      "Step 501 | Loss: 0.2789295017719269\n",
      "Step 601 | Loss: 0.33506977558135986\n",
      "Step 701 | Loss: 0.18376971781253815\n",
      "Step 801 | Loss: 0.330588161945343\n",
      "Step 901 | Loss: 0.5853796005249023\n",
      "Step 1001 | Loss: 0.333073228597641\n",
      "Step 1101 | Loss: 0.22020882368087769\n",
      "Step 1201 | Loss: 0.44600534439086914\n",
      "Step 1301 | Loss: 0.17917902767658234\n",
      "Step 1401 | Loss: 0.1578371524810791\n",
      "Step 1501 | Loss: 0.29104673862457275\n",
      "Step 1601 | Loss: 0.16867610812187195\n",
      "Step 1701 | Loss: 0.2640434503555298\n",
      "Step 1801 | Loss: 0.5243610739707947\n",
      "Step 1901 | Loss: 0.3522004187107086\n",
      "Step 2001 | Loss: 0.32564017176628113\n",
      "Step 2101 | Loss: 0.3939237892627716\n",
      "Step 2201 | Loss: 0.1953076273202896\n",
      "Step 2301 | Loss: 0.3332647681236267\n",
      "Step 2401 | Loss: 0.15896296501159668\n",
      "0.29439683714091275 0.8875\n",
      "Step 1 | Loss: 0.7097405195236206\n",
      "Step 101 | Loss: 0.16927167773246765\n",
      "Step 201 | Loss: 0.24001218378543854\n",
      "Step 301 | Loss: 0.16908545792102814\n",
      "Step 401 | Loss: 0.16522303223609924\n",
      "Step 501 | Loss: 0.35678407549858093\n",
      "Step 601 | Loss: 0.26796630024909973\n",
      "Step 701 | Loss: 0.2586221992969513\n",
      "Step 801 | Loss: 0.35419461131095886\n",
      "Step 901 | Loss: 0.40244123339653015\n",
      "Step 1001 | Loss: 0.157261461019516\n",
      "Step 1101 | Loss: 0.2793087959289551\n",
      "Step 1201 | Loss: 0.2245175987482071\n",
      "Step 1301 | Loss: 0.18364495038986206\n",
      "Step 1401 | Loss: 0.3167319595813751\n",
      "Step 1501 | Loss: 0.18288633227348328\n",
      "Step 1601 | Loss: 0.2730324864387512\n",
      "Step 1701 | Loss: 0.4034704566001892\n",
      "Step 1801 | Loss: 0.2865768373012543\n",
      "Step 1901 | Loss: 0.38146817684173584\n",
      "Step 2001 | Loss: 0.3373999297618866\n",
      "Step 2101 | Loss: 0.40864747762680054\n",
      "Step 2201 | Loss: 0.17156025767326355\n",
      "Step 2301 | Loss: 0.5192620158195496\n",
      "Step 2401 | Loss: 0.10285436362028122\n",
      "0.28605302232931457 0.89\n",
      "Step 1 | Loss: 0.633603572845459\n",
      "Step 101 | Loss: 0.4985615015029907\n",
      "Step 201 | Loss: 0.19855302572250366\n",
      "Step 301 | Loss: 0.5716191530227661\n",
      "Step 401 | Loss: 0.4797365963459015\n",
      "Step 501 | Loss: 0.16997802257537842\n",
      "Step 601 | Loss: 0.0769403949379921\n",
      "Step 701 | Loss: 0.31768739223480225\n",
      "Step 801 | Loss: 0.36294126510620117\n",
      "Step 901 | Loss: 0.33381760120391846\n",
      "Step 1001 | Loss: 0.16867539286613464\n",
      "Step 1101 | Loss: 0.24082966148853302\n",
      "Step 1201 | Loss: 0.2186499834060669\n",
      "Step 1301 | Loss: 0.30736055970191956\n",
      "Step 1401 | Loss: 0.2560379207134247\n",
      "Step 1501 | Loss: 0.24942907691001892\n",
      "Step 1601 | Loss: 0.25264185667037964\n",
      "Step 1701 | Loss: 0.3676207363605499\n",
      "Step 1801 | Loss: 0.440102219581604\n",
      "Step 1901 | Loss: 0.5081398487091064\n",
      "Step 2001 | Loss: 0.15422886610031128\n",
      "Step 2101 | Loss: 0.3060944974422455\n",
      "Step 2201 | Loss: 0.4421076476573944\n",
      "Step 2301 | Loss: 0.2702035903930664\n",
      "Step 2401 | Loss: 0.2931824326515198\n",
      "0.2971466078458673 0.8825\n",
      "Step 1 | Loss: 0.9064865112304688\n",
      "Step 101 | Loss: 0.31713294982910156\n",
      "Step 201 | Loss: 0.45375052094459534\n",
      "Step 301 | Loss: 0.20115438103675842\n",
      "Step 401 | Loss: 0.2182474434375763\n",
      "Step 501 | Loss: 0.29202911257743835\n",
      "Step 601 | Loss: 0.2611057162284851\n",
      "Step 701 | Loss: 0.3723868429660797\n",
      "Step 801 | Loss: 0.222072035074234\n",
      "Step 901 | Loss: 0.24501852691173553\n",
      "Step 1001 | Loss: 0.3400137722492218\n",
      "Step 1101 | Loss: 0.27210691571235657\n",
      "Step 1201 | Loss: 0.2619507312774658\n",
      "Step 1301 | Loss: 0.2786065936088562\n",
      "Step 1401 | Loss: 0.20596951246261597\n",
      "Step 1501 | Loss: 0.1458602100610733\n",
      "Step 1601 | Loss: 0.24639137089252472\n",
      "Step 1701 | Loss: 0.28293943405151367\n",
      "Step 1801 | Loss: 0.2332722693681717\n",
      "Step 1901 | Loss: 0.319075345993042\n",
      "Step 2001 | Loss: 0.20887447893619537\n",
      "Step 2101 | Loss: 0.18257835507392883\n",
      "Step 2201 | Loss: 0.25642716884613037\n",
      "Step 2301 | Loss: 0.3577139377593994\n",
      "Step 2401 | Loss: 0.17223550379276276\n",
      "0.26939233313379896 0.9025\n",
      "Step 1 | Loss: 1.4447758197784424\n",
      "Step 101 | Loss: 0.5368937849998474\n",
      "Step 201 | Loss: 0.5229155421257019\n",
      "Step 301 | Loss: 0.32813894748687744\n",
      "Step 401 | Loss: 0.42714226245880127\n",
      "Step 501 | Loss: 0.2994242310523987\n",
      "Step 601 | Loss: 0.4935096800327301\n",
      "Step 701 | Loss: 0.5121486186981201\n",
      "Step 801 | Loss: 0.42022931575775146\n",
      "Step 901 | Loss: 0.4510704576969147\n",
      "Step 1001 | Loss: 0.4657067656517029\n",
      "Step 1101 | Loss: 0.4612191915512085\n",
      "Step 1201 | Loss: 0.45316749811172485\n",
      "Step 1301 | Loss: 0.3171842396259308\n",
      "Step 1401 | Loss: 0.3308922052383423\n",
      "Step 1501 | Loss: 0.3347927927970886\n",
      "Step 1601 | Loss: 0.5074470043182373\n",
      "Step 1701 | Loss: 0.40311580896377563\n",
      "Step 1801 | Loss: 0.33909720182418823\n",
      "Step 1901 | Loss: 0.4490417540073395\n",
      "Step 2001 | Loss: 0.32647040486335754\n",
      "Step 2101 | Loss: 0.3296523094177246\n",
      "Step 2201 | Loss: 0.31295597553253174\n",
      "Step 2301 | Loss: 0.529273509979248\n",
      "Step 2401 | Loss: 0.3778303563594818\n",
      "0.3872457356519036 0.9075\n",
      "Step 1 | Loss: 1.210641860961914\n",
      "Step 101 | Loss: 1.1114931106567383\n",
      "Step 201 | Loss: 1.2059910297393799\n",
      "Step 301 | Loss: 0.9266427755355835\n",
      "Step 401 | Loss: 1.062470555305481\n",
      "Step 501 | Loss: 1.0368882417678833\n",
      "Step 601 | Loss: 0.6728701591491699\n",
      "Step 701 | Loss: 0.7328000664710999\n",
      "Step 801 | Loss: 0.8122271299362183\n",
      "Step 901 | Loss: 0.8134673237800598\n",
      "Step 1001 | Loss: 0.7271816730499268\n",
      "Step 1101 | Loss: 0.6991918087005615\n",
      "Step 1201 | Loss: 0.6317471265792847\n",
      "Step 1301 | Loss: 0.600913405418396\n",
      "Step 1401 | Loss: 0.831325888633728\n",
      "Step 1501 | Loss: 0.86659836769104\n",
      "Step 1601 | Loss: 0.616114616394043\n",
      "Step 1701 | Loss: 0.727561354637146\n",
      "Step 1801 | Loss: 0.6921197772026062\n",
      "Step 1901 | Loss: 0.8303936719894409\n",
      "Step 2001 | Loss: 0.6508805155754089\n",
      "Step 2101 | Loss: 0.697918176651001\n",
      "Step 2201 | Loss: 0.6189262866973877\n",
      "Step 2301 | Loss: 0.6589180827140808\n",
      "Step 2401 | Loss: 0.5800732374191284\n",
      "0.6722915828239227 0.845\n",
      "Step 1 | Loss: 0.7506935596466064\n",
      "Step 101 | Loss: 0.3494807779788971\n",
      "Step 201 | Loss: 0.31908363103866577\n",
      "Step 301 | Loss: 0.3435108959674835\n",
      "Step 401 | Loss: 0.35058915615081787\n",
      "Step 501 | Loss: 0.4465649127960205\n",
      "Step 601 | Loss: 0.35527217388153076\n",
      "Step 701 | Loss: 0.3962988555431366\n",
      "Step 801 | Loss: 0.21637330949306488\n",
      "Step 901 | Loss: 0.3673407733440399\n",
      "Step 1001 | Loss: 0.337877482175827\n",
      "Step 1101 | Loss: 0.3531484007835388\n",
      "Step 1201 | Loss: 0.30414724349975586\n",
      "Step 1301 | Loss: 0.36947837471961975\n",
      "Step 1401 | Loss: 0.31297561526298523\n",
      "Step 1501 | Loss: 0.3444455564022064\n",
      "Step 1601 | Loss: 0.2877022325992584\n",
      "Step 1701 | Loss: 0.26852959394454956\n",
      "Step 1801 | Loss: 0.22681549191474915\n",
      "Step 1901 | Loss: 0.25131523609161377\n",
      "Step 2001 | Loss: 0.35155242681503296\n",
      "Step 2101 | Loss: 0.27182915806770325\n",
      "Step 2201 | Loss: 0.3040092885494232\n",
      "Step 2301 | Loss: 0.361988365650177\n",
      "Step 2401 | Loss: 0.21908767521381378\n",
      "0.29565767923344033 0.9425\n",
      "Step 1 | Loss: 1.30060875415802\n",
      "Step 101 | Loss: 0.5757567286491394\n",
      "Step 201 | Loss: 0.5714195966720581\n",
      "Step 301 | Loss: 0.42156070470809937\n",
      "Step 401 | Loss: 0.5265244245529175\n",
      "Step 501 | Loss: 0.2008550465106964\n",
      "Step 601 | Loss: 0.3661859929561615\n",
      "Step 701 | Loss: 0.525930643081665\n",
      "Step 801 | Loss: 0.45197793841362\n",
      "Step 901 | Loss: 0.4551379084587097\n",
      "Step 1001 | Loss: 0.46021467447280884\n",
      "Step 1101 | Loss: 0.4111082851886749\n",
      "Step 1201 | Loss: 0.45365652441978455\n",
      "Step 1301 | Loss: 0.4244692623615265\n",
      "Step 1401 | Loss: 0.37996962666511536\n",
      "Step 1501 | Loss: 0.45450326800346375\n",
      "Step 1601 | Loss: 0.4152401089668274\n",
      "Step 1701 | Loss: 0.4429933726787567\n",
      "Step 1801 | Loss: 0.2910654544830322\n",
      "Step 1901 | Loss: 0.36041417717933655\n",
      "Step 2001 | Loss: 0.3006007969379425\n",
      "Step 2101 | Loss: 0.3242354691028595\n",
      "Step 2201 | Loss: 0.3719318211078644\n",
      "Step 2301 | Loss: 0.35344207286834717\n",
      "Step 2401 | Loss: 0.4630785584449768\n",
      "0.38813991254878794 0.905\n",
      "Step 1 | Loss: 0.8738651871681213\n",
      "Step 101 | Loss: 0.7230368852615356\n",
      "Step 201 | Loss: 0.8178011178970337\n",
      "Step 301 | Loss: 0.9809854030609131\n",
      "Step 401 | Loss: 1.10182785987854\n",
      "Step 501 | Loss: 0.9987090826034546\n",
      "Step 601 | Loss: 1.0760728120803833\n",
      "Step 701 | Loss: 1.03209388256073\n",
      "Step 801 | Loss: 1.033224105834961\n",
      "Step 901 | Loss: 0.9256735444068909\n",
      "Step 1001 | Loss: 0.9099448323249817\n",
      "Step 1101 | Loss: 1.0363980531692505\n",
      "Step 1201 | Loss: 1.0316524505615234\n",
      "Step 1301 | Loss: 0.8710471987724304\n",
      "Step 1401 | Loss: 0.7166330218315125\n",
      "Step 1501 | Loss: 0.8895347714424133\n",
      "Step 1601 | Loss: 0.9212168455123901\n",
      "Step 1701 | Loss: 0.8084495067596436\n",
      "Step 1801 | Loss: 0.6105787754058838\n",
      "Step 1901 | Loss: 0.8329282999038696\n",
      "Step 2001 | Loss: 0.8776658177375793\n",
      "Step 2101 | Loss: 0.6606075763702393\n",
      "Step 2201 | Loss: 0.9507749080657959\n",
      "Step 2301 | Loss: 0.893231987953186\n",
      "Step 2401 | Loss: 0.612302303314209\n",
      "0.7315198634504613 0.7975\n",
      "Step 1 | Loss: 0.41048663854599\n",
      "Step 101 | Loss: 0.30456823110580444\n",
      "Step 201 | Loss: 0.35159218311309814\n",
      "Step 301 | Loss: 0.30031630396842957\n",
      "Step 401 | Loss: 0.33951011300086975\n",
      "Step 501 | Loss: 0.3131644129753113\n",
      "Step 601 | Loss: 0.2549804151058197\n",
      "Step 701 | Loss: 0.42033088207244873\n",
      "Step 801 | Loss: 0.16915899515151978\n",
      "Step 901 | Loss: 0.16561734676361084\n",
      "Step 1001 | Loss: 0.2372422069311142\n",
      "Step 1101 | Loss: 0.34037116169929504\n",
      "Step 1201 | Loss: 0.2637684941291809\n",
      "Step 1301 | Loss: 0.21722811460494995\n",
      "Step 1401 | Loss: 0.1884690523147583\n",
      "Step 1501 | Loss: 0.21670041978359222\n",
      "Step 1601 | Loss: 0.231979101896286\n",
      "Step 1701 | Loss: 0.2044774293899536\n",
      "Step 1801 | Loss: 0.280322790145874\n",
      "Step 1901 | Loss: 0.19120123982429504\n",
      "Step 2001 | Loss: 0.3257824778556824\n",
      "Step 2101 | Loss: 0.26788419485092163\n",
      "Step 2201 | Loss: 0.22435609996318817\n",
      "Step 2301 | Loss: 0.25666576623916626\n",
      "Step 2401 | Loss: 0.19226597249507904\n",
      "0.2851744725900784 0.92\n",
      "Step 1 | Loss: 1.3049010038375854\n",
      "Step 101 | Loss: 0.5849539041519165\n",
      "Step 201 | Loss: 0.3002319931983948\n",
      "Step 301 | Loss: 0.4540731906890869\n",
      "Step 401 | Loss: 0.4441874921321869\n",
      "Step 501 | Loss: 0.43236130475997925\n",
      "Step 601 | Loss: 0.4243316948413849\n",
      "Step 701 | Loss: 0.31113189458847046\n",
      "Step 801 | Loss: 0.35739731788635254\n",
      "Step 901 | Loss: 0.4643916189670563\n",
      "Step 1001 | Loss: 0.4554917812347412\n",
      "Step 1101 | Loss: 0.4211665391921997\n",
      "Step 1201 | Loss: 0.4235435724258423\n",
      "Step 1301 | Loss: 0.3431997299194336\n",
      "Step 1401 | Loss: 0.44147926568984985\n",
      "Step 1501 | Loss: 0.39827024936676025\n",
      "Step 1601 | Loss: 0.38418057560920715\n",
      "Step 1701 | Loss: 0.36734405159950256\n",
      "Step 1801 | Loss: 0.5528671741485596\n",
      "Step 1901 | Loss: 0.35887259244918823\n",
      "Step 2001 | Loss: 0.357726514339447\n",
      "Step 2101 | Loss: 0.2889273464679718\n",
      "Step 2201 | Loss: 0.4114743769168854\n",
      "Step 2301 | Loss: 0.3728248178958893\n",
      "Step 2401 | Loss: 0.3095082640647888\n",
      "0.4004599049286354 0.9125\n",
      "Step 1 | Loss: 1.8450701236724854\n",
      "Step 101 | Loss: 0.6775768995285034\n",
      "Step 201 | Loss: 0.563256025314331\n",
      "Step 301 | Loss: 0.5918846130371094\n",
      "Step 401 | Loss: 0.5242564082145691\n",
      "Step 501 | Loss: 0.5648317933082581\n",
      "Step 601 | Loss: 0.3165629804134369\n",
      "Step 701 | Loss: 0.374362975358963\n",
      "Step 801 | Loss: 0.39684391021728516\n",
      "Step 901 | Loss: 0.33902862668037415\n",
      "Step 1001 | Loss: 0.3569873571395874\n",
      "Step 1101 | Loss: 0.3975386619567871\n",
      "Step 1201 | Loss: 0.23528623580932617\n",
      "Step 1301 | Loss: 0.24535758793354034\n",
      "Step 1401 | Loss: 0.26588183641433716\n",
      "Step 1501 | Loss: 0.30289989709854126\n",
      "Step 1601 | Loss: 0.2094452977180481\n",
      "Step 1701 | Loss: 0.3266393840312958\n",
      "Step 1801 | Loss: 0.15531516075134277\n",
      "Step 1901 | Loss: 0.3793320655822754\n",
      "Step 2001 | Loss: 0.2398073375225067\n",
      "Step 2101 | Loss: 0.19493381679058075\n",
      "Step 2201 | Loss: 0.17272420227527618\n",
      "Step 2301 | Loss: 0.33503830432891846\n",
      "Step 2401 | Loss: 0.27600014209747314\n",
      "0.28346623921655884 0.9275\n",
      "Step 1 | Loss: 1.457335114479065\n",
      "Step 101 | Loss: 0.5735549926757812\n",
      "Step 201 | Loss: 0.44292160868644714\n",
      "Step 301 | Loss: 0.4442542493343353\n",
      "Step 401 | Loss: 0.43238335847854614\n",
      "Step 501 | Loss: 0.6265750527381897\n",
      "Step 601 | Loss: 0.6451334357261658\n",
      "Step 701 | Loss: 0.40329405665397644\n",
      "Step 801 | Loss: 0.33678117394447327\n",
      "Step 901 | Loss: 0.3507480025291443\n",
      "Step 1001 | Loss: 0.30449378490448\n",
      "Step 1101 | Loss: 0.245970219373703\n",
      "Step 1201 | Loss: 0.3967677652835846\n",
      "Step 1301 | Loss: 0.2033914476633072\n",
      "Step 1401 | Loss: 0.25104519724845886\n",
      "Step 1501 | Loss: 0.2912065386772156\n",
      "Step 1601 | Loss: 0.37113848328590393\n",
      "Step 1701 | Loss: 0.20569854974746704\n",
      "Step 1801 | Loss: 0.27689623832702637\n",
      "Step 1901 | Loss: 0.3046949803829193\n",
      "Step 2001 | Loss: 0.29775869846343994\n",
      "Step 2101 | Loss: 0.26234492659568787\n",
      "Step 2201 | Loss: 0.2895980477333069\n",
      "Step 2301 | Loss: 0.20323649048805237\n",
      "Step 2401 | Loss: 0.3923698365688324\n",
      "0.282336594493621 0.9275\n",
      "Step 1 | Loss: 0.9408352971076965\n",
      "Step 101 | Loss: 0.3714519143104553\n",
      "Step 201 | Loss: 0.2292274534702301\n",
      "Step 301 | Loss: 0.4046376943588257\n",
      "Step 401 | Loss: 0.19532108306884766\n",
      "Step 501 | Loss: 0.4107557237148285\n",
      "Step 601 | Loss: 0.24165569245815277\n",
      "Step 701 | Loss: 0.29889005422592163\n",
      "Step 801 | Loss: 0.259867787361145\n",
      "Step 901 | Loss: 0.28714606165885925\n",
      "Step 1001 | Loss: 0.1928650438785553\n",
      "Step 1101 | Loss: 0.1873467117547989\n",
      "Step 1201 | Loss: 0.2775764465332031\n",
      "Step 1301 | Loss: 0.17917951941490173\n",
      "Step 1401 | Loss: 0.26546618342399597\n",
      "Step 1501 | Loss: 0.17343589663505554\n",
      "Step 1601 | Loss: 0.18194177746772766\n",
      "Step 1701 | Loss: 0.27515333890914917\n",
      "Step 1801 | Loss: 0.18179985880851746\n",
      "Step 1901 | Loss: 0.24801115691661835\n",
      "Step 2001 | Loss: 0.3629007637500763\n",
      "Step 2101 | Loss: 0.1597386747598648\n",
      "Step 2201 | Loss: 0.2546484172344208\n",
      "Step 2301 | Loss: 0.2383619248867035\n",
      "Step 2401 | Loss: 0.31046992540359497\n",
      "0.2843607579584305 0.9225\n",
      "Step 1 | Loss: 0.8873083591461182\n",
      "Step 101 | Loss: 1.0422827005386353\n",
      "Step 201 | Loss: 0.6427001357078552\n",
      "Step 301 | Loss: 0.6614928245544434\n",
      "Step 401 | Loss: 0.4512743651866913\n",
      "Step 501 | Loss: 0.8046451807022095\n",
      "Step 601 | Loss: 0.5159913301467896\n",
      "Step 701 | Loss: 0.3585917353630066\n",
      "Step 801 | Loss: 0.7710320353507996\n",
      "Step 901 | Loss: 0.6044604182243347\n",
      "Step 1001 | Loss: 0.7799093127250671\n",
      "Step 1101 | Loss: 0.7588723301887512\n",
      "Step 1201 | Loss: 0.752376914024353\n",
      "Step 1301 | Loss: 0.5282528400421143\n",
      "Step 1401 | Loss: 0.7329965829849243\n",
      "Step 1501 | Loss: 0.5140010118484497\n",
      "Step 1601 | Loss: 0.5522662401199341\n",
      "Step 1701 | Loss: 0.8408504724502563\n",
      "Step 1801 | Loss: 0.6452383995056152\n",
      "Step 1901 | Loss: 0.8007441759109497\n",
      "Step 2001 | Loss: 0.4764893054962158\n",
      "Step 2101 | Loss: 0.7565276622772217\n",
      "Step 2201 | Loss: 0.6662757396697998\n",
      "Step 2301 | Loss: 0.6182223558425903\n",
      "Step 2401 | Loss: 0.7279170155525208\n",
      "0.6149271669018795 0.7825\n",
      "Step 1 | Loss: 1.1036633253097534\n",
      "Step 101 | Loss: 0.7161425352096558\n",
      "Step 201 | Loss: 0.6724346280097961\n",
      "Step 301 | Loss: 0.5921577215194702\n",
      "Step 401 | Loss: 0.6700869798660278\n",
      "Step 501 | Loss: 0.5838953256607056\n",
      "Step 601 | Loss: 0.5047893524169922\n",
      "Step 701 | Loss: 0.6113109588623047\n",
      "Step 801 | Loss: 0.6242464780807495\n",
      "Step 901 | Loss: 0.5071321725845337\n",
      "Step 1001 | Loss: 0.5837945342063904\n",
      "Step 1101 | Loss: 0.5893720984458923\n",
      "Step 1201 | Loss: 0.6685482859611511\n",
      "Step 1301 | Loss: 0.5293684005737305\n",
      "Step 1401 | Loss: 0.5309370160102844\n",
      "Step 1501 | Loss: 0.4320117235183716\n",
      "Step 1601 | Loss: 0.4680849015712738\n",
      "Step 1701 | Loss: 0.5155442953109741\n",
      "Step 1801 | Loss: 0.5145585536956787\n",
      "Step 1901 | Loss: 0.6033023595809937\n",
      "Step 2001 | Loss: 0.5162456035614014\n",
      "Step 2101 | Loss: 0.5521928071975708\n",
      "Step 2201 | Loss: 0.6305100321769714\n",
      "Step 2301 | Loss: 0.7098039984703064\n",
      "Step 2401 | Loss: 0.4001697897911072\n",
      "0.4971548607939059 0.855\n",
      "Step 1 | Loss: 1.2105745077133179\n",
      "Step 101 | Loss: 0.8332619667053223\n",
      "Step 201 | Loss: 0.7461622357368469\n",
      "Step 301 | Loss: 0.6645714044570923\n",
      "Step 401 | Loss: 0.5884346961975098\n",
      "Step 501 | Loss: 0.7034845948219299\n",
      "Step 601 | Loss: 0.6373860239982605\n",
      "Step 701 | Loss: 0.4662012457847595\n",
      "Step 801 | Loss: 0.6037431955337524\n",
      "Step 901 | Loss: 0.4946020841598511\n",
      "Step 1001 | Loss: 0.5059599876403809\n",
      "Step 1101 | Loss: 0.7099229693412781\n",
      "Step 1201 | Loss: 0.4820326864719391\n",
      "Step 1301 | Loss: 0.49603089690208435\n",
      "Step 1401 | Loss: 0.4477376341819763\n",
      "Step 1501 | Loss: 0.5805593132972717\n",
      "Step 1601 | Loss: 0.37541672587394714\n",
      "Step 1701 | Loss: 0.5770395398139954\n",
      "Step 1801 | Loss: 0.4703635275363922\n",
      "Step 1901 | Loss: 0.48669150471687317\n",
      "Step 2001 | Loss: 0.39548325538635254\n",
      "Step 2101 | Loss: 0.43319466710090637\n",
      "Step 2201 | Loss: 0.3824925422668457\n",
      "Step 2301 | Loss: 0.5182507038116455\n",
      "Step 2401 | Loss: 0.4766909182071686\n",
      "0.5036648576230985 0.8675\n",
      "Step 1 | Loss: 0.7984527349472046\n",
      "Step 101 | Loss: 0.7309788465499878\n",
      "Step 201 | Loss: 0.8682184815406799\n",
      "Step 301 | Loss: 0.5637551546096802\n",
      "Step 401 | Loss: 0.6538158655166626\n",
      "Step 501 | Loss: 0.6617466807365417\n",
      "Step 601 | Loss: 0.6538169384002686\n",
      "Step 701 | Loss: 0.6394404172897339\n",
      "Step 801 | Loss: 0.5721454620361328\n",
      "Step 901 | Loss: 0.6306615471839905\n",
      "Step 1001 | Loss: 0.4507378041744232\n",
      "Step 1101 | Loss: 0.4782184362411499\n",
      "Step 1201 | Loss: 0.4048912823200226\n",
      "Step 1301 | Loss: 0.55433189868927\n",
      "Step 1401 | Loss: 0.4488653540611267\n",
      "Step 1501 | Loss: 0.4510720372200012\n",
      "Step 1601 | Loss: 0.5270271897315979\n",
      "Step 1701 | Loss: 0.507662832736969\n",
      "Step 1801 | Loss: 0.5442028641700745\n",
      "Step 1901 | Loss: 0.47671547532081604\n",
      "Step 2001 | Loss: 0.5384518504142761\n",
      "Step 2101 | Loss: 0.598982572555542\n",
      "Step 2201 | Loss: 0.40141257643699646\n",
      "Step 2301 | Loss: 0.4840884804725647\n",
      "Step 2401 | Loss: 0.4405857026576996\n",
      "0.473358780780077 0.85\n",
      "Step 1 | Loss: 1.2457648515701294\n",
      "Step 101 | Loss: 0.8276277184486389\n",
      "Step 201 | Loss: 0.8949569463729858\n",
      "Step 301 | Loss: 0.5239554047584534\n",
      "Step 401 | Loss: 0.5921828746795654\n",
      "Step 501 | Loss: 0.6944567561149597\n",
      "Step 601 | Loss: 0.5456056594848633\n",
      "Step 701 | Loss: 0.8547075986862183\n",
      "Step 801 | Loss: 0.5906563997268677\n",
      "Step 901 | Loss: 0.6429545879364014\n",
      "Step 1001 | Loss: 0.683222234249115\n",
      "Step 1101 | Loss: 0.8039866089820862\n",
      "Step 1201 | Loss: 0.6542111039161682\n",
      "Step 1301 | Loss: 0.6848945617675781\n",
      "Step 1401 | Loss: 0.7066378593444824\n",
      "Step 1501 | Loss: 0.3708782196044922\n",
      "Step 1601 | Loss: 0.5289645195007324\n",
      "Step 1701 | Loss: 0.5684472322463989\n",
      "Step 1801 | Loss: 0.5875865817070007\n",
      "Step 1901 | Loss: 0.5319815278053284\n",
      "Step 2001 | Loss: 0.3573509454727173\n",
      "Step 2101 | Loss: 0.6451436281204224\n",
      "Step 2201 | Loss: 0.6509917378425598\n",
      "Step 2301 | Loss: 0.7466336488723755\n",
      "Step 2401 | Loss: 0.721418559551239\n",
      "0.602604415399157 0.7975\n",
      "Step 1 | Loss: 2.1210851669311523\n",
      "Step 101 | Loss: 1.1411828994750977\n",
      "Step 201 | Loss: 0.9288109540939331\n",
      "Step 301 | Loss: 1.00137197971344\n",
      "Step 401 | Loss: 0.822461724281311\n",
      "Step 501 | Loss: 0.6493908166885376\n",
      "Step 601 | Loss: 0.6956759095191956\n",
      "Step 701 | Loss: 0.6564714312553406\n",
      "Step 801 | Loss: 0.6813850998878479\n",
      "Step 901 | Loss: 0.805228054523468\n",
      "Step 1001 | Loss: 0.6133276224136353\n",
      "Step 1101 | Loss: 0.5913681983947754\n",
      "Step 1201 | Loss: 0.8618451356887817\n",
      "Step 1301 | Loss: 0.5680018663406372\n",
      "Step 1401 | Loss: 0.5338486433029175\n",
      "Step 1501 | Loss: 0.64459228515625\n",
      "Step 1601 | Loss: 0.5758955478668213\n",
      "Step 1701 | Loss: 0.5258005261421204\n",
      "Step 1801 | Loss: 0.6249268651008606\n",
      "Step 1901 | Loss: 0.5859719514846802\n",
      "Step 2001 | Loss: 0.5689287185668945\n",
      "Step 2101 | Loss: 0.4439770579338074\n",
      "Step 2201 | Loss: 0.6762517690658569\n",
      "Step 2301 | Loss: 0.6904374361038208\n",
      "Step 2401 | Loss: 0.5101761221885681\n",
      "0.5898250053376152 0.8175\n",
      "Step 1 | Loss: 1.369605541229248\n",
      "Step 101 | Loss: 0.8674358129501343\n",
      "Step 201 | Loss: 0.7125238180160522\n",
      "Step 301 | Loss: 0.8433226346969604\n",
      "Step 401 | Loss: 0.7627753019332886\n",
      "Step 501 | Loss: 0.6334450840950012\n",
      "Step 601 | Loss: 0.736041247844696\n",
      "Step 701 | Loss: 0.6970420479774475\n",
      "Step 801 | Loss: 0.7208792567253113\n",
      "Step 901 | Loss: 0.8368408679962158\n",
      "Step 1001 | Loss: 0.6762863993644714\n",
      "Step 1101 | Loss: 0.6218153238296509\n",
      "Step 1201 | Loss: 0.655590295791626\n",
      "Step 1301 | Loss: 0.6965464353561401\n",
      "Step 1401 | Loss: 0.5507270097732544\n",
      "Step 1501 | Loss: 0.8331403732299805\n",
      "Step 1601 | Loss: 0.6948133707046509\n",
      "Step 1701 | Loss: 0.6333458423614502\n",
      "Step 1801 | Loss: 0.6736564040184021\n",
      "Step 1901 | Loss: 0.695654571056366\n",
      "Step 2001 | Loss: 0.6495436429977417\n",
      "Step 2101 | Loss: 0.5963295698165894\n",
      "Step 2201 | Loss: 0.5527251958847046\n",
      "Step 2301 | Loss: 0.5622376203536987\n",
      "Step 2401 | Loss: 0.6914010047912598\n",
      "0.6501085830295958 0.805\n",
      "Step 1 | Loss: 0.9741641283035278\n",
      "Step 101 | Loss: 0.9391409158706665\n",
      "Step 201 | Loss: 0.6857885122299194\n",
      "Step 301 | Loss: 0.5182751417160034\n",
      "Step 401 | Loss: 0.6213570833206177\n",
      "Step 501 | Loss: 0.5723985433578491\n",
      "Step 601 | Loss: 0.43099772930145264\n",
      "Step 701 | Loss: 0.7316650152206421\n",
      "Step 801 | Loss: 0.6016926169395447\n",
      "Step 901 | Loss: 0.42549067735671997\n",
      "Step 1001 | Loss: 0.5687173008918762\n",
      "Step 1101 | Loss: 0.5300754904747009\n",
      "Step 1201 | Loss: 0.5063508749008179\n",
      "Step 1301 | Loss: 0.5728297233581543\n",
      "Step 1401 | Loss: 0.4742925763130188\n",
      "Step 1501 | Loss: 0.4723930358886719\n",
      "Step 1601 | Loss: 0.5778383016586304\n",
      "Step 1701 | Loss: 0.8034642338752747\n",
      "Step 1801 | Loss: 0.4955604076385498\n",
      "Step 1901 | Loss: 0.8670156002044678\n",
      "Step 2001 | Loss: 0.6610103249549866\n",
      "Step 2101 | Loss: 0.5910729169845581\n",
      "Step 2201 | Loss: 0.6994611620903015\n",
      "Step 2301 | Loss: 0.5888488292694092\n",
      "Step 2401 | Loss: 0.6168599724769592\n",
      "0.5913421482495282 0.8225\n",
      "Step 1 | Loss: 1.9923346042633057\n",
      "Step 101 | Loss: 1.0780730247497559\n",
      "Step 201 | Loss: 0.935745358467102\n",
      "Step 301 | Loss: 0.9304287433624268\n",
      "Step 401 | Loss: 1.0451627969741821\n",
      "Step 501 | Loss: 0.9428547620773315\n",
      "Step 601 | Loss: 0.930070698261261\n",
      "Step 701 | Loss: 0.8873311281204224\n",
      "Step 801 | Loss: 0.8321378827095032\n",
      "Step 901 | Loss: 0.9239078164100647\n",
      "Step 1001 | Loss: 0.7878759503364563\n",
      "Step 1101 | Loss: 0.8711167573928833\n",
      "Step 1201 | Loss: 1.0303468704223633\n",
      "Step 1301 | Loss: 0.9049873948097229\n",
      "Step 1401 | Loss: 0.841060996055603\n",
      "Step 1501 | Loss: 0.7527751922607422\n",
      "Step 1601 | Loss: 0.8117122054100037\n",
      "Step 1701 | Loss: 0.9226628541946411\n",
      "Step 1801 | Loss: 1.030368685722351\n",
      "Step 1901 | Loss: 0.9753093719482422\n",
      "Step 2001 | Loss: 0.8784732222557068\n",
      "Step 2101 | Loss: 0.8354843854904175\n",
      "Step 2201 | Loss: 0.7293089628219604\n",
      "Step 2301 | Loss: 0.9512869119644165\n",
      "Step 2401 | Loss: 0.8596232533454895\n",
      "0.8540702814715478 0.685\n",
      "Step 1 | Loss: 1.0159958600997925\n",
      "Step 101 | Loss: 1.046894907951355\n",
      "Step 201 | Loss: 1.129872441291809\n",
      "Step 301 | Loss: 1.0637269020080566\n",
      "Step 401 | Loss: 0.9072287082672119\n",
      "Step 501 | Loss: 1.1231895685195923\n",
      "Step 601 | Loss: 0.9007363319396973\n",
      "Step 701 | Loss: 0.9485743641853333\n",
      "Step 801 | Loss: 0.809821605682373\n",
      "Step 901 | Loss: 1.0356082916259766\n",
      "Step 1001 | Loss: 0.9690312147140503\n",
      "Step 1101 | Loss: 1.0041987895965576\n",
      "Step 1201 | Loss: 0.951524019241333\n",
      "Step 1301 | Loss: 0.8701874017715454\n",
      "Step 1401 | Loss: 0.8764486908912659\n",
      "Step 1501 | Loss: 0.8697113394737244\n",
      "Step 1601 | Loss: 1.0413367748260498\n",
      "Step 1701 | Loss: 0.7163035273551941\n",
      "Step 1801 | Loss: 0.8414167165756226\n",
      "Step 1901 | Loss: 0.8557809591293335\n",
      "Step 2001 | Loss: 0.9543994069099426\n",
      "Step 2101 | Loss: 0.9715056419372559\n",
      "Step 2201 | Loss: 1.0041980743408203\n",
      "Step 2301 | Loss: 1.0117194652557373\n",
      "Step 2401 | Loss: 0.8298496007919312\n",
      "0.90539006663467 0.6575\n",
      "Step 1 | Loss: 0.8923305869102478\n",
      "Step 101 | Loss: 0.3097277581691742\n",
      "Step 201 | Loss: 0.5367903113365173\n",
      "Step 301 | Loss: 0.76776123046875\n",
      "Step 401 | Loss: 0.6198858022689819\n",
      "Step 501 | Loss: 0.6732587218284607\n",
      "Step 601 | Loss: 0.3516887426376343\n",
      "Step 701 | Loss: 0.46283668279647827\n",
      "Step 801 | Loss: 0.37769901752471924\n",
      "Step 901 | Loss: 0.5324543714523315\n",
      "Step 1001 | Loss: 0.5206195712089539\n",
      "Step 1101 | Loss: 0.6889469027519226\n",
      "Step 1201 | Loss: 0.6459437608718872\n",
      "Step 1301 | Loss: 0.5459426045417786\n",
      "Step 1401 | Loss: 0.4620712399482727\n",
      "Step 1501 | Loss: 0.5195068717002869\n",
      "Step 1601 | Loss: 0.5994121432304382\n",
      "Step 1701 | Loss: 0.8005411624908447\n",
      "Step 1801 | Loss: 0.479911208152771\n",
      "Step 1901 | Loss: 0.5920900106430054\n",
      "Step 2001 | Loss: 0.3088940382003784\n",
      "Step 2101 | Loss: 0.49173444509506226\n",
      "Step 2201 | Loss: 0.4665801227092743\n",
      "Step 2301 | Loss: 0.4052242040634155\n",
      "Step 2401 | Loss: 0.4793526828289032\n",
      "0.5391002011703953 0.81\n",
      "Step 1 | Loss: 0.5693818926811218\n",
      "Step 101 | Loss: 0.565364420413971\n",
      "Step 201 | Loss: 0.5295222997665405\n",
      "Step 301 | Loss: 0.46554476022720337\n",
      "Step 401 | Loss: 0.41526785492897034\n",
      "Step 501 | Loss: 0.4987388551235199\n",
      "Step 601 | Loss: 0.5033166408538818\n",
      "Step 701 | Loss: 0.2550651431083679\n",
      "Step 801 | Loss: 0.6276563405990601\n",
      "Step 901 | Loss: 0.5176825523376465\n",
      "Step 1001 | Loss: 0.5520663857460022\n",
      "Step 1101 | Loss: 0.5450205206871033\n",
      "Step 1201 | Loss: 0.4401276707649231\n",
      "Step 1301 | Loss: 0.6244027614593506\n",
      "Step 1401 | Loss: 0.5822264552116394\n",
      "Step 1501 | Loss: 0.6594612002372742\n",
      "Step 1601 | Loss: 0.6678473949432373\n",
      "Step 1701 | Loss: 0.7354954481124878\n",
      "Step 1801 | Loss: 0.480979859828949\n",
      "Step 1901 | Loss: 0.3093322515487671\n",
      "Step 2001 | Loss: 0.5620344877243042\n",
      "Step 2101 | Loss: 0.5675721168518066\n",
      "Step 2201 | Loss: 0.5185669660568237\n",
      "Step 2301 | Loss: 0.5439285039901733\n",
      "Step 2401 | Loss: 0.45706871151924133\n",
      "0.5382255997564873 0.8125\n",
      "Step 1 | Loss: 0.5849145650863647\n",
      "Step 101 | Loss: 0.8055336475372314\n",
      "Step 201 | Loss: 0.6034525036811829\n",
      "Step 301 | Loss: 0.46315622329711914\n",
      "Step 401 | Loss: 0.5749297142028809\n",
      "Step 501 | Loss: 0.63144850730896\n",
      "Step 601 | Loss: 0.572739839553833\n",
      "Step 701 | Loss: 0.45811933279037476\n",
      "Step 801 | Loss: 0.5452998280525208\n",
      "Step 901 | Loss: 0.524945855140686\n",
      "Step 1001 | Loss: 0.4349987208843231\n",
      "Step 1101 | Loss: 0.48076242208480835\n",
      "Step 1201 | Loss: 0.39475294947624207\n",
      "Step 1301 | Loss: 0.34733402729034424\n",
      "Step 1401 | Loss: 0.6080690622329712\n",
      "Step 1501 | Loss: 0.38136187195777893\n",
      "Step 1601 | Loss: 0.5505635738372803\n",
      "Step 1701 | Loss: 0.5395424365997314\n",
      "Step 1801 | Loss: 0.7909547686576843\n",
      "Step 1901 | Loss: 0.6815417408943176\n",
      "Step 2001 | Loss: 0.5533046126365662\n",
      "Step 2101 | Loss: 0.56593257188797\n",
      "Step 2201 | Loss: 0.4333810806274414\n",
      "Step 2301 | Loss: 0.5661914348602295\n",
      "Step 2401 | Loss: 0.4031037986278534\n",
      "0.5380593624433148 0.81\n",
      "Step 1 | Loss: 0.594308078289032\n",
      "Step 101 | Loss: 0.5128707885742188\n",
      "Step 201 | Loss: 0.5036340355873108\n",
      "Step 301 | Loss: 0.37344691157341003\n",
      "Step 401 | Loss: 0.5044610500335693\n",
      "Step 501 | Loss: 0.6290045380592346\n",
      "Step 601 | Loss: 0.3142930865287781\n",
      "Step 701 | Loss: 0.46597325801849365\n",
      "Step 801 | Loss: 0.35939112305641174\n",
      "Step 901 | Loss: 0.7134115099906921\n",
      "Step 1001 | Loss: 0.47752243280410767\n",
      "Step 1101 | Loss: 0.39089447259902954\n",
      "Step 1201 | Loss: 0.38325080275535583\n",
      "Step 1301 | Loss: 0.6369091868400574\n",
      "Step 1401 | Loss: 0.5338120460510254\n",
      "Step 1501 | Loss: 0.46638020873069763\n",
      "Step 1601 | Loss: 0.5195731520652771\n",
      "Step 1701 | Loss: 0.6115137934684753\n",
      "Step 1801 | Loss: 0.5888789296150208\n",
      "Step 1901 | Loss: 0.45627832412719727\n",
      "Step 2001 | Loss: 0.4717101752758026\n",
      "Step 2101 | Loss: 0.5972408652305603\n",
      "Step 2201 | Loss: 0.4397493898868561\n",
      "Step 2301 | Loss: 0.5608557462692261\n",
      "Step 2401 | Loss: 0.2860073149204254\n",
      "0.5381142859162501 0.81\n",
      "Step 1 | Loss: 2.0328009128570557\n",
      "Step 101 | Loss: 1.0989288091659546\n",
      "Step 201 | Loss: 0.7823176980018616\n",
      "Step 301 | Loss: 0.556087851524353\n",
      "Step 401 | Loss: 0.45921850204467773\n",
      "Step 501 | Loss: 0.5185981392860413\n",
      "Step 601 | Loss: 0.38625574111938477\n",
      "Step 701 | Loss: 0.6119989156723022\n",
      "Step 801 | Loss: 0.7262386679649353\n",
      "Step 901 | Loss: 0.5646727681159973\n",
      "Step 1001 | Loss: 0.5947695970535278\n",
      "Step 1101 | Loss: 0.3651256263256073\n",
      "Step 1201 | Loss: 0.49829018115997314\n",
      "Step 1301 | Loss: 0.614166259765625\n",
      "Step 1401 | Loss: 0.5652222037315369\n",
      "Step 1501 | Loss: 0.6064679622650146\n",
      "Step 1601 | Loss: 0.7234218716621399\n",
      "Step 1701 | Loss: 0.634698212146759\n",
      "Step 1801 | Loss: 0.5115290880203247\n",
      "Step 1901 | Loss: 0.5324748754501343\n",
      "Step 2001 | Loss: 0.6314318180084229\n",
      "Step 2101 | Loss: 0.5647095441818237\n",
      "Step 2201 | Loss: 0.5269856452941895\n",
      "Step 2301 | Loss: 0.41867688298225403\n",
      "Step 2401 | Loss: 0.5709148645401001\n",
      "0.5383807724528368 0.8125\n",
      "Step 1 | Loss: 0.8305622339248657\n",
      "Step 101 | Loss: 0.9775861501693726\n",
      "Step 201 | Loss: 0.934546709060669\n",
      "Step 301 | Loss: 1.0806961059570312\n",
      "Step 401 | Loss: 1.1060420274734497\n",
      "Step 501 | Loss: 0.7503049373626709\n",
      "Step 601 | Loss: 0.7387622594833374\n",
      "Step 701 | Loss: 0.8620534539222717\n",
      "Step 801 | Loss: 0.9501422643661499\n",
      "Step 901 | Loss: 0.6429310441017151\n",
      "Step 1001 | Loss: 0.8686457872390747\n",
      "Step 1101 | Loss: 0.9522007703781128\n",
      "Step 1201 | Loss: 0.7938207983970642\n",
      "Step 1301 | Loss: 0.713420569896698\n",
      "Step 1401 | Loss: 0.9743157625198364\n",
      "Step 1501 | Loss: 0.8054103255271912\n",
      "Step 1601 | Loss: 1.073408603668213\n",
      "Step 1701 | Loss: 0.616825520992279\n",
      "Step 1801 | Loss: 0.8061105012893677\n",
      "Step 1901 | Loss: 0.7889488935470581\n",
      "Step 2001 | Loss: 1.0154587030410767\n",
      "Step 2101 | Loss: 1.023240089416504\n",
      "Step 2201 | Loss: 0.6254562139511108\n",
      "Step 2301 | Loss: 0.5496293902397156\n",
      "Step 2401 | Loss: 0.5726217031478882\n",
      "0.4999900733134234 0.875\n",
      "Step 1 | Loss: 1.0847764015197754\n",
      "Step 101 | Loss: 1.039318561553955\n",
      "Step 201 | Loss: 0.5922839045524597\n",
      "Step 301 | Loss: 0.588040828704834\n",
      "Step 401 | Loss: 0.5947273373603821\n",
      "Step 501 | Loss: 0.5052391290664673\n",
      "Step 601 | Loss: 0.513115406036377\n",
      "Step 701 | Loss: 0.5280423760414124\n",
      "Step 801 | Loss: 0.48771387338638306\n",
      "Step 901 | Loss: 0.6154602766036987\n",
      "Step 1001 | Loss: 0.5725497603416443\n",
      "Step 1101 | Loss: 0.5739474296569824\n",
      "Step 1201 | Loss: 0.4672938883304596\n",
      "Step 1301 | Loss: 0.43165451288223267\n",
      "Step 1401 | Loss: 0.44758060574531555\n",
      "Step 1501 | Loss: 0.49234142899513245\n",
      "Step 1601 | Loss: 0.3745270073413849\n",
      "Step 1701 | Loss: 0.46161919832229614\n",
      "Step 1801 | Loss: 0.5152767300605774\n",
      "Step 1901 | Loss: 0.4785763919353485\n",
      "Step 2001 | Loss: 0.413729190826416\n",
      "Step 2101 | Loss: 0.3248656392097473\n",
      "Step 2201 | Loss: 0.4835928976535797\n",
      "Step 2301 | Loss: 0.4810792803764343\n",
      "Step 2401 | Loss: 0.4631101191043854\n",
      "0.45313507390047947 0.895\n",
      "Step 1 | Loss: 0.8838436603546143\n",
      "Step 101 | Loss: 0.7588891386985779\n",
      "Step 201 | Loss: 0.6521292328834534\n",
      "Step 301 | Loss: 0.6284920573234558\n",
      "Step 401 | Loss: 0.6200631260871887\n",
      "Step 501 | Loss: 0.6139489412307739\n",
      "Step 601 | Loss: 0.548418402671814\n",
      "Step 701 | Loss: 0.6903308629989624\n",
      "Step 801 | Loss: 0.5980582237243652\n",
      "Step 901 | Loss: 0.5570786595344543\n",
      "Step 1001 | Loss: 0.5642605423927307\n",
      "Step 1101 | Loss: 0.448148250579834\n",
      "Step 1201 | Loss: 0.5069699287414551\n",
      "Step 1301 | Loss: 0.3676404654979706\n",
      "Step 1401 | Loss: 0.4985246956348419\n",
      "Step 1501 | Loss: 0.45756128430366516\n",
      "Step 1601 | Loss: 0.49038973450660706\n",
      "Step 1701 | Loss: 0.48374316096305847\n",
      "Step 1801 | Loss: 0.5641265511512756\n",
      "Step 1901 | Loss: 0.33296531438827515\n",
      "Step 2001 | Loss: 0.3911436200141907\n",
      "Step 2101 | Loss: 0.4048636853694916\n",
      "Step 2201 | Loss: 0.5675773024559021\n",
      "Step 2301 | Loss: 0.4932587146759033\n",
      "Step 2401 | Loss: 0.36581799387931824\n",
      "0.4545598067495202 0.895\n",
      "Step 1 | Loss: 1.1935226917266846\n",
      "Step 101 | Loss: 0.7581308484077454\n",
      "Step 201 | Loss: 0.899024486541748\n",
      "Step 301 | Loss: 0.8870742321014404\n",
      "Step 401 | Loss: 0.7401869893074036\n",
      "Step 501 | Loss: 0.7389397621154785\n",
      "Step 601 | Loss: 0.4353245794773102\n",
      "Step 701 | Loss: 0.6619917154312134\n",
      "Step 801 | Loss: 0.47590115666389465\n",
      "Step 901 | Loss: 0.48945510387420654\n",
      "Step 1001 | Loss: 0.5458876490592957\n",
      "Step 1101 | Loss: 0.6672589778900146\n",
      "Step 1201 | Loss: 0.4619067907333374\n",
      "Step 1301 | Loss: 0.3588164150714874\n",
      "Step 1401 | Loss: 0.5162030458450317\n",
      "Step 1501 | Loss: 0.36515846848487854\n",
      "Step 1601 | Loss: 0.4640118479728699\n",
      "Step 1701 | Loss: 0.41257795691490173\n",
      "Step 1801 | Loss: 0.3683241307735443\n",
      "Step 1901 | Loss: 0.44584548473358154\n",
      "Step 2001 | Loss: 0.4880836606025696\n",
      "Step 2101 | Loss: 0.6085871458053589\n",
      "Step 2201 | Loss: 0.5163685083389282\n",
      "Step 2301 | Loss: 0.4336691200733185\n",
      "Step 2401 | Loss: 0.4732518494129181\n",
      "0.4559519860571412 0.8925\n",
      "Step 1 | Loss: 1.3282418251037598\n",
      "Step 101 | Loss: 0.7692867517471313\n",
      "Step 201 | Loss: 0.624161422252655\n",
      "Step 301 | Loss: 0.7352800965309143\n",
      "Step 401 | Loss: 0.6417770385742188\n",
      "Step 501 | Loss: 0.7133684158325195\n",
      "Step 601 | Loss: 0.6684807538986206\n",
      "Step 701 | Loss: 0.7378969788551331\n",
      "Step 801 | Loss: 0.7007584571838379\n",
      "Step 901 | Loss: 0.6767280101776123\n",
      "Step 1001 | Loss: 0.6741774082183838\n",
      "Step 1101 | Loss: 0.7479942440986633\n",
      "Step 1201 | Loss: 0.5475753545761108\n",
      "Step 1301 | Loss: 0.5457910895347595\n",
      "Step 1401 | Loss: 0.801249086856842\n",
      "Step 1501 | Loss: 0.6124422550201416\n",
      "Step 1601 | Loss: 0.6832208633422852\n",
      "Step 1701 | Loss: 0.5991044044494629\n",
      "Step 1801 | Loss: 0.8150225877761841\n",
      "Step 1901 | Loss: 0.5236398577690125\n",
      "Step 2001 | Loss: 0.7560714483261108\n",
      "Step 2101 | Loss: 0.6916239261627197\n",
      "Step 2201 | Loss: 0.7090561985969543\n",
      "Step 2301 | Loss: 0.6843087077140808\n",
      "Step 2401 | Loss: 0.5767562389373779\n",
      "0.6541033869997547 0.7725\n",
      "Step 1 | Loss: 2.180772542953491\n",
      "Step 101 | Loss: 0.9282904267311096\n",
      "Step 201 | Loss: 0.7983744740486145\n",
      "Step 301 | Loss: 0.7713233232498169\n",
      "Step 401 | Loss: 0.6740663647651672\n",
      "Step 501 | Loss: 0.7615676522254944\n",
      "Step 601 | Loss: 0.6500383019447327\n",
      "Step 701 | Loss: 0.6990416049957275\n",
      "Step 801 | Loss: 0.7453533411026001\n",
      "Step 901 | Loss: 0.743696928024292\n",
      "Step 1001 | Loss: 0.9035588502883911\n",
      "Step 1101 | Loss: 0.7678909301757812\n",
      "Step 1201 | Loss: 0.786560595035553\n",
      "Step 1301 | Loss: 0.6515855193138123\n",
      "Step 1401 | Loss: 0.9103366136550903\n",
      "Step 1501 | Loss: 0.6547162532806396\n",
      "Step 1601 | Loss: 0.8439005613327026\n",
      "Step 1701 | Loss: 0.720550537109375\n",
      "Step 1801 | Loss: 0.6184113621711731\n",
      "Step 1901 | Loss: 0.6879969239234924\n",
      "Step 2001 | Loss: 0.8013462424278259\n",
      "Step 2101 | Loss: 0.5719112157821655\n",
      "Step 2201 | Loss: 0.7337040305137634\n",
      "Step 2301 | Loss: 0.6584531664848328\n",
      "Step 2401 | Loss: 0.608478307723999\n",
      "0.7133347853600476 0.7325\n",
      "Step 1 | Loss: 1.4383519887924194\n",
      "Step 101 | Loss: 0.767955482006073\n",
      "Step 201 | Loss: 0.8264646530151367\n",
      "Step 301 | Loss: 0.68845534324646\n",
      "Step 401 | Loss: 0.6444602012634277\n",
      "Step 501 | Loss: 0.794686496257782\n",
      "Step 601 | Loss: 0.7397214770317078\n",
      "Step 701 | Loss: 0.7650333046913147\n",
      "Step 801 | Loss: 0.7364804744720459\n",
      "Step 901 | Loss: 0.6450023055076599\n",
      "Step 1001 | Loss: 0.6048479676246643\n",
      "Step 1101 | Loss: 0.8116728663444519\n",
      "Step 1201 | Loss: 0.8232832551002502\n",
      "Step 1301 | Loss: 0.6853630542755127\n",
      "Step 1401 | Loss: 0.8210725784301758\n",
      "Step 1501 | Loss: 0.7177666425704956\n",
      "Step 1601 | Loss: 0.8265293836593628\n",
      "Step 1701 | Loss: 0.7750479578971863\n",
      "Step 1801 | Loss: 0.8339465260505676\n",
      "Step 1901 | Loss: 0.6309984922409058\n",
      "Step 2001 | Loss: 0.8321663737297058\n",
      "Step 2101 | Loss: 0.682183563709259\n",
      "Step 2201 | Loss: 0.7461295127868652\n",
      "Step 2301 | Loss: 0.7595312595367432\n",
      "Step 2401 | Loss: 0.7042099237442017\n",
      "0.7157276285168697 0.7475\n",
      "Step 1 | Loss: 2.2724080085754395\n",
      "Step 101 | Loss: 0.8605603575706482\n",
      "Step 201 | Loss: 0.7452452182769775\n",
      "Step 301 | Loss: 0.8047177791595459\n",
      "Step 401 | Loss: 0.8051685690879822\n",
      "Step 501 | Loss: 0.7679487466812134\n",
      "Step 601 | Loss: 0.6919655203819275\n",
      "Step 701 | Loss: 0.691725492477417\n",
      "Step 801 | Loss: 0.6197064518928528\n",
      "Step 901 | Loss: 0.5812799334526062\n",
      "Step 1001 | Loss: 0.7657956480979919\n",
      "Step 1101 | Loss: 0.6749670505523682\n",
      "Step 1201 | Loss: 0.8674997091293335\n",
      "Step 1301 | Loss: 0.7180877327919006\n",
      "Step 1401 | Loss: 0.6487674117088318\n",
      "Step 1501 | Loss: 0.6918352246284485\n",
      "Step 1601 | Loss: 0.8005880117416382\n",
      "Step 1701 | Loss: 0.667866587638855\n",
      "Step 1801 | Loss: 0.6322951912879944\n",
      "Step 1901 | Loss: 0.714200496673584\n",
      "Step 2001 | Loss: 0.606909453868866\n",
      "Step 2101 | Loss: 0.7381008863449097\n",
      "Step 2201 | Loss: 0.969397246837616\n",
      "Step 2301 | Loss: 0.7172399759292603\n",
      "Step 2401 | Loss: 0.6459012031555176\n",
      "0.7330901531385243 0.73\n",
      "Step 1 | Loss: 1.2950760126113892\n",
      "Step 101 | Loss: 0.9204331040382385\n",
      "Step 201 | Loss: 0.7934563159942627\n",
      "Step 301 | Loss: 0.8287006616592407\n",
      "Step 401 | Loss: 0.8168346881866455\n",
      "Step 501 | Loss: 0.8379001617431641\n",
      "Step 601 | Loss: 0.6607915163040161\n",
      "Step 701 | Loss: 0.7842945456504822\n",
      "Step 801 | Loss: 0.688076913356781\n",
      "Step 901 | Loss: 0.6809496879577637\n",
      "Step 1001 | Loss: 0.7704226970672607\n",
      "Step 1101 | Loss: 0.6884304285049438\n",
      "Step 1201 | Loss: 0.7393843531608582\n",
      "Step 1301 | Loss: 0.7171294093132019\n",
      "Step 1401 | Loss: 0.7651522755622864\n",
      "Step 1501 | Loss: 0.79625004529953\n",
      "Step 1601 | Loss: 0.7469793558120728\n",
      "Step 1701 | Loss: 0.725862979888916\n",
      "Step 1801 | Loss: 0.7724706530570984\n",
      "Step 1901 | Loss: 0.7926065921783447\n",
      "Step 2001 | Loss: 0.8311534523963928\n",
      "Step 2101 | Loss: 0.7228869199752808\n",
      "Step 2201 | Loss: 0.6514816284179688\n",
      "Step 2301 | Loss: 0.675672173500061\n",
      "Step 2401 | Loss: 0.8122749924659729\n",
      "0.7162067087730902 0.74\n",
      "Step 1 | Loss: 1.3447742462158203\n",
      "Step 101 | Loss: 0.7853338718414307\n",
      "Step 201 | Loss: 0.7859437465667725\n",
      "Step 301 | Loss: 0.6605914831161499\n",
      "Step 401 | Loss: 0.8221774101257324\n",
      "Step 501 | Loss: 0.7345001101493835\n",
      "Step 601 | Loss: 0.7101090550422668\n",
      "Step 701 | Loss: 0.7178286910057068\n",
      "Step 801 | Loss: 0.5760451555252075\n",
      "Step 901 | Loss: 0.6891657114028931\n",
      "Step 1001 | Loss: 0.6190165877342224\n",
      "Step 1101 | Loss: 0.8088709115982056\n",
      "Step 1201 | Loss: 0.7214322090148926\n",
      "Step 1301 | Loss: 0.7079852819442749\n",
      "Step 1401 | Loss: 0.8186626434326172\n",
      "Step 1501 | Loss: 0.6992694139480591\n",
      "Step 1601 | Loss: 0.6198926568031311\n",
      "Step 1701 | Loss: 0.910335123538971\n",
      "Step 1801 | Loss: 0.7155868411064148\n",
      "Step 1901 | Loss: 0.6232855916023254\n",
      "Step 2001 | Loss: 0.7140216827392578\n",
      "Step 2101 | Loss: 0.7180842757225037\n",
      "Step 2201 | Loss: 0.7825621366500854\n",
      "Step 2301 | Loss: 0.7294551134109497\n",
      "Step 2401 | Loss: 0.5916582942008972\n",
      "0.71022830112991 0.7525\n",
      "Step 1 | Loss: 1.3923871517181396\n",
      "Step 101 | Loss: 0.6252254247665405\n",
      "Step 201 | Loss: 0.6227644681930542\n",
      "Step 301 | Loss: 0.49270057678222656\n",
      "Step 401 | Loss: 0.48921966552734375\n",
      "Step 501 | Loss: 0.5017483830451965\n",
      "Step 601 | Loss: 0.5159757733345032\n",
      "Step 701 | Loss: 0.4743981957435608\n",
      "Step 801 | Loss: 0.3567189872264862\n",
      "Step 901 | Loss: 0.4437602758407593\n",
      "Step 1001 | Loss: 0.44973400235176086\n",
      "Step 1101 | Loss: 0.4696716070175171\n",
      "Step 1201 | Loss: 0.3713323175907135\n",
      "Step 1301 | Loss: 0.28074440360069275\n",
      "Step 1401 | Loss: 0.48092013597488403\n",
      "Step 1501 | Loss: 0.32755663990974426\n",
      "Step 1601 | Loss: 0.44146353006362915\n",
      "Step 1701 | Loss: 0.4782865643501282\n",
      "Step 1801 | Loss: 0.40635859966278076\n",
      "Step 1901 | Loss: 0.35693982243537903\n",
      "Step 2001 | Loss: 0.4859447777271271\n",
      "Step 2101 | Loss: 0.40951237082481384\n",
      "Step 2201 | Loss: 0.3474377691745758\n",
      "Step 2301 | Loss: 0.41920727491378784\n",
      "Step 2401 | Loss: 0.49394989013671875\n",
      "0.3683860285414913 0.95\n",
      "Step 1 | Loss: 0.7738906741142273\n",
      "Step 101 | Loss: 0.6326717734336853\n",
      "Step 201 | Loss: 0.6234260201454163\n",
      "Step 301 | Loss: 0.6357616186141968\n",
      "Step 401 | Loss: 0.6237709522247314\n",
      "Step 501 | Loss: 0.5800376534461975\n",
      "Step 601 | Loss: 0.5260616540908813\n",
      "Step 701 | Loss: 0.5868648290634155\n",
      "Step 801 | Loss: 0.6617189645767212\n",
      "Step 901 | Loss: 0.49298903346061707\n",
      "Step 1001 | Loss: 0.5430577397346497\n",
      "Step 1101 | Loss: 0.5944052934646606\n",
      "Step 1201 | Loss: 0.5619244575500488\n",
      "Step 1301 | Loss: 0.557058572769165\n",
      "Step 1401 | Loss: 0.51589435338974\n",
      "Step 1501 | Loss: 0.5886708498001099\n",
      "Step 1601 | Loss: 0.6150491833686829\n",
      "Step 1701 | Loss: 0.589141845703125\n",
      "Step 1801 | Loss: 0.6679869890213013\n",
      "Step 1901 | Loss: 0.5261838436126709\n",
      "Step 2001 | Loss: 0.5472471714019775\n",
      "Step 2101 | Loss: 0.5311377644538879\n",
      "Step 2201 | Loss: 0.6632405519485474\n",
      "Step 2301 | Loss: 0.5458810329437256\n",
      "Step 2401 | Loss: 0.5839396715164185\n",
      "0.5232600084738664 0.945\n",
      "Step 1 | Loss: 0.9271687865257263\n",
      "Step 101 | Loss: 0.47852325439453125\n",
      "Step 201 | Loss: 0.5492072701454163\n",
      "Step 301 | Loss: 0.5791041851043701\n",
      "Step 401 | Loss: 0.5318905711174011\n",
      "Step 501 | Loss: 0.5823195576667786\n",
      "Step 601 | Loss: 0.4359513819217682\n",
      "Step 701 | Loss: 0.3033491373062134\n",
      "Step 801 | Loss: 0.4057154059410095\n",
      "Step 901 | Loss: 0.5126485824584961\n",
      "Step 1001 | Loss: 0.3779391050338745\n",
      "Step 1101 | Loss: 0.33580803871154785\n",
      "Step 1201 | Loss: 0.4225933253765106\n",
      "Step 1301 | Loss: 0.41503655910491943\n",
      "Step 1401 | Loss: 0.29170018434524536\n",
      "Step 1501 | Loss: 0.4598979949951172\n",
      "Step 1601 | Loss: 0.3791295289993286\n",
      "Step 1701 | Loss: 0.48889461159706116\n",
      "Step 1801 | Loss: 0.32688194513320923\n",
      "Step 1901 | Loss: 0.41452881693840027\n",
      "Step 2001 | Loss: 0.4792867600917816\n",
      "Step 2101 | Loss: 0.36003196239471436\n",
      "Step 2201 | Loss: 0.37415391206741333\n",
      "Step 2301 | Loss: 0.40929439663887024\n",
      "Step 2401 | Loss: 0.3670136332511902\n",
      "0.3624493379998883 0.95\n",
      "Step 1 | Loss: 1.3240888118743896\n",
      "Step 101 | Loss: 0.5051764845848083\n",
      "Step 201 | Loss: 0.43185287714004517\n",
      "Step 301 | Loss: 0.3456847369670868\n",
      "Step 401 | Loss: 0.38721543550491333\n",
      "Step 501 | Loss: 0.4574255049228668\n",
      "Step 601 | Loss: 0.4085405766963959\n",
      "Step 701 | Loss: 0.2653723359107971\n",
      "Step 801 | Loss: 0.3163887560367584\n",
      "Step 901 | Loss: 0.40772950649261475\n",
      "Step 1001 | Loss: 0.4934012293815613\n",
      "Step 1101 | Loss: 0.33573198318481445\n",
      "Step 1201 | Loss: 0.41681450605392456\n",
      "Step 1301 | Loss: 0.40487992763519287\n",
      "Step 1401 | Loss: 0.3085068464279175\n",
      "Step 1501 | Loss: 0.31667107343673706\n",
      "Step 1601 | Loss: 0.4480826258659363\n",
      "Step 1701 | Loss: 0.3552758991718292\n",
      "Step 1801 | Loss: 0.3320726752281189\n",
      "Step 1901 | Loss: 0.34387004375457764\n",
      "Step 2001 | Loss: 0.26368433237075806\n",
      "Step 2101 | Loss: 0.45917999744415283\n",
      "Step 2201 | Loss: 0.42505282163619995\n",
      "Step 2301 | Loss: 0.42481300234794617\n",
      "Step 2401 | Loss: 0.2635379731655121\n",
      "0.3631862745248679 0.9525\n",
      "Step 1 | Loss: 1.1634265184402466\n",
      "Step 101 | Loss: 0.5582379102706909\n",
      "Step 201 | Loss: 0.5179439187049866\n",
      "Step 301 | Loss: 0.5402865409851074\n",
      "Step 401 | Loss: 0.48416393995285034\n",
      "Step 501 | Loss: 0.4518686830997467\n",
      "Step 601 | Loss: 0.4767502248287201\n",
      "Step 701 | Loss: 0.397158145904541\n",
      "Step 801 | Loss: 0.3674648702144623\n",
      "Step 901 | Loss: 0.46541085839271545\n",
      "Step 1001 | Loss: 0.4056603014469147\n",
      "Step 1101 | Loss: 0.34833669662475586\n",
      "Step 1201 | Loss: 0.3950907588005066\n",
      "Step 1301 | Loss: 0.43681600689888\n",
      "Step 1401 | Loss: 0.32569971680641174\n",
      "Step 1501 | Loss: 0.4162455201148987\n",
      "Step 1601 | Loss: 0.4459378123283386\n",
      "Step 1701 | Loss: 0.37265679240226746\n",
      "Step 1801 | Loss: 0.47575002908706665\n",
      "Step 1901 | Loss: 0.41681545972824097\n",
      "Step 2001 | Loss: 0.37222766876220703\n",
      "Step 2101 | Loss: 0.3877371549606323\n",
      "Step 2201 | Loss: 0.34163305163383484\n",
      "Step 2301 | Loss: 0.36251622438430786\n",
      "Step 2401 | Loss: 0.3144336938858032\n",
      "0.36470819990800357 0.9475\n",
      "Step 1 | Loss: 0.6718515753746033\n",
      "Step 101 | Loss: 0.5203078389167786\n",
      "Step 201 | Loss: 0.45194804668426514\n",
      "Step 301 | Loss: 0.5436229109764099\n",
      "Step 401 | Loss: 0.46793872117996216\n",
      "Step 501 | Loss: 0.43423497676849365\n",
      "Step 601 | Loss: 0.42534971237182617\n",
      "Step 701 | Loss: 0.471906840801239\n",
      "Step 801 | Loss: 0.461510568857193\n",
      "Step 901 | Loss: 0.39518266916275024\n",
      "Step 1001 | Loss: 0.45520541071891785\n",
      "Step 1101 | Loss: 0.4296516478061676\n",
      "Step 1201 | Loss: 0.4586746394634247\n",
      "Step 1301 | Loss: 0.5102100372314453\n",
      "Step 1401 | Loss: 0.5777798295021057\n",
      "Step 1501 | Loss: 0.534403920173645\n",
      "Step 1601 | Loss: 0.61748206615448\n",
      "Step 1701 | Loss: 0.47529110312461853\n",
      "Step 1801 | Loss: 0.48853975534439087\n",
      "Step 1901 | Loss: 0.4433113932609558\n",
      "Step 2001 | Loss: 0.49341779947280884\n",
      "Step 2101 | Loss: 0.5052154064178467\n",
      "Step 2201 | Loss: 0.47043576836586\n",
      "Step 2301 | Loss: 0.5161005258560181\n",
      "Step 2401 | Loss: 0.5257562398910522\n",
      "0.46060382547848844 0.925\n",
      "Step 1 | Loss: 1.069521188735962\n",
      "Step 101 | Loss: 0.8338164687156677\n",
      "Step 201 | Loss: 0.7787928581237793\n",
      "Step 301 | Loss: 0.5235549211502075\n",
      "Step 401 | Loss: 0.48065459728240967\n",
      "Step 501 | Loss: 0.5856347680091858\n",
      "Step 601 | Loss: 0.4274957478046417\n",
      "Step 701 | Loss: 0.5164278745651245\n",
      "Step 801 | Loss: 0.5398654341697693\n",
      "Step 901 | Loss: 0.3861861228942871\n",
      "Step 1001 | Loss: 0.5666142702102661\n",
      "Step 1101 | Loss: 0.5018500089645386\n",
      "Step 1201 | Loss: 0.5933520197868347\n",
      "Step 1301 | Loss: 0.5695539116859436\n",
      "Step 1401 | Loss: 0.47440797090530396\n",
      "Step 1501 | Loss: 0.46398869156837463\n",
      "Step 1601 | Loss: 0.4495360851287842\n",
      "Step 1701 | Loss: 0.5082505345344543\n",
      "Step 1801 | Loss: 0.563408613204956\n",
      "Step 1901 | Loss: 0.3987843692302704\n",
      "Step 2001 | Loss: 0.5906347632408142\n",
      "Step 2101 | Loss: 0.5364251136779785\n",
      "Step 2201 | Loss: 0.546314537525177\n",
      "Step 2301 | Loss: 0.47539910674095154\n",
      "Step 2401 | Loss: 0.4876188635826111\n",
      "0.4960232395934375 0.9275\n",
      "Step 1 | Loss: 1.0613651275634766\n",
      "Step 101 | Loss: 0.6941887736320496\n",
      "Step 201 | Loss: 0.6459734439849854\n",
      "Step 301 | Loss: 0.6329830884933472\n",
      "Step 401 | Loss: 0.5686519145965576\n",
      "Step 501 | Loss: 0.5427350997924805\n",
      "Step 601 | Loss: 0.6301182508468628\n",
      "Step 701 | Loss: 0.5267698168754578\n",
      "Step 801 | Loss: 0.6190933585166931\n",
      "Step 901 | Loss: 0.5375570058822632\n",
      "Step 1001 | Loss: 0.46213433146476746\n",
      "Step 1101 | Loss: 0.5648775696754456\n",
      "Step 1201 | Loss: 0.5116382837295532\n",
      "Step 1301 | Loss: 0.5110229253768921\n",
      "Step 1401 | Loss: 0.6443714499473572\n",
      "Step 1501 | Loss: 0.5325996279716492\n",
      "Step 1601 | Loss: 0.6137551665306091\n",
      "Step 1701 | Loss: 0.5831156969070435\n",
      "Step 1801 | Loss: 0.5483071208000183\n",
      "Step 1901 | Loss: 0.6507166624069214\n",
      "Step 2001 | Loss: 0.4952581226825714\n",
      "Step 2101 | Loss: 0.6424213647842407\n",
      "Step 2201 | Loss: 0.49266231060028076\n",
      "Step 2301 | Loss: 0.4919487237930298\n",
      "Step 2401 | Loss: 0.6140929460525513\n",
      "0.5035705951741453 0.88\n",
      "Step 1 | Loss: 1.0309545993804932\n",
      "Step 101 | Loss: 0.6552298069000244\n",
      "Step 201 | Loss: 0.4826584458351135\n",
      "Step 301 | Loss: 0.443250447511673\n",
      "Step 401 | Loss: 0.5143710374832153\n",
      "Step 501 | Loss: 0.506283164024353\n",
      "Step 601 | Loss: 0.42123088240623474\n",
      "Step 701 | Loss: 0.49678266048431396\n",
      "Step 801 | Loss: 0.4446602165699005\n",
      "Step 901 | Loss: 0.41409963369369507\n",
      "Step 1001 | Loss: 0.47607362270355225\n",
      "Step 1101 | Loss: 0.30663520097732544\n",
      "Step 1201 | Loss: 0.4548482596874237\n",
      "Step 1301 | Loss: 0.4257891774177551\n",
      "Step 1401 | Loss: 0.48084911704063416\n",
      "Step 1501 | Loss: 0.32514891028404236\n",
      "Step 1601 | Loss: 0.5172628164291382\n",
      "Step 1701 | Loss: 0.41947460174560547\n",
      "Step 1801 | Loss: 0.47802138328552246\n",
      "Step 1901 | Loss: 0.3816465139389038\n",
      "Step 2001 | Loss: 0.4903790354728699\n",
      "Step 2101 | Loss: 0.43141257762908936\n",
      "Step 2201 | Loss: 0.4072682559490204\n",
      "Step 2301 | Loss: 0.4155091345310211\n",
      "Step 2401 | Loss: 0.40553852915763855\n",
      "0.4496866972431179 0.91\n",
      "Step 1 | Loss: 1.1760985851287842\n",
      "Step 101 | Loss: 0.7952927350997925\n",
      "Step 201 | Loss: 0.6194596886634827\n",
      "Step 301 | Loss: 0.5898879766464233\n",
      "Step 401 | Loss: 0.5750004053115845\n",
      "Step 501 | Loss: 0.43284881114959717\n",
      "Step 601 | Loss: 0.4072971045970917\n",
      "Step 701 | Loss: 0.6169054508209229\n",
      "Step 801 | Loss: 0.5347374081611633\n",
      "Step 901 | Loss: 0.5405882596969604\n",
      "Step 1001 | Loss: 0.4343765377998352\n",
      "Step 1101 | Loss: 0.3371015787124634\n",
      "Step 1201 | Loss: 0.41954144835472107\n",
      "Step 1301 | Loss: 0.4043419063091278\n",
      "Step 1401 | Loss: 0.5746066570281982\n",
      "Step 1501 | Loss: 0.5479850172996521\n",
      "Step 1601 | Loss: 0.38468632102012634\n",
      "Step 1701 | Loss: 0.4100736081600189\n",
      "Step 1801 | Loss: 0.5441892147064209\n",
      "Step 1901 | Loss: 0.6119396686553955\n",
      "Step 2001 | Loss: 0.6196070909500122\n",
      "Step 2101 | Loss: 0.5135063529014587\n",
      "Step 2201 | Loss: 0.45024433732032776\n",
      "Step 2301 | Loss: 0.5482174754142761\n",
      "Step 2401 | Loss: 0.5138130784034729\n",
      "0.4661265147141308 0.92\n",
      "Step 1 | Loss: 1.487630844116211\n",
      "Step 101 | Loss: 0.30947399139404297\n",
      "Step 201 | Loss: 0.523936927318573\n",
      "Step 301 | Loss: 0.340768963098526\n",
      "Step 401 | Loss: 0.5662720799446106\n",
      "Step 501 | Loss: 0.5369623303413391\n",
      "Step 601 | Loss: 0.3465898633003235\n",
      "Step 701 | Loss: 0.48574143648147583\n",
      "Step 801 | Loss: 0.3855156898498535\n",
      "Step 901 | Loss: 0.32786238193511963\n",
      "Step 1001 | Loss: 0.23094792664051056\n",
      "Step 1101 | Loss: 0.377725750207901\n",
      "Step 1201 | Loss: 0.3263581693172455\n",
      "Step 1301 | Loss: 0.38304537534713745\n",
      "Step 1401 | Loss: 0.5381150245666504\n",
      "Step 1501 | Loss: 0.2981681227684021\n",
      "Step 1601 | Loss: 0.5547531247138977\n",
      "Step 1701 | Loss: 0.3484712541103363\n",
      "Step 1801 | Loss: 0.3961485028266907\n",
      "Step 1901 | Loss: 0.494917631149292\n",
      "Step 2001 | Loss: 0.4262506067752838\n",
      "Step 2101 | Loss: 0.32084429264068604\n",
      "Step 2201 | Loss: 0.3783385157585144\n",
      "Step 2301 | Loss: 0.29661327600479126\n",
      "Step 2401 | Loss: 0.24288281798362732\n",
      "0.3730662030284741 0.8775\n",
      "Step 1 | Loss: 1.3165603876113892\n",
      "Step 101 | Loss: 0.47091490030288696\n",
      "Step 201 | Loss: 0.512404203414917\n",
      "Step 301 | Loss: 0.556155800819397\n",
      "Step 401 | Loss: 0.3016558289527893\n",
      "Step 501 | Loss: 0.5533438324928284\n",
      "Step 601 | Loss: 0.3530806303024292\n",
      "Step 701 | Loss: 0.4786146283149719\n",
      "Step 801 | Loss: 0.4151248335838318\n",
      "Step 901 | Loss: 0.43865451216697693\n",
      "Step 1001 | Loss: 0.4361756145954132\n",
      "Step 1101 | Loss: 0.3503323197364807\n",
      "Step 1201 | Loss: 0.40726515650749207\n",
      "Step 1301 | Loss: 0.5813959836959839\n",
      "Step 1401 | Loss: 0.5052323937416077\n",
      "Step 1501 | Loss: 0.4936988949775696\n",
      "Step 1601 | Loss: 0.46759334206581116\n",
      "Step 1701 | Loss: 0.22571446001529694\n",
      "Step 1801 | Loss: 0.4553990066051483\n",
      "Step 1901 | Loss: 0.4036520719528198\n",
      "Step 2001 | Loss: 0.3470897674560547\n",
      "Step 2101 | Loss: 0.5052912831306458\n",
      "Step 2201 | Loss: 0.5201008319854736\n",
      "Step 2301 | Loss: 0.4502125084400177\n",
      "Step 2401 | Loss: 0.4523882269859314\n",
      "0.4670317259944585 0.8625\n",
      "Step 1 | Loss: 1.563755750656128\n",
      "Step 101 | Loss: 0.3388015031814575\n",
      "Step 201 | Loss: 0.6180669069290161\n",
      "Step 301 | Loss: 0.2128777652978897\n",
      "Step 401 | Loss: 0.3295004963874817\n",
      "Step 501 | Loss: 0.39352354407310486\n",
      "Step 601 | Loss: 0.3346628248691559\n",
      "Step 701 | Loss: 0.4476928412914276\n",
      "Step 801 | Loss: 0.4004596173763275\n",
      "Step 901 | Loss: 0.3743666708469391\n",
      "Step 1001 | Loss: 0.3582611083984375\n",
      "Step 1101 | Loss: 0.5010478496551514\n",
      "Step 1201 | Loss: 0.3196106553077698\n",
      "Step 1301 | Loss: 0.4745231568813324\n",
      "Step 1401 | Loss: 0.23557208478450775\n",
      "Step 1501 | Loss: 0.3614294230937958\n",
      "Step 1601 | Loss: 0.37994420528411865\n",
      "Step 1701 | Loss: 0.28860580921173096\n",
      "Step 1801 | Loss: 0.2827931344509125\n",
      "Step 1901 | Loss: 0.5285496711730957\n",
      "Step 2001 | Loss: 0.4643954932689667\n",
      "Step 2101 | Loss: 0.3205864131450653\n",
      "Step 2201 | Loss: 0.43083661794662476\n",
      "Step 2301 | Loss: 0.37738192081451416\n",
      "Step 2401 | Loss: 0.48551809787750244\n",
      "0.37215280770016085 0.8825\n",
      "Step 1 | Loss: 2.0116512775421143\n",
      "Step 101 | Loss: 0.8726465106010437\n",
      "Step 201 | Loss: 0.4094884991645813\n",
      "Step 301 | Loss: 0.35800161957740784\n",
      "Step 401 | Loss: 0.42523741722106934\n",
      "Step 501 | Loss: 0.3780972957611084\n",
      "Step 601 | Loss: 0.3857327103614807\n",
      "Step 701 | Loss: 0.3328707814216614\n",
      "Step 801 | Loss: 0.6163638234138489\n",
      "Step 901 | Loss: 0.439240425825119\n",
      "Step 1001 | Loss: 0.462066650390625\n",
      "Step 1101 | Loss: 0.4193227291107178\n",
      "Step 1201 | Loss: 0.3396713137626648\n",
      "Step 1301 | Loss: 0.4018760919570923\n",
      "Step 1401 | Loss: 0.4842541813850403\n",
      "Step 1501 | Loss: 0.3634597361087799\n",
      "Step 1601 | Loss: 0.41221025586128235\n",
      "Step 1701 | Loss: 0.48541533946990967\n",
      "Step 1801 | Loss: 0.5427038669586182\n",
      "Step 1901 | Loss: 0.4028870463371277\n",
      "Step 2001 | Loss: 0.3671188950538635\n",
      "Step 2101 | Loss: 0.41418886184692383\n",
      "Step 2201 | Loss: 0.3305134177207947\n",
      "Step 2301 | Loss: 0.3967078924179077\n",
      "Step 2401 | Loss: 0.29761064052581787\n",
      "0.3713827643354417 0.8775\n",
      "Step 1 | Loss: 1.0811797380447388\n",
      "Step 101 | Loss: 0.7393043041229248\n",
      "Step 201 | Loss: 0.6276829838752747\n",
      "Step 301 | Loss: 0.491252601146698\n",
      "Step 401 | Loss: 0.40830644965171814\n",
      "Step 501 | Loss: 0.29486241936683655\n",
      "Step 601 | Loss: 0.526848554611206\n",
      "Step 701 | Loss: 0.22214742004871368\n",
      "Step 801 | Loss: 0.3014715015888214\n",
      "Step 901 | Loss: 0.3525872528553009\n",
      "Step 1001 | Loss: 0.45402228832244873\n",
      "Step 1101 | Loss: 0.3391135036945343\n",
      "Step 1201 | Loss: 0.36228588223457336\n",
      "Step 1301 | Loss: 0.24471260607242584\n",
      "Step 1401 | Loss: 0.24418537318706512\n",
      "Step 1501 | Loss: 0.20003832876682281\n",
      "Step 1601 | Loss: 0.35738086700439453\n",
      "Step 1701 | Loss: 0.3069784939289093\n",
      "Step 1801 | Loss: 0.32076776027679443\n",
      "Step 1901 | Loss: 0.3625135123729706\n",
      "Step 2001 | Loss: 0.4477308988571167\n",
      "Step 2101 | Loss: 0.359868586063385\n",
      "Step 2201 | Loss: 0.4391801953315735\n",
      "Step 2301 | Loss: 0.3750685155391693\n",
      "Step 2401 | Loss: 0.14775553345680237\n",
      "0.3000870888742927 0.9075\n",
      "Step 1 | Loss: 0.8715369701385498\n",
      "Step 101 | Loss: 0.6823569536209106\n",
      "Step 201 | Loss: 0.8741462826728821\n",
      "Step 301 | Loss: 1.0107309818267822\n",
      "Step 401 | Loss: 0.7125299572944641\n",
      "Step 501 | Loss: 0.7060741186141968\n",
      "Step 601 | Loss: 0.806942880153656\n",
      "Step 701 | Loss: 0.7840881943702698\n",
      "Step 801 | Loss: 0.4797145426273346\n",
      "Step 901 | Loss: 0.657364010810852\n",
      "Step 1001 | Loss: 0.7825730443000793\n",
      "Step 1101 | Loss: 0.4029785990715027\n",
      "Step 1201 | Loss: 0.7608050107955933\n",
      "Step 1301 | Loss: 0.4477812647819519\n",
      "Step 1401 | Loss: 0.9249160885810852\n",
      "Step 1501 | Loss: 0.6366101503372192\n",
      "Step 1601 | Loss: 0.8361265659332275\n",
      "Step 1701 | Loss: 0.7340670824050903\n",
      "Step 1801 | Loss: 0.6200094819068909\n",
      "Step 1901 | Loss: 0.5975547432899475\n",
      "Step 2001 | Loss: 1.0375275611877441\n",
      "Step 2101 | Loss: 0.8403735160827637\n",
      "Step 2201 | Loss: 0.9563722014427185\n",
      "Step 2301 | Loss: 0.7225120663642883\n",
      "Step 2401 | Loss: 0.5261301398277283\n",
      "0.7678320369934842 0.7175\n",
      "Step 1 | Loss: 1.5933042764663696\n",
      "Step 101 | Loss: 0.6118355989456177\n",
      "Step 201 | Loss: 0.7098190188407898\n",
      "Step 301 | Loss: 0.8460727334022522\n",
      "Step 401 | Loss: 0.5060999393463135\n",
      "Step 501 | Loss: 0.6618467569351196\n",
      "Step 601 | Loss: 0.7558786869049072\n",
      "Step 701 | Loss: 0.5988283157348633\n",
      "Step 801 | Loss: 0.7838959693908691\n",
      "Step 901 | Loss: 0.7226113677024841\n",
      "Step 1001 | Loss: 0.36543047428131104\n",
      "Step 1101 | Loss: 0.6743996143341064\n",
      "Step 1201 | Loss: 0.6608946323394775\n",
      "Step 1301 | Loss: 0.7052798271179199\n",
      "Step 1401 | Loss: 0.959253191947937\n",
      "Step 1501 | Loss: 0.45118823647499084\n",
      "Step 1601 | Loss: 0.8643912076950073\n",
      "Step 1701 | Loss: 1.1474522352218628\n",
      "Step 1801 | Loss: 0.5432395339012146\n",
      "Step 1901 | Loss: 0.6505259275436401\n",
      "Step 2001 | Loss: 0.6103160381317139\n",
      "Step 2101 | Loss: 0.6572102904319763\n",
      "Step 2201 | Loss: 0.9945898056030273\n",
      "Step 2301 | Loss: 0.9474544525146484\n",
      "Step 2401 | Loss: 0.7503557205200195\n",
      "0.7690063396707245 0.7175\n",
      "Step 1 | Loss: 0.6375653147697449\n",
      "Step 101 | Loss: 0.5824596285820007\n",
      "Step 201 | Loss: 0.619438886642456\n",
      "Step 301 | Loss: 0.690078616142273\n",
      "Step 401 | Loss: 0.8827714920043945\n",
      "Step 501 | Loss: 0.6521345376968384\n",
      "Step 601 | Loss: 0.965941309928894\n",
      "Step 701 | Loss: 0.6069065928459167\n",
      "Step 801 | Loss: 0.5675328969955444\n",
      "Step 901 | Loss: 0.7429479956626892\n",
      "Step 1001 | Loss: 0.7511725425720215\n",
      "Step 1101 | Loss: 0.8460866212844849\n",
      "Step 1201 | Loss: 0.6410143375396729\n",
      "Step 1301 | Loss: 0.6150871515274048\n",
      "Step 1401 | Loss: 0.6130409240722656\n",
      "Step 1501 | Loss: 0.7585073113441467\n",
      "Step 1601 | Loss: 0.6473861336708069\n",
      "Step 1701 | Loss: 0.6342782378196716\n",
      "Step 1801 | Loss: 0.9509124159812927\n",
      "Step 1901 | Loss: 0.6918963193893433\n",
      "Step 2001 | Loss: 0.8889620304107666\n",
      "Step 2101 | Loss: 0.5034717321395874\n",
      "Step 2201 | Loss: 0.7888917922973633\n",
      "Step 2301 | Loss: 0.6815886497497559\n",
      "Step 2401 | Loss: 0.5129154324531555\n",
      "0.767750213196397 0.7175\n",
      "Step 1 | Loss: 0.7587867975234985\n",
      "Step 101 | Loss: 0.610107958316803\n",
      "Step 201 | Loss: 0.8031275272369385\n",
      "Step 301 | Loss: 0.72444087266922\n",
      "Step 401 | Loss: 0.7714200615882874\n",
      "Step 501 | Loss: 0.6207926273345947\n",
      "Step 601 | Loss: 0.6767798662185669\n",
      "Step 701 | Loss: 0.9009793996810913\n",
      "Step 801 | Loss: 0.5528463125228882\n",
      "Step 901 | Loss: 0.6872565746307373\n",
      "Step 1001 | Loss: 1.0581687688827515\n",
      "Step 1101 | Loss: 0.6460338234901428\n",
      "Step 1201 | Loss: 0.6128441691398621\n",
      "Step 1301 | Loss: 0.8451281785964966\n",
      "Step 1401 | Loss: 1.0491780042648315\n",
      "Step 1501 | Loss: 0.7668483257293701\n",
      "Step 1601 | Loss: 0.6445053815841675\n",
      "Step 1701 | Loss: 0.6233923435211182\n",
      "Step 1801 | Loss: 0.9670063257217407\n",
      "Step 1901 | Loss: 0.4876313805580139\n",
      "Step 2001 | Loss: 0.6708077192306519\n",
      "Step 2101 | Loss: 0.5267575979232788\n",
      "Step 2201 | Loss: 0.6279052495956421\n",
      "Step 2301 | Loss: 0.5317713618278503\n",
      "Step 2401 | Loss: 0.6327241063117981\n",
      "0.7659227900027077 0.7175\n",
      "Step 1 | Loss: 0.6950656771659851\n",
      "Step 101 | Loss: 0.5978036522865295\n",
      "Step 201 | Loss: 1.086734414100647\n",
      "Step 301 | Loss: 0.602504551410675\n",
      "Step 401 | Loss: 0.8404740691184998\n",
      "Step 501 | Loss: 0.5431888699531555\n",
      "Step 601 | Loss: 0.7271166443824768\n",
      "Step 701 | Loss: 0.8920506834983826\n",
      "Step 801 | Loss: 0.6154709458351135\n",
      "Step 901 | Loss: 0.8091917634010315\n",
      "Step 1001 | Loss: 0.4857437312602997\n",
      "Step 1101 | Loss: 0.5675843954086304\n",
      "Step 1201 | Loss: 0.5995823740959167\n",
      "Step 1301 | Loss: 0.6477988958358765\n",
      "Step 1401 | Loss: 0.7812239527702332\n",
      "Step 1501 | Loss: 0.5409563779830933\n",
      "Step 1601 | Loss: 0.9888473749160767\n",
      "Step 1701 | Loss: 0.5577735900878906\n",
      "Step 1801 | Loss: 0.5727441310882568\n",
      "Step 1901 | Loss: 0.6544252634048462\n",
      "Step 2001 | Loss: 0.5308915972709656\n",
      "Step 2101 | Loss: 0.6737613081932068\n",
      "Step 2201 | Loss: 0.614994466304779\n",
      "Step 2301 | Loss: 0.41286271810531616\n",
      "Step 2401 | Loss: 0.589716911315918\n",
      "0.7678826058584545 0.7175\n",
      "Step 1 | Loss: 1.0955818891525269\n",
      "Step 101 | Loss: 0.9196353554725647\n",
      "Step 201 | Loss: 0.994999885559082\n",
      "Step 301 | Loss: 0.9903243184089661\n",
      "Step 401 | Loss: 0.8860946297645569\n",
      "Step 501 | Loss: 0.9181606769561768\n",
      "Step 601 | Loss: 0.8957098722457886\n",
      "Step 701 | Loss: 0.9248027801513672\n",
      "Step 801 | Loss: 0.9261158108711243\n",
      "Step 901 | Loss: 0.9900785684585571\n",
      "Step 1001 | Loss: 0.8492386341094971\n",
      "Step 1101 | Loss: 0.9366183280944824\n",
      "Step 1201 | Loss: 1.0681917667388916\n",
      "Step 1301 | Loss: 0.9109691381454468\n",
      "Step 1401 | Loss: 1.0081708431243896\n",
      "Step 1501 | Loss: 0.8981536030769348\n",
      "Step 1601 | Loss: 0.896866500377655\n",
      "Step 1701 | Loss: 0.7951551079750061\n",
      "Step 1801 | Loss: 0.937209963798523\n",
      "Step 1901 | Loss: 0.8449498414993286\n",
      "Step 2001 | Loss: 0.9136878252029419\n",
      "Step 2101 | Loss: 0.7535992860794067\n",
      "Step 2201 | Loss: 0.698098361492157\n",
      "Step 2301 | Loss: 0.8534507155418396\n",
      "Step 2401 | Loss: 0.8533936142921448\n",
      "0.7278191961076276 0.7875\n",
      "Step 1 | Loss: 0.9369372129440308\n",
      "Step 101 | Loss: 0.6427508592605591\n",
      "Step 201 | Loss: 0.643403172492981\n",
      "Step 301 | Loss: 0.8263115286827087\n",
      "Step 401 | Loss: 0.6208032369613647\n",
      "Step 501 | Loss: 0.6629774570465088\n",
      "Step 601 | Loss: 0.5023975968360901\n",
      "Step 701 | Loss: 0.5276122093200684\n",
      "Step 801 | Loss: 0.5538168549537659\n",
      "Step 901 | Loss: 0.7080057263374329\n",
      "Step 1001 | Loss: 0.8839635252952576\n",
      "Step 1101 | Loss: 0.6875437498092651\n",
      "Step 1201 | Loss: 0.5028426051139832\n",
      "Step 1301 | Loss: 0.8443325161933899\n",
      "Step 1401 | Loss: 0.5227724313735962\n",
      "Step 1501 | Loss: 0.6835989952087402\n",
      "Step 1601 | Loss: 0.3935542702674866\n",
      "Step 1701 | Loss: 0.6809833645820618\n",
      "Step 1801 | Loss: 0.6269325613975525\n",
      "Step 1901 | Loss: 0.3344552516937256\n",
      "Step 2001 | Loss: 0.7254446744918823\n",
      "Step 2101 | Loss: 0.49984240531921387\n",
      "Step 2201 | Loss: 0.49437347054481506\n",
      "Step 2301 | Loss: 0.4706108570098877\n",
      "Step 2401 | Loss: 0.5349927544593811\n",
      "0.6035902747740683 0.835\n",
      "Step 1 | Loss: 0.9447835683822632\n",
      "Step 101 | Loss: 0.8690143823623657\n",
      "Step 201 | Loss: 0.8813886642456055\n",
      "Step 301 | Loss: 0.6032236814498901\n",
      "Step 401 | Loss: 0.6543166041374207\n",
      "Step 501 | Loss: 0.6673217415809631\n",
      "Step 601 | Loss: 0.5539407730102539\n",
      "Step 701 | Loss: 0.6849507093429565\n",
      "Step 801 | Loss: 0.6386346220970154\n",
      "Step 901 | Loss: 0.568985641002655\n",
      "Step 1001 | Loss: 0.6116153597831726\n",
      "Step 1101 | Loss: 0.594317615032196\n",
      "Step 1201 | Loss: 0.6223989725112915\n",
      "Step 1301 | Loss: 0.5064027309417725\n",
      "Step 1401 | Loss: 0.7512926459312439\n",
      "Step 1501 | Loss: 0.759386420249939\n",
      "Step 1601 | Loss: 0.7621209621429443\n",
      "Step 1701 | Loss: 0.6344173550605774\n",
      "Step 1801 | Loss: 0.570070207118988\n",
      "Step 1901 | Loss: 0.4744407534599304\n",
      "Step 2001 | Loss: 0.5813366770744324\n",
      "Step 2101 | Loss: 0.4386082589626312\n",
      "Step 2201 | Loss: 0.6514476537704468\n",
      "Step 2301 | Loss: 0.5098147392272949\n",
      "Step 2401 | Loss: 0.5009235739707947\n",
      "0.6002020789161189 0.8275\n",
      "Step 1 | Loss: 1.3542150259017944\n",
      "Step 101 | Loss: 1.1307134628295898\n",
      "Step 201 | Loss: 0.6738370060920715\n",
      "Step 301 | Loss: 0.6932651400566101\n",
      "Step 401 | Loss: 0.6631631851196289\n",
      "Step 501 | Loss: 0.37006694078445435\n",
      "Step 601 | Loss: 0.5029140710830688\n",
      "Step 701 | Loss: 0.656166136264801\n",
      "Step 801 | Loss: 0.5350730419158936\n",
      "Step 901 | Loss: 0.7938727140426636\n",
      "Step 1001 | Loss: 0.45927050709724426\n",
      "Step 1101 | Loss: 0.850052535533905\n",
      "Step 1201 | Loss: 0.7258417010307312\n",
      "Step 1301 | Loss: 0.49404412508010864\n",
      "Step 1401 | Loss: 0.5863988995552063\n",
      "Step 1501 | Loss: 0.6815353035926819\n",
      "Step 1601 | Loss: 0.6502292156219482\n",
      "Step 1701 | Loss: 0.5696257948875427\n",
      "Step 1801 | Loss: 0.6376699805259705\n",
      "Step 1901 | Loss: 0.6643437147140503\n",
      "Step 2001 | Loss: 0.5779110789299011\n",
      "Step 2101 | Loss: 0.6238534450531006\n",
      "Step 2201 | Loss: 0.6272150278091431\n",
      "Step 2301 | Loss: 0.855441153049469\n",
      "Step 2401 | Loss: 0.6488772034645081\n",
      "0.6303025930610506 0.81\n",
      "Step 1 | Loss: 1.2479395866394043\n",
      "Step 101 | Loss: 1.013059139251709\n",
      "Step 201 | Loss: 0.9614023566246033\n",
      "Step 301 | Loss: 0.8662135601043701\n",
      "Step 401 | Loss: 0.8657926917076111\n",
      "Step 501 | Loss: 0.6958779692649841\n",
      "Step 601 | Loss: 0.7868233919143677\n",
      "Step 701 | Loss: 0.8037179112434387\n",
      "Step 801 | Loss: 0.7251167297363281\n",
      "Step 901 | Loss: 0.7912561893463135\n",
      "Step 1001 | Loss: 0.6064607501029968\n",
      "Step 1101 | Loss: 0.7430368661880493\n",
      "Step 1201 | Loss: 0.7243424654006958\n",
      "Step 1301 | Loss: 0.6763926148414612\n",
      "Step 1401 | Loss: 0.5522666573524475\n",
      "Step 1501 | Loss: 0.9271364808082581\n",
      "Step 1601 | Loss: 0.7458230257034302\n",
      "Step 1701 | Loss: 0.6950355172157288\n",
      "Step 1801 | Loss: 0.5538202524185181\n",
      "Step 1901 | Loss: 0.8440272808074951\n",
      "Step 2001 | Loss: 0.9009366035461426\n",
      "Step 2101 | Loss: 0.5307785272598267\n",
      "Step 2201 | Loss: 0.5461738109588623\n",
      "Step 2301 | Loss: 0.5522933006286621\n",
      "Step 2401 | Loss: 0.8609659671783447\n",
      "0.6411474876020337 0.8125\n",
      "Step 1 | Loss: 0.5400864481925964\n",
      "Step 101 | Loss: 0.5222652554512024\n",
      "Step 201 | Loss: 0.2785003185272217\n",
      "Step 301 | Loss: 0.3878910541534424\n",
      "Step 401 | Loss: 0.2745036482810974\n",
      "Step 501 | Loss: 0.25547271966934204\n",
      "Step 601 | Loss: 0.2760086953639984\n",
      "Step 701 | Loss: 0.28254806995391846\n",
      "Step 801 | Loss: 0.18231220543384552\n",
      "Step 901 | Loss: 0.24985142052173615\n",
      "Step 1001 | Loss: 0.17014451324939728\n",
      "Step 1101 | Loss: 0.12525393068790436\n",
      "Step 1201 | Loss: 0.18975920975208282\n",
      "Step 1301 | Loss: 0.1752806007862091\n",
      "Step 1401 | Loss: 0.39497798681259155\n",
      "Step 1501 | Loss: 0.3203372061252594\n",
      "Step 1601 | Loss: 0.16927169263362885\n",
      "Step 1701 | Loss: 0.11757516860961914\n",
      "Step 1801 | Loss: 0.11071006208658218\n",
      "Step 1901 | Loss: 0.1388908326625824\n",
      "Step 2001 | Loss: 0.16426394879817963\n",
      "Step 2101 | Loss: 0.27569714188575745\n",
      "Step 2201 | Loss: 0.06643352657556534\n",
      "Step 2301 | Loss: 0.16454458236694336\n",
      "Step 2401 | Loss: 0.14813435077667236\n",
      "0.1873085598869457 0.955\n",
      "Step 1 | Loss: 1.1992952823638916\n",
      "Step 101 | Loss: 0.5329859256744385\n",
      "Step 201 | Loss: 0.46662798523902893\n",
      "Step 301 | Loss: 0.19702914357185364\n",
      "Step 401 | Loss: 0.34140443801879883\n",
      "Step 501 | Loss: 0.28357234597206116\n",
      "Step 601 | Loss: 0.353175550699234\n",
      "Step 701 | Loss: 0.2379516363143921\n",
      "Step 801 | Loss: 0.34797224402427673\n",
      "Step 901 | Loss: 0.15234573185443878\n",
      "Step 1001 | Loss: 0.11626338958740234\n",
      "Step 1101 | Loss: 0.1597655713558197\n",
      "Step 1201 | Loss: 0.18941551446914673\n",
      "Step 1301 | Loss: 0.1187489777803421\n",
      "Step 1401 | Loss: 0.14729033410549164\n",
      "Step 1501 | Loss: 0.12722867727279663\n",
      "Step 1601 | Loss: 0.07058108597993851\n",
      "Step 1701 | Loss: 0.16207259893417358\n",
      "Step 1801 | Loss: 0.250809907913208\n",
      "Step 1901 | Loss: 0.2680484354496002\n",
      "Step 2001 | Loss: 0.10875758528709412\n",
      "Step 2101 | Loss: 0.16465330123901367\n",
      "Step 2201 | Loss: 0.15153859555721283\n",
      "Step 2301 | Loss: 0.3120005130767822\n",
      "Step 2401 | Loss: 0.14882154762744904\n",
      "0.1767886238724707 0.9575\n",
      "Step 1 | Loss: 0.955614447593689\n",
      "Step 101 | Loss: 0.36314135789871216\n",
      "Step 201 | Loss: 0.2995358407497406\n",
      "Step 301 | Loss: 0.298562228679657\n",
      "Step 401 | Loss: 0.2500608563423157\n",
      "Step 501 | Loss: 0.15690402686595917\n",
      "Step 601 | Loss: 0.20833489298820496\n",
      "Step 701 | Loss: 0.2895922362804413\n",
      "Step 801 | Loss: 0.2849666476249695\n",
      "Step 901 | Loss: 0.20031608641147614\n",
      "Step 1001 | Loss: 0.18217068910598755\n",
      "Step 1101 | Loss: 0.2578411102294922\n",
      "Step 1201 | Loss: 0.20332694053649902\n",
      "Step 1301 | Loss: 0.24285823106765747\n",
      "Step 1401 | Loss: 0.1766517609357834\n",
      "Step 1501 | Loss: 0.2511451542377472\n",
      "Step 1601 | Loss: 0.23108431696891785\n",
      "Step 1701 | Loss: 0.272250235080719\n",
      "Step 1801 | Loss: 0.1908940076828003\n",
      "Step 1901 | Loss: 0.23217937350273132\n",
      "Step 2001 | Loss: 0.12138296663761139\n",
      "Step 2101 | Loss: 0.1379970759153366\n",
      "Step 2201 | Loss: 0.2739129662513733\n",
      "Step 2301 | Loss: 0.2822381854057312\n",
      "Step 2401 | Loss: 0.10156236588954926\n",
      "0.20765105464103029 0.9525\n",
      "Step 1 | Loss: 1.2053920030593872\n",
      "Step 101 | Loss: 0.3206554353237152\n",
      "Step 201 | Loss: 0.14043313264846802\n",
      "Step 301 | Loss: 0.18778127431869507\n",
      "Step 401 | Loss: 0.1818704605102539\n",
      "Step 501 | Loss: 0.22645175457000732\n",
      "Step 601 | Loss: 0.14041736721992493\n",
      "Step 701 | Loss: 0.1342545449733734\n",
      "Step 801 | Loss: 0.3872056305408478\n",
      "Step 901 | Loss: 0.1015656366944313\n",
      "Step 1001 | Loss: 0.3201019763946533\n",
      "Step 1101 | Loss: 0.16715721786022186\n",
      "Step 1201 | Loss: 0.19603237509727478\n",
      "Step 1301 | Loss: 0.19710810482501984\n",
      "Step 1401 | Loss: 0.05650666356086731\n",
      "Step 1501 | Loss: 0.15978911519050598\n",
      "Step 1601 | Loss: 0.3392314910888672\n",
      "Step 1701 | Loss: 0.10221007466316223\n",
      "Step 1801 | Loss: 0.14673787355422974\n",
      "Step 1901 | Loss: 0.18991486728191376\n",
      "Step 2001 | Loss: 0.2667281925678253\n",
      "Step 2101 | Loss: 0.21143248677253723\n",
      "Step 2201 | Loss: 0.22454150021076202\n",
      "Step 2301 | Loss: 0.23893366754055023\n",
      "Step 2401 | Loss: 0.1350506842136383\n",
      "0.1869129524845537 0.9525\n",
      "Step 1 | Loss: 1.7810370922088623\n",
      "Step 101 | Loss: 0.38873034715652466\n",
      "Step 201 | Loss: 0.34511739015579224\n",
      "Step 301 | Loss: 0.5588549375534058\n",
      "Step 401 | Loss: 0.19809143245220184\n",
      "Step 501 | Loss: 0.11858837306499481\n",
      "Step 601 | Loss: 0.24689997732639313\n",
      "Step 701 | Loss: 0.1271059215068817\n",
      "Step 801 | Loss: 0.32177406549453735\n",
      "Step 901 | Loss: 0.3118932843208313\n",
      "Step 1001 | Loss: 0.127912238240242\n",
      "Step 1101 | Loss: 0.15631863474845886\n",
      "Step 1201 | Loss: 0.13761258125305176\n",
      "Step 1301 | Loss: 0.31315740942955017\n",
      "Step 1401 | Loss: 0.1369905173778534\n",
      "Step 1501 | Loss: 0.09894762933254242\n",
      "Step 1601 | Loss: 0.1833759993314743\n",
      "Step 1701 | Loss: 0.14127768576145172\n",
      "Step 1801 | Loss: 0.3441009819507599\n",
      "Step 1901 | Loss: 0.33000579476356506\n",
      "Step 2001 | Loss: 0.0720737874507904\n",
      "Step 2101 | Loss: 0.16598699986934662\n",
      "Step 2201 | Loss: 0.21614471077919006\n",
      "Step 2301 | Loss: 0.1864960491657257\n",
      "Step 2401 | Loss: 0.15308304131031036\n",
      "0.1812300822738449 0.9575\n",
      "Step 1 | Loss: 0.904762864112854\n",
      "Step 101 | Loss: 0.27927786111831665\n",
      "Step 201 | Loss: 0.5411791801452637\n",
      "Step 301 | Loss: 0.39979979395866394\n",
      "Step 401 | Loss: 0.35999590158462524\n",
      "Step 501 | Loss: 0.26510244607925415\n",
      "Step 601 | Loss: 0.32257428765296936\n",
      "Step 701 | Loss: 0.44466695189476013\n",
      "Step 801 | Loss: 0.47153598070144653\n",
      "Step 901 | Loss: 0.39084649085998535\n",
      "Step 1001 | Loss: 0.31513822078704834\n",
      "Step 1101 | Loss: 0.3078465461730957\n",
      "Step 1201 | Loss: 0.378094345331192\n",
      "Step 1301 | Loss: 0.4518663287162781\n",
      "Step 1401 | Loss: 0.4109193682670593\n",
      "Step 1501 | Loss: 0.19694070518016815\n",
      "Step 1601 | Loss: 0.35469192266464233\n",
      "Step 1701 | Loss: 0.38047629594802856\n",
      "Step 1801 | Loss: 0.3409019708633423\n",
      "Step 1901 | Loss: 0.3329789638519287\n",
      "Step 2001 | Loss: 0.3209267854690552\n",
      "Step 2101 | Loss: 0.32982882857322693\n",
      "Step 2201 | Loss: 0.28401103615760803\n",
      "Step 2301 | Loss: 0.21512949466705322\n",
      "Step 2401 | Loss: 0.4064416289329529\n",
      "0.3261845591095624 0.93\n",
      "Step 1 | Loss: 0.8116729259490967\n",
      "Step 101 | Loss: 0.4354996383190155\n",
      "Step 201 | Loss: 0.2381841391324997\n",
      "Step 301 | Loss: 0.23197412490844727\n",
      "Step 401 | Loss: 0.28925326466560364\n",
      "Step 501 | Loss: 0.28720003366470337\n",
      "Step 601 | Loss: 0.4044646620750427\n",
      "Step 701 | Loss: 0.2555653750896454\n",
      "Step 801 | Loss: 0.30771759152412415\n",
      "Step 901 | Loss: 0.27846240997314453\n",
      "Step 1001 | Loss: 0.29826968908309937\n",
      "Step 1101 | Loss: 0.3993595838546753\n",
      "Step 1201 | Loss: 0.2933967113494873\n",
      "Step 1301 | Loss: 0.35357388854026794\n",
      "Step 1401 | Loss: 0.26807886362075806\n",
      "Step 1501 | Loss: 0.34726351499557495\n",
      "Step 1601 | Loss: 0.17987321317195892\n",
      "Step 1701 | Loss: 0.39372485876083374\n",
      "Step 1801 | Loss: 0.2147272527217865\n",
      "Step 1901 | Loss: 0.3581666648387909\n",
      "Step 2001 | Loss: 0.2651630938053131\n",
      "Step 2101 | Loss: 0.2140926569700241\n",
      "Step 2201 | Loss: 0.2846536636352539\n",
      "Step 2301 | Loss: 0.21882063150405884\n",
      "Step 2401 | Loss: 0.23291516304016113\n",
      "0.28739911154090375 0.95\n",
      "Step 1 | Loss: 0.9729719758033752\n",
      "Step 101 | Loss: 0.7742275595664978\n",
      "Step 201 | Loss: 0.4831652045249939\n",
      "Step 301 | Loss: 0.4936141073703766\n",
      "Step 401 | Loss: 0.47827455401420593\n",
      "Step 501 | Loss: 0.5015824437141418\n",
      "Step 601 | Loss: 0.40981391072273254\n",
      "Step 701 | Loss: 0.6638174057006836\n",
      "Step 801 | Loss: 0.5423641204833984\n",
      "Step 901 | Loss: 0.4066702127456665\n",
      "Step 1001 | Loss: 0.46445000171661377\n",
      "Step 1101 | Loss: 0.7163026332855225\n",
      "Step 1201 | Loss: 0.6322745680809021\n",
      "Step 1301 | Loss: 0.415920615196228\n",
      "Step 1401 | Loss: 0.5579295754432678\n",
      "Step 1501 | Loss: 0.3485366702079773\n",
      "Step 1601 | Loss: 0.556627631187439\n",
      "Step 1701 | Loss: 0.521199643611908\n",
      "Step 1801 | Loss: 0.6070265769958496\n",
      "Step 1901 | Loss: 0.5028524398803711\n",
      "Step 2001 | Loss: 0.4018653929233551\n",
      "Step 2101 | Loss: 0.4258422553539276\n",
      "Step 2201 | Loss: 0.5244180560112\n",
      "Step 2301 | Loss: 0.5626020431518555\n",
      "Step 2401 | Loss: 0.508435070514679\n",
      "0.47364079224402383 0.8825\n",
      "Step 1 | Loss: 1.2896250486373901\n",
      "Step 101 | Loss: 0.696668803691864\n",
      "Step 201 | Loss: 0.5679146647453308\n",
      "Step 301 | Loss: 0.2997007966041565\n",
      "Step 401 | Loss: 0.3932303488254547\n",
      "Step 501 | Loss: 0.3973820209503174\n",
      "Step 601 | Loss: 0.623287558555603\n",
      "Step 701 | Loss: 0.3390325605869293\n",
      "Step 801 | Loss: 0.327688604593277\n",
      "Step 901 | Loss: 0.36288565397262573\n",
      "Step 1001 | Loss: 0.31306952238082886\n",
      "Step 1101 | Loss: 0.4532626271247864\n",
      "Step 1201 | Loss: 0.40610891580581665\n",
      "Step 1301 | Loss: 0.3527184724807739\n",
      "Step 1401 | Loss: 0.45230764150619507\n",
      "Step 1501 | Loss: 0.41345837712287903\n",
      "Step 1601 | Loss: 0.36892855167388916\n",
      "Step 1701 | Loss: 0.3159472346305847\n",
      "Step 1801 | Loss: 0.4234675168991089\n",
      "Step 1901 | Loss: 0.43868887424468994\n",
      "Step 2001 | Loss: 0.4693260192871094\n",
      "Step 2101 | Loss: 0.34355083107948303\n",
      "Step 2201 | Loss: 0.29668354988098145\n",
      "Step 2301 | Loss: 0.4210556447505951\n",
      "Step 2401 | Loss: 0.4920928478240967\n",
      "0.3586829183939458 0.92\n",
      "Step 1 | Loss: 1.9508129358291626\n",
      "Step 101 | Loss: 0.38531655073165894\n",
      "Step 201 | Loss: 0.32026833295822144\n",
      "Step 301 | Loss: 0.4222976863384247\n",
      "Step 401 | Loss: 0.30371958017349243\n",
      "Step 501 | Loss: 0.3080931305885315\n",
      "Step 601 | Loss: 0.2922862768173218\n",
      "Step 701 | Loss: 0.3109981119632721\n",
      "Step 801 | Loss: 0.33988484740257263\n",
      "Step 901 | Loss: 0.1929634064435959\n",
      "Step 1001 | Loss: 0.29912376403808594\n",
      "Step 1101 | Loss: 0.2777063548564911\n",
      "Step 1201 | Loss: 0.30483341217041016\n",
      "Step 1301 | Loss: 0.26493558287620544\n",
      "Step 1401 | Loss: 0.3179028630256653\n",
      "Step 1501 | Loss: 0.28858762979507446\n",
      "Step 1601 | Loss: 0.21608637273311615\n",
      "Step 1701 | Loss: 0.3155178129673004\n",
      "Step 1801 | Loss: 0.2036709189414978\n",
      "Step 1901 | Loss: 0.2557770609855652\n",
      "Step 2001 | Loss: 0.38918688893318176\n",
      "Step 2101 | Loss: 0.29846376180648804\n",
      "Step 2201 | Loss: 0.23643067479133606\n",
      "Step 2301 | Loss: 0.31578758358955383\n",
      "Step 2401 | Loss: 0.26639825105667114\n",
      "0.28658531555909367 0.9525\n",
      "Step 1 | Loss: 0.883567214012146\n",
      "Step 101 | Loss: 1.0928447246551514\n",
      "Step 201 | Loss: 1.0156277418136597\n",
      "Step 301 | Loss: 0.9785733222961426\n",
      "Step 401 | Loss: 0.7759191989898682\n",
      "Step 501 | Loss: 0.867693305015564\n",
      "Step 601 | Loss: 0.7589671015739441\n",
      "Step 701 | Loss: 0.7191286683082581\n",
      "Step 801 | Loss: 0.7900312542915344\n",
      "Step 901 | Loss: 1.277051329612732\n",
      "Step 1001 | Loss: 0.8503154516220093\n",
      "Step 1101 | Loss: 0.791225254535675\n",
      "Step 1201 | Loss: 0.7151421904563904\n",
      "Step 1301 | Loss: 0.9423572421073914\n",
      "Step 1401 | Loss: 0.8212191462516785\n",
      "Step 1501 | Loss: 0.435680627822876\n",
      "Step 1601 | Loss: 0.6862057447433472\n",
      "Step 1701 | Loss: 0.7375259399414062\n",
      "Step 1801 | Loss: 0.7843639850616455\n",
      "Step 1901 | Loss: 1.1024107933044434\n",
      "Step 2001 | Loss: 0.5637654066085815\n",
      "Step 2101 | Loss: 0.8893117308616638\n",
      "Step 2201 | Loss: 0.6085613965988159\n",
      "Step 2301 | Loss: 0.6779815554618835\n",
      "Step 2401 | Loss: 0.6613316535949707\n",
      "0.8820202490755739 0.6675\n",
      "Step 1 | Loss: 1.98289954662323\n",
      "Step 101 | Loss: 0.8790483474731445\n",
      "Step 201 | Loss: 0.8723384141921997\n",
      "Step 301 | Loss: 0.7070977687835693\n",
      "Step 401 | Loss: 1.1567254066467285\n",
      "Step 501 | Loss: 0.8626660704612732\n",
      "Step 601 | Loss: 1.0333402156829834\n",
      "Step 701 | Loss: 0.9507003426551819\n",
      "Step 801 | Loss: 0.7182859182357788\n",
      "Step 901 | Loss: 0.7787030339241028\n",
      "Step 1001 | Loss: 0.9532354474067688\n",
      "Step 1101 | Loss: 0.6709357500076294\n",
      "Step 1201 | Loss: 0.8983919620513916\n",
      "Step 1301 | Loss: 1.2084217071533203\n",
      "Step 1401 | Loss: 0.9308013319969177\n",
      "Step 1501 | Loss: 1.0230262279510498\n",
      "Step 1601 | Loss: 0.8611323237419128\n",
      "Step 1701 | Loss: 0.8561388850212097\n",
      "Step 1801 | Loss: 1.029117226600647\n",
      "Step 1901 | Loss: 0.8672661781311035\n",
      "Step 2001 | Loss: 0.9920206069946289\n",
      "Step 2101 | Loss: 0.9959561228752136\n",
      "Step 2201 | Loss: 0.8899693489074707\n",
      "Step 2301 | Loss: 0.8686447739601135\n",
      "Step 2401 | Loss: 0.6853854656219482\n",
      "0.9838396803674397 0.615\n",
      "Step 1 | Loss: 0.976568877696991\n",
      "Step 101 | Loss: 0.960046112537384\n",
      "Step 201 | Loss: 1.0377930402755737\n",
      "Step 301 | Loss: 0.6149143576622009\n",
      "Step 401 | Loss: 1.143999457359314\n",
      "Step 501 | Loss: 0.8146546483039856\n",
      "Step 601 | Loss: 1.0795652866363525\n",
      "Step 701 | Loss: 0.9094282388687134\n",
      "Step 801 | Loss: 1.0786572694778442\n",
      "Step 901 | Loss: 0.8619242310523987\n",
      "Step 1001 | Loss: 0.724040150642395\n",
      "Step 1101 | Loss: 0.8532313108444214\n",
      "Step 1201 | Loss: 0.8872792720794678\n",
      "Step 1301 | Loss: 1.1384869813919067\n",
      "Step 1401 | Loss: 0.8387178778648376\n",
      "Step 1501 | Loss: 0.849761962890625\n",
      "Step 1601 | Loss: 0.7703653573989868\n",
      "Step 1701 | Loss: 0.7008603811264038\n",
      "Step 1801 | Loss: 0.841613233089447\n",
      "Step 1901 | Loss: 0.9792059659957886\n",
      "Step 2001 | Loss: 0.577053964138031\n",
      "Step 2101 | Loss: 0.6669968366622925\n",
      "Step 2201 | Loss: 0.7048163414001465\n",
      "Step 2301 | Loss: 0.7593256831169128\n",
      "Step 2401 | Loss: 0.4206908643245697\n",
      "0.8786678545192969 0.67\n",
      "Step 1 | Loss: 1.1054513454437256\n",
      "Step 101 | Loss: 0.8309620022773743\n",
      "Step 201 | Loss: 0.6193490028381348\n",
      "Step 301 | Loss: 1.2181981801986694\n",
      "Step 401 | Loss: 1.0188870429992676\n",
      "Step 501 | Loss: 0.9210007190704346\n",
      "Step 601 | Loss: 0.7568827271461487\n",
      "Step 701 | Loss: 1.0242178440093994\n",
      "Step 801 | Loss: 0.6654007434844971\n",
      "Step 901 | Loss: 0.8623143434524536\n",
      "Step 1001 | Loss: 1.0608385801315308\n",
      "Step 1101 | Loss: 0.5595216155052185\n",
      "Step 1201 | Loss: 0.9370537996292114\n",
      "Step 1301 | Loss: 0.887799859046936\n",
      "Step 1401 | Loss: 0.8022992014884949\n",
      "Step 1501 | Loss: 0.9982589483261108\n",
      "Step 1601 | Loss: 0.8117303848266602\n",
      "Step 1701 | Loss: 0.9380730986595154\n",
      "Step 1801 | Loss: 1.2067469358444214\n",
      "Step 1901 | Loss: 0.819597601890564\n",
      "Step 2001 | Loss: 0.8913534283638\n",
      "Step 2101 | Loss: 0.9160354137420654\n",
      "Step 2201 | Loss: 0.7924008369445801\n",
      "Step 2301 | Loss: 0.8527892827987671\n",
      "Step 2401 | Loss: 0.7018092274665833\n",
      "0.9251156082723321 0.6325\n",
      "Step 1 | Loss: 1.8979297876358032\n",
      "Step 101 | Loss: 0.9451298713684082\n",
      "Step 201 | Loss: 0.8901752233505249\n",
      "Step 301 | Loss: 0.6594537496566772\n",
      "Step 401 | Loss: 1.2438944578170776\n",
      "Step 501 | Loss: 0.8288044333457947\n",
      "Step 601 | Loss: 0.8121399879455566\n",
      "Step 701 | Loss: 0.8511073589324951\n",
      "Step 801 | Loss: 0.9518643617630005\n",
      "Step 901 | Loss: 0.6708613038063049\n",
      "Step 1001 | Loss: 0.9038461446762085\n",
      "Step 1101 | Loss: 0.8845879435539246\n",
      "Step 1201 | Loss: 1.0954076051712036\n",
      "Step 1301 | Loss: 1.0122064352035522\n",
      "Step 1401 | Loss: 1.1292340755462646\n",
      "Step 1501 | Loss: 0.822728157043457\n",
      "Step 1601 | Loss: 0.6267172694206238\n",
      "Step 1701 | Loss: 0.9395616054534912\n",
      "Step 1801 | Loss: 0.7102505564689636\n",
      "Step 1901 | Loss: 0.8835470676422119\n",
      "Step 2001 | Loss: 0.5815976858139038\n",
      "Step 2101 | Loss: 0.7830127477645874\n",
      "Step 2201 | Loss: 0.6641117334365845\n",
      "Step 2301 | Loss: 0.7440533638000488\n",
      "Step 2401 | Loss: 0.5991774201393127\n",
      "0.9028769102076111 0.6525\n",
      "Step 1 | Loss: 2.001356601715088\n",
      "Step 101 | Loss: 0.45739150047302246\n",
      "Step 201 | Loss: 0.38695740699768066\n",
      "Step 301 | Loss: 0.4543764889240265\n",
      "Step 401 | Loss: 0.40308278799057007\n",
      "Step 501 | Loss: 0.3995732069015503\n",
      "Step 601 | Loss: 0.4925623834133148\n",
      "Step 701 | Loss: 0.4285100996494293\n",
      "Step 801 | Loss: 0.43709641695022583\n",
      "Step 901 | Loss: 0.5604997873306274\n",
      "Step 1001 | Loss: 0.49246957898139954\n",
      "Step 1101 | Loss: 0.3418274521827698\n",
      "Step 1201 | Loss: 0.4600905776023865\n",
      "Step 1301 | Loss: 0.3079107701778412\n",
      "Step 1401 | Loss: 0.46428415179252625\n",
      "Step 1501 | Loss: 0.43975818157196045\n",
      "Step 1601 | Loss: 0.3087693750858307\n",
      "Step 1701 | Loss: 0.38418301939964294\n",
      "Step 1801 | Loss: 0.391416996717453\n",
      "Step 1901 | Loss: 0.38747555017471313\n",
      "Step 2001 | Loss: 0.4530673027038574\n",
      "Step 2101 | Loss: 0.34955811500549316\n",
      "Step 2201 | Loss: 0.3756222724914551\n",
      "Step 2301 | Loss: 0.36571887135505676\n",
      "Step 2401 | Loss: 0.2895846366882324\n",
      "0.4155945312852989 0.8875\n",
      "Step 1 | Loss: 0.9752610921859741\n",
      "Step 101 | Loss: 1.0642422437667847\n",
      "Step 201 | Loss: 0.5413445830345154\n",
      "Step 301 | Loss: 0.32582467794418335\n",
      "Step 401 | Loss: 0.5955662131309509\n",
      "Step 501 | Loss: 0.6188637614250183\n",
      "Step 601 | Loss: 0.5567075610160828\n",
      "Step 701 | Loss: 0.4220915734767914\n",
      "Step 801 | Loss: 0.45120394229888916\n",
      "Step 901 | Loss: 0.5049865245819092\n",
      "Step 1001 | Loss: 0.6547991037368774\n",
      "Step 1101 | Loss: 0.41073814034461975\n",
      "Step 1201 | Loss: 0.5541945099830627\n",
      "Step 1301 | Loss: 0.4415476620197296\n",
      "Step 1401 | Loss: 0.19656497240066528\n",
      "Step 1501 | Loss: 0.48464906215667725\n",
      "Step 1601 | Loss: 0.36627674102783203\n",
      "Step 1701 | Loss: 0.5828391313552856\n",
      "Step 1801 | Loss: 0.36199066042900085\n",
      "Step 1901 | Loss: 0.5398587584495544\n",
      "Step 2001 | Loss: 0.3420608937740326\n",
      "Step 2101 | Loss: 0.25536444783210754\n",
      "Step 2201 | Loss: 0.32086116075515747\n",
      "Step 2301 | Loss: 0.7003706097602844\n",
      "Step 2401 | Loss: 0.4395708441734314\n",
      "0.4545251898082846 0.8575\n",
      "Step 1 | Loss: 1.666149377822876\n",
      "Step 101 | Loss: 0.4335293769836426\n",
      "Step 201 | Loss: 0.4950908422470093\n",
      "Step 301 | Loss: 0.45682793855667114\n",
      "Step 401 | Loss: 0.44599097967147827\n",
      "Step 501 | Loss: 0.3688710033893585\n",
      "Step 601 | Loss: 0.494392991065979\n",
      "Step 701 | Loss: 0.413510799407959\n",
      "Step 801 | Loss: 0.4409017562866211\n",
      "Step 901 | Loss: 0.48673415184020996\n",
      "Step 1001 | Loss: 0.29237043857574463\n",
      "Step 1101 | Loss: 0.4600815773010254\n",
      "Step 1201 | Loss: 0.3579469323158264\n",
      "Step 1301 | Loss: 0.5075049996376038\n",
      "Step 1401 | Loss: 0.5886550545692444\n",
      "Step 1501 | Loss: 0.5931317806243896\n",
      "Step 1601 | Loss: 0.5032339692115784\n",
      "Step 1701 | Loss: 0.4349944591522217\n",
      "Step 1801 | Loss: 0.4059351980686188\n",
      "Step 1901 | Loss: 0.4364094138145447\n",
      "Step 2001 | Loss: 0.5023911595344543\n",
      "Step 2101 | Loss: 0.8442071676254272\n",
      "Step 2201 | Loss: 0.5168889164924622\n",
      "Step 2301 | Loss: 0.5215373635292053\n",
      "Step 2401 | Loss: 0.44624409079551697\n",
      "0.4545966211034865 0.885\n",
      "Step 1 | Loss: 0.5186545252799988\n",
      "Step 101 | Loss: 0.49167072772979736\n",
      "Step 201 | Loss: 0.35588306188583374\n",
      "Step 301 | Loss: 0.4616214334964752\n",
      "Step 401 | Loss: 0.403623104095459\n",
      "Step 501 | Loss: 0.4245644211769104\n",
      "Step 601 | Loss: 0.35686394572257996\n",
      "Step 701 | Loss: 0.300788015127182\n",
      "Step 801 | Loss: 0.3535252511501312\n",
      "Step 901 | Loss: 0.33865198493003845\n",
      "Step 1001 | Loss: 0.6180006265640259\n",
      "Step 1101 | Loss: 0.3180777132511139\n",
      "Step 1201 | Loss: 0.46709299087524414\n",
      "Step 1301 | Loss: 0.39974793791770935\n",
      "Step 1401 | Loss: 0.3230859339237213\n",
      "Step 1501 | Loss: 0.45207810401916504\n",
      "Step 1601 | Loss: 0.447479784488678\n",
      "Step 1701 | Loss: 0.5128783583641052\n",
      "Step 1801 | Loss: 0.3505215048789978\n",
      "Step 1901 | Loss: 0.327314555644989\n",
      "Step 2001 | Loss: 0.2810359597206116\n",
      "Step 2101 | Loss: 0.34445446729660034\n",
      "Step 2201 | Loss: 0.45747292041778564\n",
      "Step 2301 | Loss: 0.42931604385375977\n",
      "Step 2401 | Loss: 0.3665688931941986\n",
      "0.4156728746735051 0.89\n",
      "Step 1 | Loss: 1.040894865989685\n",
      "Step 101 | Loss: 0.70201176404953\n",
      "Step 201 | Loss: 0.5677127242088318\n",
      "Step 301 | Loss: 0.6572103500366211\n",
      "Step 401 | Loss: 0.45773136615753174\n",
      "Step 501 | Loss: 0.31198740005493164\n",
      "Step 601 | Loss: 0.3568006753921509\n",
      "Step 701 | Loss: 0.46740078926086426\n",
      "Step 801 | Loss: 0.5522740483283997\n",
      "Step 901 | Loss: 0.4331812262535095\n",
      "Step 1001 | Loss: 0.5500627756118774\n",
      "Step 1101 | Loss: 0.4122891128063202\n",
      "Step 1201 | Loss: 0.5037922859191895\n",
      "Step 1301 | Loss: 0.5099356174468994\n",
      "Step 1401 | Loss: 0.4344685673713684\n",
      "Step 1501 | Loss: 0.42885151505470276\n",
      "Step 1601 | Loss: 0.30130451917648315\n",
      "Step 1701 | Loss: 0.46118152141571045\n",
      "Step 1801 | Loss: 0.4160533845424652\n",
      "Step 1901 | Loss: 0.44649946689605713\n",
      "Step 2001 | Loss: 0.4336775541305542\n",
      "Step 2101 | Loss: 0.28018108010292053\n",
      "Step 2201 | Loss: 0.5023974180221558\n",
      "Step 2301 | Loss: 0.2972189486026764\n",
      "Step 2401 | Loss: 0.5311547517776489\n",
      "0.4153506750729766 0.9075\n",
      "Step 1 | Loss: 2.023451566696167\n",
      "Step 101 | Loss: 1.1138134002685547\n",
      "Step 201 | Loss: 0.7322973608970642\n",
      "Step 301 | Loss: 0.5557227730751038\n",
      "Step 401 | Loss: 0.5532160997390747\n",
      "Step 501 | Loss: 0.6049704551696777\n",
      "Step 601 | Loss: 0.6534802913665771\n",
      "Step 701 | Loss: 0.5403252840042114\n",
      "Step 801 | Loss: 0.43196046352386475\n",
      "Step 901 | Loss: 0.6318100094795227\n",
      "Step 1001 | Loss: 0.6234729290008545\n",
      "Step 1101 | Loss: 0.47926202416419983\n",
      "Step 1201 | Loss: 0.5596069097518921\n",
      "Step 1301 | Loss: 0.5269719362258911\n",
      "Step 1401 | Loss: 0.5031224489212036\n",
      "Step 1501 | Loss: 0.6047601699829102\n",
      "Step 1601 | Loss: 0.7556955814361572\n",
      "Step 1701 | Loss: 0.47715091705322266\n",
      "Step 1801 | Loss: 0.49792709946632385\n",
      "Step 1901 | Loss: 0.710963785648346\n",
      "Step 2001 | Loss: 0.5868235230445862\n",
      "Step 2101 | Loss: 0.4788275361061096\n",
      "Step 2201 | Loss: 0.5854465961456299\n",
      "Step 2301 | Loss: 0.6111143231391907\n",
      "Step 2401 | Loss: 0.6252198219299316\n",
      "0.5836171669578241 0.785\n",
      "Step 1 | Loss: 1.2520694732666016\n",
      "Step 101 | Loss: 0.9139333963394165\n",
      "Step 201 | Loss: 0.8374911546707153\n",
      "Step 301 | Loss: 0.8256752490997314\n",
      "Step 401 | Loss: 0.7670743465423584\n",
      "Step 501 | Loss: 0.7893754839897156\n",
      "Step 601 | Loss: 0.8204078674316406\n",
      "Step 701 | Loss: 0.8809101581573486\n",
      "Step 801 | Loss: 0.6138994693756104\n",
      "Step 901 | Loss: 0.7041760683059692\n",
      "Step 1001 | Loss: 0.5591387152671814\n",
      "Step 1101 | Loss: 0.7217190265655518\n",
      "Step 1201 | Loss: 0.5469591021537781\n",
      "Step 1301 | Loss: 0.5585909485816956\n",
      "Step 1401 | Loss: 0.6351356506347656\n",
      "Step 1501 | Loss: 0.557735800743103\n",
      "Step 1601 | Loss: 0.7002893686294556\n",
      "Step 1701 | Loss: 0.5241738557815552\n",
      "Step 1801 | Loss: 0.37246501445770264\n",
      "Step 1901 | Loss: 0.5644084215164185\n",
      "Step 2001 | Loss: 0.45436426997184753\n",
      "Step 2101 | Loss: 0.5208742022514343\n",
      "Step 2201 | Loss: 0.4467971920967102\n",
      "Step 2301 | Loss: 0.5791553258895874\n",
      "Step 2401 | Loss: 0.5703064203262329\n",
      "0.5669316543837716 0.78\n",
      "Step 1 | Loss: 0.822740912437439\n",
      "Step 101 | Loss: 0.7832316756248474\n",
      "Step 201 | Loss: 0.5801787972450256\n",
      "Step 301 | Loss: 0.5814223885536194\n",
      "Step 401 | Loss: 0.4502055048942566\n",
      "Step 501 | Loss: 0.4660835266113281\n",
      "Step 601 | Loss: 0.4528365731239319\n",
      "Step 701 | Loss: 0.3695121705532074\n",
      "Step 801 | Loss: 0.39809268712997437\n",
      "Step 901 | Loss: 0.4219580888748169\n",
      "Step 1001 | Loss: 0.479002982378006\n",
      "Step 1101 | Loss: 0.42170658707618713\n",
      "Step 1201 | Loss: 0.4989781081676483\n",
      "Step 1301 | Loss: 0.5429245233535767\n",
      "Step 1401 | Loss: 0.5098594427108765\n",
      "Step 1501 | Loss: 0.47094547748565674\n",
      "Step 1601 | Loss: 0.4372520446777344\n",
      "Step 1701 | Loss: 0.3838542401790619\n",
      "Step 1801 | Loss: 0.39959409832954407\n",
      "Step 1901 | Loss: 0.46232879161834717\n",
      "Step 2001 | Loss: 0.515424370765686\n",
      "Step 2101 | Loss: 0.4150584936141968\n",
      "Step 2201 | Loss: 0.5449166893959045\n",
      "Step 2301 | Loss: 0.5143769979476929\n",
      "Step 2401 | Loss: 0.5312811136245728\n",
      "0.4715933820021122 0.8875\n",
      "Step 1 | Loss: 1.1490119695663452\n",
      "Step 101 | Loss: 1.0860737562179565\n",
      "Step 201 | Loss: 0.9326117038726807\n",
      "Step 301 | Loss: 0.7811349034309387\n",
      "Step 401 | Loss: 0.7801397442817688\n",
      "Step 501 | Loss: 0.7010563611984253\n",
      "Step 601 | Loss: 0.5867038369178772\n",
      "Step 701 | Loss: 0.625005841255188\n",
      "Step 801 | Loss: 0.5181371569633484\n",
      "Step 901 | Loss: 0.6993010640144348\n",
      "Step 1001 | Loss: 0.5448633432388306\n",
      "Step 1101 | Loss: 0.5684136152267456\n",
      "Step 1201 | Loss: 0.4469327926635742\n",
      "Step 1301 | Loss: 0.6876591444015503\n",
      "Step 1401 | Loss: 0.5860823392868042\n",
      "Step 1501 | Loss: 0.6989315748214722\n",
      "Step 1601 | Loss: 0.5343460440635681\n",
      "Step 1701 | Loss: 0.5489470958709717\n",
      "Step 1801 | Loss: 0.7412900328636169\n",
      "Step 1901 | Loss: 0.5877730846405029\n",
      "Step 2001 | Loss: 0.5950859189033508\n",
      "Step 2101 | Loss: 0.5970411896705627\n",
      "Step 2201 | Loss: 0.6050596833229065\n",
      "Step 2301 | Loss: 0.5959093570709229\n",
      "Step 2401 | Loss: 0.7238157987594604\n",
      "0.5845839939367642 0.785\n",
      "Step 1 | Loss: 1.1940927505493164\n",
      "Step 101 | Loss: 1.103083848953247\n",
      "Step 201 | Loss: 0.9075093269348145\n",
      "Step 301 | Loss: 0.6972939968109131\n",
      "Step 401 | Loss: 0.5220134258270264\n",
      "Step 501 | Loss: 0.5838330388069153\n",
      "Step 601 | Loss: 0.5994883179664612\n",
      "Step 701 | Loss: 0.6074172854423523\n",
      "Step 801 | Loss: 0.5099006295204163\n",
      "Step 901 | Loss: 0.6918210983276367\n",
      "Step 1001 | Loss: 0.5628125667572021\n",
      "Step 1101 | Loss: 0.42227309942245483\n",
      "Step 1201 | Loss: 0.5588128566741943\n",
      "Step 1301 | Loss: 0.6297230124473572\n",
      "Step 1401 | Loss: 0.6655557155609131\n",
      "Step 1501 | Loss: 0.5659652948379517\n",
      "Step 1601 | Loss: 0.6155803203582764\n",
      "Step 1701 | Loss: 0.4534822404384613\n",
      "Step 1801 | Loss: 0.5866745710372925\n",
      "Step 1901 | Loss: 0.4888262450695038\n",
      "Step 2001 | Loss: 0.6866235733032227\n",
      "Step 2101 | Loss: 0.5808891654014587\n",
      "Step 2201 | Loss: 0.5718987584114075\n",
      "Step 2301 | Loss: 0.7138420343399048\n",
      "Step 2401 | Loss: 0.6043760180473328\n",
      "0.5849278175984587 0.775\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.875\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 1.375\n",
      "Step 501 | Loss: 1.875\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.625\n",
      "Step 801 | Loss: 1.75\n",
      "Step 901 | Loss: 1.625\n",
      "Step 1001 | Loss: 2.375\n",
      "Step 1101 | Loss: 2.5\n",
      "Step 1201 | Loss: 2.25\n",
      "Step 1301 | Loss: 1.625\n",
      "Step 1401 | Loss: 1.625\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 1.375\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 2.25\n",
      "Step 1901 | Loss: 2.5\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 2.125\n",
      "Step 2201 | Loss: 1.75\n",
      "Step 2301 | Loss: 1.75\n",
      "Step 2401 | Loss: 2.625\n",
      "2.0192307084798857 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 2.625\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 2.25\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.75\n",
      "Step 601 | Loss: 1.25\n",
      "Step 701 | Loss: 1.625\n",
      "Step 801 | Loss: 1.375\n",
      "Step 901 | Loss: 2.125\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 1.75\n",
      "Step 1201 | Loss: 1.75\n",
      "Step 1301 | Loss: 2.5\n",
      "Step 1401 | Loss: 2.25\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 2.125\n",
      "Step 1701 | Loss: 2.0\n",
      "Step 1801 | Loss: 2.249999761581421\n",
      "Step 1901 | Loss: 1.625\n",
      "Step 2001 | Loss: 2.75\n",
      "Step 2101 | Loss: 2.25\n",
      "Step 2201 | Loss: 1.75\n",
      "Step 2301 | Loss: 1.875\n",
      "Step 2401 | Loss: 2.5\n",
      "2.0192307084798857 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 2.375\n",
      "Step 201 | Loss: 1.5\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 1.25\n",
      "Step 501 | Loss: 2.0\n",
      "Step 601 | Loss: 1.75\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 1.375\n",
      "Step 901 | Loss: 2.25\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 1.75\n",
      "Step 1301 | Loss: 1.625\n",
      "Step 1401 | Loss: 1.5\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 2.375\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 1.5\n",
      "Step 1901 | Loss: 1.375\n",
      "Step 2001 | Loss: 2.0\n",
      "Step 2101 | Loss: 2.375\n",
      "Step 2201 | Loss: 1.75\n",
      "Step 2301 | Loss: 2.375\n",
      "Step 2401 | Loss: 2.375\n",
      "2.0192307084798857 0.5\n",
      "Step 1 | Loss: 2.625\n",
      "Step 101 | Loss: 1.625\n",
      "Step 201 | Loss: 2.0\n",
      "Step 301 | Loss: 2.75\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 2.625\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 2.0\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 1.25\n",
      "Step 1101 | Loss: 2.5\n",
      "Step 1201 | Loss: 2.125\n",
      "Step 1301 | Loss: 2.625\n",
      "Step 1401 | Loss: 1.5\n",
      "Step 1501 | Loss: 2.125\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 1.75\n",
      "Step 1801 | Loss: 2.5\n",
      "Step 1901 | Loss: 2.375\n",
      "Step 2001 | Loss: 1.625\n",
      "Step 2101 | Loss: 1.375\n",
      "Step 2201 | Loss: 2.5\n",
      "Step 2301 | Loss: 2.5\n",
      "Step 2401 | Loss: 2.375\n",
      "2.0192307084798857 0.5\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 2.375\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 1.75\n",
      "Step 401 | Loss: 2.25\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 1.125\n",
      "Step 701 | Loss: 2.25\n",
      "Step 801 | Loss: 2.375\n",
      "Step 901 | Loss: 2.125\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 2.125\n",
      "Step 1301 | Loss: 1.5\n",
      "Step 1401 | Loss: 2.25\n",
      "Step 1501 | Loss: 2.0\n",
      "Step 1601 | Loss: 2.0\n",
      "Step 1701 | Loss: 1.5\n",
      "Step 1801 | Loss: 2.5\n",
      "Step 1901 | Loss: 1.125\n",
      "Step 2001 | Loss: 2.125\n",
      "Step 2101 | Loss: 2.375\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 1.875\n",
      "Step 2401 | Loss: 2.125\n",
      "2.0192307084798857 0.5\n",
      "Step 1 | Loss: 0.4729708433151245\n",
      "Step 101 | Loss: 0.58180832862854\n",
      "Step 201 | Loss: 0.49234041571617126\n",
      "Step 301 | Loss: 0.4280402362346649\n",
      "Step 401 | Loss: 0.38202911615371704\n",
      "Step 501 | Loss: 0.39136946201324463\n",
      "Step 601 | Loss: 0.4468703866004944\n",
      "Step 701 | Loss: 0.4371089041233063\n",
      "Step 801 | Loss: 0.379393994808197\n",
      "Step 901 | Loss: 0.45324161648750305\n",
      "Step 1001 | Loss: 0.41480082273483276\n",
      "Step 1101 | Loss: 0.43754827976226807\n",
      "Step 1201 | Loss: 0.35284072160720825\n",
      "Step 1301 | Loss: 0.5288568139076233\n",
      "Step 1401 | Loss: 0.42508387565612793\n",
      "Step 1501 | Loss: 0.3012447655200958\n",
      "Step 1601 | Loss: 0.2912414073944092\n",
      "Step 1701 | Loss: 0.38698434829711914\n",
      "Step 1801 | Loss: 0.4325962960720062\n",
      "Step 1901 | Loss: 0.39873576164245605\n",
      "Step 2001 | Loss: 0.44017493724823\n",
      "Step 2101 | Loss: 0.48394396901130676\n",
      "Step 2201 | Loss: 0.4317210018634796\n",
      "Step 2301 | Loss: 0.4009895920753479\n",
      "Step 2401 | Loss: 0.26207447052001953\n",
      "0.3675815541103639 0.945\n",
      "Step 1 | Loss: 0.9465747475624084\n",
      "Step 101 | Loss: 0.44598665833473206\n",
      "Step 201 | Loss: 0.3183614909648895\n",
      "Step 301 | Loss: 0.3601284623146057\n",
      "Step 401 | Loss: 0.421418160200119\n",
      "Step 501 | Loss: 0.3878291845321655\n",
      "Step 601 | Loss: 0.35749685764312744\n",
      "Step 701 | Loss: 0.2841036915779114\n",
      "Step 801 | Loss: 0.39905625581741333\n",
      "Step 901 | Loss: 0.34733808040618896\n",
      "Step 1001 | Loss: 0.389571875333786\n",
      "Step 1101 | Loss: 0.5535311102867126\n",
      "Step 1201 | Loss: 0.5108528733253479\n",
      "Step 1301 | Loss: 0.3993162512779236\n",
      "Step 1401 | Loss: 0.3713546395301819\n",
      "Step 1501 | Loss: 0.41226473450660706\n",
      "Step 1601 | Loss: 0.34412682056427\n",
      "Step 1701 | Loss: 0.3588215410709381\n",
      "Step 1801 | Loss: 0.3615673780441284\n",
      "Step 1901 | Loss: 0.3989902138710022\n",
      "Step 2001 | Loss: 0.38209009170532227\n",
      "Step 2101 | Loss: 0.34983402490615845\n",
      "Step 2201 | Loss: 0.3448909819126129\n",
      "Step 2301 | Loss: 0.40726715326309204\n",
      "Step 2401 | Loss: 0.4063592851161957\n",
      "0.3663785964682648 0.9375\n",
      "Step 1 | Loss: 0.8916674256324768\n",
      "Step 101 | Loss: 0.6256543397903442\n",
      "Step 201 | Loss: 0.4788931906223297\n",
      "Step 301 | Loss: 0.4590985178947449\n",
      "Step 401 | Loss: 0.4567600190639496\n",
      "Step 501 | Loss: 0.5197473168373108\n",
      "Step 601 | Loss: 0.49071943759918213\n",
      "Step 701 | Loss: 0.5120438933372498\n",
      "Step 801 | Loss: 0.4312482476234436\n",
      "Step 901 | Loss: 0.44484376907348633\n",
      "Step 1001 | Loss: 0.3512001037597656\n",
      "Step 1101 | Loss: 0.3312358856201172\n",
      "Step 1201 | Loss: 0.37427911162376404\n",
      "Step 1301 | Loss: 0.41616082191467285\n",
      "Step 1401 | Loss: 0.3258711099624634\n",
      "Step 1501 | Loss: 0.3223194479942322\n",
      "Step 1601 | Loss: 0.324601411819458\n",
      "Step 1701 | Loss: 0.3618953227996826\n",
      "Step 1801 | Loss: 0.35105082392692566\n",
      "Step 1901 | Loss: 0.3574359714984894\n",
      "Step 2001 | Loss: 0.3409191966056824\n",
      "Step 2101 | Loss: 0.31737178564071655\n",
      "Step 2201 | Loss: 0.4469665288925171\n",
      "Step 2301 | Loss: 0.3749663233757019\n",
      "Step 2401 | Loss: 0.48510265350341797\n",
      "0.35994954257542966 0.94\n",
      "Step 1 | Loss: 1.1601982116699219\n",
      "Step 101 | Loss: 0.48023083806037903\n",
      "Step 201 | Loss: 0.5866160988807678\n",
      "Step 301 | Loss: 0.39500686526298523\n",
      "Step 401 | Loss: 0.34783312678337097\n",
      "Step 501 | Loss: 0.34564635157585144\n",
      "Step 601 | Loss: 0.3705994188785553\n",
      "Step 701 | Loss: 0.4136815369129181\n",
      "Step 801 | Loss: 0.4672602713108063\n",
      "Step 901 | Loss: 0.42077937722206116\n",
      "Step 1001 | Loss: 0.38683566451072693\n",
      "Step 1101 | Loss: 0.5384230613708496\n",
      "Step 1201 | Loss: 0.5207759737968445\n",
      "Step 1301 | Loss: 0.38306230306625366\n",
      "Step 1401 | Loss: 0.29582786560058594\n",
      "Step 1501 | Loss: 0.3923332095146179\n",
      "Step 1601 | Loss: 0.3972361087799072\n",
      "Step 1701 | Loss: 0.38504090905189514\n",
      "Step 1801 | Loss: 0.3313813805580139\n",
      "Step 1901 | Loss: 0.2851426303386688\n",
      "Step 2001 | Loss: 0.40309661626815796\n",
      "Step 2101 | Loss: 0.34904971718788147\n",
      "Step 2201 | Loss: 0.28387972712516785\n",
      "Step 2301 | Loss: 0.3626459240913391\n",
      "Step 2401 | Loss: 0.4382379949092865\n",
      "0.3570586647501685 0.945\n",
      "Step 1 | Loss: 1.1841905117034912\n",
      "Step 101 | Loss: 0.7749618887901306\n",
      "Step 201 | Loss: 0.6733948588371277\n",
      "Step 301 | Loss: 0.4599308669567108\n",
      "Step 401 | Loss: 0.5511593222618103\n",
      "Step 501 | Loss: 0.5307525396347046\n",
      "Step 601 | Loss: 0.41121435165405273\n",
      "Step 701 | Loss: 0.504622220993042\n",
      "Step 801 | Loss: 0.45503976941108704\n",
      "Step 901 | Loss: 0.6574655771255493\n",
      "Step 1001 | Loss: 0.41490599513053894\n",
      "Step 1101 | Loss: 0.4255029857158661\n",
      "Step 1201 | Loss: 0.4162806272506714\n",
      "Step 1301 | Loss: 0.3676651120185852\n",
      "Step 1401 | Loss: 0.5935688018798828\n",
      "Step 1501 | Loss: 0.43492335081100464\n",
      "Step 1601 | Loss: 0.44173872470855713\n",
      "Step 1701 | Loss: 0.4882824718952179\n",
      "Step 1801 | Loss: 0.567499577999115\n",
      "Step 1901 | Loss: 0.5697486996650696\n",
      "Step 2001 | Loss: 0.4782875180244446\n",
      "Step 2101 | Loss: 0.44974377751350403\n",
      "Step 2201 | Loss: 0.3008119463920593\n",
      "Step 2301 | Loss: 0.43933555483818054\n",
      "Step 2401 | Loss: 0.33703848719596863\n",
      "0.39541257145395126 0.935\n",
      "Step 1 | Loss: 1.6864984035491943\n",
      "Step 101 | Loss: 0.9894285202026367\n",
      "Step 201 | Loss: 0.6792487502098083\n",
      "Step 301 | Loss: 0.6284734606742859\n",
      "Step 401 | Loss: 0.7141180038452148\n",
      "Step 501 | Loss: 0.8921773433685303\n",
      "Step 601 | Loss: 0.7515769600868225\n",
      "Step 701 | Loss: 0.6813946962356567\n",
      "Step 801 | Loss: 0.9364923238754272\n",
      "Step 901 | Loss: 0.8394781351089478\n",
      "Step 1001 | Loss: 0.7181268930435181\n",
      "Step 1101 | Loss: 0.8310279846191406\n",
      "Step 1201 | Loss: 0.7095159292221069\n",
      "Step 1301 | Loss: 0.73419189453125\n",
      "Step 1401 | Loss: 0.8217827081680298\n",
      "Step 1501 | Loss: 0.8232064843177795\n",
      "Step 1601 | Loss: 0.7389217019081116\n",
      "Step 1701 | Loss: 0.7516119480133057\n",
      "Step 1801 | Loss: 0.7371332049369812\n",
      "Step 1901 | Loss: 0.7669517993927002\n",
      "Step 2001 | Loss: 0.884597897529602\n",
      "Step 2101 | Loss: 1.1549383401870728\n",
      "Step 2201 | Loss: 0.6381586790084839\n",
      "Step 2301 | Loss: 0.3851843476295471\n",
      "Step 2401 | Loss: 0.6106323003768921\n",
      "0.9274890276400657 0.6575\n",
      "Step 1 | Loss: 1.2865699529647827\n",
      "Step 101 | Loss: 0.788044810295105\n",
      "Step 201 | Loss: 1.0657235383987427\n",
      "Step 301 | Loss: 1.041497826576233\n",
      "Step 401 | Loss: 0.5968368053436279\n",
      "Step 501 | Loss: 0.9589407444000244\n",
      "Step 601 | Loss: 0.5591845512390137\n",
      "Step 701 | Loss: 0.9181281924247742\n",
      "Step 801 | Loss: 0.6624945998191833\n",
      "Step 901 | Loss: 0.688264787197113\n",
      "Step 1001 | Loss: 0.8282168507575989\n",
      "Step 1101 | Loss: 0.6847888231277466\n",
      "Step 1201 | Loss: 1.0082476139068604\n",
      "Step 1301 | Loss: 0.8424040079116821\n",
      "Step 1401 | Loss: 0.7100345492362976\n",
      "Step 1501 | Loss: 0.5056095719337463\n",
      "Step 1601 | Loss: 0.8387651443481445\n",
      "Step 1701 | Loss: 0.5581893920898438\n",
      "Step 1801 | Loss: 0.9600144028663635\n",
      "Step 1901 | Loss: 0.8879365921020508\n",
      "Step 2001 | Loss: 0.6867378354072571\n",
      "Step 2101 | Loss: 0.6677086353302002\n",
      "Step 2201 | Loss: 0.5807993412017822\n",
      "Step 2301 | Loss: 0.6840711832046509\n",
      "Step 2401 | Loss: 0.5733640193939209\n",
      "0.7531012465650563 0.74\n",
      "Step 1 | Loss: 1.6002963781356812\n",
      "Step 101 | Loss: 1.0216470956802368\n",
      "Step 201 | Loss: 0.6793630123138428\n",
      "Step 301 | Loss: 0.4781855642795563\n",
      "Step 401 | Loss: 0.6884220838546753\n",
      "Step 501 | Loss: 0.5540192127227783\n",
      "Step 601 | Loss: 0.5434352159500122\n",
      "Step 701 | Loss: 0.8367819786071777\n",
      "Step 801 | Loss: 0.7561968564987183\n",
      "Step 901 | Loss: 0.9577699899673462\n",
      "Step 1001 | Loss: 0.703765869140625\n",
      "Step 1101 | Loss: 0.6971059441566467\n",
      "Step 1201 | Loss: 0.6770469546318054\n",
      "Step 1301 | Loss: 0.4673245847225189\n",
      "Step 1401 | Loss: 0.9508524537086487\n",
      "Step 1501 | Loss: 0.7912231683731079\n",
      "Step 1601 | Loss: 0.7329723834991455\n",
      "Step 1701 | Loss: 0.45438453555107117\n",
      "Step 1801 | Loss: 0.766502320766449\n",
      "Step 1901 | Loss: 0.875649094581604\n",
      "Step 2001 | Loss: 0.9985814094543457\n",
      "Step 2101 | Loss: 0.8406854867935181\n",
      "Step 2201 | Loss: 0.920601487159729\n",
      "Step 2301 | Loss: 0.5806732773780823\n",
      "Step 2401 | Loss: 0.8559862971305847\n",
      "0.7575412137382855 0.7275\n",
      "Step 1 | Loss: 0.7593917846679688\n",
      "Step 101 | Loss: 0.4521254599094391\n",
      "Step 201 | Loss: 0.5618468523025513\n",
      "Step 301 | Loss: 0.49861735105514526\n",
      "Step 401 | Loss: 0.3007335066795349\n",
      "Step 501 | Loss: 0.4761987030506134\n",
      "Step 601 | Loss: 0.35784056782722473\n",
      "Step 701 | Loss: 0.4522910416126251\n",
      "Step 801 | Loss: 0.5370282530784607\n",
      "Step 901 | Loss: 0.7466581463813782\n",
      "Step 1001 | Loss: 0.5399972200393677\n",
      "Step 1101 | Loss: 0.6268331408500671\n",
      "Step 1201 | Loss: 0.3191579580307007\n",
      "Step 1301 | Loss: 0.710847020149231\n",
      "Step 1401 | Loss: 0.4252282679080963\n",
      "Step 1501 | Loss: 0.3758338987827301\n",
      "Step 1601 | Loss: 0.574540913105011\n",
      "Step 1701 | Loss: 0.5564761161804199\n",
      "Step 1801 | Loss: 0.7233096361160278\n",
      "Step 1901 | Loss: 0.48268648982048035\n",
      "Step 2001 | Loss: 0.5573371052742004\n",
      "Step 2101 | Loss: 0.3452848792076111\n",
      "Step 2201 | Loss: 0.28762638568878174\n",
      "Step 2301 | Loss: 0.4972943663597107\n",
      "Step 2401 | Loss: 0.638644278049469\n",
      "0.5626422381866475 0.82\n",
      "Step 1 | Loss: 1.2084720134735107\n",
      "Step 101 | Loss: 0.8374345898628235\n",
      "Step 201 | Loss: 0.9057597517967224\n",
      "Step 301 | Loss: 0.7040954232215881\n",
      "Step 401 | Loss: 0.584153413772583\n",
      "Step 501 | Loss: 0.9999843835830688\n",
      "Step 601 | Loss: 0.7545034885406494\n",
      "Step 701 | Loss: 0.984121561050415\n",
      "Step 801 | Loss: 0.8483195304870605\n",
      "Step 901 | Loss: 0.6391060948371887\n",
      "Step 1001 | Loss: 0.7891031503677368\n",
      "Step 1101 | Loss: 0.5654457807540894\n",
      "Step 1201 | Loss: 0.9722428321838379\n",
      "Step 1301 | Loss: 0.8908384442329407\n",
      "Step 1401 | Loss: 0.697834312915802\n",
      "Step 1501 | Loss: 0.6615486145019531\n",
      "Step 1601 | Loss: 0.565321683883667\n",
      "Step 1701 | Loss: 0.8203622698783875\n",
      "Step 1801 | Loss: 0.5647642612457275\n",
      "Step 1901 | Loss: 0.913057267665863\n",
      "Step 2001 | Loss: 0.719734787940979\n",
      "Step 2101 | Loss: 0.6775383949279785\n",
      "Step 2201 | Loss: 0.46500545740127563\n",
      "Step 2301 | Loss: 0.5969111919403076\n",
      "Step 2401 | Loss: 0.4462159276008606\n",
      "0.8217701627433593 0.705\n",
      "Step 1 | Loss: 1.4530434608459473\n",
      "Step 101 | Loss: 0.606810450553894\n",
      "Step 201 | Loss: 0.6898317337036133\n",
      "Step 301 | Loss: 0.9766836762428284\n",
      "Step 401 | Loss: 0.9648268818855286\n",
      "Step 501 | Loss: 0.9932163953781128\n",
      "Step 601 | Loss: 0.9813478589057922\n",
      "Step 701 | Loss: 0.721311092376709\n",
      "Step 801 | Loss: 0.8081411123275757\n",
      "Step 901 | Loss: 0.8595307469367981\n",
      "Step 1001 | Loss: 0.7762796878814697\n",
      "Step 1101 | Loss: 0.8792445063591003\n",
      "Step 1201 | Loss: 0.8418918251991272\n",
      "Step 1301 | Loss: 0.9968975186347961\n",
      "Step 1401 | Loss: 0.731214165687561\n",
      "Step 1501 | Loss: 0.7289925217628479\n",
      "Step 1601 | Loss: 0.7658213376998901\n",
      "Step 1701 | Loss: 0.7531594038009644\n",
      "Step 1801 | Loss: 0.6901391744613647\n",
      "Step 1901 | Loss: 0.7931308746337891\n",
      "Step 2001 | Loss: 0.8721916675567627\n",
      "Step 2101 | Loss: 0.7592644095420837\n",
      "Step 2201 | Loss: 0.9068377614021301\n",
      "Step 2301 | Loss: 0.9072932004928589\n",
      "Step 2401 | Loss: 0.8887612223625183\n",
      "0.7695713056650005 0.74\n",
      "Step 1 | Loss: 1.5870002508163452\n",
      "Step 101 | Loss: 0.9555691480636597\n",
      "Step 201 | Loss: 0.8998818397521973\n",
      "Step 301 | Loss: 0.5458189845085144\n",
      "Step 401 | Loss: 1.00617516040802\n",
      "Step 501 | Loss: 0.8884102702140808\n",
      "Step 601 | Loss: 0.8635988831520081\n",
      "Step 701 | Loss: 0.8816769123077393\n",
      "Step 801 | Loss: 0.7784950733184814\n",
      "Step 901 | Loss: 0.7499537467956543\n",
      "Step 1001 | Loss: 0.9045141339302063\n",
      "Step 1101 | Loss: 0.8189324140548706\n",
      "Step 1201 | Loss: 0.7908269166946411\n",
      "Step 1301 | Loss: 0.7423802614212036\n",
      "Step 1401 | Loss: 0.9241347908973694\n",
      "Step 1501 | Loss: 0.898304283618927\n",
      "Step 1601 | Loss: 0.8008111715316772\n",
      "Step 1701 | Loss: 0.9343679547309875\n",
      "Step 1801 | Loss: 0.6627877354621887\n",
      "Step 1901 | Loss: 0.7786749601364136\n",
      "Step 2001 | Loss: 0.7849398851394653\n",
      "Step 2101 | Loss: 0.8391405940055847\n",
      "Step 2201 | Loss: 0.8049753308296204\n",
      "Step 2301 | Loss: 0.7052434682846069\n",
      "Step 2401 | Loss: 0.7153222560882568\n",
      "0.7666250763564114 0.7325\n",
      "Step 1 | Loss: 1.574474573135376\n",
      "Step 101 | Loss: 0.7803991436958313\n",
      "Step 201 | Loss: 0.7966766953468323\n",
      "Step 301 | Loss: 0.7552996277809143\n",
      "Step 401 | Loss: 0.7970439791679382\n",
      "Step 501 | Loss: 0.6650504469871521\n",
      "Step 601 | Loss: 0.7329592704772949\n",
      "Step 701 | Loss: 0.6841531991958618\n",
      "Step 801 | Loss: 0.7791496515274048\n",
      "Step 901 | Loss: 0.9016215801239014\n",
      "Step 1001 | Loss: 0.7399799227714539\n",
      "Step 1101 | Loss: 0.7642785310745239\n",
      "Step 1201 | Loss: 0.8053387999534607\n",
      "Step 1301 | Loss: 0.795055627822876\n",
      "Step 1401 | Loss: 0.714218258857727\n",
      "Step 1501 | Loss: 0.7759814262390137\n",
      "Step 1601 | Loss: 0.610634446144104\n",
      "Step 1701 | Loss: 0.569746732711792\n",
      "Step 1801 | Loss: 0.7531024813652039\n",
      "Step 1901 | Loss: 0.8219395279884338\n",
      "Step 2001 | Loss: 0.771399974822998\n",
      "Step 2101 | Loss: 0.8671560287475586\n",
      "Step 2201 | Loss: 0.6941611766815186\n",
      "Step 2301 | Loss: 0.8092887997627258\n",
      "Step 2401 | Loss: 0.6918483972549438\n",
      "0.7492205448176309 0.705\n",
      "Step 1 | Loss: 1.3665332794189453\n",
      "Step 101 | Loss: 0.7425327897071838\n",
      "Step 201 | Loss: 0.837009072303772\n",
      "Step 301 | Loss: 0.7536767721176147\n",
      "Step 401 | Loss: 0.8244708180427551\n",
      "Step 501 | Loss: 0.7166279554367065\n",
      "Step 601 | Loss: 0.6908918619155884\n",
      "Step 701 | Loss: 0.7635824084281921\n",
      "Step 801 | Loss: 0.9095357060432434\n",
      "Step 901 | Loss: 0.6690162420272827\n",
      "Step 1001 | Loss: 0.8252453207969666\n",
      "Step 1101 | Loss: 0.7903319001197815\n",
      "Step 1201 | Loss: 0.6738900542259216\n",
      "Step 1301 | Loss: 0.8926886320114136\n",
      "Step 1401 | Loss: 0.6512181162834167\n",
      "Step 1501 | Loss: 0.7731519341468811\n",
      "Step 1601 | Loss: 0.7979784607887268\n",
      "Step 1701 | Loss: 0.9017846584320068\n",
      "Step 1801 | Loss: 0.6920093297958374\n",
      "Step 1901 | Loss: 0.8486329913139343\n",
      "Step 2001 | Loss: 0.7108120918273926\n",
      "Step 2101 | Loss: 0.832059919834137\n",
      "Step 2201 | Loss: 0.7860746383666992\n",
      "Step 2301 | Loss: 0.783724844455719\n",
      "Step 2401 | Loss: 0.8120687007904053\n",
      "0.7386544394199628 0.6975\n",
      "Step 1 | Loss: 1.5907862186431885\n",
      "Step 101 | Loss: 1.2683296203613281\n",
      "Step 201 | Loss: 1.2265077829360962\n",
      "Step 301 | Loss: 0.7337070107460022\n",
      "Step 401 | Loss: 0.9844078421592712\n",
      "Step 501 | Loss: 0.8120900392532349\n",
      "Step 601 | Loss: 0.8240382671356201\n",
      "Step 701 | Loss: 0.799935519695282\n",
      "Step 801 | Loss: 0.7087541818618774\n",
      "Step 901 | Loss: 0.8381949663162231\n",
      "Step 1001 | Loss: 0.8831921815872192\n",
      "Step 1101 | Loss: 0.6639293432235718\n",
      "Step 1201 | Loss: 0.9737847447395325\n",
      "Step 1301 | Loss: 0.7537387609481812\n",
      "Step 1401 | Loss: 0.6240754127502441\n",
      "Step 1501 | Loss: 0.8020544648170471\n",
      "Step 1601 | Loss: 0.8092474341392517\n",
      "Step 1701 | Loss: 0.6723993420600891\n",
      "Step 1801 | Loss: 1.0820759534835815\n",
      "Step 1901 | Loss: 0.937608003616333\n",
      "Step 2001 | Loss: 0.645285964012146\n",
      "Step 2101 | Loss: 0.8263996839523315\n",
      "Step 2201 | Loss: 0.7570459246635437\n",
      "Step 2301 | Loss: 0.6864509582519531\n",
      "Step 2401 | Loss: 0.8948236107826233\n",
      "0.7664548257001713 0.725\n",
      "Step 1 | Loss: 1.3229293823242188\n",
      "Step 101 | Loss: 1.3876713514328003\n",
      "Step 201 | Loss: 1.0614893436431885\n",
      "Step 301 | Loss: 1.301006555557251\n",
      "Step 401 | Loss: 1.285290241241455\n",
      "Step 501 | Loss: 1.020487904548645\n",
      "Step 601 | Loss: 0.8835849165916443\n",
      "Step 701 | Loss: 0.8145524859428406\n",
      "Step 801 | Loss: 0.8169640898704529\n",
      "Step 901 | Loss: 1.1154847145080566\n",
      "Step 1001 | Loss: 1.3145151138305664\n",
      "Step 1101 | Loss: 1.1361831426620483\n",
      "Step 1201 | Loss: 1.0379546880722046\n",
      "Step 1301 | Loss: 1.1126549243927002\n",
      "Step 1401 | Loss: 0.9508225321769714\n",
      "Step 1501 | Loss: 1.195465087890625\n",
      "Step 1601 | Loss: 0.8302961587905884\n",
      "Step 1701 | Loss: 1.2177475690841675\n",
      "Step 1801 | Loss: 1.0540950298309326\n",
      "Step 1901 | Loss: 1.1878893375396729\n",
      "Step 2001 | Loss: 1.0546070337295532\n",
      "Step 2101 | Loss: 1.081744909286499\n",
      "Step 2201 | Loss: 1.279390573501587\n",
      "Step 2301 | Loss: 1.0263115167617798\n",
      "Step 2401 | Loss: 1.1925201416015625\n",
      "1.078957954936466 0.535\n",
      "Step 1 | Loss: 1.3978039026260376\n",
      "Step 101 | Loss: 1.3294320106506348\n",
      "Step 201 | Loss: 1.5202834606170654\n",
      "Step 301 | Loss: 0.9886249303817749\n",
      "Step 401 | Loss: 1.2261182069778442\n",
      "Step 501 | Loss: 0.9869256615638733\n",
      "Step 601 | Loss: 1.1230456829071045\n",
      "Step 701 | Loss: 0.9037692546844482\n",
      "Step 801 | Loss: 1.1233874559402466\n",
      "Step 901 | Loss: 1.0471959114074707\n",
      "Step 1001 | Loss: 1.097104787826538\n",
      "Step 1101 | Loss: 0.9553391933441162\n",
      "Step 1201 | Loss: 1.1002223491668701\n",
      "Step 1301 | Loss: 1.0749249458312988\n",
      "Step 1401 | Loss: 0.8388037085533142\n",
      "Step 1501 | Loss: 0.9925718307495117\n",
      "Step 1601 | Loss: 1.0801582336425781\n",
      "Step 1701 | Loss: 1.0282270908355713\n",
      "Step 1801 | Loss: 1.1349793672561646\n",
      "Step 1901 | Loss: 1.1169050931930542\n",
      "Step 2001 | Loss: 0.9845958948135376\n",
      "Step 2101 | Loss: 1.0487849712371826\n",
      "Step 2201 | Loss: 1.1695027351379395\n",
      "Step 2301 | Loss: 0.9580724239349365\n",
      "Step 2401 | Loss: 0.7737565040588379\n",
      "1.0688083880504597 0.5325\n",
      "Step 1 | Loss: 1.6750051975250244\n",
      "Step 101 | Loss: 0.757678747177124\n",
      "Step 201 | Loss: 0.8193716406822205\n",
      "Step 301 | Loss: 0.8074766993522644\n",
      "Step 401 | Loss: 0.85433429479599\n",
      "Step 501 | Loss: 0.7221475839614868\n",
      "Step 601 | Loss: 0.8241087794303894\n",
      "Step 701 | Loss: 0.7363920211791992\n",
      "Step 801 | Loss: 0.7530397772789001\n",
      "Step 901 | Loss: 0.8406901359558105\n",
      "Step 1001 | Loss: 0.8538469076156616\n",
      "Step 1101 | Loss: 0.7366347312927246\n",
      "Step 1201 | Loss: 0.7682002782821655\n",
      "Step 1301 | Loss: 0.6703643202781677\n",
      "Step 1401 | Loss: 0.7797948718070984\n",
      "Step 1501 | Loss: 0.744678795337677\n",
      "Step 1601 | Loss: 0.7878198623657227\n",
      "Step 1701 | Loss: 0.8052678108215332\n",
      "Step 1801 | Loss: 0.8841158151626587\n",
      "Step 1901 | Loss: 0.7217742204666138\n",
      "Step 2001 | Loss: 0.9039167165756226\n",
      "Step 2101 | Loss: 0.7562641501426697\n",
      "Step 2201 | Loss: 0.8502827286720276\n",
      "Step 2301 | Loss: 0.7913126945495605\n",
      "Step 2401 | Loss: 0.8674501180648804\n",
      "0.7990571792110244 0.6525\n",
      "Step 1 | Loss: 1.3928662538528442\n",
      "Step 101 | Loss: 0.9964370131492615\n",
      "Step 201 | Loss: 0.8497774600982666\n",
      "Step 301 | Loss: 0.7555750608444214\n",
      "Step 401 | Loss: 0.7850902676582336\n",
      "Step 501 | Loss: 0.7752825617790222\n",
      "Step 601 | Loss: 0.8329545855522156\n",
      "Step 701 | Loss: 0.8192607164382935\n",
      "Step 801 | Loss: 0.7852922677993774\n",
      "Step 901 | Loss: 0.7703901529312134\n",
      "Step 1001 | Loss: 0.7890902161598206\n",
      "Step 1101 | Loss: 0.9015793800354004\n",
      "Step 1201 | Loss: 0.7864051461219788\n",
      "Step 1301 | Loss: 0.8502637147903442\n",
      "Step 1401 | Loss: 0.5853344202041626\n",
      "Step 1501 | Loss: 0.8570680618286133\n",
      "Step 1601 | Loss: 0.771821141242981\n",
      "Step 1701 | Loss: 0.7776980400085449\n",
      "Step 1801 | Loss: 0.7631320953369141\n",
      "Step 1901 | Loss: 0.8509752750396729\n",
      "Step 2001 | Loss: 0.7152175903320312\n",
      "Step 2101 | Loss: 0.7551621794700623\n",
      "Step 2201 | Loss: 0.8848244547843933\n",
      "Step 2301 | Loss: 0.7976868152618408\n",
      "Step 2401 | Loss: 0.7603734135627747\n",
      "0.7991518367561228 0.645\n",
      "Step 1 | Loss: 1.3067762851715088\n",
      "Step 101 | Loss: 1.2033299207687378\n",
      "Step 201 | Loss: 0.9832409620285034\n",
      "Step 301 | Loss: 0.8677868247032166\n",
      "Step 401 | Loss: 0.746836245059967\n",
      "Step 501 | Loss: 0.7860930562019348\n",
      "Step 601 | Loss: 0.7824229001998901\n",
      "Step 701 | Loss: 0.6994074583053589\n",
      "Step 801 | Loss: 0.75654137134552\n",
      "Step 901 | Loss: 0.7789682149887085\n",
      "Step 1001 | Loss: 0.8720494508743286\n",
      "Step 1101 | Loss: 0.8629539608955383\n",
      "Step 1201 | Loss: 0.8078056573867798\n",
      "Step 1301 | Loss: 0.8942123055458069\n",
      "Step 1401 | Loss: 0.6648590564727783\n",
      "Step 1501 | Loss: 0.8286049962043762\n",
      "Step 1601 | Loss: 0.8844542503356934\n",
      "Step 1701 | Loss: 0.7641067504882812\n",
      "Step 1801 | Loss: 0.8985430002212524\n",
      "Step 1901 | Loss: 0.8051832318305969\n",
      "Step 2001 | Loss: 0.8238938450813293\n",
      "Step 2101 | Loss: 0.8794390559196472\n",
      "Step 2201 | Loss: 0.6967905163764954\n",
      "Step 2301 | Loss: 0.8399435877799988\n",
      "Step 2401 | Loss: 0.8347406387329102\n",
      "0.8002002963040181 0.66\n",
      "Step 1 | Loss: 0.9621418714523315\n",
      "Step 101 | Loss: 0.7863170504570007\n",
      "Step 201 | Loss: 0.9391865134239197\n",
      "Step 301 | Loss: 0.5558933019638062\n",
      "Step 401 | Loss: 0.7101934552192688\n",
      "Step 501 | Loss: 0.4794766306877136\n",
      "Step 601 | Loss: 0.7059441804885864\n",
      "Step 701 | Loss: 0.6964215636253357\n",
      "Step 801 | Loss: 0.5694442391395569\n",
      "Step 901 | Loss: 0.639698326587677\n",
      "Step 1001 | Loss: 0.736770749092102\n",
      "Step 1101 | Loss: 0.7424266934394836\n",
      "Step 1201 | Loss: 0.5909874439239502\n",
      "Step 1301 | Loss: 0.9040269255638123\n",
      "Step 1401 | Loss: 0.8636355400085449\n",
      "Step 1501 | Loss: 0.6835991144180298\n",
      "Step 1601 | Loss: 0.6077005863189697\n",
      "Step 1701 | Loss: 0.8420935869216919\n",
      "Step 1801 | Loss: 0.6044396758079529\n",
      "Step 1901 | Loss: 0.5790795087814331\n",
      "Step 2001 | Loss: 0.6703789234161377\n",
      "Step 2101 | Loss: 0.7744007706642151\n",
      "Step 2201 | Loss: 0.7952086925506592\n",
      "Step 2301 | Loss: 0.6275029182434082\n",
      "Step 2401 | Loss: 0.8202639818191528\n",
      "0.7035600800944888 0.75\n",
      "Step 1 | Loss: 1.2072803974151611\n",
      "Step 101 | Loss: 0.9214799404144287\n",
      "Step 201 | Loss: 0.8782936334609985\n",
      "Step 301 | Loss: 0.9345342516899109\n",
      "Step 401 | Loss: 0.6610565781593323\n",
      "Step 501 | Loss: 0.5625573396682739\n",
      "Step 601 | Loss: 0.754828929901123\n",
      "Step 701 | Loss: 0.7166876792907715\n",
      "Step 801 | Loss: 0.8066535592079163\n",
      "Step 901 | Loss: 0.7806023955345154\n",
      "Step 1001 | Loss: 0.6970182061195374\n",
      "Step 1101 | Loss: 0.7323173880577087\n",
      "Step 1201 | Loss: 0.7140618562698364\n",
      "Step 1301 | Loss: 0.7030156850814819\n",
      "Step 1401 | Loss: 0.8576968908309937\n",
      "Step 1501 | Loss: 0.7564870715141296\n",
      "Step 1601 | Loss: 0.7654443979263306\n",
      "Step 1701 | Loss: 0.547642707824707\n",
      "Step 1801 | Loss: 0.5346370935440063\n",
      "Step 1901 | Loss: 0.654608964920044\n",
      "Step 2001 | Loss: 0.7185677289962769\n",
      "Step 2101 | Loss: 0.4891543388366699\n",
      "Step 2201 | Loss: 0.452380508184433\n",
      "Step 2301 | Loss: 0.7775218486785889\n",
      "Step 2401 | Loss: 0.5367269515991211\n",
      "0.7035361682608919 0.7475\n",
      "Step 1 | Loss: 1.3102190494537354\n",
      "Step 101 | Loss: 1.490343451499939\n",
      "Step 201 | Loss: 0.9968017339706421\n",
      "Step 301 | Loss: 0.9619905948638916\n",
      "Step 401 | Loss: 0.695331335067749\n",
      "Step 501 | Loss: 0.7691961526870728\n",
      "Step 601 | Loss: 0.8288386464118958\n",
      "Step 701 | Loss: 0.6508018970489502\n",
      "Step 801 | Loss: 0.5474040508270264\n",
      "Step 901 | Loss: 0.5192047357559204\n",
      "Step 1001 | Loss: 0.48587581515312195\n",
      "Step 1101 | Loss: 0.598628580570221\n",
      "Step 1201 | Loss: 0.5944363474845886\n",
      "Step 1301 | Loss: 0.6909173130989075\n",
      "Step 1401 | Loss: 0.524024248123169\n",
      "Step 1501 | Loss: 0.4892648756504059\n",
      "Step 1601 | Loss: 0.7151945233345032\n",
      "Step 1701 | Loss: 0.6314529180526733\n",
      "Step 1801 | Loss: 0.5943241119384766\n",
      "Step 1901 | Loss: 0.7907747626304626\n",
      "Step 2001 | Loss: 0.7078636884689331\n",
      "Step 2101 | Loss: 0.5789352655410767\n",
      "Step 2201 | Loss: 0.5940671563148499\n",
      "Step 2301 | Loss: 0.6784259080886841\n",
      "Step 2401 | Loss: 0.6311647295951843\n",
      "0.7046706342081271 0.7525\n",
      "Step 1 | Loss: 0.8868763446807861\n",
      "Step 101 | Loss: 0.8033533096313477\n",
      "Step 201 | Loss: 0.8249943256378174\n",
      "Step 301 | Loss: 0.7846503257751465\n",
      "Step 401 | Loss: 0.6660282015800476\n",
      "Step 501 | Loss: 0.6544780731201172\n",
      "Step 601 | Loss: 0.6451847553253174\n",
      "Step 701 | Loss: 0.7426822781562805\n",
      "Step 801 | Loss: 0.6636343598365784\n",
      "Step 901 | Loss: 0.7522976398468018\n",
      "Step 1001 | Loss: 0.7010688781738281\n",
      "Step 1101 | Loss: 0.8144489526748657\n",
      "Step 1201 | Loss: 1.0177760124206543\n",
      "Step 1301 | Loss: 0.738023579120636\n",
      "Step 1401 | Loss: 0.8282089233398438\n",
      "Step 1501 | Loss: 0.5739885568618774\n",
      "Step 1601 | Loss: 0.7257351875305176\n",
      "Step 1701 | Loss: 0.5849523544311523\n",
      "Step 1801 | Loss: 0.6443201899528503\n",
      "Step 1901 | Loss: 0.8011898398399353\n",
      "Step 2001 | Loss: 0.6570245623588562\n",
      "Step 2101 | Loss: 0.7216557264328003\n",
      "Step 2201 | Loss: 0.5847455859184265\n",
      "Step 2301 | Loss: 0.6894261837005615\n",
      "Step 2401 | Loss: 0.6920970678329468\n",
      "0.7034323277368568 0.755\n",
      "Step 1 | Loss: 0.903811514377594\n",
      "Step 101 | Loss: 1.0453804731369019\n",
      "Step 201 | Loss: 0.9231098890304565\n",
      "Step 301 | Loss: 0.7450156211853027\n",
      "Step 401 | Loss: 0.7159027457237244\n",
      "Step 501 | Loss: 1.0208817720413208\n",
      "Step 601 | Loss: 0.891248345375061\n",
      "Step 701 | Loss: 0.8765581846237183\n",
      "Step 801 | Loss: 0.6377676725387573\n",
      "Step 901 | Loss: 0.7140063047409058\n",
      "Step 1001 | Loss: 0.647957980632782\n",
      "Step 1101 | Loss: 0.5146162509918213\n",
      "Step 1201 | Loss: 0.7649281024932861\n",
      "Step 1301 | Loss: 0.7245463132858276\n",
      "Step 1401 | Loss: 0.5284683704376221\n",
      "Step 1501 | Loss: 0.5808316469192505\n",
      "Step 1601 | Loss: 0.665374755859375\n",
      "Step 1701 | Loss: 0.725277304649353\n",
      "Step 1801 | Loss: 1.0108038187026978\n",
      "Step 1901 | Loss: 0.9028185606002808\n",
      "Step 2001 | Loss: 0.6810206770896912\n",
      "Step 2101 | Loss: 0.666619062423706\n",
      "Step 2201 | Loss: 0.7037271857261658\n",
      "Step 2301 | Loss: 0.5587188601493835\n",
      "Step 2401 | Loss: 0.710300624370575\n",
      "0.7040580653021586 0.7525\n",
      "Step 1 | Loss: 1.7477803230285645\n",
      "Step 101 | Loss: 1.0085079669952393\n",
      "Step 201 | Loss: 0.9999355673789978\n",
      "Step 301 | Loss: 1.0184144973754883\n",
      "Step 401 | Loss: 0.9652948379516602\n",
      "Step 501 | Loss: 0.9940171241760254\n",
      "Step 601 | Loss: 0.9961376786231995\n",
      "Step 701 | Loss: 0.9961007833480835\n",
      "Step 801 | Loss: 0.9984978437423706\n",
      "Step 901 | Loss: 1.0168803930282593\n",
      "Step 1001 | Loss: 0.9982350468635559\n",
      "Step 1101 | Loss: 1.0280903577804565\n",
      "Step 1201 | Loss: 0.9871258735656738\n",
      "Step 1301 | Loss: 1.0302408933639526\n",
      "Step 1401 | Loss: 1.0032880306243896\n",
      "Step 1501 | Loss: 0.994459867477417\n",
      "Step 1601 | Loss: 1.0001654624938965\n",
      "Step 1701 | Loss: 1.003068208694458\n",
      "Step 1801 | Loss: 1.0098786354064941\n",
      "Step 1901 | Loss: 1.0172858238220215\n",
      "Step 2001 | Loss: 1.007256031036377\n",
      "Step 2101 | Loss: 0.9844455718994141\n",
      "Step 2201 | Loss: 0.9998631477355957\n",
      "Step 2301 | Loss: 1.0003423690795898\n",
      "Step 2401 | Loss: 0.9984828233718872\n",
      "1.0003801095010147 0.5\n",
      "Step 1 | Loss: 1.0110386610031128\n",
      "Step 101 | Loss: 1.0376609563827515\n",
      "Step 201 | Loss: 1.0127017498016357\n",
      "Step 301 | Loss: 0.9907643795013428\n",
      "Step 401 | Loss: 1.014524221420288\n",
      "Step 501 | Loss: 1.0001776218414307\n",
      "Step 601 | Loss: 1.0024919509887695\n",
      "Step 701 | Loss: 1.006379246711731\n",
      "Step 801 | Loss: 0.9809894561767578\n",
      "Step 901 | Loss: 0.9797754287719727\n",
      "Step 1001 | Loss: 1.0995546579360962\n",
      "Step 1101 | Loss: 0.9991437792778015\n",
      "Step 1201 | Loss: 0.9870644211769104\n",
      "Step 1301 | Loss: 1.0108959674835205\n",
      "Step 1401 | Loss: 1.002375602722168\n",
      "Step 1501 | Loss: 1.0318019390106201\n",
      "Step 1601 | Loss: 1.0007569789886475\n",
      "Step 1701 | Loss: 1.008832335472107\n",
      "Step 1801 | Loss: 1.000288486480713\n",
      "Step 1901 | Loss: 1.007163405418396\n",
      "Step 2001 | Loss: 0.9915050268173218\n",
      "Step 2101 | Loss: 0.9961069226264954\n",
      "Step 2201 | Loss: 1.0070910453796387\n",
      "Step 2301 | Loss: 0.98377525806427\n",
      "Step 2401 | Loss: 0.997519850730896\n",
      "1.0013104410161218 0.5\n",
      "Step 1 | Loss: 1.7943016290664673\n",
      "Step 101 | Loss: 0.9962180256843567\n",
      "Step 201 | Loss: 1.0025914907455444\n",
      "Step 301 | Loss: 0.9904919862747192\n",
      "Step 401 | Loss: 1.0323612689971924\n",
      "Step 501 | Loss: 1.038633108139038\n",
      "Step 601 | Loss: 1.0011098384857178\n",
      "Step 701 | Loss: 1.0049623250961304\n",
      "Step 801 | Loss: 0.9856163263320923\n",
      "Step 901 | Loss: 1.0018503665924072\n",
      "Step 1001 | Loss: 0.9821783900260925\n",
      "Step 1101 | Loss: 1.0045450925827026\n",
      "Step 1201 | Loss: 0.9987596273422241\n",
      "Step 1301 | Loss: 1.0072969198226929\n",
      "Step 1401 | Loss: 0.9380925297737122\n",
      "Step 1501 | Loss: 1.0009160041809082\n",
      "Step 1601 | Loss: 0.9979308247566223\n",
      "Step 1701 | Loss: 1.0134022235870361\n",
      "Step 1801 | Loss: 0.9985126256942749\n",
      "Step 1901 | Loss: 0.999062180519104\n",
      "Step 2001 | Loss: 1.0014408826828003\n",
      "Step 2101 | Loss: 1.0181870460510254\n",
      "Step 2201 | Loss: 0.9936724901199341\n",
      "Step 2301 | Loss: 1.0280609130859375\n",
      "Step 2401 | Loss: 0.9912112951278687\n",
      "1.0038304612064126 0.5\n",
      "Step 1 | Loss: 2.12080717086792\n",
      "Step 101 | Loss: 1.0052293539047241\n",
      "Step 201 | Loss: 1.0297019481658936\n",
      "Step 301 | Loss: 1.0019985437393188\n",
      "Step 401 | Loss: 1.0166937112808228\n",
      "Step 501 | Loss: 0.9763530492782593\n",
      "Step 601 | Loss: 1.0001713037490845\n",
      "Step 701 | Loss: 1.0192784070968628\n",
      "Step 801 | Loss: 1.0717754364013672\n",
      "Step 901 | Loss: 0.9664889574050903\n",
      "Step 1001 | Loss: 1.0098824501037598\n",
      "Step 1101 | Loss: 1.0010861158370972\n",
      "Step 1201 | Loss: 0.9999216794967651\n",
      "Step 1301 | Loss: 1.0191986560821533\n",
      "Step 1401 | Loss: 1.0061274766921997\n",
      "Step 1501 | Loss: 0.9965470433235168\n",
      "Step 1601 | Loss: 0.9993102550506592\n",
      "Step 1701 | Loss: 0.9995136260986328\n",
      "Step 1801 | Loss: 1.00005304813385\n",
      "Step 1901 | Loss: 0.9979051351547241\n",
      "Step 2001 | Loss: 1.0221471786499023\n",
      "Step 2101 | Loss: 1.0030385255813599\n",
      "Step 2201 | Loss: 1.016769528388977\n",
      "Step 2301 | Loss: 1.0050989389419556\n",
      "Step 2401 | Loss: 1.0018258094787598\n",
      "1.0029758330183123 0.5\n",
      "Step 1 | Loss: 0.9721581935882568\n",
      "Step 101 | Loss: 1.0298517942428589\n",
      "Step 201 | Loss: 1.0008317232131958\n",
      "Step 301 | Loss: 1.0225882530212402\n",
      "Step 401 | Loss: 0.9963923692703247\n",
      "Step 501 | Loss: 0.9976727962493896\n",
      "Step 601 | Loss: 1.0007184743881226\n",
      "Step 701 | Loss: 0.9994074702262878\n",
      "Step 801 | Loss: 1.0292707681655884\n",
      "Step 901 | Loss: 1.0010452270507812\n",
      "Step 1001 | Loss: 1.001707911491394\n",
      "Step 1101 | Loss: 0.9832539558410645\n",
      "Step 1201 | Loss: 0.9991662502288818\n",
      "Step 1301 | Loss: 0.99895840883255\n",
      "Step 1401 | Loss: 0.9968617558479309\n",
      "Step 1501 | Loss: 1.027471661567688\n",
      "Step 1601 | Loss: 0.9867908358573914\n",
      "Step 1701 | Loss: 0.9794724583625793\n",
      "Step 1801 | Loss: 1.013677954673767\n",
      "Step 1901 | Loss: 0.9902502298355103\n",
      "Step 2001 | Loss: 0.9953551292419434\n",
      "Step 2101 | Loss: 0.9962338805198669\n",
      "Step 2201 | Loss: 0.9973770380020142\n",
      "Step 2301 | Loss: 0.9860799312591553\n",
      "Step 2401 | Loss: 1.023829698562622\n",
      "0.9999828676146874 0.5\n",
      "Step 1 | Loss: 1.676630973815918\n",
      "Step 101 | Loss: 0.11333820223808289\n",
      "Step 201 | Loss: 0.11084049940109253\n",
      "Step 301 | Loss: 0.21788591146469116\n",
      "Step 401 | Loss: 0.18197396397590637\n",
      "Step 501 | Loss: 0.14893117547035217\n",
      "Step 601 | Loss: 0.17015232145786285\n",
      "Step 701 | Loss: 0.0667303279042244\n",
      "Step 801 | Loss: 0.09741158783435822\n",
      "Step 901 | Loss: 0.11877508461475372\n",
      "Step 1001 | Loss: 0.2700614929199219\n",
      "Step 1101 | Loss: 0.12722161412239075\n",
      "Step 1201 | Loss: 0.10075188428163528\n",
      "Step 1301 | Loss: 0.16130277514457703\n",
      "Step 1401 | Loss: 0.16556255519390106\n",
      "Step 1501 | Loss: 0.1062181368470192\n",
      "Step 1601 | Loss: 0.12738588452339172\n",
      "Step 1701 | Loss: 0.11710095405578613\n",
      "Step 1801 | Loss: 0.11014607548713684\n",
      "Step 1901 | Loss: 0.09230722486972809\n",
      "Step 2001 | Loss: 0.09295970946550369\n",
      "Step 2101 | Loss: 0.11435405910015106\n",
      "Step 2201 | Loss: 0.09809590876102448\n",
      "Step 2301 | Loss: 0.0944022536277771\n",
      "Step 2401 | Loss: 0.09419527649879456\n",
      "0.11943483198713803 0.9875\n",
      "Step 1 | Loss: 2.3570027351379395\n",
      "Step 101 | Loss: 0.3837157189846039\n",
      "Step 201 | Loss: 0.10884996503591537\n",
      "Step 301 | Loss: 0.1454271674156189\n",
      "Step 401 | Loss: 0.23067447543144226\n",
      "Step 501 | Loss: 0.15344403684139252\n",
      "Step 601 | Loss: 0.1056583896279335\n",
      "Step 701 | Loss: 0.08936206251382828\n",
      "Step 801 | Loss: 0.0882740244269371\n",
      "Step 901 | Loss: 0.16310590505599976\n",
      "Step 1001 | Loss: 0.08659876883029938\n",
      "Step 1101 | Loss: 0.2552291452884674\n",
      "Step 1201 | Loss: 0.10867690294981003\n",
      "Step 1301 | Loss: 0.19120484590530396\n",
      "Step 1401 | Loss: 0.0924479141831398\n",
      "Step 1501 | Loss: 0.09674298018217087\n",
      "Step 1601 | Loss: 0.14583531022071838\n",
      "Step 1701 | Loss: 0.20359861850738525\n",
      "Step 1801 | Loss: 0.09256530553102493\n",
      "Step 1901 | Loss: 0.09699058532714844\n",
      "Step 2001 | Loss: 0.17948585748672485\n",
      "Step 2101 | Loss: 0.09103342890739441\n",
      "Step 2201 | Loss: 0.08950468897819519\n",
      "Step 2301 | Loss: 0.08070166409015656\n",
      "Step 2401 | Loss: 0.13274137675762177\n",
      "0.12143866203473874 0.9875\n",
      "Step 1 | Loss: 0.8837624788284302\n",
      "Step 101 | Loss: 0.0930016040802002\n",
      "Step 201 | Loss: 0.0907125473022461\n",
      "Step 301 | Loss: 0.16948047280311584\n",
      "Step 401 | Loss: 0.17810620367527008\n",
      "Step 501 | Loss: 0.17087699472904205\n",
      "Step 601 | Loss: 0.09713266789913177\n",
      "Step 701 | Loss: 0.11387673020362854\n",
      "Step 801 | Loss: 0.16369512677192688\n",
      "Step 901 | Loss: 0.12191040068864822\n",
      "Step 1001 | Loss: 0.10691143572330475\n",
      "Step 1101 | Loss: 0.13355784118175507\n",
      "Step 1201 | Loss: 0.2122734934091568\n",
      "Step 1301 | Loss: 0.10101719200611115\n",
      "Step 1401 | Loss: 0.16991405189037323\n",
      "Step 1501 | Loss: 0.11207331717014313\n",
      "Step 1601 | Loss: 0.10889004170894623\n",
      "Step 1701 | Loss: 0.12140339612960815\n",
      "Step 1801 | Loss: 0.13803152740001678\n",
      "Step 1901 | Loss: 0.12336061894893646\n",
      "Step 2001 | Loss: 0.0972822830080986\n",
      "Step 2101 | Loss: 0.08378979563713074\n",
      "Step 2201 | Loss: 0.1066078320145607\n",
      "Step 2301 | Loss: 0.18087533116340637\n",
      "Step 2401 | Loss: 0.1622890681028366\n",
      "0.1196456213854505 0.9875\n",
      "Step 1 | Loss: 0.8509325385093689\n",
      "Step 101 | Loss: 0.10232585668563843\n",
      "Step 201 | Loss: 0.12522850930690765\n",
      "Step 301 | Loss: 0.0653759241104126\n",
      "Step 401 | Loss: 0.1031566932797432\n",
      "Step 501 | Loss: 0.12514613568782806\n",
      "Step 601 | Loss: 0.08960960805416107\n",
      "Step 701 | Loss: 0.09692537039518356\n",
      "Step 801 | Loss: 0.11695335805416107\n",
      "Step 901 | Loss: 0.12468206882476807\n",
      "Step 1001 | Loss: 0.03897624835371971\n",
      "Step 1101 | Loss: 0.1238020583987236\n",
      "Step 1201 | Loss: 0.12662623822689056\n",
      "Step 1301 | Loss: 0.06108778715133667\n",
      "Step 1401 | Loss: 0.1887616217136383\n",
      "Step 1501 | Loss: 0.09442878514528275\n",
      "Step 1601 | Loss: 0.08456351608037949\n",
      "Step 1701 | Loss: 0.2144574671983719\n",
      "Step 1801 | Loss: 0.17022423446178436\n",
      "Step 1901 | Loss: 0.06243184581398964\n",
      "Step 2001 | Loss: 0.08027415722608566\n",
      "Step 2101 | Loss: 0.14554919302463531\n",
      "Step 2201 | Loss: 0.05450977757573128\n",
      "Step 2301 | Loss: 0.12742488086223602\n",
      "Step 2401 | Loss: 0.07594545185565948\n",
      "0.11967760696470044 0.9875\n",
      "Step 1 | Loss: 0.5447103381156921\n",
      "Step 101 | Loss: 0.252852201461792\n",
      "Step 201 | Loss: 0.12484289705753326\n",
      "Step 301 | Loss: 0.12780044972896576\n",
      "Step 401 | Loss: 0.11279157549142838\n",
      "Step 501 | Loss: 0.1200493723154068\n",
      "Step 601 | Loss: 0.12514463067054749\n",
      "Step 701 | Loss: 0.07300794869661331\n",
      "Step 801 | Loss: 0.1707514077425003\n",
      "Step 901 | Loss: 0.1043175533413887\n",
      "Step 1001 | Loss: 0.07457037270069122\n",
      "Step 1101 | Loss: 0.08875958621501923\n",
      "Step 1201 | Loss: 0.10763595998287201\n",
      "Step 1301 | Loss: 0.13495533168315887\n",
      "Step 1401 | Loss: 0.11657160520553589\n",
      "Step 1501 | Loss: 0.22203172743320465\n",
      "Step 1601 | Loss: 0.11613183468580246\n",
      "Step 1701 | Loss: 0.10392577201128006\n",
      "Step 1801 | Loss: 0.14045020937919617\n",
      "Step 1901 | Loss: 0.14550042152404785\n",
      "Step 2001 | Loss: 0.12052220106124878\n",
      "Step 2101 | Loss: 0.10474415123462677\n",
      "Step 2201 | Loss: 0.13932102918624878\n",
      "Step 2301 | Loss: 0.12119358777999878\n",
      "Step 2401 | Loss: 0.1868894100189209\n",
      "0.11976442710375115 0.9875\n",
      "Step 1 | Loss: 1.4390273094177246\n",
      "Step 101 | Loss: 0.8484624624252319\n",
      "Step 201 | Loss: 0.447942852973938\n",
      "Step 301 | Loss: 0.3666967451572418\n",
      "Step 401 | Loss: 0.4588810205459595\n",
      "Step 501 | Loss: 0.4150864779949188\n",
      "Step 601 | Loss: 0.5141010284423828\n",
      "Step 701 | Loss: 0.43647730350494385\n",
      "Step 801 | Loss: 0.2917739450931549\n",
      "Step 901 | Loss: 0.4129100441932678\n",
      "Step 1001 | Loss: 0.37997984886169434\n",
      "Step 1101 | Loss: 0.42750149965286255\n",
      "Step 1201 | Loss: 0.4515233635902405\n",
      "Step 1301 | Loss: 0.4573631286621094\n",
      "Step 1401 | Loss: 0.44305962324142456\n",
      "Step 1501 | Loss: 0.2919478416442871\n",
      "Step 1601 | Loss: 0.33766528964042664\n",
      "Step 1701 | Loss: 0.3549576997756958\n",
      "Step 1801 | Loss: 0.38338690996170044\n",
      "Step 1901 | Loss: 0.5208513140678406\n",
      "Step 2001 | Loss: 0.47778940200805664\n",
      "Step 2101 | Loss: 0.3201477527618408\n",
      "Step 2201 | Loss: 0.36598631739616394\n",
      "Step 2301 | Loss: 0.37762385606765747\n",
      "Step 2401 | Loss: 0.5075840353965759\n",
      "0.4027520296985527 0.9275\n",
      "Step 1 | Loss: 1.1089614629745483\n",
      "Step 101 | Loss: 0.8834247589111328\n",
      "Step 201 | Loss: 0.6940052509307861\n",
      "Step 301 | Loss: 0.4711495637893677\n",
      "Step 401 | Loss: 0.3731938898563385\n",
      "Step 501 | Loss: 0.3300519585609436\n",
      "Step 601 | Loss: 0.45769912004470825\n",
      "Step 701 | Loss: 0.47370824217796326\n",
      "Step 801 | Loss: 0.35662516951560974\n",
      "Step 901 | Loss: 0.47539734840393066\n",
      "Step 1001 | Loss: 0.27722668647766113\n",
      "Step 1101 | Loss: 0.31355321407318115\n",
      "Step 1201 | Loss: 0.4723876416683197\n",
      "Step 1301 | Loss: 0.5370250344276428\n",
      "Step 1401 | Loss: 0.31439393758773804\n",
      "Step 1501 | Loss: 0.4537216126918793\n",
      "Step 1601 | Loss: 0.40121322870254517\n",
      "Step 1701 | Loss: 0.5994783639907837\n",
      "Step 1801 | Loss: 0.3826506733894348\n",
      "Step 1901 | Loss: 0.5859568119049072\n",
      "Step 2001 | Loss: 0.2930312156677246\n",
      "Step 2101 | Loss: 0.3463384509086609\n",
      "Step 2201 | Loss: 0.38851720094680786\n",
      "Step 2301 | Loss: 0.3844969868659973\n",
      "Step 2401 | Loss: 0.4451669156551361\n",
      "0.4023203927870596 0.9325\n",
      "Step 1 | Loss: 1.1751996278762817\n",
      "Step 101 | Loss: 0.8300155401229858\n",
      "Step 201 | Loss: 0.811295211315155\n",
      "Step 301 | Loss: 0.7659479379653931\n",
      "Step 401 | Loss: 0.7218872308731079\n",
      "Step 501 | Loss: 0.6105280518531799\n",
      "Step 601 | Loss: 0.6980722546577454\n",
      "Step 701 | Loss: 0.7435812950134277\n",
      "Step 801 | Loss: 0.6338841319084167\n",
      "Step 901 | Loss: 0.6484267711639404\n",
      "Step 1001 | Loss: 0.5955510139465332\n",
      "Step 1101 | Loss: 0.5946912169456482\n",
      "Step 1201 | Loss: 0.6449559330940247\n",
      "Step 1301 | Loss: 0.56861412525177\n",
      "Step 1401 | Loss: 0.663442075252533\n",
      "Step 1501 | Loss: 0.5370016694068909\n",
      "Step 1601 | Loss: 0.6315975189208984\n",
      "Step 1701 | Loss: 0.45970550179481506\n",
      "Step 1801 | Loss: 0.4726625680923462\n",
      "Step 1901 | Loss: 0.5191356539726257\n",
      "Step 2001 | Loss: 0.4390479028224945\n",
      "Step 2101 | Loss: 0.43238916993141174\n",
      "Step 2201 | Loss: 0.31025195121765137\n",
      "Step 2301 | Loss: 0.4975588917732239\n",
      "Step 2401 | Loss: 0.41519439220428467\n",
      "0.40372608921567044 0.9225\n",
      "Step 1 | Loss: 1.099331259727478\n",
      "Step 101 | Loss: 0.7744618058204651\n",
      "Step 201 | Loss: 0.6940722465515137\n",
      "Step 301 | Loss: 0.7067791223526001\n",
      "Step 401 | Loss: 0.6354565024375916\n",
      "Step 501 | Loss: 0.6126527786254883\n",
      "Step 601 | Loss: 0.587723970413208\n",
      "Step 701 | Loss: 0.43212151527404785\n",
      "Step 801 | Loss: 0.5465148091316223\n",
      "Step 901 | Loss: 0.5859171152114868\n",
      "Step 1001 | Loss: 0.5304780602455139\n",
      "Step 1101 | Loss: 0.5840444564819336\n",
      "Step 1201 | Loss: 0.6311410069465637\n",
      "Step 1301 | Loss: 0.5926305651664734\n",
      "Step 1401 | Loss: 0.6220200657844543\n",
      "Step 1501 | Loss: 0.5495237112045288\n",
      "Step 1601 | Loss: 0.5373027920722961\n",
      "Step 1701 | Loss: 0.5713467001914978\n",
      "Step 1801 | Loss: 0.5010087490081787\n",
      "Step 1901 | Loss: 0.41690629720687866\n",
      "Step 2001 | Loss: 0.5649497509002686\n",
      "Step 2101 | Loss: 0.4887600541114807\n",
      "Step 2201 | Loss: 0.6407588720321655\n",
      "Step 2301 | Loss: 0.5094242095947266\n",
      "Step 2401 | Loss: 0.6044090390205383\n",
      "0.5304484143851782 0.9075\n",
      "Step 1 | Loss: 1.5923467874526978\n",
      "Step 101 | Loss: 0.7627747058868408\n",
      "Step 201 | Loss: 0.6580697298049927\n",
      "Step 301 | Loss: 0.644972026348114\n",
      "Step 401 | Loss: 0.6082674860954285\n",
      "Step 501 | Loss: 0.559512734413147\n",
      "Step 601 | Loss: 0.5404461026191711\n",
      "Step 701 | Loss: 0.5362027287483215\n",
      "Step 801 | Loss: 0.5278264880180359\n",
      "Step 901 | Loss: 0.5464290976524353\n",
      "Step 1001 | Loss: 0.47022780776023865\n",
      "Step 1101 | Loss: 0.4892628490924835\n",
      "Step 1201 | Loss: 0.473635733127594\n",
      "Step 1301 | Loss: 0.5349295139312744\n",
      "Step 1401 | Loss: 0.5114200115203857\n",
      "Step 1501 | Loss: 0.4163222014904022\n",
      "Step 1601 | Loss: 0.45130544900894165\n",
      "Step 1701 | Loss: 0.5016853213310242\n",
      "Step 1801 | Loss: 0.6014379262924194\n",
      "Step 1901 | Loss: 0.4421539306640625\n",
      "Step 2001 | Loss: 0.5817421078681946\n",
      "Step 2101 | Loss: 0.5541301965713501\n",
      "Step 2201 | Loss: 0.6061721444129944\n",
      "Step 2301 | Loss: 0.5313165187835693\n",
      "Step 2401 | Loss: 0.5979697108268738\n",
      "0.5027112422101142 0.9225\n",
      "Step 1 | Loss: 1.0474964380264282\n",
      "Step 101 | Loss: 0.6289803981781006\n",
      "Step 201 | Loss: 0.5730087161064148\n",
      "Step 301 | Loss: 0.5035586357116699\n",
      "Step 401 | Loss: 0.5197846293449402\n",
      "Step 501 | Loss: 0.5437183380126953\n",
      "Step 601 | Loss: 0.5123906135559082\n",
      "Step 701 | Loss: 0.6282443404197693\n",
      "Step 801 | Loss: 0.743736207485199\n",
      "Step 901 | Loss: 0.6365930438041687\n",
      "Step 1001 | Loss: 0.5615344643592834\n",
      "Step 1101 | Loss: 0.4700659513473511\n",
      "Step 1201 | Loss: 0.4900606870651245\n",
      "Step 1301 | Loss: 0.6046326160430908\n",
      "Step 1401 | Loss: 0.5358178615570068\n",
      "Step 1501 | Loss: 0.48754939436912537\n",
      "Step 1601 | Loss: 0.6357771158218384\n",
      "Step 1701 | Loss: 0.5511798858642578\n",
      "Step 1801 | Loss: 0.5346661806106567\n",
      "Step 1901 | Loss: 0.5667463541030884\n",
      "Step 2001 | Loss: 0.4197407364845276\n",
      "Step 2101 | Loss: 0.4956473112106323\n",
      "Step 2201 | Loss: 0.4933301508426666\n",
      "Step 2301 | Loss: 0.47520899772644043\n",
      "Step 2401 | Loss: 0.6469883918762207\n",
      "0.5189403779568796 0.8375\n",
      "Step 1 | Loss: 1.2185306549072266\n",
      "Step 101 | Loss: 0.70868980884552\n",
      "Step 201 | Loss: 0.5252494812011719\n",
      "Step 301 | Loss: 0.6302044987678528\n",
      "Step 401 | Loss: 0.6246439218521118\n",
      "Step 501 | Loss: 0.5842407941818237\n",
      "Step 601 | Loss: 0.5677337646484375\n",
      "Step 701 | Loss: 0.48187533020973206\n",
      "Step 801 | Loss: 0.6422022581100464\n",
      "Step 901 | Loss: 0.4627451002597809\n",
      "Step 1001 | Loss: 0.4580869972705841\n",
      "Step 1101 | Loss: 0.5898952484130859\n",
      "Step 1201 | Loss: 0.5756559371948242\n",
      "Step 1301 | Loss: 0.5392343997955322\n",
      "Step 1401 | Loss: 0.5448982119560242\n",
      "Step 1501 | Loss: 0.6689522862434387\n",
      "Step 1601 | Loss: 0.5287062525749207\n",
      "Step 1701 | Loss: 0.516717791557312\n",
      "Step 1801 | Loss: 0.6639204621315002\n",
      "Step 1901 | Loss: 0.5051901936531067\n",
      "Step 2001 | Loss: 0.5380269885063171\n",
      "Step 2101 | Loss: 0.4491553008556366\n",
      "Step 2201 | Loss: 0.6612677574157715\n",
      "Step 2301 | Loss: 0.543183445930481\n",
      "Step 2401 | Loss: 0.487185001373291\n",
      "0.5229027464018169 0.84\n",
      "Step 1 | Loss: 1.3697326183319092\n",
      "Step 101 | Loss: 0.502339780330658\n",
      "Step 201 | Loss: 0.5048873424530029\n",
      "Step 301 | Loss: 0.34446024894714355\n",
      "Step 401 | Loss: 0.6587514281272888\n",
      "Step 501 | Loss: 0.4659048914909363\n",
      "Step 601 | Loss: 0.7048405408859253\n",
      "Step 701 | Loss: 0.3431447148323059\n",
      "Step 801 | Loss: 0.40160542726516724\n",
      "Step 901 | Loss: 0.3980969786643982\n",
      "Step 1001 | Loss: 0.404747873544693\n",
      "Step 1101 | Loss: 0.5826192498207092\n",
      "Step 1201 | Loss: 0.5034998059272766\n",
      "Step 1301 | Loss: 0.47155046463012695\n",
      "Step 1401 | Loss: 0.46897220611572266\n",
      "Step 1501 | Loss: 0.5971480011940002\n",
      "Step 1601 | Loss: 0.45821964740753174\n",
      "Step 1701 | Loss: 0.44386741518974304\n",
      "Step 1801 | Loss: 0.616783618927002\n",
      "Step 1901 | Loss: 0.5178588032722473\n",
      "Step 2001 | Loss: 0.5871781706809998\n",
      "Step 2101 | Loss: 0.48782879114151\n",
      "Step 2201 | Loss: 0.48384883999824524\n",
      "Step 2301 | Loss: 0.5288585424423218\n",
      "Step 2401 | Loss: 0.5109391212463379\n",
      "0.5012363560816372 0.835\n",
      "Step 1 | Loss: 1.6803083419799805\n",
      "Step 101 | Loss: 0.7460318803787231\n",
      "Step 201 | Loss: 0.5187875628471375\n",
      "Step 301 | Loss: 0.6744207143783569\n",
      "Step 401 | Loss: 0.6233912110328674\n",
      "Step 501 | Loss: 0.7361438870429993\n",
      "Step 601 | Loss: 0.37107187509536743\n",
      "Step 701 | Loss: 0.5372005105018616\n",
      "Step 801 | Loss: 0.6137514114379883\n",
      "Step 901 | Loss: 0.5899407267570496\n",
      "Step 1001 | Loss: 0.39745962619781494\n",
      "Step 1101 | Loss: 0.4753982126712799\n",
      "Step 1201 | Loss: 0.5035212635993958\n",
      "Step 1301 | Loss: 0.4201086461544037\n",
      "Step 1401 | Loss: 0.6609043478965759\n",
      "Step 1501 | Loss: 0.39972394704818726\n",
      "Step 1601 | Loss: 0.4648808240890503\n",
      "Step 1701 | Loss: 0.5489334464073181\n",
      "Step 1801 | Loss: 0.6362394690513611\n",
      "Step 1901 | Loss: 0.44199538230895996\n",
      "Step 2001 | Loss: 0.5509774088859558\n",
      "Step 2101 | Loss: 0.5495467185974121\n",
      "Step 2201 | Loss: 0.6092343330383301\n",
      "Step 2301 | Loss: 0.41341593861579895\n",
      "Step 2401 | Loss: 0.4241741895675659\n",
      "0.49935120173658354 0.835\n",
      "Step 1 | Loss: 1.1168709993362427\n",
      "Step 101 | Loss: 0.6937723159790039\n",
      "Step 201 | Loss: 0.6547579765319824\n",
      "Step 301 | Loss: 0.8614194989204407\n",
      "Step 401 | Loss: 0.64256352186203\n",
      "Step 501 | Loss: 0.6462185382843018\n",
      "Step 601 | Loss: 0.671899139881134\n",
      "Step 701 | Loss: 0.7437254190444946\n",
      "Step 801 | Loss: 0.632145881652832\n",
      "Step 901 | Loss: 0.6173768639564514\n",
      "Step 1001 | Loss: 0.6325206756591797\n",
      "Step 1101 | Loss: 0.5928748250007629\n",
      "Step 1201 | Loss: 0.4171925485134125\n",
      "Step 1301 | Loss: 0.6916699409484863\n",
      "Step 1401 | Loss: 0.5617932677268982\n",
      "Step 1501 | Loss: 0.5983119010925293\n",
      "Step 1601 | Loss: 0.6411116719245911\n",
      "Step 1701 | Loss: 0.5649389028549194\n",
      "Step 1801 | Loss: 0.35961660742759705\n",
      "Step 1901 | Loss: 0.690004289150238\n",
      "Step 2001 | Loss: 0.39570140838623047\n",
      "Step 2101 | Loss: 0.4514999985694885\n",
      "Step 2201 | Loss: 0.5468305349349976\n",
      "Step 2301 | Loss: 0.5281239748001099\n",
      "Step 2401 | Loss: 0.5664520263671875\n",
      "0.49944317337642613 0.835\n",
      "Step 1 | Loss: 1.5417143106460571\n",
      "Step 101 | Loss: 1.1150662899017334\n",
      "Step 201 | Loss: 0.8597690463066101\n",
      "Step 301 | Loss: 1.1270314455032349\n",
      "Step 401 | Loss: 1.3118560314178467\n",
      "Step 501 | Loss: 0.8016802668571472\n",
      "Step 601 | Loss: 1.3086965084075928\n",
      "Step 701 | Loss: 1.0818421840667725\n",
      "Step 801 | Loss: 1.4404222965240479\n",
      "Step 901 | Loss: 1.6240007877349854\n",
      "Step 1001 | Loss: 1.3843657970428467\n",
      "Step 1101 | Loss: 1.0275425910949707\n",
      "Step 1201 | Loss: 0.9334210753440857\n",
      "Step 1301 | Loss: 0.9301192164421082\n",
      "Step 1401 | Loss: 0.9982042908668518\n",
      "Step 1501 | Loss: 0.5600051283836365\n",
      "Step 1601 | Loss: 1.0694990158081055\n",
      "Step 1701 | Loss: 1.2044824361801147\n",
      "Step 1801 | Loss: 1.0541092157363892\n",
      "Step 1901 | Loss: 1.3930467367172241\n",
      "Step 2001 | Loss: 1.1063836812973022\n",
      "Step 2101 | Loss: 0.899262547492981\n",
      "Step 2201 | Loss: 0.9714028835296631\n",
      "Step 2301 | Loss: 1.303816556930542\n",
      "Step 2401 | Loss: 0.8779339790344238\n",
      "0.9545717381052038 0.575\n",
      "Step 1 | Loss: 1.2160406112670898\n",
      "Step 101 | Loss: 1.4469521045684814\n",
      "Step 201 | Loss: 1.4167084693908691\n",
      "Step 301 | Loss: 1.3000214099884033\n",
      "Step 401 | Loss: 1.4634696245193481\n",
      "Step 501 | Loss: 0.7554540634155273\n",
      "Step 601 | Loss: 1.206057071685791\n",
      "Step 701 | Loss: 1.41364586353302\n",
      "Step 801 | Loss: 1.4998202323913574\n",
      "Step 901 | Loss: 1.0718684196472168\n",
      "Step 1001 | Loss: 1.1855437755584717\n",
      "Step 1101 | Loss: 1.4762850999832153\n",
      "Step 1201 | Loss: 1.1035487651824951\n",
      "Step 1301 | Loss: 1.067618489265442\n",
      "Step 1401 | Loss: 1.094092845916748\n",
      "Step 1501 | Loss: 0.9128380417823792\n",
      "Step 1601 | Loss: 1.1355314254760742\n",
      "Step 1701 | Loss: 1.0872161388397217\n",
      "Step 1801 | Loss: 0.9031623005867004\n",
      "Step 1901 | Loss: 1.1602542400360107\n",
      "Step 2001 | Loss: 1.0314385890960693\n",
      "Step 2101 | Loss: 0.9877843856811523\n",
      "Step 2201 | Loss: 0.9896576404571533\n",
      "Step 2301 | Loss: 1.077745795249939\n",
      "Step 2401 | Loss: 0.9241507649421692\n",
      "0.9543566297682262 0.575\n",
      "Step 1 | Loss: 1.1104559898376465\n",
      "Step 101 | Loss: 1.0605385303497314\n",
      "Step 201 | Loss: 0.8201032876968384\n",
      "Step 301 | Loss: 1.0965420007705688\n",
      "Step 401 | Loss: 0.8514653444290161\n",
      "Step 501 | Loss: 0.921532392501831\n",
      "Step 601 | Loss: 1.1275923252105713\n",
      "Step 701 | Loss: 0.9372157454490662\n",
      "Step 801 | Loss: 1.1484298706054688\n",
      "Step 901 | Loss: 1.1556731462478638\n",
      "Step 1001 | Loss: 0.8482534289360046\n",
      "Step 1101 | Loss: 1.116295337677002\n",
      "Step 1201 | Loss: 0.8598722815513611\n",
      "Step 1301 | Loss: 1.0035196542739868\n",
      "Step 1401 | Loss: 0.802728533744812\n",
      "Step 1501 | Loss: 0.9390900731086731\n",
      "Step 1601 | Loss: 1.0174332857131958\n",
      "Step 1701 | Loss: 0.8203350305557251\n",
      "Step 1801 | Loss: 1.031804084777832\n",
      "Step 1901 | Loss: 1.252020239830017\n",
      "Step 2001 | Loss: 0.8551735877990723\n",
      "Step 2101 | Loss: 0.9434545040130615\n",
      "Step 2201 | Loss: 1.2176965475082397\n",
      "Step 2301 | Loss: 0.9092169404029846\n",
      "Step 2401 | Loss: 1.0960009098052979\n",
      "0.9538279771892538 0.575\n",
      "Step 1 | Loss: 1.8280342817306519\n",
      "Step 101 | Loss: 1.0299947261810303\n",
      "Step 201 | Loss: 1.4455406665802002\n",
      "Step 301 | Loss: 1.0616117715835571\n",
      "Step 401 | Loss: 1.1510658264160156\n",
      "Step 501 | Loss: 1.3188018798828125\n",
      "Step 601 | Loss: 1.1903164386749268\n",
      "Step 701 | Loss: 1.1614009141921997\n",
      "Step 801 | Loss: 1.0009775161743164\n",
      "Step 901 | Loss: 1.0266156196594238\n",
      "Step 1001 | Loss: 1.3767681121826172\n",
      "Step 1101 | Loss: 1.0033830404281616\n",
      "Step 1201 | Loss: 0.8931140303611755\n",
      "Step 1301 | Loss: 1.1823571920394897\n",
      "Step 1401 | Loss: 1.2175220251083374\n",
      "Step 1501 | Loss: 0.7785290479660034\n",
      "Step 1601 | Loss: 0.9122731685638428\n",
      "Step 1701 | Loss: 1.3569689989089966\n",
      "Step 1801 | Loss: 0.9292995929718018\n",
      "Step 1901 | Loss: 0.9798563718795776\n",
      "Step 2001 | Loss: 1.0877553224563599\n",
      "Step 2101 | Loss: 1.0463573932647705\n",
      "Step 2201 | Loss: 0.8228574991226196\n",
      "Step 2301 | Loss: 1.0569164752960205\n",
      "Step 2401 | Loss: 1.2162213325500488\n",
      "0.9532932707849663 0.575\n",
      "Step 1 | Loss: 0.9882090091705322\n",
      "Step 101 | Loss: 1.018222689628601\n",
      "Step 201 | Loss: 0.8370690941810608\n",
      "Step 301 | Loss: 1.0503257513046265\n",
      "Step 401 | Loss: 1.1241950988769531\n",
      "Step 501 | Loss: 0.961647093296051\n",
      "Step 601 | Loss: 1.165777564048767\n",
      "Step 701 | Loss: 0.9268330931663513\n",
      "Step 801 | Loss: 1.1947166919708252\n",
      "Step 901 | Loss: 1.0461686849594116\n",
      "Step 1001 | Loss: 1.126408338546753\n",
      "Step 1101 | Loss: 1.1925575733184814\n",
      "Step 1201 | Loss: 0.8974725604057312\n",
      "Step 1301 | Loss: 0.9081311821937561\n",
      "Step 1401 | Loss: 0.7416065335273743\n",
      "Step 1501 | Loss: 0.919942319393158\n",
      "Step 1601 | Loss: 0.7939488887786865\n",
      "Step 1701 | Loss: 0.9957871437072754\n",
      "Step 1801 | Loss: 1.1450155973434448\n",
      "Step 1901 | Loss: 1.2666230201721191\n",
      "Step 2001 | Loss: 0.7898200750350952\n",
      "Step 2101 | Loss: 0.797538161277771\n",
      "Step 2201 | Loss: 1.0408176183700562\n",
      "Step 2301 | Loss: 0.9064762592315674\n",
      "Step 2401 | Loss: 1.1304311752319336\n",
      "0.9577861095995311 0.5825\n",
      "Step 1 | Loss: 1.1226284503936768\n",
      "Step 101 | Loss: 0.9968770146369934\n",
      "Step 201 | Loss: 1.0092400312423706\n",
      "Step 301 | Loss: 1.0011308193206787\n",
      "Step 401 | Loss: 1.0226688385009766\n",
      "Step 501 | Loss: 1.001577615737915\n",
      "Step 601 | Loss: 0.9779970645904541\n",
      "Step 701 | Loss: 1.0024702548980713\n",
      "Step 801 | Loss: 1.0108891725540161\n",
      "Step 901 | Loss: 1.0066617727279663\n",
      "Step 1001 | Loss: 0.994827389717102\n",
      "Step 1101 | Loss: 1.0006680488586426\n",
      "Step 1201 | Loss: 0.9877296686172485\n",
      "Step 1301 | Loss: 1.0013208389282227\n",
      "Step 1401 | Loss: 0.9935252070426941\n",
      "Step 1501 | Loss: 0.9945456385612488\n",
      "Step 1601 | Loss: 1.010392189025879\n",
      "Step 1701 | Loss: 0.9869976043701172\n",
      "Step 1801 | Loss: 1.0067375898361206\n",
      "Step 1901 | Loss: 0.9938656091690063\n",
      "Step 2001 | Loss: 0.9829750061035156\n",
      "Step 2101 | Loss: 1.0024670362472534\n",
      "Step 2201 | Loss: 1.0461623668670654\n",
      "Step 2301 | Loss: 0.9875341057777405\n",
      "Step 2401 | Loss: 0.9852703809738159\n",
      "0.99989509705316 0.515\n",
      "Step 1 | Loss: 0.9181753396987915\n",
      "Step 101 | Loss: 1.0097228288650513\n",
      "Step 201 | Loss: 1.003892421722412\n",
      "Step 301 | Loss: 0.9986258745193481\n",
      "Step 401 | Loss: 1.0143251419067383\n",
      "Step 501 | Loss: 1.0106117725372314\n",
      "Step 601 | Loss: 0.9890937805175781\n",
      "Step 701 | Loss: 0.9971802830696106\n",
      "Step 801 | Loss: 1.0218465328216553\n",
      "Step 901 | Loss: 1.0044044256210327\n",
      "Step 1001 | Loss: 1.0146123170852661\n",
      "Step 1101 | Loss: 0.9803995490074158\n",
      "Step 1201 | Loss: 1.0072543621063232\n",
      "Step 1301 | Loss: 0.9998243451118469\n",
      "Step 1401 | Loss: 0.9995526075363159\n",
      "Step 1501 | Loss: 1.026700735092163\n",
      "Step 1601 | Loss: 1.0280592441558838\n",
      "Step 1701 | Loss: 0.9960642457008362\n",
      "Step 1801 | Loss: 0.9796642065048218\n",
      "Step 1901 | Loss: 1.0003496408462524\n",
      "Step 2001 | Loss: 1.0006293058395386\n",
      "Step 2101 | Loss: 1.0135611295700073\n",
      "Step 2201 | Loss: 1.0154895782470703\n",
      "Step 2301 | Loss: 1.0325908660888672\n",
      "Step 2401 | Loss: 0.9981825351715088\n",
      "1.0008265635174658 0.485\n",
      "Step 1 | Loss: 1.124230980873108\n",
      "Step 101 | Loss: 0.9983526468276978\n",
      "Step 201 | Loss: 0.9894868731498718\n",
      "Step 301 | Loss: 1.0078208446502686\n",
      "Step 401 | Loss: 0.987769365310669\n",
      "Step 501 | Loss: 0.9916250705718994\n",
      "Step 601 | Loss: 1.0094311237335205\n",
      "Step 701 | Loss: 0.9990087747573853\n",
      "Step 801 | Loss: 1.0085948705673218\n",
      "Step 901 | Loss: 1.0006179809570312\n",
      "Step 1001 | Loss: 1.0100488662719727\n",
      "Step 1101 | Loss: 1.0007282495498657\n",
      "Step 1201 | Loss: 0.9997385144233704\n",
      "Step 1301 | Loss: 0.9998708963394165\n",
      "Step 1401 | Loss: 0.9921226501464844\n",
      "Step 1501 | Loss: 1.0081517696380615\n",
      "Step 1601 | Loss: 0.9811602234840393\n",
      "Step 1701 | Loss: 1.0078314542770386\n",
      "Step 1801 | Loss: 0.9974756836891174\n",
      "Step 1901 | Loss: 0.998720645904541\n",
      "Step 2001 | Loss: 1.008406162261963\n",
      "Step 2101 | Loss: 1.0024311542510986\n",
      "Step 2201 | Loss: 1.0163742303848267\n",
      "Step 2301 | Loss: 1.008591890335083\n",
      "Step 2401 | Loss: 1.0032122135162354\n",
      "1.0075714507773217 0.485\n",
      "Step 1 | Loss: 1.0506598949432373\n",
      "Step 101 | Loss: 0.9990311861038208\n",
      "Step 201 | Loss: 0.9977128505706787\n",
      "Step 301 | Loss: 0.9969485402107239\n",
      "Step 401 | Loss: 0.9978054761886597\n",
      "Step 501 | Loss: 0.9980260729789734\n",
      "Step 601 | Loss: 1.0096086263656616\n",
      "Step 701 | Loss: 0.9866642355918884\n",
      "Step 801 | Loss: 0.9680178761482239\n",
      "Step 901 | Loss: 1.0050663948059082\n",
      "Step 1001 | Loss: 0.9974455237388611\n",
      "Step 1101 | Loss: 1.0170321464538574\n",
      "Step 1201 | Loss: 1.0051476955413818\n",
      "Step 1301 | Loss: 0.9985411167144775\n",
      "Step 1401 | Loss: 1.0480525493621826\n",
      "Step 1501 | Loss: 1.0044480562210083\n",
      "Step 1601 | Loss: 0.9998393654823303\n",
      "Step 1701 | Loss: 1.0205729007720947\n",
      "Step 1801 | Loss: 1.0146235227584839\n",
      "Step 1901 | Loss: 0.9951045513153076\n",
      "Step 2001 | Loss: 0.9928876161575317\n",
      "Step 2101 | Loss: 0.9687231779098511\n",
      "Step 2201 | Loss: 0.996371865272522\n",
      "Step 2301 | Loss: 1.0263843536376953\n",
      "Step 2401 | Loss: 1.0125727653503418\n",
      "0.9994374275583547 0.515\n",
      "Step 1 | Loss: 1.40841805934906\n",
      "Step 101 | Loss: 1.0029082298278809\n",
      "Step 201 | Loss: 1.0105595588684082\n",
      "Step 301 | Loss: 1.0048913955688477\n",
      "Step 401 | Loss: 0.9905772805213928\n",
      "Step 501 | Loss: 1.0126007795333862\n",
      "Step 601 | Loss: 1.002477765083313\n",
      "Step 701 | Loss: 1.0271234512329102\n",
      "Step 801 | Loss: 1.013697624206543\n",
      "Step 901 | Loss: 0.984810471534729\n",
      "Step 1001 | Loss: 0.9957414269447327\n",
      "Step 1101 | Loss: 1.0218400955200195\n",
      "Step 1201 | Loss: 0.9962724447250366\n",
      "Step 1301 | Loss: 1.0020357370376587\n",
      "Step 1401 | Loss: 1.0016132593154907\n",
      "Step 1501 | Loss: 1.0011696815490723\n",
      "Step 1601 | Loss: 1.0011179447174072\n",
      "Step 1701 | Loss: 0.9852604866027832\n",
      "Step 1801 | Loss: 1.0095250606536865\n",
      "Step 1901 | Loss: 1.0095349550247192\n",
      "Step 2001 | Loss: 0.9842789173126221\n",
      "Step 2101 | Loss: 1.0017104148864746\n",
      "Step 2201 | Loss: 1.0039663314819336\n",
      "Step 2301 | Loss: 0.970491349697113\n",
      "Step 2401 | Loss: 1.0048984289169312\n",
      "1.0001942364270016 0.515\n",
      "Step 1 | Loss: 1.9999998807907104\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.375\n",
      "Step 501 | Loss: 1.5\n",
      "Step 601 | Loss: 2.375\n",
      "Step 701 | Loss: 1.5\n",
      "Step 801 | Loss: 2.5\n",
      "Step 901 | Loss: 2.375\n",
      "Step 1001 | Loss: 1.375\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 2.375\n",
      "Step 1301 | Loss: 2.125\n",
      "Step 1401 | Loss: 1.999999761581421\n",
      "Step 1501 | Loss: 1.75\n",
      "Step 1601 | Loss: 1.749999761581421\n",
      "Step 1701 | Loss: 2.124999761581421\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 1.4999998807907104\n",
      "Step 2001 | Loss: 2.375\n",
      "Step 2101 | Loss: 1.6249998807907104\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.124999761581421\n",
      "Step 2401 | Loss: 1.75\n",
      "2.0192306362665717 0.5\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 1.625\n",
      "Step 201 | Loss: 2.625\n",
      "Step 301 | Loss: 2.125\n",
      "Step 401 | Loss: 1.875\n",
      "Step 501 | Loss: 2.375\n",
      "Step 601 | Loss: 1.75\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 2.375\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 1.375\n",
      "Step 1101 | Loss: 2.0\n",
      "Step 1201 | Loss: 1.875\n",
      "Step 1301 | Loss: 1.375\n",
      "Step 1401 | Loss: 2.5\n",
      "Step 1501 | Loss: 1.875\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 1.25\n",
      "Step 2201 | Loss: 2.0\n",
      "Step 2301 | Loss: 1.75\n",
      "Step 2401 | Loss: 1.75\n",
      "2.0192307325509904 0.5\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.75\n",
      "Step 301 | Loss: 2.375\n",
      "Step 401 | Loss: 2.5\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 2.125\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 1.375\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 2.375\n",
      "Step 1401 | Loss: 1.75\n",
      "Step 1501 | Loss: 2.375\n",
      "Step 1601 | Loss: 1.875\n",
      "Step 1701 | Loss: 1.75\n",
      "Step 1801 | Loss: 1.875\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 1.875\n",
      "Step 2101 | Loss: 1.125\n",
      "Step 2201 | Loss: 1.75\n",
      "Step 2301 | Loss: 1.75\n",
      "Step 2401 | Loss: 1.75\n",
      "2.0192307634995554 0.5\n",
      "Step 1 | Loss: 1.875\n",
      "Step 101 | Loss: 2.0\n",
      "Step 201 | Loss: 2.125\n",
      "Step 301 | Loss: 2.25\n",
      "Step 401 | Loss: 2.375\n",
      "Step 501 | Loss: 2.25\n",
      "Step 601 | Loss: 1.9999998807907104\n",
      "Step 701 | Loss: 2.374999761581421\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 2.25\n",
      "Step 1001 | Loss: 1.9999998807907104\n",
      "Step 1101 | Loss: 2.75\n",
      "Step 1201 | Loss: 2.125\n",
      "Step 1301 | Loss: 2.5\n",
      "Step 1401 | Loss: 2.0\n",
      "Step 1501 | Loss: 1.7499998807907104\n",
      "Step 1601 | Loss: 2.0\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 1.4999998807907104\n",
      "Step 1901 | Loss: 2.249999761581421\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 2.5\n",
      "Step 2201 | Loss: 1.7499998807907104\n",
      "Step 2301 | Loss: 2.25\n",
      "Step 2401 | Loss: 3.0\n",
      "2.0192306534602174 0.5\n",
      "Step 1 | Loss: 2.0\n",
      "Step 101 | Loss: 0.625\n",
      "Step 201 | Loss: 2.624999761581421\n",
      "Step 301 | Loss: 1.125\n",
      "Step 401 | Loss: 1.875\n",
      "Step 501 | Loss: 2.125\n",
      "Step 601 | Loss: 2.25\n",
      "Step 701 | Loss: 2.125\n",
      "Step 801 | Loss: 1.75\n",
      "Step 901 | Loss: 2.375\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 2.375\n",
      "Step 1201 | Loss: 1.5\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 2.25\n",
      "Step 1601 | Loss: 1.9999998807907104\n",
      "Step 1701 | Loss: 2.25\n",
      "Step 1801 | Loss: 1.75\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 2.75\n",
      "Step 2201 | Loss: 1.875\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 2.125\n",
      "2.01923068097005 0.5\n",
      "Step 1 | Loss: 0.8193469643592834\n",
      "Step 101 | Loss: 0.710195779800415\n",
      "Step 201 | Loss: 0.5485584139823914\n",
      "Step 301 | Loss: 0.6125643253326416\n",
      "Step 401 | Loss: 0.48462343215942383\n",
      "Step 501 | Loss: 0.6982289552688599\n",
      "Step 601 | Loss: 0.6286507248878479\n",
      "Step 701 | Loss: 0.5233607888221741\n",
      "Step 801 | Loss: 0.6395009160041809\n",
      "Step 901 | Loss: 0.5665608644485474\n",
      "Step 1001 | Loss: 0.6269394159317017\n",
      "Step 1101 | Loss: 0.5075965523719788\n",
      "Step 1201 | Loss: 0.6002534031867981\n",
      "Step 1301 | Loss: 0.6065349578857422\n",
      "Step 1401 | Loss: 0.568594753742218\n",
      "Step 1501 | Loss: 0.6890507340431213\n",
      "Step 1601 | Loss: 0.5639940500259399\n",
      "Step 1701 | Loss: 0.7284746766090393\n",
      "Step 1801 | Loss: 0.6684809923171997\n",
      "Step 1901 | Loss: 0.5928287506103516\n",
      "Step 2001 | Loss: 0.5734187364578247\n",
      "Step 2101 | Loss: 0.5835649371147156\n",
      "Step 2201 | Loss: 0.6491442322731018\n",
      "Step 2301 | Loss: 0.5216599702835083\n",
      "Step 2401 | Loss: 0.508065938949585\n",
      "0.595088306931608 0.9025\n",
      "Step 1 | Loss: 0.9143523573875427\n",
      "Step 101 | Loss: 0.8528947830200195\n",
      "Step 201 | Loss: 0.7078467607498169\n",
      "Step 301 | Loss: 0.6579483151435852\n",
      "Step 401 | Loss: 0.6529238820075989\n",
      "Step 501 | Loss: 0.6001737713813782\n",
      "Step 601 | Loss: 0.5298961400985718\n",
      "Step 701 | Loss: 0.7236093282699585\n",
      "Step 801 | Loss: 0.6235746145248413\n",
      "Step 901 | Loss: 0.6297824382781982\n",
      "Step 1001 | Loss: 0.6898789405822754\n",
      "Step 1101 | Loss: 0.6165603995323181\n",
      "Step 1201 | Loss: 0.6462631225585938\n",
      "Step 1301 | Loss: 0.7267568111419678\n",
      "Step 1401 | Loss: 0.6370856761932373\n",
      "Step 1501 | Loss: 0.6255564093589783\n",
      "Step 1601 | Loss: 0.7849583625793457\n",
      "Step 1701 | Loss: 0.7247164249420166\n",
      "Step 1801 | Loss: 0.6080334186553955\n",
      "Step 1901 | Loss: 0.6423879265785217\n",
      "Step 2001 | Loss: 0.5966446995735168\n",
      "Step 2101 | Loss: 0.660507321357727\n",
      "Step 2201 | Loss: 0.6794960498809814\n",
      "Step 2301 | Loss: 0.727825403213501\n",
      "Step 2401 | Loss: 0.6102492809295654\n",
      "0.6799633456286571 0.8325\n",
      "Step 1 | Loss: 1.3018527030944824\n",
      "Step 101 | Loss: 0.7975283861160278\n",
      "Step 201 | Loss: 0.7395472526550293\n",
      "Step 301 | Loss: 0.668287992477417\n",
      "Step 401 | Loss: 0.7952533960342407\n",
      "Step 501 | Loss: 0.7895693182945251\n",
      "Step 601 | Loss: 0.709736704826355\n",
      "Step 701 | Loss: 0.7255992889404297\n",
      "Step 801 | Loss: 0.6065163612365723\n",
      "Step 901 | Loss: 0.660079836845398\n",
      "Step 1001 | Loss: 0.8266657590866089\n",
      "Step 1101 | Loss: 0.6688207387924194\n",
      "Step 1201 | Loss: 0.8064489364624023\n",
      "Step 1301 | Loss: 0.7382574081420898\n",
      "Step 1401 | Loss: 0.6884138584136963\n",
      "Step 1501 | Loss: 0.7364939451217651\n",
      "Step 1601 | Loss: 0.653088390827179\n",
      "Step 1701 | Loss: 0.7243055105209351\n",
      "Step 1801 | Loss: 0.6226201057434082\n",
      "Step 1901 | Loss: 0.7900611162185669\n",
      "Step 2001 | Loss: 0.6123996376991272\n",
      "Step 2101 | Loss: 0.6003583669662476\n",
      "Step 2201 | Loss: 0.6273795366287231\n",
      "Step 2301 | Loss: 0.7424694299697876\n",
      "Step 2401 | Loss: 0.6265245079994202\n",
      "0.6802335938273167 0.835\n",
      "Step 1 | Loss: 0.8925212621688843\n",
      "Step 101 | Loss: 0.6294201612472534\n",
      "Step 201 | Loss: 0.5825782418251038\n",
      "Step 301 | Loss: 0.5540353059768677\n",
      "Step 401 | Loss: 0.5877608060836792\n",
      "Step 501 | Loss: 0.725105345249176\n",
      "Step 601 | Loss: 0.5649586915969849\n",
      "Step 701 | Loss: 0.6270250082015991\n",
      "Step 801 | Loss: 0.5546247959136963\n",
      "Step 901 | Loss: 0.6261566877365112\n",
      "Step 1001 | Loss: 0.4964297413825989\n",
      "Step 1101 | Loss: 0.5329023003578186\n",
      "Step 1201 | Loss: 0.6798810362815857\n",
      "Step 1301 | Loss: 0.6041589975357056\n",
      "Step 1401 | Loss: 0.5962469577789307\n",
      "Step 1501 | Loss: 0.5958543419837952\n",
      "Step 1601 | Loss: 0.5129797458648682\n",
      "Step 1701 | Loss: 0.6048738956451416\n",
      "Step 1801 | Loss: 0.6392404437065125\n",
      "Step 1901 | Loss: 0.6280838251113892\n",
      "Step 2001 | Loss: 0.6196539402008057\n",
      "Step 2101 | Loss: 0.49740442633628845\n",
      "Step 2201 | Loss: 0.5960288643836975\n",
      "Step 2301 | Loss: 0.6516185998916626\n",
      "Step 2401 | Loss: 0.6050365567207336\n",
      "0.5926893694121934 0.8975\n",
      "Step 1 | Loss: 0.9423790574073792\n",
      "Step 101 | Loss: 0.6165093779563904\n",
      "Step 201 | Loss: 0.6470226049423218\n",
      "Step 301 | Loss: 0.5311831831932068\n",
      "Step 401 | Loss: 0.562950849533081\n",
      "Step 501 | Loss: 0.5579760074615479\n",
      "Step 601 | Loss: 0.6671335697174072\n",
      "Step 701 | Loss: 0.5509809851646423\n",
      "Step 801 | Loss: 0.5272383093833923\n",
      "Step 901 | Loss: 0.6113066673278809\n",
      "Step 1001 | Loss: 0.5087433457374573\n",
      "Step 1101 | Loss: 0.6262792348861694\n",
      "Step 1201 | Loss: 0.5644986629486084\n",
      "Step 1301 | Loss: 0.5063925385475159\n",
      "Step 1401 | Loss: 0.5597835183143616\n",
      "Step 1501 | Loss: 0.5223601460456848\n",
      "Step 1601 | Loss: 0.5744664669036865\n",
      "Step 1701 | Loss: 0.7626121044158936\n",
      "Step 1801 | Loss: 0.5984182953834534\n",
      "Step 1901 | Loss: 0.5159392952919006\n",
      "Step 2001 | Loss: 0.6485018730163574\n",
      "Step 2101 | Loss: 0.5577906370162964\n",
      "Step 2201 | Loss: 0.5480744242668152\n",
      "Step 2301 | Loss: 0.5791560411453247\n",
      "Step 2401 | Loss: 0.48435044288635254\n",
      "0.5879872041013964 0.9025\n",
      "Step 1 | Loss: 0.8596091270446777\n",
      "Step 101 | Loss: 0.6186975836753845\n",
      "Step 201 | Loss: 0.610643208026886\n",
      "Step 301 | Loss: 0.5743882060050964\n",
      "Step 401 | Loss: 0.7005321383476257\n",
      "Step 501 | Loss: 0.7285637855529785\n",
      "Step 601 | Loss: 0.5314918160438538\n",
      "Step 701 | Loss: 0.5219489336013794\n",
      "Step 801 | Loss: 0.5876966714859009\n",
      "Step 901 | Loss: 0.5333426594734192\n",
      "Step 1001 | Loss: 0.5901912450790405\n",
      "Step 1101 | Loss: 0.4486139416694641\n",
      "Step 1201 | Loss: 0.5468151569366455\n",
      "Step 1301 | Loss: 0.5012063384056091\n",
      "Step 1401 | Loss: 0.5028754472732544\n",
      "Step 1501 | Loss: 0.43042030930519104\n",
      "Step 1601 | Loss: 0.4585757851600647\n",
      "Step 1701 | Loss: 0.5315238833427429\n",
      "Step 1801 | Loss: 0.44614100456237793\n",
      "Step 1901 | Loss: 0.5697706341743469\n",
      "Step 2001 | Loss: 0.5369353294372559\n",
      "Step 2101 | Loss: 0.5690605640411377\n",
      "Step 2201 | Loss: 0.35046425461769104\n",
      "Step 2301 | Loss: 0.44714176654815674\n",
      "Step 2401 | Loss: 0.45393285155296326\n",
      "0.455812311205415 0.8775\n",
      "Step 1 | Loss: 1.2338087558746338\n",
      "Step 101 | Loss: 0.9192992448806763\n",
      "Step 201 | Loss: 0.6734362840652466\n",
      "Step 301 | Loss: 0.4628825783729553\n",
      "Step 401 | Loss: 0.5429901480674744\n",
      "Step 501 | Loss: 0.5611253976821899\n",
      "Step 601 | Loss: 0.36051005125045776\n",
      "Step 701 | Loss: 0.4895882308483124\n",
      "Step 801 | Loss: 0.3674793839454651\n",
      "Step 901 | Loss: 0.3888236880302429\n",
      "Step 1001 | Loss: 0.605236291885376\n",
      "Step 1101 | Loss: 0.5746492147445679\n",
      "Step 1201 | Loss: 0.318589448928833\n",
      "Step 1301 | Loss: 0.49072158336639404\n",
      "Step 1401 | Loss: 0.3827051520347595\n",
      "Step 1501 | Loss: 0.4202629327774048\n",
      "Step 1601 | Loss: 0.4252106845378876\n",
      "Step 1701 | Loss: 0.4090333878993988\n",
      "Step 1801 | Loss: 0.4415980279445648\n",
      "Step 1901 | Loss: 0.6326757669448853\n",
      "Step 2001 | Loss: 0.40157005190849304\n",
      "Step 2101 | Loss: 0.5528268814086914\n",
      "Step 2201 | Loss: 0.4756653904914856\n",
      "Step 2301 | Loss: 0.6285698413848877\n",
      "Step 2401 | Loss: 0.5428941249847412\n",
      "0.4514903695009837 0.8675\n",
      "Step 1 | Loss: 0.9981421232223511\n",
      "Step 101 | Loss: 0.5299022793769836\n",
      "Step 201 | Loss: 0.45434829592704773\n",
      "Step 301 | Loss: 0.5249423384666443\n",
      "Step 401 | Loss: 0.5645140409469604\n",
      "Step 501 | Loss: 0.6164523959159851\n",
      "Step 601 | Loss: 0.396480530500412\n",
      "Step 701 | Loss: 0.5706735253334045\n",
      "Step 801 | Loss: 0.5932230353355408\n",
      "Step 901 | Loss: 0.5589295029640198\n",
      "Step 1001 | Loss: 0.48615553975105286\n",
      "Step 1101 | Loss: 0.5915694832801819\n",
      "Step 1201 | Loss: 0.4195574223995209\n",
      "Step 1301 | Loss: 0.5286800861358643\n",
      "Step 1401 | Loss: 0.523569643497467\n",
      "Step 1501 | Loss: 0.39750590920448303\n",
      "Step 1601 | Loss: 0.4349168539047241\n",
      "Step 1701 | Loss: 0.36926865577697754\n",
      "Step 1801 | Loss: 0.6355064511299133\n",
      "Step 1901 | Loss: 0.3974497616291046\n",
      "Step 2001 | Loss: 0.40933552384376526\n",
      "Step 2101 | Loss: 0.3858318030834198\n",
      "Step 2201 | Loss: 0.4548467695713043\n",
      "Step 2301 | Loss: 0.44968485832214355\n",
      "Step 2401 | Loss: 0.4218366742134094\n",
      "0.45902789843433883 0.875\n",
      "Step 1 | Loss: 1.425720453262329\n",
      "Step 101 | Loss: 0.4716191291809082\n",
      "Step 201 | Loss: 0.45249873399734497\n",
      "Step 301 | Loss: 0.5224135518074036\n",
      "Step 401 | Loss: 0.436322420835495\n",
      "Step 501 | Loss: 0.5140972137451172\n",
      "Step 601 | Loss: 0.529426097869873\n",
      "Step 701 | Loss: 0.3941555321216583\n",
      "Step 801 | Loss: 0.467370867729187\n",
      "Step 901 | Loss: 0.5020705461502075\n",
      "Step 1001 | Loss: 0.4910200834274292\n",
      "Step 1101 | Loss: 0.5814952254295349\n",
      "Step 1201 | Loss: 0.6151583194732666\n",
      "Step 1301 | Loss: 0.3227815628051758\n",
      "Step 1401 | Loss: 0.5153898596763611\n",
      "Step 1501 | Loss: 0.42429232597351074\n",
      "Step 1601 | Loss: 0.3338436782360077\n",
      "Step 1701 | Loss: 0.3006046414375305\n",
      "Step 1801 | Loss: 0.38728639483451843\n",
      "Step 1901 | Loss: 0.4764270782470703\n",
      "Step 2001 | Loss: 0.21455587446689606\n",
      "Step 2101 | Loss: 0.5275184512138367\n",
      "Step 2201 | Loss: 0.4427835941314697\n",
      "Step 2301 | Loss: 0.35962429642677307\n",
      "Step 2401 | Loss: 0.5182957649230957\n",
      "0.44785216507709574 0.87\n",
      "Step 1 | Loss: 1.1144285202026367\n",
      "Step 101 | Loss: 0.7648228406906128\n",
      "Step 201 | Loss: 0.6743265986442566\n",
      "Step 301 | Loss: 0.6314271092414856\n",
      "Step 401 | Loss: 0.7861998081207275\n",
      "Step 501 | Loss: 0.5487178564071655\n",
      "Step 601 | Loss: 0.6694955229759216\n",
      "Step 701 | Loss: 0.5416488647460938\n",
      "Step 801 | Loss: 0.5701472163200378\n",
      "Step 901 | Loss: 0.552478015422821\n",
      "Step 1001 | Loss: 0.6790830492973328\n",
      "Step 1101 | Loss: 0.6844796538352966\n",
      "Step 1201 | Loss: 0.5335055589675903\n",
      "Step 1301 | Loss: 0.6096393465995789\n",
      "Step 1401 | Loss: 0.43491581082344055\n",
      "Step 1501 | Loss: 0.5894836187362671\n",
      "Step 1601 | Loss: 0.5293052196502686\n",
      "Step 1701 | Loss: 0.4755791425704956\n",
      "Step 1801 | Loss: 0.46748003363609314\n",
      "Step 1901 | Loss: 0.3899221420288086\n",
      "Step 2001 | Loss: 0.45282164216041565\n",
      "Step 2101 | Loss: 0.5837658047676086\n",
      "Step 2201 | Loss: 0.5056124925613403\n",
      "Step 2301 | Loss: 0.3177582621574402\n",
      "Step 2401 | Loss: 0.36608096957206726\n",
      "0.4710360427701819 0.85\n",
      "Step 1 | Loss: 1.0441176891326904\n",
      "Step 101 | Loss: 0.551135778427124\n",
      "Step 201 | Loss: 0.4023359715938568\n",
      "Step 301 | Loss: 0.45752179622650146\n",
      "Step 401 | Loss: 0.3637063801288605\n",
      "Step 501 | Loss: 0.5393298268318176\n",
      "Step 601 | Loss: 0.5783584713935852\n",
      "Step 701 | Loss: 0.6364738345146179\n",
      "Step 801 | Loss: 0.7005184888839722\n",
      "Step 901 | Loss: 0.5536555051803589\n",
      "Step 1001 | Loss: 0.646409273147583\n",
      "Step 1101 | Loss: 0.41236379742622375\n",
      "Step 1201 | Loss: 0.6127175092697144\n",
      "Step 1301 | Loss: 0.47202298045158386\n",
      "Step 1401 | Loss: 0.44255074858665466\n",
      "Step 1501 | Loss: 0.581687867641449\n",
      "Step 1601 | Loss: 0.4957171678543091\n",
      "Step 1701 | Loss: 0.5747658610343933\n",
      "Step 1801 | Loss: 0.5750536322593689\n",
      "Step 1901 | Loss: 0.5812596082687378\n",
      "Step 2001 | Loss: 0.32390788197517395\n",
      "Step 2101 | Loss: 0.5698692202568054\n",
      "Step 2201 | Loss: 0.42428213357925415\n",
      "Step 2301 | Loss: 0.579989492893219\n",
      "Step 2401 | Loss: 0.6199678182601929\n",
      "0.5588879971594077 0.7925\n",
      "Step 1 | Loss: 0.6073964834213257\n",
      "Step 101 | Loss: 0.6022943258285522\n",
      "Step 201 | Loss: 0.7352226972579956\n",
      "Step 301 | Loss: 0.5581362247467041\n",
      "Step 401 | Loss: 0.6071146726608276\n",
      "Step 501 | Loss: 0.44412627816200256\n",
      "Step 601 | Loss: 0.4908095598220825\n",
      "Step 701 | Loss: 0.591383695602417\n",
      "Step 801 | Loss: 0.3820621371269226\n",
      "Step 901 | Loss: 0.4226706922054291\n",
      "Step 1001 | Loss: 0.5943581461906433\n",
      "Step 1101 | Loss: 0.6035467982292175\n",
      "Step 1201 | Loss: 0.5176570415496826\n",
      "Step 1301 | Loss: 0.502620279788971\n",
      "Step 1401 | Loss: 0.6336962580680847\n",
      "Step 1501 | Loss: 0.442457914352417\n",
      "Step 1601 | Loss: 0.545832097530365\n",
      "Step 1701 | Loss: 0.4490622282028198\n",
      "Step 1801 | Loss: 0.6025699377059937\n",
      "Step 1901 | Loss: 0.4700561761856079\n",
      "Step 2001 | Loss: 0.6622288227081299\n",
      "Step 2101 | Loss: 0.3932625353336334\n",
      "Step 2201 | Loss: 0.48584866523742676\n",
      "Step 2301 | Loss: 0.4134177267551422\n",
      "Step 2401 | Loss: 0.5678991675376892\n",
      "0.5583908122588895 0.7875\n",
      "Step 1 | Loss: 2.231333017349243\n",
      "Step 101 | Loss: 0.6848487854003906\n",
      "Step 201 | Loss: 0.560424268245697\n",
      "Step 301 | Loss: 0.5543891787528992\n",
      "Step 401 | Loss: 0.5317690372467041\n",
      "Step 501 | Loss: 0.36384138464927673\n",
      "Step 601 | Loss: 0.530426561832428\n",
      "Step 701 | Loss: 0.6745861768722534\n",
      "Step 801 | Loss: 0.5987908840179443\n",
      "Step 901 | Loss: 0.5517675280570984\n",
      "Step 1001 | Loss: 0.4940324127674103\n",
      "Step 1101 | Loss: 0.6431382894515991\n",
      "Step 1201 | Loss: 0.44344013929367065\n",
      "Step 1301 | Loss: 0.39351874589920044\n",
      "Step 1401 | Loss: 0.6187483072280884\n",
      "Step 1501 | Loss: 0.530475914478302\n",
      "Step 1601 | Loss: 0.558588981628418\n",
      "Step 1701 | Loss: 0.49632319808006287\n",
      "Step 1801 | Loss: 0.6487956047058105\n",
      "Step 1901 | Loss: 0.579236626625061\n",
      "Step 2001 | Loss: 0.34614524245262146\n",
      "Step 2101 | Loss: 0.6514690518379211\n",
      "Step 2201 | Loss: 0.6480764150619507\n",
      "Step 2301 | Loss: 0.6352790594100952\n",
      "Step 2401 | Loss: 0.6770021319389343\n",
      "0.5590161302129687 0.7875\n",
      "Step 1 | Loss: 0.3633325397968292\n",
      "Step 101 | Loss: 0.6418026685714722\n",
      "Step 201 | Loss: 0.4986344575881958\n",
      "Step 301 | Loss: 0.8030152916908264\n",
      "Step 401 | Loss: 0.4223703444004059\n",
      "Step 501 | Loss: 0.6510382294654846\n",
      "Step 601 | Loss: 0.5361549258232117\n",
      "Step 701 | Loss: 0.6147512197494507\n",
      "Step 801 | Loss: 0.7881494164466858\n",
      "Step 901 | Loss: 0.5015657544136047\n",
      "Step 1001 | Loss: 0.5560725331306458\n",
      "Step 1101 | Loss: 0.7718898057937622\n",
      "Step 1201 | Loss: 0.6792473793029785\n",
      "Step 1301 | Loss: 0.6447680592536926\n",
      "Step 1401 | Loss: 0.6143395304679871\n",
      "Step 1501 | Loss: 0.499345600605011\n",
      "Step 1601 | Loss: 0.557392418384552\n",
      "Step 1701 | Loss: 0.7215204834938049\n",
      "Step 1801 | Loss: 0.46229520440101624\n",
      "Step 1901 | Loss: 0.5316909551620483\n",
      "Step 2001 | Loss: 0.6555414199829102\n",
      "Step 2101 | Loss: 0.6641837954521179\n",
      "Step 2201 | Loss: 0.545059323310852\n",
      "Step 2301 | Loss: 0.5618418455123901\n",
      "Step 2401 | Loss: 0.5270583629608154\n",
      "0.5585486156626517 0.7875\n",
      "Step 1 | Loss: 1.8729336261749268\n",
      "Step 101 | Loss: 0.40119028091430664\n",
      "Step 201 | Loss: 0.5847121477127075\n",
      "Step 301 | Loss: 0.42137518525123596\n",
      "Step 401 | Loss: 0.4209926128387451\n",
      "Step 501 | Loss: 0.541369616985321\n",
      "Step 601 | Loss: 0.6433058977127075\n",
      "Step 701 | Loss: 0.5658053755760193\n",
      "Step 801 | Loss: 0.6187625527381897\n",
      "Step 901 | Loss: 0.48057854175567627\n",
      "Step 1001 | Loss: 0.3943181037902832\n",
      "Step 1101 | Loss: 0.5312902331352234\n",
      "Step 1201 | Loss: 0.5621325969696045\n",
      "Step 1301 | Loss: 0.5230666995048523\n",
      "Step 1401 | Loss: 0.5494915246963501\n",
      "Step 1501 | Loss: 0.6375427842140198\n",
      "Step 1601 | Loss: 0.574190616607666\n",
      "Step 1701 | Loss: 0.6118629574775696\n",
      "Step 1801 | Loss: 0.44131818413734436\n",
      "Step 1901 | Loss: 0.5840820670127869\n",
      "Step 2001 | Loss: 0.5543331503868103\n",
      "Step 2101 | Loss: 0.38999122381210327\n",
      "Step 2201 | Loss: 0.5083603262901306\n",
      "Step 2301 | Loss: 0.9933438301086426\n",
      "Step 2401 | Loss: 0.6503033638000488\n",
      "0.5583912123150468 0.79\n",
      "Step 1 | Loss: 1.2544127702713013\n",
      "Step 101 | Loss: 1.1181097030639648\n",
      "Step 201 | Loss: 0.9419792294502258\n",
      "Step 301 | Loss: 0.7884722352027893\n",
      "Step 401 | Loss: 0.9432507157325745\n",
      "Step 501 | Loss: 1.0940237045288086\n",
      "Step 601 | Loss: 1.143567681312561\n",
      "Step 701 | Loss: 0.8624624609947205\n",
      "Step 801 | Loss: 1.0915447473526\n",
      "Step 901 | Loss: 1.0939244031906128\n",
      "Step 1001 | Loss: 1.1246483325958252\n",
      "Step 1101 | Loss: 0.8800807595252991\n",
      "Step 1201 | Loss: 1.2922412157058716\n",
      "Step 1301 | Loss: 1.1043064594268799\n",
      "Step 1401 | Loss: 1.3299167156219482\n",
      "Step 1501 | Loss: 1.1546494960784912\n",
      "Step 1601 | Loss: 0.8183709383010864\n",
      "Step 1701 | Loss: 0.9677129983901978\n",
      "Step 1801 | Loss: 1.1971654891967773\n",
      "Step 1901 | Loss: 1.003775954246521\n",
      "Step 2001 | Loss: 1.2106916904449463\n",
      "Step 2101 | Loss: 1.2314858436584473\n",
      "Step 2201 | Loss: 1.1409953832626343\n",
      "Step 2301 | Loss: 1.0870678424835205\n",
      "Step 2401 | Loss: 0.8828371167182922\n",
      "1.0849638687500642 0.52\n",
      "Step 1 | Loss: 1.0624593496322632\n",
      "Step 101 | Loss: 1.141461730003357\n",
      "Step 201 | Loss: 1.036171317100525\n",
      "Step 301 | Loss: 1.0563985109329224\n",
      "Step 401 | Loss: 0.9508732557296753\n",
      "Step 501 | Loss: 1.1663434505462646\n",
      "Step 601 | Loss: 1.1025714874267578\n",
      "Step 701 | Loss: 1.0517250299453735\n",
      "Step 801 | Loss: 1.0863302946090698\n",
      "Step 901 | Loss: 0.9341769218444824\n",
      "Step 1001 | Loss: 1.1767168045043945\n",
      "Step 1101 | Loss: 1.0166727304458618\n",
      "Step 1201 | Loss: 1.1864395141601562\n",
      "Step 1301 | Loss: 1.3274352550506592\n",
      "Step 1401 | Loss: 1.3018878698349\n",
      "Step 1501 | Loss: 0.47932547330856323\n",
      "Step 1601 | Loss: 0.9027272462844849\n",
      "Step 1701 | Loss: 0.9525235891342163\n",
      "Step 1801 | Loss: 0.8771187663078308\n",
      "Step 1901 | Loss: 1.3212006092071533\n",
      "Step 2001 | Loss: 1.2004892826080322\n",
      "Step 2101 | Loss: 1.2561523914337158\n",
      "Step 2201 | Loss: 0.9724990129470825\n",
      "Step 2301 | Loss: 0.8471355438232422\n",
      "Step 2401 | Loss: 1.2592713832855225\n",
      "1.0849638904694745 0.52\n",
      "Step 1 | Loss: 1.0292625427246094\n",
      "Step 101 | Loss: 1.3448446989059448\n",
      "Step 201 | Loss: 1.2694079875946045\n",
      "Step 301 | Loss: 0.9692334532737732\n",
      "Step 401 | Loss: 0.9879133701324463\n",
      "Step 501 | Loss: 0.8483383655548096\n",
      "Step 601 | Loss: 0.9858621954917908\n",
      "Step 701 | Loss: 1.1097416877746582\n",
      "Step 801 | Loss: 0.9870287179946899\n",
      "Step 901 | Loss: 1.3749539852142334\n",
      "Step 1001 | Loss: 1.3588314056396484\n",
      "Step 1101 | Loss: 0.9947251677513123\n",
      "Step 1201 | Loss: 1.1806581020355225\n",
      "Step 1301 | Loss: 1.2445018291473389\n",
      "Step 1401 | Loss: 0.9316830635070801\n",
      "Step 1501 | Loss: 1.0813910961151123\n",
      "Step 1601 | Loss: 0.9843432307243347\n",
      "Step 1701 | Loss: 1.110190749168396\n",
      "Step 1801 | Loss: 1.0100488662719727\n",
      "Step 1901 | Loss: 1.0846995115280151\n",
      "Step 2001 | Loss: 1.0295743942260742\n",
      "Step 2101 | Loss: 0.9735337495803833\n",
      "Step 2201 | Loss: 1.0838439464569092\n",
      "Step 2301 | Loss: 0.9861535429954529\n",
      "Step 2401 | Loss: 1.0783061981201172\n",
      "1.084963873573885 0.52\n",
      "Step 1 | Loss: 1.4986851215362549\n",
      "Step 101 | Loss: 1.1602908372879028\n",
      "Step 201 | Loss: 1.214295506477356\n",
      "Step 301 | Loss: 1.0787097215652466\n",
      "Step 401 | Loss: 1.3304327726364136\n",
      "Step 501 | Loss: 0.971840500831604\n",
      "Step 601 | Loss: 1.2751625776290894\n",
      "Step 701 | Loss: 1.0904719829559326\n",
      "Step 801 | Loss: 0.6844437122344971\n",
      "Step 901 | Loss: 1.0259008407592773\n",
      "Step 1001 | Loss: 1.1619117259979248\n",
      "Step 1101 | Loss: 1.195455551147461\n",
      "Step 1201 | Loss: 1.1251847743988037\n",
      "Step 1301 | Loss: 1.0088509321212769\n",
      "Step 1401 | Loss: 0.7457870244979858\n",
      "Step 1501 | Loss: 1.3226797580718994\n",
      "Step 1601 | Loss: 1.123342514038086\n",
      "Step 1701 | Loss: 1.038751244544983\n",
      "Step 1801 | Loss: 1.241623878479004\n",
      "Step 1901 | Loss: 0.988961935043335\n",
      "Step 2001 | Loss: 0.9298056364059448\n",
      "Step 2101 | Loss: 1.1210664510726929\n",
      "Step 2201 | Loss: 0.8751989603042603\n",
      "Step 2301 | Loss: 0.7761706709861755\n",
      "Step 2401 | Loss: 0.8306027054786682\n",
      "1.0849638894171496 0.52\n",
      "Step 1 | Loss: 1.0152462720870972\n",
      "Step 101 | Loss: 1.3931143283843994\n",
      "Step 201 | Loss: 1.1957855224609375\n",
      "Step 301 | Loss: 0.959269106388092\n",
      "Step 401 | Loss: 0.9569503664970398\n",
      "Step 501 | Loss: 1.0728509426116943\n",
      "Step 601 | Loss: 0.8407092690467834\n",
      "Step 701 | Loss: 0.9511054754257202\n",
      "Step 801 | Loss: 0.9974963665008545\n",
      "Step 901 | Loss: 1.09803307056427\n",
      "Step 1001 | Loss: 1.1878997087478638\n",
      "Step 1101 | Loss: 0.982534646987915\n",
      "Step 1201 | Loss: 1.1017581224441528\n",
      "Step 1301 | Loss: 1.4373666048049927\n",
      "Step 1401 | Loss: 1.0925873517990112\n",
      "Step 1501 | Loss: 1.1038345098495483\n",
      "Step 1601 | Loss: 1.4334304332733154\n",
      "Step 1701 | Loss: 1.0820283889770508\n",
      "Step 1801 | Loss: 0.9839557409286499\n",
      "Step 1901 | Loss: 1.232077717781067\n",
      "Step 2001 | Loss: 1.037339448928833\n",
      "Step 2101 | Loss: 1.2510147094726562\n",
      "Step 2201 | Loss: 0.9484625458717346\n",
      "Step 2301 | Loss: 1.214179515838623\n",
      "Step 2401 | Loss: 1.1557246446609497\n",
      "1.0849638998428224 0.52\n",
      "Step 1 | Loss: 2.1308462619781494\n",
      "Step 101 | Loss: 0.6085091233253479\n",
      "Step 201 | Loss: 0.6361888647079468\n",
      "Step 301 | Loss: 0.6859756708145142\n",
      "Step 401 | Loss: 0.5035452246665955\n",
      "Step 501 | Loss: 0.6483516693115234\n",
      "Step 601 | Loss: 0.46455687284469604\n",
      "Step 701 | Loss: 0.5497559309005737\n",
      "Step 801 | Loss: 0.42016372084617615\n",
      "Step 901 | Loss: 0.48514628410339355\n",
      "Step 1001 | Loss: 0.6110557913780212\n",
      "Step 1101 | Loss: 0.5719629526138306\n",
      "Step 1201 | Loss: 0.4646029472351074\n",
      "Step 1301 | Loss: 0.6540653109550476\n",
      "Step 1401 | Loss: 0.31952744722366333\n",
      "Step 1501 | Loss: 0.5680022239685059\n",
      "Step 1601 | Loss: 0.5882498025894165\n",
      "Step 1701 | Loss: 0.5918709635734558\n",
      "Step 1801 | Loss: 0.4729697108268738\n",
      "Step 1901 | Loss: 0.5602893829345703\n",
      "Step 2001 | Loss: 0.42226535081863403\n",
      "Step 2101 | Loss: 0.4930201470851898\n",
      "Step 2201 | Loss: 0.4562608599662781\n",
      "Step 2301 | Loss: 0.660111129283905\n",
      "Step 2401 | Loss: 0.6421549916267395\n",
      "0.556820709429041 0.7875\n",
      "Step 1 | Loss: 1.455116629600525\n",
      "Step 101 | Loss: 0.5617729425430298\n",
      "Step 201 | Loss: 0.5708993673324585\n",
      "Step 301 | Loss: 0.5395059585571289\n",
      "Step 401 | Loss: 0.4444727301597595\n",
      "Step 501 | Loss: 0.7200911641120911\n",
      "Step 601 | Loss: 0.6179596781730652\n",
      "Step 701 | Loss: 0.42305266857147217\n",
      "Step 801 | Loss: 0.4363609552383423\n",
      "Step 901 | Loss: 0.5515406131744385\n",
      "Step 1001 | Loss: 0.6105706095695496\n",
      "Step 1101 | Loss: 0.500134289264679\n",
      "Step 1201 | Loss: 0.5626582503318787\n",
      "Step 1301 | Loss: 0.6643911004066467\n",
      "Step 1401 | Loss: 0.4454241096973419\n",
      "Step 1501 | Loss: 0.5395275354385376\n",
      "Step 1601 | Loss: 0.6486683487892151\n",
      "Step 1701 | Loss: 0.4994434416294098\n",
      "Step 1801 | Loss: 0.5650843381881714\n",
      "Step 1901 | Loss: 0.4825765788555145\n",
      "Step 2001 | Loss: 0.41950613260269165\n",
      "Step 2101 | Loss: 0.5466145873069763\n",
      "Step 2201 | Loss: 0.5084818005561829\n",
      "Step 2301 | Loss: 0.4404018223285675\n",
      "Step 2401 | Loss: 0.5749244689941406\n",
      "0.5572688012307927 0.785\n",
      "Step 1 | Loss: 1.452269196510315\n",
      "Step 101 | Loss: 0.6508622765541077\n",
      "Step 201 | Loss: 0.48927679657936096\n",
      "Step 301 | Loss: 0.5357831716537476\n",
      "Step 401 | Loss: 0.5033733248710632\n",
      "Step 501 | Loss: 0.5196567177772522\n",
      "Step 601 | Loss: 0.7598068118095398\n",
      "Step 701 | Loss: 0.521081268787384\n",
      "Step 801 | Loss: 0.4299659729003906\n",
      "Step 901 | Loss: 0.3988076448440552\n",
      "Step 1001 | Loss: 0.4861220717430115\n",
      "Step 1101 | Loss: 0.4032835066318512\n",
      "Step 1201 | Loss: 0.5257614850997925\n",
      "Step 1301 | Loss: 0.7084658741950989\n",
      "Step 1401 | Loss: 0.6940193176269531\n",
      "Step 1501 | Loss: 0.6063016057014465\n",
      "Step 1601 | Loss: 0.4771043062210083\n",
      "Step 1701 | Loss: 0.4621846675872803\n",
      "Step 1801 | Loss: 0.5285319089889526\n",
      "Step 1901 | Loss: 0.549126148223877\n",
      "Step 2001 | Loss: 0.585769534111023\n",
      "Step 2101 | Loss: 0.5392804741859436\n",
      "Step 2201 | Loss: 0.4893631637096405\n",
      "Step 2301 | Loss: 0.5053536295890808\n",
      "Step 2401 | Loss: 0.6093353629112244\n",
      "0.5569547081844551 0.7825\n",
      "Step 1 | Loss: 0.7723378539085388\n",
      "Step 101 | Loss: 0.6905186772346497\n",
      "Step 201 | Loss: 0.5101292729377747\n",
      "Step 301 | Loss: 0.5532048940658569\n",
      "Step 401 | Loss: 0.6271494626998901\n",
      "Step 501 | Loss: 0.4981655180454254\n",
      "Step 601 | Loss: 0.6571069955825806\n",
      "Step 701 | Loss: 0.5660520195960999\n",
      "Step 801 | Loss: 0.558891773223877\n",
      "Step 901 | Loss: 0.682643711566925\n",
      "Step 1001 | Loss: 0.49971190094947815\n",
      "Step 1101 | Loss: 0.595503032207489\n",
      "Step 1201 | Loss: 0.4521958529949188\n",
      "Step 1301 | Loss: 0.5748255252838135\n",
      "Step 1401 | Loss: 0.39750295877456665\n",
      "Step 1501 | Loss: 0.5681315064430237\n",
      "Step 1601 | Loss: 0.5065372586250305\n",
      "Step 1701 | Loss: 0.428650826215744\n",
      "Step 1801 | Loss: 0.42322030663490295\n",
      "Step 1901 | Loss: 0.4518948793411255\n",
      "Step 2001 | Loss: 0.5532210469245911\n",
      "Step 2101 | Loss: 0.5509964823722839\n",
      "Step 2201 | Loss: 0.6640908122062683\n",
      "Step 2301 | Loss: 0.5124576687812805\n",
      "Step 2401 | Loss: 0.6206225752830505\n",
      "0.5577126390218082 0.785\n",
      "Step 1 | Loss: 0.9350731372833252\n",
      "Step 101 | Loss: 0.6513489484786987\n",
      "Step 201 | Loss: 0.38812056183815\n",
      "Step 301 | Loss: 0.5082941055297852\n",
      "Step 401 | Loss: 0.7694255113601685\n",
      "Step 501 | Loss: 0.45552679896354675\n",
      "Step 601 | Loss: 0.40024900436401367\n",
      "Step 701 | Loss: 0.5285846590995789\n",
      "Step 801 | Loss: 0.3328855633735657\n",
      "Step 901 | Loss: 0.4822525382041931\n",
      "Step 1001 | Loss: 0.6208773851394653\n",
      "Step 1101 | Loss: 0.6502343416213989\n",
      "Step 1201 | Loss: 0.5750051140785217\n",
      "Step 1301 | Loss: 0.7417799234390259\n",
      "Step 1401 | Loss: 0.4777606725692749\n",
      "Step 1501 | Loss: 0.4902253746986389\n",
      "Step 1601 | Loss: 0.4807085692882538\n",
      "Step 1701 | Loss: 0.4710271954536438\n",
      "Step 1801 | Loss: 0.3967874050140381\n",
      "Step 1901 | Loss: 0.5972363352775574\n",
      "Step 2001 | Loss: 0.5813443064689636\n",
      "Step 2101 | Loss: 0.48208263516426086\n",
      "Step 2201 | Loss: 0.4566686153411865\n",
      "Step 2301 | Loss: 0.44289907813072205\n",
      "Step 2401 | Loss: 0.7277916669845581\n",
      "0.5572361126142625 0.785\n",
      "Step 1 | Loss: 1.0317741632461548\n",
      "Step 101 | Loss: 0.7459226846694946\n",
      "Step 201 | Loss: 0.7671394348144531\n",
      "Step 301 | Loss: 0.5324386358261108\n",
      "Step 401 | Loss: 0.7425209879875183\n",
      "Step 501 | Loss: 0.6484487056732178\n",
      "Step 601 | Loss: 0.4556235671043396\n",
      "Step 701 | Loss: 0.6175991296768188\n",
      "Step 801 | Loss: 0.5053281784057617\n",
      "Step 901 | Loss: 0.37438690662384033\n",
      "Step 1001 | Loss: 0.7149762511253357\n",
      "Step 1101 | Loss: 0.5167863368988037\n",
      "Step 1201 | Loss: 0.550790011882782\n",
      "Step 1301 | Loss: 0.5813649296760559\n",
      "Step 1401 | Loss: 0.5308883190155029\n",
      "Step 1501 | Loss: 0.6593842506408691\n",
      "Step 1601 | Loss: 0.5896973013877869\n",
      "Step 1701 | Loss: 0.6211522817611694\n",
      "Step 1801 | Loss: 0.3446713089942932\n",
      "Step 1901 | Loss: 0.6158791780471802\n",
      "Step 2001 | Loss: 0.5891568660736084\n",
      "Step 2101 | Loss: 0.5279040336608887\n",
      "Step 2201 | Loss: 0.544887900352478\n",
      "Step 2301 | Loss: 0.7675014138221741\n",
      "Step 2401 | Loss: 0.7434147000312805\n",
      "0.5830456691722371 0.7725\n",
      "Step 1 | Loss: 0.8947513699531555\n",
      "Step 101 | Loss: 0.6630988121032715\n",
      "Step 201 | Loss: 0.5874584317207336\n",
      "Step 301 | Loss: 0.600407600402832\n",
      "Step 401 | Loss: 0.4913388192653656\n",
      "Step 501 | Loss: 0.6409919261932373\n",
      "Step 601 | Loss: 0.5760766863822937\n",
      "Step 701 | Loss: 0.6286441087722778\n",
      "Step 801 | Loss: 0.5837348103523254\n",
      "Step 901 | Loss: 0.2839306890964508\n",
      "Step 1001 | Loss: 0.6757979393005371\n",
      "Step 1101 | Loss: 0.8920871019363403\n",
      "Step 1201 | Loss: 0.7003222107887268\n",
      "Step 1301 | Loss: 0.5188576579093933\n",
      "Step 1401 | Loss: 0.7285104990005493\n",
      "Step 1501 | Loss: 0.5606279969215393\n",
      "Step 1601 | Loss: 0.6472092270851135\n",
      "Step 1701 | Loss: 0.5994604825973511\n",
      "Step 1801 | Loss: 0.8100094795227051\n",
      "Step 1901 | Loss: 0.7667288184165955\n",
      "Step 2001 | Loss: 0.7432631254196167\n",
      "Step 2101 | Loss: 0.6007593870162964\n",
      "Step 2201 | Loss: 0.6045465469360352\n",
      "Step 2301 | Loss: 0.4167158305644989\n",
      "Step 2401 | Loss: 0.7597418427467346\n",
      "0.5793258905037943 0.78\n",
      "Step 1 | Loss: 1.3370131254196167\n",
      "Step 101 | Loss: 0.731019139289856\n",
      "Step 201 | Loss: 0.5187256336212158\n",
      "Step 301 | Loss: 0.6916399002075195\n",
      "Step 401 | Loss: 0.4998692274093628\n",
      "Step 501 | Loss: 0.7715957164764404\n",
      "Step 601 | Loss: 0.38371819257736206\n",
      "Step 701 | Loss: 0.7536838054656982\n",
      "Step 801 | Loss: 0.5532933473587036\n",
      "Step 901 | Loss: 0.62080979347229\n",
      "Step 1001 | Loss: 0.7051602005958557\n",
      "Step 1101 | Loss: 0.5997635126113892\n",
      "Step 1201 | Loss: 0.6393929719924927\n",
      "Step 1301 | Loss: 0.7094692587852478\n",
      "Step 1401 | Loss: 0.6513593196868896\n",
      "Step 1501 | Loss: 0.5030266046524048\n",
      "Step 1601 | Loss: 0.49258625507354736\n",
      "Step 1701 | Loss: 0.5294427871704102\n",
      "Step 1801 | Loss: 0.7979050278663635\n",
      "Step 1901 | Loss: 0.5240340232849121\n",
      "Step 2001 | Loss: 0.619295597076416\n",
      "Step 2101 | Loss: 0.8531198501586914\n",
      "Step 2201 | Loss: 0.7482897043228149\n",
      "Step 2301 | Loss: 0.4402233958244324\n",
      "Step 2401 | Loss: 0.518019437789917\n",
      "0.5797277256786045 0.7825\n",
      "Step 1 | Loss: 0.9949740171432495\n",
      "Step 101 | Loss: 0.940466582775116\n",
      "Step 201 | Loss: 0.8184545040130615\n",
      "Step 301 | Loss: 0.8973631262779236\n",
      "Step 401 | Loss: 1.0976848602294922\n",
      "Step 501 | Loss: 0.8381415009498596\n",
      "Step 601 | Loss: 0.8442566990852356\n",
      "Step 701 | Loss: 0.9445992112159729\n",
      "Step 801 | Loss: 1.0572926998138428\n",
      "Step 901 | Loss: 0.9059537649154663\n",
      "Step 1001 | Loss: 0.9148649573326111\n",
      "Step 1101 | Loss: 0.8988028764724731\n",
      "Step 1201 | Loss: 0.9052842855453491\n",
      "Step 1301 | Loss: 0.8716455101966858\n",
      "Step 1401 | Loss: 0.7865312695503235\n",
      "Step 1501 | Loss: 0.9413591623306274\n",
      "Step 1601 | Loss: 0.8772724866867065\n",
      "Step 1701 | Loss: 0.9228097200393677\n",
      "Step 1801 | Loss: 0.8996021747589111\n",
      "Step 1901 | Loss: 0.8346942663192749\n",
      "Step 2001 | Loss: 0.9202900528907776\n",
      "Step 2101 | Loss: 0.9962279200553894\n",
      "Step 2201 | Loss: 0.901286244392395\n",
      "Step 2301 | Loss: 0.8242616653442383\n",
      "Step 2401 | Loss: 1.0287749767303467\n",
      "0.9209416149879537 0.62\n",
      "Step 1 | Loss: 0.967799961566925\n",
      "Step 101 | Loss: 0.9831488728523254\n",
      "Step 201 | Loss: 0.6169439554214478\n",
      "Step 301 | Loss: 0.4605658948421478\n",
      "Step 401 | Loss: 0.5952451825141907\n",
      "Step 501 | Loss: 0.8046813607215881\n",
      "Step 601 | Loss: 0.6098004579544067\n",
      "Step 701 | Loss: 0.46793925762176514\n",
      "Step 801 | Loss: 0.7345669865608215\n",
      "Step 901 | Loss: 0.6520441770553589\n",
      "Step 1001 | Loss: 0.7596950531005859\n",
      "Step 1101 | Loss: 0.47508448362350464\n",
      "Step 1201 | Loss: 0.4134746193885803\n",
      "Step 1301 | Loss: 0.4646911323070526\n",
      "Step 1401 | Loss: 0.5888820290565491\n",
      "Step 1501 | Loss: 0.49783360958099365\n",
      "Step 1601 | Loss: 0.5175312757492065\n",
      "Step 1701 | Loss: 0.5165480971336365\n",
      "Step 1801 | Loss: 0.5742584466934204\n",
      "Step 1901 | Loss: 0.7038547992706299\n",
      "Step 2001 | Loss: 0.549407958984375\n",
      "Step 2101 | Loss: 0.5682950019836426\n",
      "Step 2201 | Loss: 0.5985744595527649\n",
      "Step 2301 | Loss: 0.8269475698471069\n",
      "Step 2401 | Loss: 0.5374569296836853\n",
      "0.579260791862475 0.785\n",
      "Step 1 | Loss: 2.202915668487549\n",
      "Step 101 | Loss: 1.7368613481521606\n",
      "Step 201 | Loss: 1.2966991662979126\n",
      "Step 301 | Loss: 0.9777920246124268\n",
      "Step 401 | Loss: 0.7816879153251648\n",
      "Step 501 | Loss: 0.6307035088539124\n",
      "Step 601 | Loss: 0.5354790687561035\n",
      "Step 701 | Loss: 0.5716389417648315\n",
      "Step 801 | Loss: 0.6447357535362244\n",
      "Step 901 | Loss: 0.7180453538894653\n",
      "Step 1001 | Loss: 0.5662199258804321\n",
      "Step 1101 | Loss: 0.5984547734260559\n",
      "Step 1201 | Loss: 0.49526268243789673\n",
      "Step 1301 | Loss: 0.5767596960067749\n",
      "Step 1401 | Loss: 0.5216050148010254\n",
      "Step 1501 | Loss: 0.5684103965759277\n",
      "Step 1601 | Loss: 0.49135392904281616\n",
      "Step 1701 | Loss: 0.5636387467384338\n",
      "Step 1801 | Loss: 0.5307859182357788\n",
      "Step 1901 | Loss: 0.39815089106559753\n",
      "Step 2001 | Loss: 0.5696046948432922\n",
      "Step 2101 | Loss: 0.5120814442634583\n",
      "Step 2201 | Loss: 0.5520517230033875\n",
      "Step 2301 | Loss: 0.593443751335144\n",
      "Step 2401 | Loss: 0.49508750438690186\n",
      "0.5836050072005354 0.8325\n",
      "Step 1 | Loss: 1.4958174228668213\n",
      "Step 101 | Loss: 1.0576391220092773\n",
      "Step 201 | Loss: 0.9383973479270935\n",
      "Step 301 | Loss: 0.6153029203414917\n",
      "Step 401 | Loss: 0.5516512393951416\n",
      "Step 501 | Loss: 0.5927187204360962\n",
      "Step 601 | Loss: 0.5503396391868591\n",
      "Step 701 | Loss: 0.5661213397979736\n",
      "Step 801 | Loss: 0.5563640594482422\n",
      "Step 901 | Loss: 0.5538061857223511\n",
      "Step 1001 | Loss: 0.5588461756706238\n",
      "Step 1101 | Loss: 0.6165667176246643\n",
      "Step 1201 | Loss: 0.5056045651435852\n",
      "Step 1301 | Loss: 0.5267044305801392\n",
      "Step 1401 | Loss: 0.6367213129997253\n",
      "Step 1501 | Loss: 0.5520821213722229\n",
      "Step 1601 | Loss: 0.5172721147537231\n",
      "Step 1701 | Loss: 0.5006105899810791\n",
      "Step 1801 | Loss: 0.5543234348297119\n",
      "Step 1901 | Loss: 0.4818870723247528\n",
      "Step 2001 | Loss: 0.506671667098999\n",
      "Step 2101 | Loss: 0.542353093624115\n",
      "Step 2201 | Loss: 0.5351808667182922\n",
      "Step 2301 | Loss: 0.5135648250579834\n",
      "Step 2401 | Loss: 0.5989410281181335\n",
      "0.5958153492600862 0.84\n",
      "Step 1 | Loss: 1.7503044605255127\n",
      "Step 101 | Loss: 0.5909745097160339\n",
      "Step 201 | Loss: 0.4339345097541809\n",
      "Step 301 | Loss: 0.5369787216186523\n",
      "Step 401 | Loss: 0.5684292316436768\n",
      "Step 501 | Loss: 0.5460902452468872\n",
      "Step 601 | Loss: 0.4980464279651642\n",
      "Step 701 | Loss: 0.5710426568984985\n",
      "Step 801 | Loss: 0.6517738103866577\n",
      "Step 901 | Loss: 0.5601783990859985\n",
      "Step 1001 | Loss: 0.6337155103683472\n",
      "Step 1101 | Loss: 0.5729674100875854\n",
      "Step 1201 | Loss: 0.45686033368110657\n",
      "Step 1301 | Loss: 0.561018705368042\n",
      "Step 1401 | Loss: 0.4076067805290222\n",
      "Step 1501 | Loss: 0.5270069241523743\n",
      "Step 1601 | Loss: 0.5894412398338318\n",
      "Step 1701 | Loss: 0.5933867692947388\n",
      "Step 1801 | Loss: 0.6271169185638428\n",
      "Step 1901 | Loss: 0.5432161092758179\n",
      "Step 2001 | Loss: 0.4876202940940857\n",
      "Step 2101 | Loss: 0.5419980883598328\n",
      "Step 2201 | Loss: 0.5368465185165405\n",
      "Step 2301 | Loss: 0.5214487910270691\n",
      "Step 2401 | Loss: 0.5631953477859497\n",
      "0.5806893195904054 0.86\n",
      "Step 1 | Loss: 1.8667038679122925\n",
      "Step 101 | Loss: 0.947609543800354\n",
      "Step 201 | Loss: 0.6312029957771301\n",
      "Step 301 | Loss: 0.6639755964279175\n",
      "Step 401 | Loss: 0.48974931240081787\n",
      "Step 501 | Loss: 0.5037556886672974\n",
      "Step 601 | Loss: 0.620848536491394\n",
      "Step 701 | Loss: 0.6036949157714844\n",
      "Step 801 | Loss: 0.5695468783378601\n",
      "Step 901 | Loss: 0.5593805313110352\n",
      "Step 1001 | Loss: 0.5411642789840698\n",
      "Step 1101 | Loss: 0.6348441243171692\n",
      "Step 1201 | Loss: 0.5202981233596802\n",
      "Step 1301 | Loss: 0.4685966968536377\n",
      "Step 1401 | Loss: 0.5866186022758484\n",
      "Step 1501 | Loss: 0.6335397958755493\n",
      "Step 1601 | Loss: 0.5229139924049377\n",
      "Step 1701 | Loss: 0.45714494585990906\n",
      "Step 1801 | Loss: 0.5499497056007385\n",
      "Step 1901 | Loss: 0.578278660774231\n",
      "Step 2001 | Loss: 0.5066688656806946\n",
      "Step 2101 | Loss: 0.5762933492660522\n",
      "Step 2201 | Loss: 0.5004119873046875\n",
      "Step 2301 | Loss: 0.5713611245155334\n",
      "Step 2401 | Loss: 0.48599445819854736\n",
      "0.5823330798848153 0.845\n",
      "Step 1 | Loss: 1.5451854467391968\n",
      "Step 101 | Loss: 0.7389893531799316\n",
      "Step 201 | Loss: 0.613027036190033\n",
      "Step 301 | Loss: 0.6152526140213013\n",
      "Step 401 | Loss: 0.5284348726272583\n",
      "Step 501 | Loss: 0.485615074634552\n",
      "Step 601 | Loss: 0.4958783984184265\n",
      "Step 701 | Loss: 0.5278170108795166\n",
      "Step 801 | Loss: 0.5650362968444824\n",
      "Step 901 | Loss: 0.5486432313919067\n",
      "Step 1001 | Loss: 0.43241459131240845\n",
      "Step 1101 | Loss: 0.5493919849395752\n",
      "Step 1201 | Loss: 0.625880777835846\n",
      "Step 1301 | Loss: 0.6089387536048889\n",
      "Step 1401 | Loss: 0.4359742999076843\n",
      "Step 1501 | Loss: 0.4581640660762787\n",
      "Step 1601 | Loss: 0.4938206076622009\n",
      "Step 1701 | Loss: 0.5544624924659729\n",
      "Step 1801 | Loss: 0.4759824872016907\n",
      "Step 1901 | Loss: 0.5792548060417175\n",
      "Step 2001 | Loss: 0.5413334965705872\n",
      "Step 2101 | Loss: 0.5247143507003784\n",
      "Step 2201 | Loss: 0.5089944005012512\n",
      "Step 2301 | Loss: 0.5882208347320557\n",
      "Step 2401 | Loss: 0.5590270161628723\n",
      "0.5803293192248576 0.8525\n",
      "Step 1 | Loss: 2.625\n",
      "Step 101 | Loss: 2.375\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 1.875\n",
      "Step 701 | Loss: 2.0\n",
      "Step 801 | Loss: 2.125\n",
      "Step 901 | Loss: 1.25\n",
      "Step 1001 | Loss: 1.75\n",
      "Step 1101 | Loss: 2.125\n",
      "Step 1201 | Loss: 2.875\n",
      "Step 1301 | Loss: 1.5\n",
      "Step 1401 | Loss: 1.625\n",
      "Step 1501 | Loss: 2.125\n",
      "Step 1601 | Loss: 1.875\n",
      "Step 1701 | Loss: 2.0\n",
      "Step 1801 | Loss: 2.0\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.25\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 2.0\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 2.0\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 1.75\n",
      "Step 101 | Loss: 1.625\n",
      "Step 201 | Loss: 2.25\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 1.75\n",
      "Step 501 | Loss: 2.0\n",
      "Step 601 | Loss: 2.25\n",
      "Step 701 | Loss: 1.75\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 2.375\n",
      "Step 1001 | Loss: 2.125\n",
      "Step 1101 | Loss: 2.25\n",
      "Step 1201 | Loss: 1.625\n",
      "Step 1301 | Loss: 1.875\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 1.5\n",
      "Step 1601 | Loss: 2.25\n",
      "Step 1701 | Loss: 1.375\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 2.125\n",
      "Step 2001 | Loss: 2.375\n",
      "Step 2101 | Loss: 2.75\n",
      "Step 2201 | Loss: 1.75\n",
      "Step 2301 | Loss: 2.375\n",
      "Step 2401 | Loss: 2.5\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.25\n",
      "Step 101 | Loss: 2.125\n",
      "Step 201 | Loss: 1.875\n",
      "Step 301 | Loss: 1.75\n",
      "Step 401 | Loss: 1.625\n",
      "Step 501 | Loss: 2.0\n",
      "Step 601 | Loss: 2.375\n",
      "Step 701 | Loss: 1.5\n",
      "Step 801 | Loss: 2.25\n",
      "Step 901 | Loss: 2.0\n",
      "Step 1001 | Loss: 2.5\n",
      "Step 1101 | Loss: 2.125\n",
      "Step 1201 | Loss: 2.0\n",
      "Step 1301 | Loss: 2.625\n",
      "Step 1401 | Loss: 2.125\n",
      "Step 1501 | Loss: 2.5\n",
      "Step 1601 | Loss: 2.375\n",
      "Step 1701 | Loss: 1.625\n",
      "Step 1801 | Loss: 1.625\n",
      "Step 1901 | Loss: 2.25\n",
      "Step 2001 | Loss: 2.125\n",
      "Step 2101 | Loss: 1.25\n",
      "Step 2201 | Loss: 1.375\n",
      "Step 2301 | Loss: 2.125\n",
      "Step 2401 | Loss: 2.5\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 2.375\n",
      "Step 201 | Loss: 1.75\n",
      "Step 301 | Loss: 2.0\n",
      "Step 401 | Loss: 2.5\n",
      "Step 501 | Loss: 1.875\n",
      "Step 601 | Loss: 2.375\n",
      "Step 701 | Loss: 2.875\n",
      "Step 801 | Loss: 2.125\n",
      "Step 901 | Loss: 1.25\n",
      "Step 1001 | Loss: 1.5\n",
      "Step 1101 | Loss: 1.75\n",
      "Step 1201 | Loss: 1.25\n",
      "Step 1301 | Loss: 2.125\n",
      "Step 1401 | Loss: 1.75\n",
      "Step 1501 | Loss: 2.375\n",
      "Step 1601 | Loss: 1.5\n",
      "Step 1701 | Loss: 2.125\n",
      "Step 1801 | Loss: 2.5\n",
      "Step 1901 | Loss: 2.0\n",
      "Step 2001 | Loss: 1.75\n",
      "Step 2101 | Loss: 1.875\n",
      "Step 2201 | Loss: 2.625\n",
      "Step 2301 | Loss: 2.375\n",
      "Step 2401 | Loss: 2.375\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 2.125\n",
      "Step 101 | Loss: 1.875\n",
      "Step 201 | Loss: 2.5\n",
      "Step 301 | Loss: 2.125\n",
      "Step 401 | Loss: 2.0\n",
      "Step 501 | Loss: 1.625\n",
      "Step 601 | Loss: 2.5\n",
      "Step 701 | Loss: 2.5\n",
      "Step 801 | Loss: 2.625\n",
      "Step 901 | Loss: 2.5\n",
      "Step 1001 | Loss: 1.5\n",
      "Step 1101 | Loss: 1.5\n",
      "Step 1201 | Loss: 1.75\n",
      "Step 1301 | Loss: 2.125\n",
      "Step 1401 | Loss: 1.875\n",
      "Step 1501 | Loss: 2.0\n",
      "Step 1601 | Loss: 2.125\n",
      "Step 1701 | Loss: 1.875\n",
      "Step 1801 | Loss: 2.125\n",
      "Step 1901 | Loss: 2.375\n",
      "Step 2001 | Loss: 2.375\n",
      "Step 2101 | Loss: 2.25\n",
      "Step 2201 | Loss: 2.375\n",
      "Step 2301 | Loss: 2.0\n",
      "Step 2401 | Loss: 2.5\n",
      "2.019230769230769 0.5\n",
      "Step 1 | Loss: 1.2733111381530762\n",
      "Step 101 | Loss: 0.8646669983863831\n",
      "Step 201 | Loss: 0.5308095812797546\n",
      "Step 301 | Loss: 0.44655001163482666\n",
      "Step 401 | Loss: 0.35333603620529175\n",
      "Step 501 | Loss: 0.5135695934295654\n",
      "Step 601 | Loss: 0.4377322494983673\n",
      "Step 701 | Loss: 0.5412092208862305\n",
      "Step 801 | Loss: 0.4512796401977539\n",
      "Step 901 | Loss: 0.3996330201625824\n",
      "Step 1001 | Loss: 0.385282963514328\n",
      "Step 1101 | Loss: 0.5732110738754272\n",
      "Step 1201 | Loss: 0.35522279143333435\n",
      "Step 1301 | Loss: 0.27780404686927795\n",
      "Step 1401 | Loss: 0.4278556704521179\n",
      "Step 1501 | Loss: 0.4131239652633667\n",
      "Step 1601 | Loss: 0.36660444736480713\n",
      "Step 1701 | Loss: 0.3951627016067505\n",
      "Step 1801 | Loss: 0.40642088651657104\n",
      "Step 1901 | Loss: 0.3690488636493683\n",
      "Step 2001 | Loss: 0.34537220001220703\n",
      "Step 2101 | Loss: 0.3040669560432434\n",
      "Step 2201 | Loss: 0.36154741048812866\n",
      "Step 2301 | Loss: 0.4112963378429413\n",
      "Step 2401 | Loss: 0.41386809945106506\n",
      "0.35465042462413365 0.9075\n",
      "Step 1 | Loss: 1.2022548913955688\n",
      "Step 101 | Loss: 0.6860252618789673\n",
      "Step 201 | Loss: 0.4307103157043457\n",
      "Step 301 | Loss: 0.2478809654712677\n",
      "Step 401 | Loss: 0.30402928590774536\n",
      "Step 501 | Loss: 0.5031342506408691\n",
      "Step 601 | Loss: 0.6556618213653564\n",
      "Step 701 | Loss: 0.6012159585952759\n",
      "Step 801 | Loss: 0.2888208031654358\n",
      "Step 901 | Loss: 0.3341520428657532\n",
      "Step 1001 | Loss: 0.4234236180782318\n",
      "Step 1101 | Loss: 0.3602166771888733\n",
      "Step 1201 | Loss: 0.45777708292007446\n",
      "Step 1301 | Loss: 0.3463541269302368\n",
      "Step 1401 | Loss: 0.4941602945327759\n",
      "Step 1501 | Loss: 0.5364724397659302\n",
      "Step 1601 | Loss: 0.38751381635665894\n",
      "Step 1701 | Loss: 0.37608760595321655\n",
      "Step 1801 | Loss: 0.37061166763305664\n",
      "Step 1901 | Loss: 0.510132908821106\n",
      "Step 2001 | Loss: 0.44539037346839905\n",
      "Step 2101 | Loss: 0.4464060068130493\n",
      "Step 2201 | Loss: 0.38044804334640503\n",
      "Step 2301 | Loss: 0.42589232325553894\n",
      "Step 2401 | Loss: 0.506152868270874\n",
      "0.3546132688009568 0.9075\n",
      "Step 1 | Loss: 1.1339054107666016\n",
      "Step 101 | Loss: 1.0868390798568726\n",
      "Step 201 | Loss: 0.8258947134017944\n",
      "Step 301 | Loss: 0.7828517556190491\n",
      "Step 401 | Loss: 0.7373696565628052\n",
      "Step 501 | Loss: 0.789806067943573\n",
      "Step 601 | Loss: 0.648836612701416\n",
      "Step 701 | Loss: 0.6380271911621094\n",
      "Step 801 | Loss: 0.6625782251358032\n",
      "Step 901 | Loss: 0.7188138961791992\n",
      "Step 1001 | Loss: 0.6016577482223511\n",
      "Step 1101 | Loss: 0.5381165146827698\n",
      "Step 1201 | Loss: 0.44146645069122314\n",
      "Step 1301 | Loss: 0.43052545189857483\n",
      "Step 1401 | Loss: 0.35098400712013245\n",
      "Step 1501 | Loss: 0.4268213212490082\n",
      "Step 1601 | Loss: 0.3908034861087799\n",
      "Step 1701 | Loss: 0.4676515758037567\n",
      "Step 1801 | Loss: 0.47348034381866455\n",
      "Step 1901 | Loss: 0.46727436780929565\n",
      "Step 2001 | Loss: 0.3646015226840973\n",
      "Step 2101 | Loss: 0.4113540053367615\n",
      "Step 2201 | Loss: 0.36191684007644653\n",
      "Step 2301 | Loss: 0.33892107009887695\n",
      "Step 2401 | Loss: 0.4397294819355011\n",
      "0.4069588129241068 0.895\n",
      "Step 1 | Loss: 1.1568405628204346\n",
      "Step 101 | Loss: 0.9894994497299194\n",
      "Step 201 | Loss: 0.7602211236953735\n",
      "Step 301 | Loss: 0.6023510098457336\n",
      "Step 401 | Loss: 0.404533714056015\n",
      "Step 501 | Loss: 0.6279163360595703\n",
      "Step 601 | Loss: 0.36016300320625305\n",
      "Step 701 | Loss: 0.44430673122406006\n",
      "Step 801 | Loss: 0.6654894948005676\n",
      "Step 901 | Loss: 0.4021797478199005\n",
      "Step 1001 | Loss: 0.32377326488494873\n",
      "Step 1101 | Loss: 0.37897273898124695\n",
      "Step 1201 | Loss: 0.37354496121406555\n",
      "Step 1301 | Loss: 0.42095494270324707\n",
      "Step 1401 | Loss: 0.36441776156425476\n",
      "Step 1501 | Loss: 0.3189442753791809\n",
      "Step 1601 | Loss: 0.3135858476161957\n",
      "Step 1701 | Loss: 0.5175763368606567\n",
      "Step 1801 | Loss: 0.25919225811958313\n",
      "Step 1901 | Loss: 0.33619943261146545\n",
      "Step 2001 | Loss: 0.5212974548339844\n",
      "Step 2101 | Loss: 0.41632407903671265\n",
      "Step 2201 | Loss: 0.2713867127895355\n",
      "Step 2301 | Loss: 0.3982401490211487\n",
      "Step 2401 | Loss: 0.4076708257198334\n",
      "0.35483146827942796 0.9075\n",
      "Step 1 | Loss: 1.5488402843475342\n",
      "Step 101 | Loss: 0.627770721912384\n",
      "Step 201 | Loss: 0.5243469476699829\n",
      "Step 301 | Loss: 0.6723222136497498\n",
      "Step 401 | Loss: 0.4886568784713745\n",
      "Step 501 | Loss: 0.2596920132637024\n",
      "Step 601 | Loss: 0.7558916211128235\n",
      "Step 701 | Loss: 0.5677266716957092\n",
      "Step 801 | Loss: 0.5098549127578735\n",
      "Step 901 | Loss: 0.48106253147125244\n",
      "Step 1001 | Loss: 0.648105263710022\n",
      "Step 1101 | Loss: 0.36653316020965576\n",
      "Step 1201 | Loss: 0.5700163245201111\n",
      "Step 1301 | Loss: 0.44694286584854126\n",
      "Step 1401 | Loss: 0.6223115921020508\n",
      "Step 1501 | Loss: 0.6681324243545532\n",
      "Step 1601 | Loss: 0.4425066113471985\n",
      "Step 1701 | Loss: 0.48001670837402344\n",
      "Step 1801 | Loss: 0.37700238823890686\n",
      "Step 1901 | Loss: 0.4170597791671753\n",
      "Step 2001 | Loss: 0.7066282033920288\n",
      "Step 2101 | Loss: 0.40253105759620667\n",
      "Step 2201 | Loss: 0.37787923216819763\n",
      "Step 2301 | Loss: 0.4720509648323059\n",
      "Step 2401 | Loss: 0.3682594895362854\n",
      "0.4348680901771858 0.8875\n",
      "Step 1 | Loss: 0.7567610740661621\n",
      "Step 101 | Loss: 0.8840147256851196\n",
      "Step 201 | Loss: 0.7805504202842712\n",
      "Step 301 | Loss: 0.7884621620178223\n",
      "Step 401 | Loss: 0.721545398235321\n",
      "Step 501 | Loss: 0.6434438228607178\n",
      "Step 601 | Loss: 0.7974308133125305\n",
      "Step 701 | Loss: 0.7556048631668091\n",
      "Step 801 | Loss: 0.6918643116950989\n",
      "Step 901 | Loss: 0.7841002345085144\n",
      "Step 1001 | Loss: 0.8043381571769714\n",
      "Step 1101 | Loss: 0.7197996973991394\n",
      "Step 1201 | Loss: 0.8249704241752625\n",
      "Step 1301 | Loss: 0.9034423232078552\n",
      "Step 1401 | Loss: 0.7486518025398254\n",
      "Step 1501 | Loss: 0.6535869836807251\n",
      "Step 1601 | Loss: 0.7354193329811096\n",
      "Step 1701 | Loss: 0.7851127982139587\n",
      "Step 1801 | Loss: 0.6619429588317871\n",
      "Step 1901 | Loss: 0.9745010733604431\n",
      "Step 2001 | Loss: 0.6524857878684998\n",
      "Step 2101 | Loss: 0.7243316173553467\n",
      "Step 2201 | Loss: 0.8165526986122131\n",
      "Step 2301 | Loss: 0.6285714507102966\n",
      "Step 2401 | Loss: 0.6414118409156799\n",
      "0.728985960433373 0.69\n",
      "Step 1 | Loss: 1.4574095010757446\n",
      "Step 101 | Loss: 0.9114372134208679\n",
      "Step 201 | Loss: 0.7778596878051758\n",
      "Step 301 | Loss: 0.8584516644477844\n",
      "Step 401 | Loss: 0.8265517354011536\n",
      "Step 501 | Loss: 0.7784703969955444\n",
      "Step 601 | Loss: 0.8865665793418884\n",
      "Step 701 | Loss: 0.6131207942962646\n",
      "Step 801 | Loss: 0.6237610578536987\n",
      "Step 901 | Loss: 0.667086124420166\n",
      "Step 1001 | Loss: 0.8116609454154968\n",
      "Step 1101 | Loss: 0.7393354177474976\n",
      "Step 1201 | Loss: 0.9278836250305176\n",
      "Step 1301 | Loss: 0.7891212105751038\n",
      "Step 1401 | Loss: 0.6027795672416687\n",
      "Step 1501 | Loss: 0.5834372639656067\n",
      "Step 1601 | Loss: 0.894249677658081\n",
      "Step 1701 | Loss: 0.9600942134857178\n",
      "Step 1801 | Loss: 0.9129736423492432\n",
      "Step 1901 | Loss: 0.6992783546447754\n",
      "Step 2001 | Loss: 0.7046715021133423\n",
      "Step 2101 | Loss: 0.771447479724884\n",
      "Step 2201 | Loss: 0.7929733395576477\n",
      "Step 2301 | Loss: 0.7235398888587952\n",
      "Step 2401 | Loss: 0.5867191553115845\n",
      "0.7304388158237045 0.685\n",
      "Step 1 | Loss: 1.3749115467071533\n",
      "Step 101 | Loss: 0.9820781946182251\n",
      "Step 201 | Loss: 0.7841969132423401\n",
      "Step 301 | Loss: 1.037013053894043\n",
      "Step 401 | Loss: 0.7599167227745056\n",
      "Step 501 | Loss: 0.6869689226150513\n",
      "Step 601 | Loss: 0.7264803647994995\n",
      "Step 701 | Loss: 0.7441564798355103\n",
      "Step 801 | Loss: 0.9240894317626953\n",
      "Step 901 | Loss: 0.8257487416267395\n",
      "Step 1001 | Loss: 0.806143581867218\n",
      "Step 1101 | Loss: 0.7620499134063721\n",
      "Step 1201 | Loss: 0.8002779483795166\n",
      "Step 1301 | Loss: 0.6010297536849976\n",
      "Step 1401 | Loss: 0.7584941387176514\n",
      "Step 1501 | Loss: 0.824802577495575\n",
      "Step 1601 | Loss: 0.7977831959724426\n",
      "Step 1701 | Loss: 0.7090246677398682\n",
      "Step 1801 | Loss: 0.7045808434486389\n",
      "Step 1901 | Loss: 0.6173811554908752\n",
      "Step 2001 | Loss: 0.6042704582214355\n",
      "Step 2101 | Loss: 0.6960493922233582\n",
      "Step 2201 | Loss: 0.8079364895820618\n",
      "Step 2301 | Loss: 0.8761048913002014\n",
      "Step 2401 | Loss: 0.7825888395309448\n",
      "0.7266993406229045 0.69\n",
      "Step 1 | Loss: 1.0979715585708618\n",
      "Step 101 | Loss: 1.1638526916503906\n",
      "Step 201 | Loss: 0.858150839805603\n",
      "Step 301 | Loss: 1.101104736328125\n",
      "Step 401 | Loss: 0.7224985361099243\n",
      "Step 501 | Loss: 0.9223260879516602\n",
      "Step 601 | Loss: 0.8268848061561584\n",
      "Step 701 | Loss: 0.8958603143692017\n",
      "Step 801 | Loss: 0.7375161051750183\n",
      "Step 901 | Loss: 0.767169713973999\n",
      "Step 1001 | Loss: 0.831332266330719\n",
      "Step 1101 | Loss: 0.7927128076553345\n",
      "Step 1201 | Loss: 0.9387385249137878\n",
      "Step 1301 | Loss: 0.673211932182312\n",
      "Step 1401 | Loss: 0.9718986749649048\n",
      "Step 1501 | Loss: 0.6902697086334229\n",
      "Step 1601 | Loss: 0.7602089047431946\n",
      "Step 1701 | Loss: 0.6811227798461914\n",
      "Step 1801 | Loss: 0.7613378763198853\n",
      "Step 1901 | Loss: 0.7320399284362793\n",
      "Step 2001 | Loss: 0.9322066307067871\n",
      "Step 2101 | Loss: 0.6959624290466309\n",
      "Step 2201 | Loss: 0.800538182258606\n",
      "Step 2301 | Loss: 0.8204405307769775\n",
      "Step 2401 | Loss: 0.9129742383956909\n",
      "0.7295091615891323 0.685\n",
      "Step 1 | Loss: 0.9257640838623047\n",
      "Step 101 | Loss: 0.7188903093338013\n",
      "Step 201 | Loss: 0.7892042398452759\n",
      "Step 301 | Loss: 0.8406253457069397\n",
      "Step 401 | Loss: 0.6993957161903381\n",
      "Step 501 | Loss: 0.7135276794433594\n",
      "Step 601 | Loss: 0.7513909339904785\n",
      "Step 701 | Loss: 1.0942895412445068\n",
      "Step 801 | Loss: 0.711110532283783\n",
      "Step 901 | Loss: 0.8211318254470825\n",
      "Step 1001 | Loss: 0.8362425565719604\n",
      "Step 1101 | Loss: 0.7762418985366821\n",
      "Step 1201 | Loss: 1.0201921463012695\n",
      "Step 1301 | Loss: 0.587572455406189\n",
      "Step 1401 | Loss: 0.9326160550117493\n",
      "Step 1501 | Loss: 0.7658548355102539\n",
      "Step 1601 | Loss: 0.9644919037818909\n",
      "Step 1701 | Loss: 0.5696092844009399\n",
      "Step 1801 | Loss: 0.9647234678268433\n",
      "Step 1901 | Loss: 0.7754160165786743\n",
      "Step 2001 | Loss: 1.0551869869232178\n",
      "Step 2101 | Loss: 0.8755878806114197\n",
      "Step 2201 | Loss: 0.7652595043182373\n",
      "Step 2301 | Loss: 1.104910135269165\n",
      "Step 2401 | Loss: 0.8529649972915649\n",
      "0.8702824196288853 0.6175\n",
      "Step 1 | Loss: 0.9870967864990234\n",
      "Step 101 | Loss: 0.8622142672538757\n",
      "Step 201 | Loss: 0.8278787732124329\n",
      "Step 301 | Loss: 0.5917342901229858\n",
      "Step 401 | Loss: 0.8197797536849976\n",
      "Step 501 | Loss: 0.7194998860359192\n",
      "Step 601 | Loss: 0.7437465786933899\n",
      "Step 701 | Loss: 0.8593702912330627\n",
      "Step 801 | Loss: 0.8046503663063049\n",
      "Step 901 | Loss: 0.5322247743606567\n",
      "Step 1001 | Loss: 0.8072269558906555\n",
      "Step 1101 | Loss: 0.6626649498939514\n",
      "Step 1201 | Loss: 0.7722457647323608\n",
      "Step 1301 | Loss: 0.7614672780036926\n",
      "Step 1401 | Loss: 0.6819722652435303\n",
      "Step 1501 | Loss: 0.6458234786987305\n",
      "Step 1601 | Loss: 0.722378134727478\n",
      "Step 1701 | Loss: 0.7038026452064514\n",
      "Step 1801 | Loss: 0.8062134981155396\n",
      "Step 1901 | Loss: 0.6644777059555054\n",
      "Step 2001 | Loss: 0.8742640018463135\n",
      "Step 2101 | Loss: 0.8125332593917847\n",
      "Step 2201 | Loss: 0.7939667701721191\n",
      "Step 2301 | Loss: 0.7604538202285767\n",
      "Step 2401 | Loss: 0.7419159412384033\n",
      "0.721776856308553 0.7425\n",
      "Step 1 | Loss: 0.9763501286506653\n",
      "Step 101 | Loss: 1.0529985427856445\n",
      "Step 201 | Loss: 0.9160751104354858\n",
      "Step 301 | Loss: 1.0154995918273926\n",
      "Step 401 | Loss: 0.776542067527771\n",
      "Step 501 | Loss: 0.6870616674423218\n",
      "Step 601 | Loss: 0.5819169878959656\n",
      "Step 701 | Loss: 0.6695098280906677\n",
      "Step 801 | Loss: 0.7327494621276855\n",
      "Step 901 | Loss: 0.6052413582801819\n",
      "Step 1001 | Loss: 0.7516263127326965\n",
      "Step 1101 | Loss: 0.6453248262405396\n",
      "Step 1201 | Loss: 0.6513694524765015\n",
      "Step 1301 | Loss: 0.5927134156227112\n",
      "Step 1401 | Loss: 0.7320963740348816\n",
      "Step 1501 | Loss: 0.857948899269104\n",
      "Step 1601 | Loss: 0.778877854347229\n",
      "Step 1701 | Loss: 0.9608613848686218\n",
      "Step 1801 | Loss: 0.7542954087257385\n",
      "Step 1901 | Loss: 0.8548588752746582\n",
      "Step 2001 | Loss: 0.7030524015426636\n",
      "Step 2101 | Loss: 0.8067282438278198\n",
      "Step 2201 | Loss: 0.7267913818359375\n",
      "Step 2301 | Loss: 0.7551476955413818\n",
      "Step 2401 | Loss: 0.6685547828674316\n",
      "0.7371222412433429 0.745\n",
      "Step 1 | Loss: 0.9747954607009888\n",
      "Step 101 | Loss: 0.5363462567329407\n",
      "Step 201 | Loss: 0.5569787621498108\n",
      "Step 301 | Loss: 0.6433417201042175\n",
      "Step 401 | Loss: 0.7872847318649292\n",
      "Step 501 | Loss: 0.8040368556976318\n",
      "Step 601 | Loss: 0.5650045275688171\n",
      "Step 701 | Loss: 0.6496042609214783\n",
      "Step 801 | Loss: 0.7797750234603882\n",
      "Step 901 | Loss: 0.6436874270439148\n",
      "Step 1001 | Loss: 0.6786090731620789\n",
      "Step 1101 | Loss: 0.7617353200912476\n",
      "Step 1201 | Loss: 0.5885553956031799\n",
      "Step 1301 | Loss: 0.7835839986801147\n",
      "Step 1401 | Loss: 0.5598942041397095\n",
      "Step 1501 | Loss: 0.5068231225013733\n",
      "Step 1601 | Loss: 0.6723958849906921\n",
      "Step 1701 | Loss: 0.8097652196884155\n",
      "Step 1801 | Loss: 0.5298671126365662\n",
      "Step 1901 | Loss: 0.6977651715278625\n",
      "Step 2001 | Loss: 0.45612889528274536\n",
      "Step 2101 | Loss: 0.7260632514953613\n",
      "Step 2201 | Loss: 0.5252181887626648\n",
      "Step 2301 | Loss: 0.5958663821220398\n",
      "Step 2401 | Loss: 0.603151798248291\n",
      "0.5859945972493489 0.805\n",
      "Step 1 | Loss: 0.74553382396698\n",
      "Step 101 | Loss: 0.6382330060005188\n",
      "Step 201 | Loss: 0.6495307683944702\n",
      "Step 301 | Loss: 0.6491913795471191\n",
      "Step 401 | Loss: 0.759407639503479\n",
      "Step 501 | Loss: 0.813254177570343\n",
      "Step 601 | Loss: 0.8565798401832581\n",
      "Step 701 | Loss: 0.5509583353996277\n",
      "Step 801 | Loss: 0.8769636154174805\n",
      "Step 901 | Loss: 0.608629584312439\n",
      "Step 1001 | Loss: 0.6647000312805176\n",
      "Step 1101 | Loss: 0.5037807822227478\n",
      "Step 1201 | Loss: 0.46809276938438416\n",
      "Step 1301 | Loss: 0.6747121810913086\n",
      "Step 1401 | Loss: 0.49330055713653564\n",
      "Step 1501 | Loss: 0.6082906126976013\n",
      "Step 1601 | Loss: 0.5948560833930969\n",
      "Step 1701 | Loss: 0.49912869930267334\n",
      "Step 1801 | Loss: 0.4911207854747772\n",
      "Step 1901 | Loss: 0.6782293915748596\n",
      "Step 2001 | Loss: 0.664084255695343\n",
      "Step 2101 | Loss: 0.5340768098831177\n",
      "Step 2201 | Loss: 0.4763697683811188\n",
      "Step 2301 | Loss: 0.5515990257263184\n",
      "Step 2401 | Loss: 0.44354313611984253\n",
      "0.5755473739514894 0.8125\n",
      "Step 1 | Loss: 0.9832292795181274\n",
      "Step 101 | Loss: 0.9473783373832703\n",
      "Step 201 | Loss: 0.9121659994125366\n",
      "Step 301 | Loss: 0.7616313695907593\n",
      "Step 401 | Loss: 0.7966195344924927\n",
      "Step 501 | Loss: 0.7160481214523315\n",
      "Step 601 | Loss: 0.905062198638916\n",
      "Step 701 | Loss: 0.7024112343788147\n",
      "Step 801 | Loss: 0.6573044657707214\n",
      "Step 901 | Loss: 0.8011705279350281\n",
      "Step 1001 | Loss: 0.7143709659576416\n",
      "Step 1101 | Loss: 0.7961078882217407\n",
      "Step 1201 | Loss: 0.8128104209899902\n",
      "Step 1301 | Loss: 0.7903597354888916\n",
      "Step 1401 | Loss: 0.6449572443962097\n",
      "Step 1501 | Loss: 0.7172071933746338\n",
      "Step 1601 | Loss: 0.80106520652771\n",
      "Step 1701 | Loss: 0.8561877012252808\n",
      "Step 1801 | Loss: 0.6344903111457825\n",
      "Step 1901 | Loss: 0.750077486038208\n",
      "Step 2001 | Loss: 0.481891393661499\n",
      "Step 2101 | Loss: 0.6088491082191467\n",
      "Step 2201 | Loss: 0.7735040187835693\n",
      "Step 2301 | Loss: 0.7232614159584045\n",
      "Step 2401 | Loss: 0.6368837952613831\n",
      "0.7201839959592925 0.735\n",
      "Step 1 | Loss: 1.887061595916748\n",
      "Step 101 | Loss: 0.6399495601654053\n",
      "Step 201 | Loss: 0.5327540040016174\n",
      "Step 301 | Loss: 0.49721458554267883\n",
      "Step 401 | Loss: 0.38560834527015686\n",
      "Step 501 | Loss: 0.47956007719039917\n",
      "Step 601 | Loss: 0.4863540828227997\n",
      "Step 701 | Loss: 0.27240315079689026\n",
      "Step 801 | Loss: 0.5492066144943237\n",
      "Step 901 | Loss: 0.4174917936325073\n",
      "Step 1001 | Loss: 0.32591527700424194\n",
      "Step 1101 | Loss: 0.24195119738578796\n",
      "Step 1201 | Loss: 0.42915078997612\n",
      "Step 1301 | Loss: 0.34145694971084595\n",
      "Step 1401 | Loss: 0.45178163051605225\n",
      "Step 1501 | Loss: 0.31420937180519104\n",
      "Step 1601 | Loss: 0.3941422998905182\n",
      "Step 1701 | Loss: 0.4104381501674652\n",
      "Step 1801 | Loss: 0.3580000102519989\n",
      "Step 1901 | Loss: 0.5199580788612366\n",
      "Step 2001 | Loss: 0.48919010162353516\n",
      "Step 2101 | Loss: 0.4485887289047241\n",
      "Step 2201 | Loss: 0.39353007078170776\n",
      "Step 2301 | Loss: 0.5457177758216858\n",
      "Step 2401 | Loss: 0.3663167953491211\n",
      "0.4581263217219747 0.8825\n",
      "Step 1 | Loss: 1.0336755514144897\n",
      "Step 101 | Loss: 0.7494585514068604\n",
      "Step 201 | Loss: 0.7130661606788635\n",
      "Step 301 | Loss: 0.4860232174396515\n",
      "Step 401 | Loss: 0.4742969870567322\n",
      "Step 501 | Loss: 0.35774245858192444\n",
      "Step 601 | Loss: 0.39894577860832214\n",
      "Step 701 | Loss: 0.4154537320137024\n",
      "Step 801 | Loss: 0.42495059967041016\n",
      "Step 901 | Loss: 0.4329628050327301\n",
      "Step 1001 | Loss: 0.5107776522636414\n",
      "Step 1101 | Loss: 0.46809524297714233\n",
      "Step 1201 | Loss: 0.63510662317276\n",
      "Step 1301 | Loss: 0.525996208190918\n",
      "Step 1401 | Loss: 0.5751281976699829\n",
      "Step 1501 | Loss: 0.6244063973426819\n",
      "Step 1601 | Loss: 0.3816996216773987\n",
      "Step 1701 | Loss: 0.48071154952049255\n",
      "Step 1801 | Loss: 0.4356652498245239\n",
      "Step 1901 | Loss: 0.5128260254859924\n",
      "Step 2001 | Loss: 0.5960099101066589\n",
      "Step 2101 | Loss: 0.5688108205795288\n",
      "Step 2201 | Loss: 0.5557516813278198\n",
      "Step 2301 | Loss: 0.4849400222301483\n",
      "Step 2401 | Loss: 0.34205833077430725\n",
      "0.4964010335097536 0.8775\n",
      "Step 1 | Loss: 1.0389599800109863\n",
      "Step 101 | Loss: 0.40429362654685974\n",
      "Step 201 | Loss: 0.40839019417762756\n",
      "Step 301 | Loss: 0.6105471253395081\n",
      "Step 401 | Loss: 0.4701606333255768\n",
      "Step 501 | Loss: 0.4088553786277771\n",
      "Step 601 | Loss: 0.46370482444763184\n",
      "Step 701 | Loss: 0.5112524032592773\n",
      "Step 801 | Loss: 0.4909124970436096\n",
      "Step 901 | Loss: 0.446763277053833\n",
      "Step 1001 | Loss: 0.5658935904502869\n",
      "Step 1101 | Loss: 0.545133650302887\n",
      "Step 1201 | Loss: 0.6041356921195984\n",
      "Step 1301 | Loss: 0.4392537474632263\n",
      "Step 1401 | Loss: 0.32960912585258484\n",
      "Step 1501 | Loss: 0.37574490904808044\n",
      "Step 1601 | Loss: 0.4965934753417969\n",
      "Step 1701 | Loss: 0.44563859701156616\n",
      "Step 1801 | Loss: 0.4913020730018616\n",
      "Step 1901 | Loss: 0.41059765219688416\n",
      "Step 2001 | Loss: 0.3899554908275604\n",
      "Step 2101 | Loss: 0.4104177951812744\n",
      "Step 2201 | Loss: 0.50779128074646\n",
      "Step 2301 | Loss: 0.6773021817207336\n",
      "Step 2401 | Loss: 0.4659436047077179\n",
      "0.4961070659803941 0.8825\n",
      "Step 1 | Loss: 1.1502121686935425\n",
      "Step 101 | Loss: 0.6902059316635132\n",
      "Step 201 | Loss: 0.6796056032180786\n",
      "Step 301 | Loss: 0.6378993391990662\n",
      "Step 401 | Loss: 0.6260913610458374\n",
      "Step 501 | Loss: 0.6251365542411804\n",
      "Step 601 | Loss: 0.7092554569244385\n",
      "Step 701 | Loss: 0.48492830991744995\n",
      "Step 801 | Loss: 0.5080692768096924\n",
      "Step 901 | Loss: 0.5029547214508057\n",
      "Step 1001 | Loss: 0.38613802194595337\n",
      "Step 1101 | Loss: 0.4477626383304596\n",
      "Step 1201 | Loss: 0.5336014032363892\n",
      "Step 1301 | Loss: 0.4856105446815491\n",
      "Step 1401 | Loss: 0.42814403772354126\n",
      "Step 1501 | Loss: 0.4901612401008606\n",
      "Step 1601 | Loss: 0.4215165972709656\n",
      "Step 1701 | Loss: 0.3964720368385315\n",
      "Step 1801 | Loss: 0.5065256357192993\n",
      "Step 1901 | Loss: 0.5679011344909668\n",
      "Step 2001 | Loss: 0.4417811930179596\n",
      "Step 2101 | Loss: 0.5669832825660706\n",
      "Step 2201 | Loss: 0.3025713562965393\n",
      "Step 2301 | Loss: 0.3760913014411926\n",
      "Step 2401 | Loss: 0.39952975511550903\n",
      "0.4579870621845777 0.885\n",
      "Step 1 | Loss: 1.410321831703186\n",
      "Step 101 | Loss: 0.6846638917922974\n",
      "Step 201 | Loss: 0.5431991815567017\n",
      "Step 301 | Loss: 0.5176268815994263\n",
      "Step 401 | Loss: 0.45983824133872986\n",
      "Step 501 | Loss: 0.6179249286651611\n",
      "Step 601 | Loss: 0.5498650074005127\n",
      "Step 701 | Loss: 0.5325129628181458\n",
      "Step 801 | Loss: 0.5456777811050415\n",
      "Step 901 | Loss: 0.4026007354259491\n",
      "Step 1001 | Loss: 0.5428677797317505\n",
      "Step 1101 | Loss: 0.6241899728775024\n",
      "Step 1201 | Loss: 0.4869900643825531\n",
      "Step 1301 | Loss: 0.4469527006149292\n",
      "Step 1401 | Loss: 0.4692819118499756\n",
      "Step 1501 | Loss: 0.5476975440979004\n",
      "Step 1601 | Loss: 0.6045949459075928\n",
      "Step 1701 | Loss: 0.5131977200508118\n",
      "Step 1801 | Loss: 0.4661931097507477\n",
      "Step 1901 | Loss: 0.7135124802589417\n",
      "Step 2001 | Loss: 0.5766189694404602\n",
      "Step 2101 | Loss: 0.48760566115379333\n",
      "Step 2201 | Loss: 0.5613217949867249\n",
      "Step 2301 | Loss: 0.575806200504303\n",
      "Step 2401 | Loss: 0.5379418730735779\n",
      "0.5314523514735161 0.8825\n",
      "Step 1 | Loss: 2.3699984550476074\n",
      "Step 101 | Loss: 1.8673906326293945\n",
      "Step 201 | Loss: 1.7264665365219116\n",
      "Step 301 | Loss: 1.8647059202194214\n",
      "Step 401 | Loss: 1.2410985231399536\n",
      "Step 501 | Loss: 1.9959354400634766\n",
      "Step 601 | Loss: 1.6184037923812866\n",
      "Step 701 | Loss: 1.7521882057189941\n",
      "Step 801 | Loss: 2.185823917388916\n",
      "Step 901 | Loss: 2.467102289199829\n",
      "Step 1001 | Loss: 2.6031808853149414\n",
      "Step 1101 | Loss: 2.2413761615753174\n",
      "Step 1201 | Loss: 2.0596914291381836\n",
      "Step 1301 | Loss: 1.8396495580673218\n",
      "Step 1401 | Loss: 2.4554190635681152\n",
      "Step 1501 | Loss: 1.7943376302719116\n",
      "Step 1601 | Loss: 1.825042486190796\n",
      "Step 1701 | Loss: 1.8405746221542358\n",
      "Step 1801 | Loss: 2.0058465003967285\n",
      "Step 1901 | Loss: 2.2459511756896973\n",
      "Step 2001 | Loss: 1.7008020877838135\n",
      "Step 2101 | Loss: 1.6305625438690186\n",
      "Step 2201 | Loss: 2.559851884841919\n",
      "Step 2301 | Loss: 1.8362573385238647\n",
      "Step 2401 | Loss: 1.4832950830459595\n",
      "1.8360140977153585 0.4975\n",
      "Step 1 | Loss: 1.7395448684692383\n",
      "Step 101 | Loss: 1.7316184043884277\n",
      "Step 201 | Loss: 1.7245497703552246\n",
      "Step 301 | Loss: 2.107147693634033\n",
      "Step 401 | Loss: 2.3339478969573975\n",
      "Step 501 | Loss: 1.9575530290603638\n",
      "Step 601 | Loss: 2.147761821746826\n",
      "Step 701 | Loss: 2.4039762020111084\n",
      "Step 801 | Loss: 1.5665940046310425\n",
      "Step 901 | Loss: 1.6471012830734253\n",
      "Step 1001 | Loss: 2.4310662746429443\n",
      "Step 1101 | Loss: 1.4699020385742188\n",
      "Step 1201 | Loss: 2.0002553462982178\n",
      "Step 1301 | Loss: 1.5037736892700195\n",
      "Step 1401 | Loss: 2.4875338077545166\n",
      "Step 1501 | Loss: 1.1536297798156738\n",
      "Step 1601 | Loss: 1.9209059476852417\n",
      "Step 1701 | Loss: 2.1047351360321045\n",
      "Step 1801 | Loss: 1.954283595085144\n",
      "Step 1901 | Loss: 1.4429047107696533\n",
      "Step 2001 | Loss: 1.8388621807098389\n",
      "Step 2101 | Loss: 1.3698184490203857\n",
      "Step 2201 | Loss: 1.994368553161621\n",
      "Step 2301 | Loss: 2.0593767166137695\n",
      "Step 2401 | Loss: 1.9558569192886353\n",
      "1.82195023090318 0.4975\n",
      "Step 1 | Loss: 2.2486367225646973\n",
      "Step 101 | Loss: 2.4554643630981445\n",
      "Step 201 | Loss: 1.1918171644210815\n",
      "Step 301 | Loss: 1.7574944496154785\n",
      "Step 401 | Loss: 2.339921236038208\n",
      "Step 501 | Loss: 2.3864011764526367\n",
      "Step 601 | Loss: 1.2583099603652954\n",
      "Step 701 | Loss: 1.6171228885650635\n",
      "Step 801 | Loss: 1.708465814590454\n",
      "Step 901 | Loss: 1.9387736320495605\n",
      "Step 1001 | Loss: 1.7582796812057495\n",
      "Step 1101 | Loss: 2.104417324066162\n",
      "Step 1201 | Loss: 2.4320857524871826\n",
      "Step 1301 | Loss: 1.3388404846191406\n",
      "Step 1401 | Loss: 1.3949944972991943\n",
      "Step 1501 | Loss: 1.8344403505325317\n",
      "Step 1601 | Loss: 2.183077573776245\n",
      "Step 1701 | Loss: 2.0965065956115723\n",
      "Step 1801 | Loss: 1.8396060466766357\n",
      "Step 1901 | Loss: 2.1663401126861572\n",
      "Step 2001 | Loss: 2.5622568130493164\n",
      "Step 2101 | Loss: 1.5793455839157104\n",
      "Step 2201 | Loss: 1.2807776927947998\n",
      "Step 2301 | Loss: 1.6353356838226318\n",
      "Step 2401 | Loss: 1.610120415687561\n",
      "1.8207635201276662 0.4975\n",
      "Step 1 | Loss: 1.7185832262039185\n",
      "Step 101 | Loss: 1.9320862293243408\n",
      "Step 201 | Loss: 2.323471784591675\n",
      "Step 301 | Loss: 2.5113165378570557\n",
      "Step 401 | Loss: 1.4218231439590454\n",
      "Step 501 | Loss: 1.5693163871765137\n",
      "Step 601 | Loss: 1.6805065870285034\n",
      "Step 701 | Loss: 1.457914113998413\n",
      "Step 801 | Loss: 1.6795521974563599\n",
      "Step 901 | Loss: 2.030764102935791\n",
      "Step 1001 | Loss: 1.4874024391174316\n",
      "Step 1101 | Loss: 1.6861069202423096\n",
      "Step 1201 | Loss: 1.9461452960968018\n",
      "Step 1301 | Loss: 1.2286837100982666\n",
      "Step 1401 | Loss: 2.122819423675537\n",
      "Step 1501 | Loss: 1.6944905519485474\n",
      "Step 1601 | Loss: 1.7150835990905762\n",
      "Step 1701 | Loss: 1.9684126377105713\n",
      "Step 1801 | Loss: 1.438477873802185\n",
      "Step 1901 | Loss: 1.386021614074707\n",
      "Step 2001 | Loss: 1.979019284248352\n",
      "Step 2101 | Loss: 1.9058680534362793\n",
      "Step 2201 | Loss: 1.8081015348434448\n",
      "Step 2301 | Loss: 1.123624324798584\n",
      "Step 2401 | Loss: 2.112722396850586\n",
      "1.8209441192404403 0.4975\n",
      "Step 1 | Loss: 1.8278582096099854\n",
      "Step 101 | Loss: 2.1210663318634033\n",
      "Step 201 | Loss: 2.0037031173706055\n",
      "Step 301 | Loss: 2.1449990272521973\n",
      "Step 401 | Loss: 1.2521625757217407\n",
      "Step 501 | Loss: 1.658841609954834\n",
      "Step 601 | Loss: 1.8521732091903687\n",
      "Step 701 | Loss: 2.0075817108154297\n",
      "Step 801 | Loss: 2.1858203411102295\n",
      "Step 901 | Loss: 1.5441203117370605\n",
      "Step 1001 | Loss: 1.830061912536621\n",
      "Step 1101 | Loss: 1.6134816408157349\n",
      "Step 1201 | Loss: 1.3502914905548096\n",
      "Step 1301 | Loss: 1.5048792362213135\n",
      "Step 1401 | Loss: 1.8369925022125244\n",
      "Step 1501 | Loss: 2.1216650009155273\n",
      "Step 1601 | Loss: 2.0287866592407227\n",
      "Step 1701 | Loss: 2.0921878814697266\n",
      "Step 1801 | Loss: 2.1031880378723145\n",
      "Step 1901 | Loss: 1.8943642377853394\n",
      "Step 2001 | Loss: 2.1007659435272217\n",
      "Step 2101 | Loss: 1.2083989381790161\n",
      "Step 2201 | Loss: 1.7955358028411865\n",
      "Step 2301 | Loss: 1.784435749053955\n",
      "Step 2401 | Loss: 1.5777469873428345\n",
      "1.8207493122339202 0.4975\n",
      "Step 1 | Loss: 1.0367169380187988\n",
      "Step 101 | Loss: 0.6722939610481262\n",
      "Step 201 | Loss: 0.46239161491394043\n",
      "Step 301 | Loss: 0.27835917472839355\n",
      "Step 401 | Loss: 0.2341722548007965\n",
      "Step 501 | Loss: 0.20245608687400818\n",
      "Step 601 | Loss: 0.1171969547867775\n",
      "Step 701 | Loss: 0.17471200227737427\n",
      "Step 801 | Loss: 0.1975238025188446\n",
      "Step 901 | Loss: 0.35862603783607483\n",
      "Step 1001 | Loss: 0.28837302327156067\n",
      "Step 1101 | Loss: 0.31948381662368774\n",
      "Step 1201 | Loss: 0.1946684867143631\n",
      "Step 1301 | Loss: 0.17113064229488373\n",
      "Step 1401 | Loss: 0.22171449661254883\n",
      "Step 1501 | Loss: 0.2157726287841797\n",
      "Step 1601 | Loss: 0.15545722842216492\n",
      "Step 1701 | Loss: 0.15381982922554016\n",
      "Step 1801 | Loss: 0.24768973886966705\n",
      "Step 1901 | Loss: 0.3015035092830658\n",
      "Step 2001 | Loss: 0.24559557437896729\n",
      "Step 2101 | Loss: 0.18336650729179382\n",
      "Step 2201 | Loss: 0.16556058824062347\n",
      "Step 2301 | Loss: 0.16492849588394165\n",
      "Step 2401 | Loss: 0.16888472437858582\n",
      "0.1905975369796571 0.96\n",
      "Step 1 | Loss: 1.1938241720199585\n",
      "Step 101 | Loss: 0.5293053388595581\n",
      "Step 201 | Loss: 0.4587787091732025\n",
      "Step 301 | Loss: 0.40964362025260925\n",
      "Step 401 | Loss: 0.19601184129714966\n",
      "Step 501 | Loss: 0.19456979632377625\n",
      "Step 601 | Loss: 0.15434710681438446\n",
      "Step 701 | Loss: 0.26403331756591797\n",
      "Step 801 | Loss: 0.14819054305553436\n",
      "Step 901 | Loss: 0.12876155972480774\n",
      "Step 1001 | Loss: 0.13684113323688507\n",
      "Step 1101 | Loss: 0.21795327961444855\n",
      "Step 1201 | Loss: 0.15909019112586975\n",
      "Step 1301 | Loss: 0.14760832488536835\n",
      "Step 1401 | Loss: 0.29770511388778687\n",
      "Step 1501 | Loss: 0.18258753418922424\n",
      "Step 1601 | Loss: 0.18950995802879333\n",
      "Step 1701 | Loss: 0.2549704313278198\n",
      "Step 1801 | Loss: 0.17496192455291748\n",
      "Step 1901 | Loss: 0.23251120746135712\n",
      "Step 2001 | Loss: 0.22188659012317657\n",
      "Step 2101 | Loss: 0.12215271592140198\n",
      "Step 2201 | Loss: 0.4019661247730255\n",
      "Step 2301 | Loss: 0.24092160165309906\n",
      "Step 2401 | Loss: 0.11452433466911316\n",
      "0.2050337956129416 0.96\n",
      "Step 1 | Loss: 0.9766085147857666\n",
      "Step 101 | Loss: 0.37568533420562744\n",
      "Step 201 | Loss: 0.2771627604961395\n",
      "Step 301 | Loss: 0.275119811296463\n",
      "Step 401 | Loss: 0.3856163024902344\n",
      "Step 501 | Loss: 0.3294917941093445\n",
      "Step 601 | Loss: 0.25163450837135315\n",
      "Step 701 | Loss: 0.19954419136047363\n",
      "Step 801 | Loss: 0.1446736454963684\n",
      "Step 901 | Loss: 0.10515161603689194\n",
      "Step 1001 | Loss: 0.19665880501270294\n",
      "Step 1101 | Loss: 0.24927446246147156\n",
      "Step 1201 | Loss: 0.2802242040634155\n",
      "Step 1301 | Loss: 0.2796308100223541\n",
      "Step 1401 | Loss: 0.12609286606311798\n",
      "Step 1501 | Loss: 0.1237204521894455\n",
      "Step 1601 | Loss: 0.24749909341335297\n",
      "Step 1701 | Loss: 0.1757575273513794\n",
      "Step 1801 | Loss: 0.26241275668144226\n",
      "Step 1901 | Loss: 0.25600650906562805\n",
      "Step 2001 | Loss: 0.10876492410898209\n",
      "Step 2101 | Loss: 0.10896956920623779\n",
      "Step 2201 | Loss: 0.2398117184638977\n",
      "Step 2301 | Loss: 0.255221426486969\n",
      "Step 2401 | Loss: 0.1304495632648468\n",
      "0.19036003205291405 0.9625\n",
      "Step 1 | Loss: 0.9932695031166077\n",
      "Step 101 | Loss: 0.748803973197937\n",
      "Step 201 | Loss: 0.762668251991272\n",
      "Step 301 | Loss: 0.5143123865127563\n",
      "Step 401 | Loss: 0.31049561500549316\n",
      "Step 501 | Loss: 0.20891109108924866\n",
      "Step 601 | Loss: 0.25747692584991455\n",
      "Step 701 | Loss: 0.18339085578918457\n",
      "Step 801 | Loss: 0.2366103231906891\n",
      "Step 901 | Loss: 0.22779425978660583\n",
      "Step 1001 | Loss: 0.17602333426475525\n",
      "Step 1101 | Loss: 0.245824933052063\n",
      "Step 1201 | Loss: 0.19266103208065033\n",
      "Step 1301 | Loss: 0.19285957515239716\n",
      "Step 1401 | Loss: 0.20914149284362793\n",
      "Step 1501 | Loss: 0.13681869208812714\n",
      "Step 1601 | Loss: 0.18378028273582458\n",
      "Step 1701 | Loss: 0.19614991545677185\n",
      "Step 1801 | Loss: 0.23615795373916626\n",
      "Step 1901 | Loss: 0.19781479239463806\n",
      "Step 2001 | Loss: 0.13018707931041718\n",
      "Step 2101 | Loss: 0.22767561674118042\n",
      "Step 2201 | Loss: 0.10277751088142395\n",
      "Step 2301 | Loss: 0.20883113145828247\n",
      "Step 2401 | Loss: 0.23880517482757568\n",
      "0.200816088349902 0.9725\n",
      "Step 1 | Loss: 0.8379212021827698\n",
      "Step 101 | Loss: 0.22362568974494934\n",
      "Step 201 | Loss: 0.2912583351135254\n",
      "Step 301 | Loss: 0.21478457748889923\n",
      "Step 401 | Loss: 0.25943654775619507\n",
      "Step 501 | Loss: 0.19826078414916992\n",
      "Step 601 | Loss: 0.24614831805229187\n",
      "Step 701 | Loss: 0.15996111929416656\n",
      "Step 801 | Loss: 0.28134840726852417\n",
      "Step 901 | Loss: 0.19068345427513123\n",
      "Step 1001 | Loss: 0.1706010401248932\n",
      "Step 1101 | Loss: 0.24609434604644775\n",
      "Step 1201 | Loss: 0.21811652183532715\n",
      "Step 1301 | Loss: 0.29422134160995483\n",
      "Step 1401 | Loss: 0.2124137282371521\n",
      "Step 1501 | Loss: 0.15739880502223969\n",
      "Step 1601 | Loss: 0.24187983572483063\n",
      "Step 1701 | Loss: 0.24072574079036713\n",
      "Step 1801 | Loss: 0.28481733798980713\n",
      "Step 1901 | Loss: 0.20574146509170532\n",
      "Step 2001 | Loss: 0.14912891387939453\n",
      "Step 2101 | Loss: 0.2187759429216385\n",
      "Step 2201 | Loss: 0.17027489840984344\n",
      "Step 2301 | Loss: 0.2590163052082062\n",
      "Step 2401 | Loss: 0.12102149426937103\n",
      "0.20947473115394483 0.96\n",
      "Step 1 | Loss: 1.3063938617706299\n",
      "Step 101 | Loss: 1.0288305282592773\n",
      "Step 201 | Loss: 0.8705507516860962\n",
      "Step 301 | Loss: 0.6650338172912598\n",
      "Step 401 | Loss: 0.7586520910263062\n",
      "Step 501 | Loss: 0.8020575642585754\n",
      "Step 601 | Loss: 0.853635847568512\n",
      "Step 701 | Loss: 0.8968890905380249\n",
      "Step 801 | Loss: 0.7910102009773254\n",
      "Step 901 | Loss: 0.8310782313346863\n",
      "Step 1001 | Loss: 0.9203205108642578\n",
      "Step 1101 | Loss: 0.6879242658615112\n",
      "Step 1201 | Loss: 0.9655105471611023\n",
      "Step 1301 | Loss: 0.7320067286491394\n",
      "Step 1401 | Loss: 0.8196882009506226\n",
      "Step 1501 | Loss: 0.7937572002410889\n",
      "Step 1601 | Loss: 0.5838993787765503\n",
      "Step 1701 | Loss: 0.8492364883422852\n",
      "Step 1801 | Loss: 0.6114253401756287\n",
      "Step 1901 | Loss: 0.800637423992157\n",
      "Step 2001 | Loss: 0.871509850025177\n",
      "Step 2101 | Loss: 0.5774096250534058\n",
      "Step 2201 | Loss: 0.6166309118270874\n",
      "Step 2301 | Loss: 0.7834045886993408\n",
      "Step 2401 | Loss: 0.7340137958526611\n",
      "0.7686283810674861 0.7525\n",
      "Step 1 | Loss: 1.1020692586898804\n",
      "Step 101 | Loss: 0.9560583233833313\n",
      "Step 201 | Loss: 0.7856442928314209\n",
      "Step 301 | Loss: 0.6572037935256958\n",
      "Step 401 | Loss: 0.7535874247550964\n",
      "Step 501 | Loss: 0.6481350660324097\n",
      "Step 601 | Loss: 0.8626766204833984\n",
      "Step 701 | Loss: 0.7582926750183105\n",
      "Step 801 | Loss: 0.5987879037857056\n",
      "Step 901 | Loss: 0.8713430166244507\n",
      "Step 1001 | Loss: 0.715120792388916\n",
      "Step 1101 | Loss: 0.7074260115623474\n",
      "Step 1201 | Loss: 0.5867776274681091\n",
      "Step 1301 | Loss: 0.7977715134620667\n",
      "Step 1401 | Loss: 0.7273169159889221\n",
      "Step 1501 | Loss: 0.6933678388595581\n",
      "Step 1601 | Loss: 0.7321783304214478\n",
      "Step 1701 | Loss: 0.6990900039672852\n",
      "Step 1801 | Loss: 0.7101330757141113\n",
      "Step 1901 | Loss: 0.7042335867881775\n",
      "Step 2001 | Loss: 0.7232316732406616\n",
      "Step 2101 | Loss: 0.7498277425765991\n",
      "Step 2201 | Loss: 0.6722481846809387\n",
      "Step 2301 | Loss: 0.7074236869812012\n",
      "Step 2401 | Loss: 0.669607400894165\n",
      "0.7971936734609493 0.7525\n",
      "Step 1 | Loss: 1.1766844987869263\n",
      "Step 101 | Loss: 0.7662935256958008\n",
      "Step 201 | Loss: 0.8199588656425476\n",
      "Step 301 | Loss: 0.8525382280349731\n",
      "Step 401 | Loss: 0.7782047986984253\n",
      "Step 501 | Loss: 0.6746917963027954\n",
      "Step 601 | Loss: 0.7892799377441406\n",
      "Step 701 | Loss: 0.831644594669342\n",
      "Step 801 | Loss: 0.7767669558525085\n",
      "Step 901 | Loss: 0.8153813481330872\n",
      "Step 1001 | Loss: 0.8143086433410645\n",
      "Step 1101 | Loss: 0.693450927734375\n",
      "Step 1201 | Loss: 0.7299776077270508\n",
      "Step 1301 | Loss: 0.7010406255722046\n",
      "Step 1401 | Loss: 0.7473493814468384\n",
      "Step 1501 | Loss: 0.8162716627120972\n",
      "Step 1601 | Loss: 0.8419196605682373\n",
      "Step 1701 | Loss: 0.7445215582847595\n",
      "Step 1801 | Loss: 0.7854392528533936\n",
      "Step 1901 | Loss: 0.7264164090156555\n",
      "Step 2001 | Loss: 0.8635119199752808\n",
      "Step 2101 | Loss: 0.7822778224945068\n",
      "Step 2201 | Loss: 0.6989598870277405\n",
      "Step 2301 | Loss: 0.63160240650177\n",
      "Step 2401 | Loss: 0.6828750371932983\n",
      "0.7691351447706665 0.7875\n",
      "Step 1 | Loss: 0.8160926699638367\n",
      "Step 101 | Loss: 0.8099343776702881\n",
      "Step 201 | Loss: 0.7516650557518005\n",
      "Step 301 | Loss: 0.7103724479675293\n",
      "Step 401 | Loss: 0.7816262245178223\n",
      "Step 501 | Loss: 0.8941619396209717\n",
      "Step 601 | Loss: 0.8268088102340698\n",
      "Step 701 | Loss: 0.7690836191177368\n",
      "Step 801 | Loss: 0.7838390469551086\n",
      "Step 901 | Loss: 0.8797343373298645\n",
      "Step 1001 | Loss: 0.838774561882019\n",
      "Step 1101 | Loss: 0.69936603307724\n",
      "Step 1201 | Loss: 0.6803752779960632\n",
      "Step 1301 | Loss: 0.7655115723609924\n",
      "Step 1401 | Loss: 0.7152379751205444\n",
      "Step 1501 | Loss: 0.7724587321281433\n",
      "Step 1601 | Loss: 0.6893374919891357\n",
      "Step 1701 | Loss: 0.694202721118927\n",
      "Step 1801 | Loss: 0.8599197864532471\n",
      "Step 1901 | Loss: 0.6563480496406555\n",
      "Step 2001 | Loss: 0.7232608795166016\n",
      "Step 2101 | Loss: 0.7571632266044617\n",
      "Step 2201 | Loss: 0.7793019413948059\n",
      "Step 2301 | Loss: 0.8207358121871948\n",
      "Step 2401 | Loss: 0.7879588007926941\n",
      "0.76166909398635 0.78\n",
      "Step 1 | Loss: 1.1170276403427124\n",
      "Step 101 | Loss: 0.8857465386390686\n",
      "Step 201 | Loss: 0.672919511795044\n",
      "Step 301 | Loss: 0.5998519062995911\n",
      "Step 401 | Loss: 0.5322406888008118\n",
      "Step 501 | Loss: 0.4703012704849243\n",
      "Step 601 | Loss: 0.5785545706748962\n",
      "Step 701 | Loss: 0.5646572113037109\n",
      "Step 801 | Loss: 0.646734893321991\n",
      "Step 901 | Loss: 0.5472050905227661\n",
      "Step 1001 | Loss: 0.4667508602142334\n",
      "Step 1101 | Loss: 0.5336142778396606\n",
      "Step 1201 | Loss: 0.6540504693984985\n",
      "Step 1301 | Loss: 0.6052824258804321\n",
      "Step 1401 | Loss: 0.5022397637367249\n",
      "Step 1501 | Loss: 0.5396442413330078\n",
      "Step 1601 | Loss: 0.5972949862480164\n",
      "Step 1701 | Loss: 0.5207459330558777\n",
      "Step 1801 | Loss: 0.553787350654602\n",
      "Step 1901 | Loss: 0.6353118419647217\n",
      "Step 2001 | Loss: 0.6614415049552917\n",
      "Step 2101 | Loss: 0.6270118355751038\n",
      "Step 2201 | Loss: 0.5057067275047302\n",
      "Step 2301 | Loss: 0.5423097014427185\n",
      "Step 2401 | Loss: 0.604698121547699\n",
      "0.553849612663274 0.8775\n",
      "Step 1 | Loss: 0.9713096618652344\n",
      "Step 101 | Loss: 0.6113872528076172\n",
      "Step 201 | Loss: 0.32182803750038147\n",
      "Step 301 | Loss: 0.5446376800537109\n",
      "Step 401 | Loss: 0.6894312500953674\n",
      "Step 501 | Loss: 0.5338709950447083\n",
      "Step 601 | Loss: 0.5785015821456909\n",
      "Step 701 | Loss: 0.5898157358169556\n",
      "Step 801 | Loss: 0.6498790979385376\n",
      "Step 901 | Loss: 0.509730339050293\n",
      "Step 1001 | Loss: 0.6641491055488586\n",
      "Step 1101 | Loss: 0.5169188976287842\n",
      "Step 1201 | Loss: 0.7257950901985168\n",
      "Step 1301 | Loss: 0.6284012198448181\n",
      "Step 1401 | Loss: 0.7504234910011292\n",
      "Step 1501 | Loss: 0.6103634834289551\n",
      "Step 1601 | Loss: 0.8757205605506897\n",
      "Step 1701 | Loss: 0.8372005820274353\n",
      "Step 1801 | Loss: 0.6112349629402161\n",
      "Step 1901 | Loss: 0.5737242102622986\n",
      "Step 2001 | Loss: 0.7108786106109619\n",
      "Step 2101 | Loss: 0.5977438688278198\n",
      "Step 2201 | Loss: 0.49999165534973145\n",
      "Step 2301 | Loss: 0.8511263728141785\n",
      "Step 2401 | Loss: 0.8592934608459473\n",
      "0.6414406907193442 0.7225\n",
      "Step 1 | Loss: 1.2627716064453125\n",
      "Step 101 | Loss: 0.7096181511878967\n",
      "Step 201 | Loss: 0.7723468542098999\n",
      "Step 301 | Loss: 0.9806170463562012\n",
      "Step 401 | Loss: 0.6439008116722107\n",
      "Step 501 | Loss: 0.6237581968307495\n",
      "Step 601 | Loss: 0.6665358543395996\n",
      "Step 701 | Loss: 0.7176602482795715\n",
      "Step 801 | Loss: 0.3465909957885742\n",
      "Step 901 | Loss: 0.6686626672744751\n",
      "Step 1001 | Loss: 0.6059273481369019\n",
      "Step 1101 | Loss: 0.6301032304763794\n",
      "Step 1201 | Loss: 0.7468417882919312\n",
      "Step 1301 | Loss: 0.6209390163421631\n",
      "Step 1401 | Loss: 0.6208093166351318\n",
      "Step 1501 | Loss: 0.5561516880989075\n",
      "Step 1601 | Loss: 0.9447077512741089\n",
      "Step 1701 | Loss: 0.6530432105064392\n",
      "Step 1801 | Loss: 0.5611778497695923\n",
      "Step 1901 | Loss: 0.5750076770782471\n",
      "Step 2001 | Loss: 0.7776095867156982\n",
      "Step 2101 | Loss: 0.5617716908454895\n",
      "Step 2201 | Loss: 0.5794671773910522\n",
      "Step 2301 | Loss: 0.7603907585144043\n",
      "Step 2401 | Loss: 0.5518221855163574\n",
      "0.6454672109731916 0.7275\n",
      "Step 1 | Loss: 2.5575497150421143\n",
      "Step 101 | Loss: 0.7152076959609985\n",
      "Step 201 | Loss: 0.6997063159942627\n",
      "Step 301 | Loss: 0.6028500199317932\n",
      "Step 401 | Loss: 0.5280535221099854\n",
      "Step 501 | Loss: 0.6014215350151062\n",
      "Step 601 | Loss: 0.9130083918571472\n",
      "Step 701 | Loss: 0.5850884318351746\n",
      "Step 801 | Loss: 0.8042818903923035\n",
      "Step 901 | Loss: 0.45000728964805603\n",
      "Step 1001 | Loss: 0.41137897968292236\n",
      "Step 1101 | Loss: 0.8016547560691833\n",
      "Step 1201 | Loss: 0.5807439088821411\n",
      "Step 1301 | Loss: 0.6621521711349487\n",
      "Step 1401 | Loss: 0.821971595287323\n",
      "Step 1501 | Loss: 0.749517023563385\n",
      "Step 1601 | Loss: 0.6579926609992981\n",
      "Step 1701 | Loss: 0.5755138397216797\n",
      "Step 1801 | Loss: 0.7053796052932739\n",
      "Step 1901 | Loss: 0.8422989249229431\n",
      "Step 2001 | Loss: 0.6104006767272949\n",
      "Step 2101 | Loss: 0.7756361365318298\n",
      "Step 2201 | Loss: 0.6574227809906006\n",
      "Step 2301 | Loss: 0.599764347076416\n",
      "Step 2401 | Loss: 0.6766018271446228\n",
      "0.6539250681117882 0.7\n",
      "Step 1 | Loss: 0.8182013630867004\n",
      "Step 101 | Loss: 0.41492152214050293\n",
      "Step 201 | Loss: 0.5593081116676331\n",
      "Step 301 | Loss: 0.506415605545044\n",
      "Step 401 | Loss: 0.6590152978897095\n",
      "Step 501 | Loss: 0.4054814279079437\n",
      "Step 601 | Loss: 0.6765916347503662\n",
      "Step 701 | Loss: 0.5341204404830933\n",
      "Step 801 | Loss: 0.6542204022407532\n",
      "Step 901 | Loss: 0.6906295418739319\n",
      "Step 1001 | Loss: 0.6641240119934082\n",
      "Step 1101 | Loss: 0.4232807159423828\n",
      "Step 1201 | Loss: 0.8292608261108398\n",
      "Step 1301 | Loss: 0.38774171471595764\n",
      "Step 1401 | Loss: 0.620254397392273\n",
      "Step 1501 | Loss: 0.6351605653762817\n",
      "Step 1601 | Loss: 0.48896312713623047\n",
      "Step 1701 | Loss: 0.6429992914199829\n",
      "Step 1801 | Loss: 0.691512942314148\n",
      "Step 1901 | Loss: 0.7244203090667725\n",
      "Step 2001 | Loss: 0.6271423697471619\n",
      "Step 2101 | Loss: 0.935941219329834\n",
      "Step 2201 | Loss: 0.6340706944465637\n",
      "Step 2301 | Loss: 0.6049312949180603\n",
      "Step 2401 | Loss: 0.7145611047744751\n",
      "0.6322311197758649 0.725\n",
      "Step 1 | Loss: 0.784643292427063\n",
      "Step 101 | Loss: 0.6885870099067688\n",
      "Step 201 | Loss: 0.6402156949043274\n",
      "Step 301 | Loss: 0.5850449800491333\n",
      "Step 401 | Loss: 0.48561641573905945\n",
      "Step 501 | Loss: 0.5393801927566528\n",
      "Step 601 | Loss: 0.6828626990318298\n",
      "Step 701 | Loss: 0.7332905530929565\n",
      "Step 801 | Loss: 0.6420263051986694\n",
      "Step 901 | Loss: 0.7104942202568054\n",
      "Step 1001 | Loss: 0.4027184545993805\n",
      "Step 1101 | Loss: 0.3971029818058014\n",
      "Step 1201 | Loss: 0.5719080567359924\n",
      "Step 1301 | Loss: 0.7404416799545288\n",
      "Step 1401 | Loss: 0.8463435173034668\n",
      "Step 1501 | Loss: 0.5592252612113953\n",
      "Step 1601 | Loss: 0.43003493547439575\n",
      "Step 1701 | Loss: 0.6171364784240723\n",
      "Step 1801 | Loss: 0.6711950898170471\n",
      "Step 1901 | Loss: 0.9015555381774902\n",
      "Step 2001 | Loss: 0.6253800988197327\n",
      "Step 2101 | Loss: 0.6289908289909363\n",
      "Step 2201 | Loss: 0.41438430547714233\n",
      "Step 2301 | Loss: 0.5356518626213074\n",
      "Step 2401 | Loss: 0.5055934190750122\n",
      "0.6411670185899548 0.72\n",
      "Step 1 | Loss: 2.122002363204956\n",
      "Step 101 | Loss: 1.0000008344650269\n",
      "Step 201 | Loss: 0.9876859188079834\n",
      "Step 301 | Loss: 0.9976444244384766\n",
      "Step 401 | Loss: 1.0087946653366089\n",
      "Step 501 | Loss: 1.0187814235687256\n",
      "Step 601 | Loss: 1.0045831203460693\n",
      "Step 701 | Loss: 1.0074704885482788\n",
      "Step 801 | Loss: 1.002396821975708\n",
      "Step 901 | Loss: 0.9806910753250122\n",
      "Step 1001 | Loss: 1.0045245885849\n",
      "Step 1101 | Loss: 0.9843965172767639\n",
      "Step 1201 | Loss: 1.0162187814712524\n",
      "Step 1301 | Loss: 1.0095325708389282\n",
      "Step 1401 | Loss: 1.000020980834961\n",
      "Step 1501 | Loss: 1.0000509023666382\n",
      "Step 1601 | Loss: 1.0217996835708618\n",
      "Step 1701 | Loss: 1.0060710906982422\n",
      "Step 1801 | Loss: 0.9866896271705627\n",
      "Step 1901 | Loss: 1.0261386632919312\n",
      "Step 2001 | Loss: 0.998466432094574\n",
      "Step 2101 | Loss: 1.0203595161437988\n",
      "Step 2201 | Loss: 1.0\n",
      "Step 2301 | Loss: 1.037987470626831\n",
      "Step 2401 | Loss: 0.9943304657936096\n",
      "1.000036454231556 0.5\n",
      "Step 1 | Loss: 1.1612248420715332\n",
      "Step 101 | Loss: 1.0028302669525146\n",
      "Step 201 | Loss: 0.9906342625617981\n",
      "Step 301 | Loss: 1.050233244895935\n",
      "Step 401 | Loss: 0.9997220039367676\n",
      "Step 501 | Loss: 1.0015491247177124\n",
      "Step 601 | Loss: 0.9835700988769531\n",
      "Step 701 | Loss: 0.9858115315437317\n",
      "Step 801 | Loss: 0.9963458776473999\n",
      "Step 901 | Loss: 0.9960942268371582\n",
      "Step 1001 | Loss: 0.999040424823761\n",
      "Step 1101 | Loss: 0.9978242516517639\n",
      "Step 1201 | Loss: 1.0033864974975586\n",
      "Step 1301 | Loss: 1.0141551494598389\n",
      "Step 1401 | Loss: 1.000652551651001\n",
      "Step 1501 | Loss: 0.9935654997825623\n",
      "Step 1601 | Loss: 0.9975472688674927\n",
      "Step 1701 | Loss: 0.9961128234863281\n",
      "Step 1801 | Loss: 0.9835456609725952\n",
      "Step 1901 | Loss: 0.9980446100234985\n",
      "Step 2001 | Loss: 0.9966908097267151\n",
      "Step 2101 | Loss: 1.0217677354812622\n",
      "Step 2201 | Loss: 1.001910924911499\n",
      "Step 2301 | Loss: 1.0008792877197266\n",
      "Step 2401 | Loss: 1.0008282661437988\n",
      "1.0002924402671878 0.5\n",
      "Step 1 | Loss: 1.0223889350891113\n",
      "Step 101 | Loss: 0.9840500354766846\n",
      "Step 201 | Loss: 1.0033890008926392\n",
      "Step 301 | Loss: 1.0092518329620361\n",
      "Step 401 | Loss: 0.9963873624801636\n",
      "Step 501 | Loss: 1.00753915309906\n",
      "Step 601 | Loss: 0.9989923238754272\n",
      "Step 701 | Loss: 1.0044735670089722\n",
      "Step 801 | Loss: 1.0101137161254883\n",
      "Step 901 | Loss: 1.0001944303512573\n",
      "Step 1001 | Loss: 1.0366754531860352\n",
      "Step 1101 | Loss: 1.0011796951293945\n",
      "Step 1201 | Loss: 0.9967522621154785\n",
      "Step 1301 | Loss: 1.0097708702087402\n",
      "Step 1401 | Loss: 1.0014194250106812\n",
      "Step 1501 | Loss: 0.9897899627685547\n",
      "Step 1601 | Loss: 1.0173810720443726\n",
      "Step 1701 | Loss: 1.0000008344650269\n",
      "Step 1801 | Loss: 1.0001698732376099\n",
      "Step 1901 | Loss: 1.0127712488174438\n",
      "Step 2001 | Loss: 1.0163475275039673\n",
      "Step 2101 | Loss: 0.998286247253418\n",
      "Step 2201 | Loss: 1.0008370876312256\n",
      "Step 2301 | Loss: 0.9998083710670471\n",
      "Step 2401 | Loss: 1.003680944442749\n",
      "1.0030393862023594 0.5\n",
      "Step 1 | Loss: 1.056244134902954\n",
      "Step 101 | Loss: 1.0004777908325195\n",
      "Step 201 | Loss: 1.0004771947860718\n",
      "Step 301 | Loss: 1.0050305128097534\n",
      "Step 401 | Loss: 0.9904919266700745\n",
      "Step 501 | Loss: 1.0058362483978271\n",
      "Step 601 | Loss: 0.989424467086792\n",
      "Step 701 | Loss: 1.0087722539901733\n",
      "Step 801 | Loss: 1.002780795097351\n",
      "Step 901 | Loss: 1.0021088123321533\n",
      "Step 1001 | Loss: 1.0546066761016846\n",
      "Step 1101 | Loss: 0.9998820424079895\n",
      "Step 1201 | Loss: 0.9845232367515564\n",
      "Step 1301 | Loss: 1.0056304931640625\n",
      "Step 1401 | Loss: 1.0149178504943848\n",
      "Step 1501 | Loss: 1.012529969215393\n",
      "Step 1601 | Loss: 0.9985283613204956\n",
      "Step 1701 | Loss: 0.9966801404953003\n",
      "Step 1801 | Loss: 1.0208215713500977\n",
      "Step 1901 | Loss: 0.996098518371582\n",
      "Step 2001 | Loss: 0.9946906566619873\n",
      "Step 2101 | Loss: 0.9930092096328735\n",
      "Step 2201 | Loss: 1.0011409521102905\n",
      "Step 2301 | Loss: 0.9938211441040039\n",
      "Step 2401 | Loss: 0.9573124647140503\n",
      "1.004658986598541 0.5\n",
      "Step 1 | Loss: 1.0287692546844482\n",
      "Step 101 | Loss: 1.0112570524215698\n",
      "Step 201 | Loss: 0.997208833694458\n",
      "Step 301 | Loss: 1.006141185760498\n",
      "Step 401 | Loss: 0.996329665184021\n",
      "Step 501 | Loss: 0.9882417917251587\n",
      "Step 601 | Loss: 0.9736419916152954\n",
      "Step 701 | Loss: 0.9981629848480225\n",
      "Step 801 | Loss: 1.0002574920654297\n",
      "Step 901 | Loss: 0.9995087385177612\n",
      "Step 1001 | Loss: 0.9968237280845642\n",
      "Step 1101 | Loss: 1.01118004322052\n",
      "Step 1201 | Loss: 0.9783573746681213\n",
      "Step 1301 | Loss: 0.9919309020042419\n",
      "Step 1401 | Loss: 0.9973140954971313\n",
      "Step 1501 | Loss: 1.00019109249115\n",
      "Step 1601 | Loss: 1.0021297931671143\n",
      "Step 1701 | Loss: 1.00840163230896\n",
      "Step 1801 | Loss: 1.005706787109375\n",
      "Step 1901 | Loss: 0.9973962903022766\n",
      "Step 2001 | Loss: 1.0125621557235718\n",
      "Step 2101 | Loss: 1.0033855438232422\n",
      "Step 2201 | Loss: 1.009384036064148\n",
      "Step 2301 | Loss: 0.9911838173866272\n",
      "Step 2401 | Loss: 1.0273792743682861\n",
      "1.0044921066376729 0.5\n",
      "Step 1 | Loss: 2.2535552978515625\n",
      "Step 101 | Loss: 0.45507025718688965\n",
      "Step 201 | Loss: 0.6110423803329468\n",
      "Step 301 | Loss: 0.5563352108001709\n",
      "Step 401 | Loss: 0.5057607889175415\n",
      "Step 501 | Loss: 0.3921549916267395\n",
      "Step 601 | Loss: 0.5906904339790344\n",
      "Step 701 | Loss: 0.47829386591911316\n",
      "Step 801 | Loss: 0.20670273900032043\n",
      "Step 901 | Loss: 0.741866946220398\n",
      "Step 1001 | Loss: 0.38078925013542175\n",
      "Step 1101 | Loss: 0.6035343408584595\n",
      "Step 1201 | Loss: 0.4257670044898987\n",
      "Step 1301 | Loss: 0.33619239926338196\n",
      "Step 1401 | Loss: 0.4932714104652405\n",
      "Step 1501 | Loss: 0.5112314224243164\n",
      "Step 1601 | Loss: 0.5226754546165466\n",
      "Step 1701 | Loss: 0.34172874689102173\n",
      "Step 1801 | Loss: 0.48284590244293213\n",
      "Step 1901 | Loss: 0.4226216971874237\n",
      "Step 2001 | Loss: 0.4078238010406494\n",
      "Step 2101 | Loss: 0.49701401591300964\n",
      "Step 2201 | Loss: 0.4681878685951233\n",
      "Step 2301 | Loss: 0.5762461423873901\n",
      "Step 2401 | Loss: 0.37634146213531494\n",
      "0.44145247452177167 0.835\n",
      "Step 1 | Loss: 1.5175650119781494\n",
      "Step 101 | Loss: 0.5590468049049377\n",
      "Step 201 | Loss: 0.6612861752510071\n",
      "Step 301 | Loss: 0.30659106373786926\n",
      "Step 401 | Loss: 0.383031964302063\n",
      "Step 501 | Loss: 0.5380991101264954\n",
      "Step 601 | Loss: 0.3633711338043213\n",
      "Step 701 | Loss: 0.5103738903999329\n",
      "Step 801 | Loss: 0.33640170097351074\n",
      "Step 901 | Loss: 0.3078022599220276\n",
      "Step 1001 | Loss: 0.3468174636363983\n",
      "Step 1101 | Loss: 0.4347474277019501\n",
      "Step 1201 | Loss: 0.37513768672943115\n",
      "Step 1301 | Loss: 0.5123555660247803\n",
      "Step 1401 | Loss: 0.2614818215370178\n",
      "Step 1501 | Loss: 0.4633413851261139\n",
      "Step 1601 | Loss: 0.5657734274864197\n",
      "Step 1701 | Loss: 0.5526981353759766\n",
      "Step 1801 | Loss: 0.5350024700164795\n",
      "Step 1901 | Loss: 0.4176575541496277\n",
      "Step 2001 | Loss: 0.6690021753311157\n",
      "Step 2101 | Loss: 0.31958937644958496\n",
      "Step 2201 | Loss: 0.3725363612174988\n",
      "Step 2301 | Loss: 0.34522247314453125\n",
      "Step 2401 | Loss: 0.5513563752174377\n",
      "0.44160588511691873 0.835\n",
      "Step 1 | Loss: 0.6753741502761841\n",
      "Step 101 | Loss: 0.43512246012687683\n",
      "Step 201 | Loss: 0.39291658997535706\n",
      "Step 301 | Loss: 0.42254918813705444\n",
      "Step 401 | Loss: 0.5187414884567261\n",
      "Step 501 | Loss: 0.28860101103782654\n",
      "Step 601 | Loss: 0.30524352192878723\n",
      "Step 701 | Loss: 0.24899280071258545\n",
      "Step 801 | Loss: 0.5972742438316345\n",
      "Step 901 | Loss: 0.3723284900188446\n",
      "Step 1001 | Loss: 0.43780848383903503\n",
      "Step 1101 | Loss: 0.4926738142967224\n",
      "Step 1201 | Loss: 0.39021340012550354\n",
      "Step 1301 | Loss: 0.3955872654914856\n",
      "Step 1401 | Loss: 0.4525352418422699\n",
      "Step 1501 | Loss: 0.4392837882041931\n",
      "Step 1601 | Loss: 0.42708203196525574\n",
      "Step 1701 | Loss: 0.5701448321342468\n",
      "Step 1801 | Loss: 0.5104364156723022\n",
      "Step 1901 | Loss: 0.4869077801704407\n",
      "Step 2001 | Loss: 0.62548828125\n",
      "Step 2101 | Loss: 0.4294782280921936\n",
      "Step 2201 | Loss: 0.7259694337844849\n",
      "Step 2301 | Loss: 0.883952260017395\n",
      "Step 2401 | Loss: 0.31549257040023804\n",
      "0.4488308945977308 0.8475\n",
      "Step 1 | Loss: 1.6084057092666626\n",
      "Step 101 | Loss: 0.7419031262397766\n",
      "Step 201 | Loss: 0.3483215868473053\n",
      "Step 301 | Loss: 0.3024364709854126\n",
      "Step 401 | Loss: 0.43050438165664673\n",
      "Step 501 | Loss: 0.4956492483615875\n",
      "Step 601 | Loss: 0.4275667071342468\n",
      "Step 701 | Loss: 0.5367064476013184\n",
      "Step 801 | Loss: 0.4632788300514221\n",
      "Step 901 | Loss: 0.19131505489349365\n",
      "Step 1001 | Loss: 0.3166360855102539\n",
      "Step 1101 | Loss: 0.40592294931411743\n",
      "Step 1201 | Loss: 0.281566858291626\n",
      "Step 1301 | Loss: 0.47371160984039307\n",
      "Step 1401 | Loss: 0.5462867617607117\n",
      "Step 1501 | Loss: 0.35385581851005554\n",
      "Step 1601 | Loss: 0.4007610082626343\n",
      "Step 1701 | Loss: 0.5139505863189697\n",
      "Step 1801 | Loss: 0.49152520298957825\n",
      "Step 1901 | Loss: 0.4394680857658386\n",
      "Step 2001 | Loss: 0.6009108424186707\n",
      "Step 2101 | Loss: 0.482820600271225\n",
      "Step 2201 | Loss: 0.24335543811321259\n",
      "Step 2301 | Loss: 0.4448760449886322\n",
      "Step 2401 | Loss: 0.26960235834121704\n",
      "0.44950008504164857 0.85\n",
      "Step 1 | Loss: 2.060656785964966\n",
      "Step 101 | Loss: 1.1000269651412964\n",
      "Step 201 | Loss: 0.8821803331375122\n",
      "Step 301 | Loss: 0.3515469431877136\n",
      "Step 401 | Loss: 0.34506645798683167\n",
      "Step 501 | Loss: 0.35703328251838684\n",
      "Step 601 | Loss: 0.31249451637268066\n",
      "Step 701 | Loss: 0.47993046045303345\n",
      "Step 801 | Loss: 0.45297926664352417\n",
      "Step 901 | Loss: 0.4939362108707428\n",
      "Step 1001 | Loss: 0.36536693572998047\n",
      "Step 1101 | Loss: 0.6314464807510376\n",
      "Step 1201 | Loss: 0.41210055351257324\n",
      "Step 1301 | Loss: 0.2758685052394867\n",
      "Step 1401 | Loss: 0.37769263982772827\n",
      "Step 1501 | Loss: 0.4513619542121887\n",
      "Step 1601 | Loss: 0.1568794846534729\n",
      "Step 1701 | Loss: 0.39828404784202576\n",
      "Step 1801 | Loss: 0.3860260248184204\n",
      "Step 1901 | Loss: 0.4456498324871063\n",
      "Step 2001 | Loss: 0.4867456555366516\n",
      "Step 2101 | Loss: 0.5608910322189331\n",
      "Step 2201 | Loss: 0.4856499135494232\n",
      "Step 2301 | Loss: 0.43661513924598694\n",
      "Step 2401 | Loss: 0.30239880084991455\n",
      "0.4482083688010619 0.85\n",
      "Step 1 | Loss: 2.454542398452759\n",
      "Step 101 | Loss: 0.39005139470100403\n",
      "Step 201 | Loss: 0.2895975708961487\n",
      "Step 301 | Loss: 0.2614264190196991\n",
      "Step 401 | Loss: 0.16114206612110138\n",
      "Step 501 | Loss: 0.25523799657821655\n",
      "Step 601 | Loss: 0.1641407459974289\n",
      "Step 701 | Loss: 0.4002178907394409\n",
      "Step 801 | Loss: 0.2978682816028595\n",
      "Step 901 | Loss: 0.3366449475288391\n",
      "Step 1001 | Loss: 0.3246248960494995\n",
      "Step 1101 | Loss: 0.33589601516723633\n",
      "Step 1201 | Loss: 0.20772993564605713\n",
      "Step 1301 | Loss: 0.35405412316322327\n",
      "Step 1401 | Loss: 0.3269246220588684\n",
      "Step 1501 | Loss: 0.16694143414497375\n",
      "Step 1601 | Loss: 0.1801077127456665\n",
      "Step 1701 | Loss: 0.20348572731018066\n",
      "Step 1801 | Loss: 0.23581752181053162\n",
      "Step 1901 | Loss: 0.3051276206970215\n",
      "Step 2001 | Loss: 0.2796395421028137\n",
      "Step 2101 | Loss: 0.32834723591804504\n",
      "Step 2201 | Loss: 0.24023699760437012\n",
      "Step 2301 | Loss: 0.17572574317455292\n",
      "Step 2401 | Loss: 0.2382284551858902\n",
      "0.2597727456923686 0.9475\n",
      "Step 1 | Loss: 1.1452927589416504\n",
      "Step 101 | Loss: 0.27869662642478943\n",
      "Step 201 | Loss: 0.42090073227882385\n",
      "Step 301 | Loss: 0.26108479499816895\n",
      "Step 401 | Loss: 0.23088014125823975\n",
      "Step 501 | Loss: 0.3603898882865906\n",
      "Step 601 | Loss: 0.23460090160369873\n",
      "Step 701 | Loss: 0.362014502286911\n",
      "Step 801 | Loss: 0.21863296627998352\n",
      "Step 901 | Loss: 0.1894019991159439\n",
      "Step 1001 | Loss: 0.2006676346063614\n",
      "Step 1101 | Loss: 0.1923033446073532\n",
      "Step 1201 | Loss: 0.3235057592391968\n",
      "Step 1301 | Loss: 0.3058054745197296\n",
      "Step 1401 | Loss: 0.20726826786994934\n",
      "Step 1501 | Loss: 0.2518180310726166\n",
      "Step 1601 | Loss: 0.30876967310905457\n",
      "Step 1701 | Loss: 0.24690766632556915\n",
      "Step 1801 | Loss: 0.20091882348060608\n",
      "Step 1901 | Loss: 0.19537322223186493\n",
      "Step 2001 | Loss: 0.4664439558982849\n",
      "Step 2101 | Loss: 0.2347181886434555\n",
      "Step 2201 | Loss: 0.29083341360092163\n",
      "Step 2301 | Loss: 0.4181285798549652\n",
      "Step 2401 | Loss: 0.26780229806900024\n",
      "0.25982220239560366 0.945\n",
      "Step 1 | Loss: 1.274234652519226\n",
      "Step 101 | Loss: 0.3418300747871399\n",
      "Step 201 | Loss: 0.38740453124046326\n",
      "Step 301 | Loss: 0.25511443614959717\n",
      "Step 401 | Loss: 0.2406926155090332\n",
      "Step 501 | Loss: 0.2319718450307846\n",
      "Step 601 | Loss: 0.2723207175731659\n",
      "Step 701 | Loss: 0.2551732063293457\n",
      "Step 801 | Loss: 0.2447262853384018\n",
      "Step 901 | Loss: 0.2934989035129547\n",
      "Step 1001 | Loss: 0.19040057063102722\n",
      "Step 1101 | Loss: 0.2490827888250351\n",
      "Step 1201 | Loss: 0.25338801741600037\n",
      "Step 1301 | Loss: 0.20279306173324585\n",
      "Step 1401 | Loss: 0.408672034740448\n",
      "Step 1501 | Loss: 0.2352984994649887\n",
      "Step 1601 | Loss: 0.3448989689350128\n",
      "Step 1701 | Loss: 0.2869859039783478\n",
      "Step 1801 | Loss: 0.3299366235733032\n",
      "Step 1901 | Loss: 0.21060748398303986\n",
      "Step 2001 | Loss: 0.2033945620059967\n",
      "Step 2101 | Loss: 0.35737818479537964\n",
      "Step 2201 | Loss: 0.2752956748008728\n",
      "Step 2301 | Loss: 0.288433700799942\n",
      "Step 2401 | Loss: 0.25713080167770386\n",
      "0.2599033650294429 0.9425\n",
      "Step 1 | Loss: 2.0486533641815186\n",
      "Step 101 | Loss: 0.2739499509334564\n",
      "Step 201 | Loss: 0.23875173926353455\n",
      "Step 301 | Loss: 0.28973570466041565\n",
      "Step 401 | Loss: 0.3177799582481384\n",
      "Step 501 | Loss: 0.3726799786090851\n",
      "Step 601 | Loss: 0.2642684280872345\n",
      "Step 701 | Loss: 0.15049594640731812\n",
      "Step 801 | Loss: 0.40036511421203613\n",
      "Step 901 | Loss: 0.25171804428100586\n",
      "Step 1001 | Loss: 0.19570350646972656\n",
      "Step 1101 | Loss: 0.2628241777420044\n",
      "Step 1201 | Loss: 0.27991992235183716\n",
      "Step 1301 | Loss: 0.4136420786380768\n",
      "Step 1401 | Loss: 0.20405744016170502\n",
      "Step 1501 | Loss: 0.23113568127155304\n",
      "Step 1601 | Loss: 0.22901925444602966\n",
      "Step 1701 | Loss: 0.26366135478019714\n",
      "Step 1801 | Loss: 0.3546167016029358\n",
      "Step 1901 | Loss: 0.29565727710723877\n",
      "Step 2001 | Loss: 0.29028335213661194\n",
      "Step 2101 | Loss: 0.23545005917549133\n",
      "Step 2201 | Loss: 0.35832735896110535\n",
      "Step 2301 | Loss: 0.3356390595436096\n",
      "Step 2401 | Loss: 0.266313761472702\n",
      "0.2598806394499581 0.95\n",
      "Step 1 | Loss: 0.7460103034973145\n",
      "Step 101 | Loss: 0.3063809871673584\n",
      "Step 201 | Loss: 0.2237333208322525\n",
      "Step 301 | Loss: 0.25554734468460083\n",
      "Step 401 | Loss: 0.2453824281692505\n",
      "Step 501 | Loss: 0.2896586060523987\n",
      "Step 601 | Loss: 0.2994210124015808\n",
      "Step 701 | Loss: 0.3119231164455414\n",
      "Step 801 | Loss: 0.21225397288799286\n",
      "Step 901 | Loss: 0.1807699352502823\n",
      "Step 1001 | Loss: 0.25524818897247314\n",
      "Step 1101 | Loss: 0.33987724781036377\n",
      "Step 1201 | Loss: 0.19431930780410767\n",
      "Step 1301 | Loss: 0.2510218024253845\n",
      "Step 1401 | Loss: 0.2020283043384552\n",
      "Step 1501 | Loss: 0.37571462988853455\n",
      "Step 1601 | Loss: 0.26370880007743835\n",
      "Step 1701 | Loss: 0.28104105591773987\n",
      "Step 1801 | Loss: 0.24902330338954926\n",
      "Step 1901 | Loss: 0.21991762518882751\n",
      "Step 2001 | Loss: 0.4093554615974426\n",
      "Step 2101 | Loss: 0.1562345027923584\n",
      "Step 2201 | Loss: 0.19841687381267548\n",
      "Step 2301 | Loss: 0.2743724286556244\n",
      "Step 2401 | Loss: 0.2595187723636627\n",
      "0.2599659168678525 0.94\n",
      "Step 1 | Loss: 1.4820430278778076\n",
      "Step 101 | Loss: 0.38470694422721863\n",
      "Step 201 | Loss: 0.22293204069137573\n",
      "Step 301 | Loss: 0.25058645009994507\n",
      "Step 401 | Loss: 0.39434272050857544\n",
      "Step 501 | Loss: 0.4110138714313507\n",
      "Step 601 | Loss: 0.2317938506603241\n",
      "Step 701 | Loss: 0.2472464144229889\n",
      "Step 801 | Loss: 0.2698304057121277\n",
      "Step 901 | Loss: 0.13886326551437378\n",
      "Step 1001 | Loss: 0.30498409271240234\n",
      "Step 1101 | Loss: 0.2732633352279663\n",
      "Step 1201 | Loss: 0.3107946217060089\n",
      "Step 1301 | Loss: 0.2547709345817566\n",
      "Step 1401 | Loss: 0.20895397663116455\n",
      "Step 1501 | Loss: 0.245341494679451\n",
      "Step 1601 | Loss: 0.21836858987808228\n",
      "Step 1701 | Loss: 0.2769653797149658\n",
      "Step 1801 | Loss: 0.2781858444213867\n",
      "Step 1901 | Loss: 0.23475471138954163\n",
      "Step 2001 | Loss: 0.3101842999458313\n",
      "Step 2101 | Loss: 0.2663581371307373\n",
      "Step 2201 | Loss: 0.2595900297164917\n",
      "Step 2301 | Loss: 0.2742658257484436\n",
      "Step 2401 | Loss: 0.19708004593849182\n",
      "0.27257840895581187 0.965\n",
      "Step 1 | Loss: 1.2315095663070679\n",
      "Step 101 | Loss: 0.31637489795684814\n",
      "Step 201 | Loss: 0.2795496881008148\n",
      "Step 301 | Loss: 0.2775074541568756\n",
      "Step 401 | Loss: 0.2952534258365631\n",
      "Step 501 | Loss: 0.21447448432445526\n",
      "Step 601 | Loss: 0.2686980366706848\n",
      "Step 701 | Loss: 0.2625569701194763\n",
      "Step 801 | Loss: 0.18594032526016235\n",
      "Step 901 | Loss: 0.21947360038757324\n",
      "Step 1001 | Loss: 0.4096184968948364\n",
      "Step 1101 | Loss: 0.24592989683151245\n",
      "Step 1201 | Loss: 0.2594138979911804\n",
      "Step 1301 | Loss: 0.28952398896217346\n",
      "Step 1401 | Loss: 0.22148409485816956\n",
      "Step 1501 | Loss: 0.36755993962287903\n",
      "Step 1601 | Loss: 0.30434370040893555\n",
      "Step 1701 | Loss: 0.31545907258987427\n",
      "Step 1801 | Loss: 0.31756791472435\n",
      "Step 1901 | Loss: 0.33146312832832336\n",
      "Step 2001 | Loss: 0.29338720440864563\n",
      "Step 2101 | Loss: 0.25751110911369324\n",
      "Step 2201 | Loss: 0.2010141760110855\n",
      "Step 2301 | Loss: 0.22250664234161377\n",
      "Step 2401 | Loss: 0.3974030911922455\n",
      "0.2745306613565638 0.9625\n",
      "Step 1 | Loss: 0.4987839460372925\n",
      "Step 101 | Loss: 0.29763880372047424\n",
      "Step 201 | Loss: 0.30675244331359863\n",
      "Step 301 | Loss: 0.1196814626455307\n",
      "Step 401 | Loss: 0.27938905358314514\n",
      "Step 501 | Loss: 0.35104188323020935\n",
      "Step 601 | Loss: 0.29383644461631775\n",
      "Step 701 | Loss: 0.36866986751556396\n",
      "Step 801 | Loss: 0.17519932985305786\n",
      "Step 901 | Loss: 0.24566753208637238\n",
      "Step 1001 | Loss: 0.3515697121620178\n",
      "Step 1101 | Loss: 0.34241437911987305\n",
      "Step 1201 | Loss: 0.302739679813385\n",
      "Step 1301 | Loss: 0.21134376525878906\n",
      "Step 1401 | Loss: 0.32207727432250977\n",
      "Step 1501 | Loss: 0.27683907747268677\n",
      "Step 1601 | Loss: 0.31295904517173767\n",
      "Step 1701 | Loss: 0.26938265562057495\n",
      "Step 1801 | Loss: 0.2591193616390228\n",
      "Step 1901 | Loss: 0.1594626009464264\n",
      "Step 2001 | Loss: 0.4545010030269623\n",
      "Step 2101 | Loss: 0.2551562786102295\n",
      "Step 2201 | Loss: 0.20406116545200348\n",
      "Step 2301 | Loss: 0.3502391576766968\n",
      "Step 2401 | Loss: 0.3371697664260864\n",
      "0.27338424001309175 0.97\n",
      "Step 1 | Loss: 0.9585686922073364\n",
      "Step 101 | Loss: 0.3405027687549591\n",
      "Step 201 | Loss: 0.2915673851966858\n",
      "Step 301 | Loss: 0.3302948474884033\n",
      "Step 401 | Loss: 0.3752067983150482\n",
      "Step 501 | Loss: 0.3222600519657135\n",
      "Step 601 | Loss: 0.27190113067626953\n",
      "Step 701 | Loss: 0.30758288502693176\n",
      "Step 801 | Loss: 0.2589774429798126\n",
      "Step 901 | Loss: 0.18423055112361908\n",
      "Step 1001 | Loss: 0.3009975254535675\n",
      "Step 1101 | Loss: 0.20268191397190094\n",
      "Step 1201 | Loss: 0.38297614455223083\n",
      "Step 1301 | Loss: 0.233955517411232\n",
      "Step 1401 | Loss: 0.2242448627948761\n",
      "Step 1501 | Loss: 0.1913391351699829\n",
      "Step 1601 | Loss: 0.22361750900745392\n",
      "Step 1701 | Loss: 0.19431960582733154\n",
      "Step 1801 | Loss: 0.2345755398273468\n",
      "Step 1901 | Loss: 0.24585482478141785\n",
      "Step 2001 | Loss: 0.3696329891681671\n",
      "Step 2101 | Loss: 0.2609409987926483\n",
      "Step 2201 | Loss: 0.2990607023239136\n",
      "Step 2301 | Loss: 0.21101631224155426\n",
      "Step 2401 | Loss: 0.20971371233463287\n",
      "0.2757921990789878 0.97\n",
      "Step 1 | Loss: 1.8695341348648071\n",
      "Step 101 | Loss: 0.2628346085548401\n",
      "Step 201 | Loss: 0.29713475704193115\n",
      "Step 301 | Loss: 0.22980394959449768\n",
      "Step 401 | Loss: 0.26684221625328064\n",
      "Step 501 | Loss: 0.4252933859825134\n",
      "Step 601 | Loss: 0.30127739906311035\n",
      "Step 701 | Loss: 0.3797209560871124\n",
      "Step 801 | Loss: 0.28657400608062744\n",
      "Step 901 | Loss: 0.20753256976604462\n",
      "Step 1001 | Loss: 0.23842942714691162\n",
      "Step 1101 | Loss: 0.2699134051799774\n",
      "Step 1201 | Loss: 0.3017764687538147\n",
      "Step 1301 | Loss: 0.2875564396381378\n",
      "Step 1401 | Loss: 0.2507496178150177\n",
      "Step 1501 | Loss: 0.3437485694885254\n",
      "Step 1601 | Loss: 0.2193596065044403\n",
      "Step 1701 | Loss: 0.25017687678337097\n",
      "Step 1801 | Loss: 0.21432530879974365\n",
      "Step 1901 | Loss: 0.2653287947177887\n",
      "Step 2001 | Loss: 0.24361011385917664\n",
      "Step 2101 | Loss: 0.3669145405292511\n",
      "Step 2201 | Loss: 0.2601836323738098\n",
      "Step 2301 | Loss: 0.38072288036346436\n",
      "Step 2401 | Loss: 0.345412015914917\n",
      "0.2774123790660379 0.9675\n",
      "Step 1 | Loss: 1.5131556987762451\n",
      "Step 101 | Loss: 0.857642650604248\n",
      "Step 201 | Loss: 0.7748156785964966\n",
      "Step 301 | Loss: 0.526165783405304\n",
      "Step 401 | Loss: 0.6983902454376221\n",
      "Step 501 | Loss: 0.7716898918151855\n",
      "Step 601 | Loss: 0.7206639051437378\n",
      "Step 701 | Loss: 0.6635125279426575\n",
      "Step 801 | Loss: 0.6028346419334412\n",
      "Step 901 | Loss: 0.6158062815666199\n",
      "Step 1001 | Loss: 0.4328334331512451\n",
      "Step 1101 | Loss: 0.4834878742694855\n",
      "Step 1201 | Loss: 0.5789769887924194\n",
      "Step 1301 | Loss: 0.6799172759056091\n",
      "Step 1401 | Loss: 0.4829495847225189\n",
      "Step 1501 | Loss: 0.4408050775527954\n",
      "Step 1601 | Loss: 0.5251139402389526\n",
      "Step 1701 | Loss: 0.5997471213340759\n",
      "Step 1801 | Loss: 0.5693570971488953\n",
      "Step 1901 | Loss: 0.46104395389556885\n",
      "Step 2001 | Loss: 0.5279912352561951\n",
      "Step 2101 | Loss: 0.5206169486045837\n",
      "Step 2201 | Loss: 0.5469416379928589\n",
      "Step 2301 | Loss: 0.5421174168586731\n",
      "Step 2401 | Loss: 0.47100433707237244\n",
      "0.5753105675856602 0.8325\n",
      "Step 1 | Loss: 1.1333959102630615\n",
      "Step 101 | Loss: 0.6507178544998169\n",
      "Step 201 | Loss: 0.6015204191207886\n",
      "Step 301 | Loss: 0.6149060726165771\n",
      "Step 401 | Loss: 0.38339757919311523\n",
      "Step 501 | Loss: 0.43242475390434265\n",
      "Step 601 | Loss: 0.43870630860328674\n",
      "Step 701 | Loss: 0.4485691487789154\n",
      "Step 801 | Loss: 0.4350360631942749\n",
      "Step 901 | Loss: 0.4108259379863739\n",
      "Step 1001 | Loss: 0.4862751066684723\n",
      "Step 1101 | Loss: 0.44472140073776245\n",
      "Step 1201 | Loss: 0.4577988386154175\n",
      "Step 1301 | Loss: 0.30967530608177185\n",
      "Step 1401 | Loss: 0.4486426115036011\n",
      "Step 1501 | Loss: 0.3484666049480438\n",
      "Step 1601 | Loss: 0.4453199505805969\n",
      "Step 1701 | Loss: 0.44688668847084045\n",
      "Step 1801 | Loss: 0.4754353165626526\n",
      "Step 1901 | Loss: 0.35019275546073914\n",
      "Step 2001 | Loss: 0.3384894132614136\n",
      "Step 2101 | Loss: 0.31332096457481384\n",
      "Step 2201 | Loss: 0.4893772304058075\n",
      "Step 2301 | Loss: 0.4078514575958252\n",
      "Step 2401 | Loss: 0.2579699158668518\n",
      "0.4419210255292628 0.92\n",
      "Step 1 | Loss: 1.1146368980407715\n",
      "Step 101 | Loss: 0.5545604228973389\n",
      "Step 201 | Loss: 0.5420485734939575\n",
      "Step 301 | Loss: 0.6089504361152649\n",
      "Step 401 | Loss: 0.5552167892456055\n",
      "Step 501 | Loss: 0.6162943840026855\n",
      "Step 601 | Loss: 0.5286703109741211\n",
      "Step 701 | Loss: 0.4419974386692047\n",
      "Step 801 | Loss: 0.5870305895805359\n",
      "Step 901 | Loss: 0.5718258619308472\n",
      "Step 1001 | Loss: 0.5432127714157104\n",
      "Step 1101 | Loss: 0.35369348526000977\n",
      "Step 1201 | Loss: 0.6450574994087219\n",
      "Step 1301 | Loss: 0.4806497097015381\n",
      "Step 1401 | Loss: 0.4301435351371765\n",
      "Step 1501 | Loss: 0.41936370730400085\n",
      "Step 1601 | Loss: 0.4523204565048218\n",
      "Step 1701 | Loss: 0.4546304941177368\n",
      "Step 1801 | Loss: 0.3820059597492218\n",
      "Step 1901 | Loss: 0.4392630457878113\n",
      "Step 2001 | Loss: 0.37171703577041626\n",
      "Step 2101 | Loss: 0.3238363564014435\n",
      "Step 2201 | Loss: 0.6091812252998352\n",
      "Step 2301 | Loss: 0.3935839533805847\n",
      "Step 2401 | Loss: 0.4306877851486206\n",
      "0.47956319402242636 0.9075\n",
      "Step 1 | Loss: 1.2188208103179932\n",
      "Step 101 | Loss: 0.9891715049743652\n",
      "Step 201 | Loss: 0.7006497979164124\n",
      "Step 301 | Loss: 0.5579348206520081\n",
      "Step 401 | Loss: 0.44034138321876526\n",
      "Step 501 | Loss: 0.5134459137916565\n",
      "Step 601 | Loss: 0.5660473704338074\n",
      "Step 701 | Loss: 0.5153101682662964\n",
      "Step 801 | Loss: 0.5567148327827454\n",
      "Step 901 | Loss: 0.413404256105423\n",
      "Step 1001 | Loss: 0.513189971446991\n",
      "Step 1101 | Loss: 0.4246029555797577\n",
      "Step 1201 | Loss: 0.49087992310523987\n",
      "Step 1301 | Loss: 0.4731979966163635\n",
      "Step 1401 | Loss: 0.4460553526878357\n",
      "Step 1501 | Loss: 0.4310940206050873\n",
      "Step 1601 | Loss: 0.5003515481948853\n",
      "Step 1701 | Loss: 0.48839789628982544\n",
      "Step 1801 | Loss: 0.3927121162414551\n",
      "Step 1901 | Loss: 0.4390740692615509\n",
      "Step 2001 | Loss: 0.44742605090141296\n",
      "Step 2101 | Loss: 0.48762813210487366\n",
      "Step 2201 | Loss: 0.4234376549720764\n",
      "Step 2301 | Loss: 0.442138671875\n",
      "Step 2401 | Loss: 0.3956736624240875\n",
      "0.4809517730996364 0.9\n",
      "Step 1 | Loss: 0.9894453883171082\n",
      "Step 101 | Loss: 0.6554586291313171\n",
      "Step 201 | Loss: 0.5899695754051208\n",
      "Step 301 | Loss: 0.5139740705490112\n",
      "Step 401 | Loss: 0.5395686030387878\n",
      "Step 501 | Loss: 0.4551701545715332\n",
      "Step 601 | Loss: 0.5778586864471436\n",
      "Step 701 | Loss: 0.5042396187782288\n",
      "Step 801 | Loss: 0.4744129478931427\n",
      "Step 901 | Loss: 0.4382627010345459\n",
      "Step 1001 | Loss: 0.437508225440979\n",
      "Step 1101 | Loss: 0.46762269735336304\n",
      "Step 1201 | Loss: 0.485119104385376\n",
      "Step 1301 | Loss: 0.5307052731513977\n",
      "Step 1401 | Loss: 0.3649739623069763\n",
      "Step 1501 | Loss: 0.5216638445854187\n",
      "Step 1601 | Loss: 0.47018545866012573\n",
      "Step 1701 | Loss: 0.5227263569831848\n",
      "Step 1801 | Loss: 0.5085070729255676\n",
      "Step 1901 | Loss: 0.37246981263160706\n",
      "Step 2001 | Loss: 0.4755568206310272\n",
      "Step 2101 | Loss: 0.50382399559021\n",
      "Step 2201 | Loss: 0.5013756155967712\n",
      "Step 2301 | Loss: 0.35240811109542847\n",
      "Step 2401 | Loss: 0.3836383819580078\n",
      "0.4841996016098444 0.8825\n",
      "Step 1 | Loss: 1.0898364782333374\n",
      "Step 101 | Loss: 0.3075072765350342\n",
      "Step 201 | Loss: 0.3453258275985718\n",
      "Step 301 | Loss: 0.3532031774520874\n",
      "Step 401 | Loss: 0.3506965935230255\n",
      "Step 501 | Loss: 0.23090961575508118\n",
      "Step 601 | Loss: 0.3080354332923889\n",
      "Step 701 | Loss: 0.32536545395851135\n",
      "Step 801 | Loss: 0.37295591831207275\n",
      "Step 901 | Loss: 0.2891775071620941\n",
      "Step 1001 | Loss: 0.36266303062438965\n",
      "Step 1101 | Loss: 0.2876666784286499\n",
      "Step 1201 | Loss: 0.34368789196014404\n",
      "Step 1301 | Loss: 0.33225688338279724\n",
      "Step 1401 | Loss: 0.25285303592681885\n",
      "Step 1501 | Loss: 0.3266844153404236\n",
      "Step 1601 | Loss: 0.44204476475715637\n",
      "Step 1701 | Loss: 0.24192792177200317\n",
      "Step 1801 | Loss: 0.456524521112442\n",
      "Step 1901 | Loss: 0.1793774664402008\n",
      "Step 2001 | Loss: 0.30689308047294617\n",
      "Step 2101 | Loss: 0.3190363347530365\n",
      "Step 2201 | Loss: 0.2614086866378784\n",
      "Step 2301 | Loss: 0.2937244772911072\n",
      "Step 2401 | Loss: 0.3209269344806671\n",
      "0.3782144509675155 0.915\n",
      "Step 1 | Loss: 0.7269744873046875\n",
      "Step 101 | Loss: 0.3640899062156677\n",
      "Step 201 | Loss: 0.35000061988830566\n",
      "Step 301 | Loss: 0.2675181031227112\n",
      "Step 401 | Loss: 0.38822034001350403\n",
      "Step 501 | Loss: 0.2183230072259903\n",
      "Step 601 | Loss: 0.32927146553993225\n",
      "Step 701 | Loss: 0.2875150442123413\n",
      "Step 801 | Loss: 0.3958997130393982\n",
      "Step 901 | Loss: 0.2862461805343628\n",
      "Step 1001 | Loss: 0.3787321150302887\n",
      "Step 1101 | Loss: 0.3450462520122528\n",
      "Step 1201 | Loss: 0.3849005103111267\n",
      "Step 1301 | Loss: 0.3693661689758301\n",
      "Step 1401 | Loss: 0.3662068545818329\n",
      "Step 1501 | Loss: 0.2931485176086426\n",
      "Step 1601 | Loss: 0.31385907530784607\n",
      "Step 1701 | Loss: 0.3336997926235199\n",
      "Step 1801 | Loss: 0.3136592507362366\n",
      "Step 1901 | Loss: 0.3052668273448944\n",
      "Step 2001 | Loss: 0.32841041684150696\n",
      "Step 2101 | Loss: 0.41539257764816284\n",
      "Step 2201 | Loss: 0.3853861391544342\n",
      "Step 2301 | Loss: 0.33010417222976685\n",
      "Step 2401 | Loss: 0.4760589599609375\n",
      "0.37787168235429536 0.915\n",
      "Step 1 | Loss: 1.077319860458374\n",
      "Step 101 | Loss: 0.3997815251350403\n",
      "Step 201 | Loss: 0.41313469409942627\n",
      "Step 301 | Loss: 0.2934013307094574\n",
      "Step 401 | Loss: 0.3766588568687439\n",
      "Step 501 | Loss: 0.4282402992248535\n",
      "Step 601 | Loss: 0.3549982011318207\n",
      "Step 701 | Loss: 0.5203734636306763\n",
      "Step 801 | Loss: 0.24613144993782043\n",
      "Step 901 | Loss: 0.3703928589820862\n",
      "Step 1001 | Loss: 0.37332865595817566\n",
      "Step 1101 | Loss: 0.21021020412445068\n",
      "Step 1201 | Loss: 0.3692428171634674\n",
      "Step 1301 | Loss: 0.33473432064056396\n",
      "Step 1401 | Loss: 0.3678334951400757\n",
      "Step 1501 | Loss: 0.29365354776382446\n",
      "Step 1601 | Loss: 0.26979416608810425\n",
      "Step 1701 | Loss: 0.3921806812286377\n",
      "Step 1801 | Loss: 0.3589704632759094\n",
      "Step 1901 | Loss: 0.29876506328582764\n",
      "Step 2001 | Loss: 0.30775949358940125\n",
      "Step 2101 | Loss: 0.2880003750324249\n",
      "Step 2201 | Loss: 0.3137849271297455\n",
      "Step 2301 | Loss: 0.3495595157146454\n",
      "Step 2401 | Loss: 0.40180450677871704\n",
      "0.38085867809227214 0.9075\n",
      "Step 1 | Loss: 0.6511565446853638\n",
      "Step 101 | Loss: 0.36243993043899536\n",
      "Step 201 | Loss: 0.37152519822120667\n",
      "Step 301 | Loss: 0.4468848705291748\n",
      "Step 401 | Loss: 0.3765566349029541\n",
      "Step 501 | Loss: 0.3475104868412018\n",
      "Step 601 | Loss: 0.36626386642456055\n",
      "Step 701 | Loss: 0.3039954900741577\n",
      "Step 801 | Loss: 0.5670257210731506\n",
      "Step 901 | Loss: 0.22353683412075043\n",
      "Step 1001 | Loss: 0.3275910019874573\n",
      "Step 1101 | Loss: 0.32889729738235474\n",
      "Step 1201 | Loss: 0.35408419370651245\n",
      "Step 1301 | Loss: 0.2784775197505951\n",
      "Step 1401 | Loss: 0.3288925290107727\n",
      "Step 1501 | Loss: 0.3987257778644562\n",
      "Step 1601 | Loss: 0.3842623233795166\n",
      "Step 1701 | Loss: 0.2989388704299927\n",
      "Step 1801 | Loss: 0.30861973762512207\n",
      "Step 1901 | Loss: 0.3398876488208771\n",
      "Step 2001 | Loss: 0.3041137754917145\n",
      "Step 2101 | Loss: 0.29272085428237915\n",
      "Step 2201 | Loss: 0.3680478632450104\n",
      "Step 2301 | Loss: 0.3404620289802551\n",
      "Step 2401 | Loss: 0.40373510122299194\n",
      "0.3781143068284451 0.915\n",
      "Step 1 | Loss: 1.6031376123428345\n",
      "Step 101 | Loss: 0.37818312644958496\n",
      "Step 201 | Loss: 0.38833457231521606\n",
      "Step 301 | Loss: 0.2933200001716614\n",
      "Step 401 | Loss: 0.3438031077384949\n",
      "Step 501 | Loss: 0.41901853680610657\n",
      "Step 601 | Loss: 0.29140371084213257\n",
      "Step 701 | Loss: 0.3322147727012634\n",
      "Step 801 | Loss: 0.4342282712459564\n",
      "Step 901 | Loss: 0.45108386874198914\n",
      "Step 1001 | Loss: 0.23086662590503693\n",
      "Step 1101 | Loss: 0.3321874141693115\n",
      "Step 1201 | Loss: 0.3014987111091614\n",
      "Step 1301 | Loss: 0.2660589814186096\n",
      "Step 1401 | Loss: 0.293683797121048\n",
      "Step 1501 | Loss: 0.365175724029541\n",
      "Step 1601 | Loss: 0.3236950933933258\n",
      "Step 1701 | Loss: 0.27456918358802795\n",
      "Step 1801 | Loss: 0.38983750343322754\n",
      "Step 1901 | Loss: 0.2504615783691406\n",
      "Step 2001 | Loss: 0.34296727180480957\n",
      "Step 2101 | Loss: 0.3816714882850647\n",
      "Step 2201 | Loss: 0.2964172661304474\n",
      "Step 2301 | Loss: 0.29512113332748413\n",
      "Step 2401 | Loss: 0.36377811431884766\n",
      "0.3826550416131597 0.9\n",
      "Step 1 | Loss: 1.1452885866165161\n",
      "Step 101 | Loss: 0.8645483255386353\n",
      "Step 201 | Loss: 0.5455929040908813\n",
      "Step 301 | Loss: 0.6997959017753601\n",
      "Step 401 | Loss: 0.48478662967681885\n",
      "Step 501 | Loss: 0.4880484938621521\n",
      "Step 601 | Loss: 0.6032220125198364\n",
      "Step 701 | Loss: 0.41988763213157654\n",
      "Step 801 | Loss: 0.6894124746322632\n",
      "Step 901 | Loss: 0.560248613357544\n",
      "Step 1001 | Loss: 0.6169460415840149\n",
      "Step 1101 | Loss: 0.6830577254295349\n",
      "Step 1201 | Loss: 0.701326310634613\n",
      "Step 1301 | Loss: 0.4620819389820099\n",
      "Step 1401 | Loss: 0.5421338677406311\n",
      "Step 1501 | Loss: 0.6304064989089966\n",
      "Step 1601 | Loss: 0.4643249213695526\n",
      "Step 1701 | Loss: 0.638671875\n",
      "Step 1801 | Loss: 0.5760111212730408\n",
      "Step 1901 | Loss: 0.7344607710838318\n",
      "Step 2001 | Loss: 0.8550392389297485\n",
      "Step 2101 | Loss: 0.7067427039146423\n",
      "Step 2201 | Loss: 0.4599382281303406\n",
      "Step 2301 | Loss: 0.7883110642433167\n",
      "Step 2401 | Loss: 0.8079731464385986\n",
      "0.5812287662291209 0.7625\n",
      "Step 1 | Loss: 1.0366883277893066\n",
      "Step 101 | Loss: 1.0226199626922607\n",
      "Step 201 | Loss: 0.4206281006336212\n",
      "Step 301 | Loss: 0.24175304174423218\n",
      "Step 401 | Loss: 0.2436005175113678\n",
      "Step 501 | Loss: 0.2796153724193573\n",
      "Step 601 | Loss: 0.29493558406829834\n",
      "Step 701 | Loss: 0.2743355929851532\n",
      "Step 801 | Loss: 0.15839557349681854\n",
      "Step 901 | Loss: 0.21843180060386658\n",
      "Step 1001 | Loss: 0.22855347394943237\n",
      "Step 1101 | Loss: 0.2538197338581085\n",
      "Step 1201 | Loss: 0.24930399656295776\n",
      "Step 1301 | Loss: 0.17049354314804077\n",
      "Step 1401 | Loss: 0.30431681871414185\n",
      "Step 1501 | Loss: 0.19101546704769135\n",
      "Step 1601 | Loss: 0.29743286967277527\n",
      "Step 1701 | Loss: 0.23751403391361237\n",
      "Step 1801 | Loss: 0.261311799287796\n",
      "Step 1901 | Loss: 0.1881229132413864\n",
      "Step 2001 | Loss: 0.22822456061840057\n",
      "Step 2101 | Loss: 0.23410840332508087\n",
      "Step 2201 | Loss: 0.24435511231422424\n",
      "Step 2301 | Loss: 0.23924998939037323\n",
      "Step 2401 | Loss: 0.3104897737503052\n",
      "0.24816579949661957 0.9725\n",
      "Step 1 | Loss: 1.4531011581420898\n",
      "Step 101 | Loss: 0.5421980023384094\n",
      "Step 201 | Loss: 0.6295555830001831\n",
      "Step 301 | Loss: 0.4997374713420868\n",
      "Step 401 | Loss: 0.5245761275291443\n",
      "Step 501 | Loss: 0.6037589311599731\n",
      "Step 601 | Loss: 0.5138348340988159\n",
      "Step 701 | Loss: 0.6856176853179932\n",
      "Step 801 | Loss: 0.4376208186149597\n",
      "Step 901 | Loss: 0.5016588568687439\n",
      "Step 1001 | Loss: 0.5257279276847839\n",
      "Step 1101 | Loss: 0.6056403517723083\n",
      "Step 1201 | Loss: 0.5932332873344421\n",
      "Step 1301 | Loss: 0.44763028621673584\n",
      "Step 1401 | Loss: 0.42456120252609253\n",
      "Step 1501 | Loss: 0.5877196788787842\n",
      "Step 1601 | Loss: 0.5005991458892822\n",
      "Step 1701 | Loss: 0.5245458483695984\n",
      "Step 1801 | Loss: 0.4866275489330292\n",
      "Step 1901 | Loss: 0.48375874757766724\n",
      "Step 2001 | Loss: 0.628123939037323\n",
      "Step 2101 | Loss: 0.5836366415023804\n",
      "Step 2201 | Loss: 0.4946807324886322\n",
      "Step 2301 | Loss: 0.7044690847396851\n",
      "Step 2401 | Loss: 0.4387474060058594\n",
      "0.4889090275203717 0.82\n",
      "Step 1 | Loss: 0.9520489573478699\n",
      "Step 101 | Loss: 0.39921799302101135\n",
      "Step 201 | Loss: 0.31877225637435913\n",
      "Step 301 | Loss: 0.22578099370002747\n",
      "Step 401 | Loss: 0.2906908690929413\n",
      "Step 501 | Loss: 0.3712478578090668\n",
      "Step 601 | Loss: 0.20003315806388855\n",
      "Step 701 | Loss: 0.23160208761692047\n",
      "Step 801 | Loss: 0.19342871010303497\n",
      "Step 901 | Loss: 0.3194672465324402\n",
      "Step 1001 | Loss: 0.23312000930309296\n",
      "Step 1101 | Loss: 0.1951875388622284\n",
      "Step 1201 | Loss: 0.24764516949653625\n",
      "Step 1301 | Loss: 0.2083926647901535\n",
      "Step 1401 | Loss: 0.2518792152404785\n",
      "Step 1501 | Loss: 0.22730086743831635\n",
      "Step 1601 | Loss: 0.2537025511264801\n",
      "Step 1701 | Loss: 0.1677275002002716\n",
      "Step 1801 | Loss: 0.30934739112854004\n",
      "Step 1901 | Loss: 0.24421100318431854\n",
      "Step 2001 | Loss: 0.23563949763774872\n",
      "Step 2101 | Loss: 0.2695270776748657\n",
      "Step 2201 | Loss: 0.19909822940826416\n",
      "Step 2301 | Loss: 0.2523114085197449\n",
      "Step 2401 | Loss: 0.27613404393196106\n",
      "0.24888626491188268 0.9725\n",
      "Step 1 | Loss: 1.163426399230957\n",
      "Step 101 | Loss: 0.627879798412323\n",
      "Step 201 | Loss: 0.24454186856746674\n",
      "Step 301 | Loss: 0.2510424554347992\n",
      "Step 401 | Loss: 0.26952865719795227\n",
      "Step 501 | Loss: 0.2899363934993744\n",
      "Step 601 | Loss: 0.2942151725292206\n",
      "Step 701 | Loss: 0.28030386567115784\n",
      "Step 801 | Loss: 0.2792273461818695\n",
      "Step 901 | Loss: 0.26299747824668884\n",
      "Step 1001 | Loss: 0.30504751205444336\n",
      "Step 1101 | Loss: 0.2230646163225174\n",
      "Step 1201 | Loss: 0.22828127443790436\n",
      "Step 1301 | Loss: 0.2665751874446869\n",
      "Step 1401 | Loss: 0.20422497391700745\n",
      "Step 1501 | Loss: 0.2858292758464813\n",
      "Step 1601 | Loss: 0.2370048612356186\n",
      "Step 1701 | Loss: 0.3082906901836395\n",
      "Step 1801 | Loss: 0.13554847240447998\n",
      "Step 1901 | Loss: 0.2246367633342743\n",
      "Step 2001 | Loss: 0.27656182646751404\n",
      "Step 2101 | Loss: 0.3311973512172699\n",
      "Step 2201 | Loss: 0.23149117827415466\n",
      "Step 2301 | Loss: 0.275473952293396\n",
      "Step 2401 | Loss: 0.2319074273109436\n",
      "0.2481460832951439 0.97\n",
      "Step 1 | Loss: 1.3143397569656372\n",
      "Step 101 | Loss: 0.9478815793991089\n",
      "Step 201 | Loss: 0.8311367034912109\n",
      "Step 301 | Loss: 0.8865822553634644\n",
      "Step 401 | Loss: 0.7938323020935059\n",
      "Step 501 | Loss: 0.8264500498771667\n",
      "Step 601 | Loss: 0.8456605672836304\n",
      "Step 701 | Loss: 0.7661628723144531\n",
      "Step 801 | Loss: 0.6545510292053223\n",
      "Step 901 | Loss: 0.8288869857788086\n",
      "Step 1001 | Loss: 0.5580955743789673\n",
      "Step 1101 | Loss: 0.7327109575271606\n",
      "Step 1201 | Loss: 0.7059750556945801\n",
      "Step 1301 | Loss: 0.7050055265426636\n",
      "Step 1401 | Loss: 0.9234716892242432\n",
      "Step 1501 | Loss: 0.707616925239563\n",
      "Step 1601 | Loss: 0.6886051297187805\n",
      "Step 1701 | Loss: 0.6903603672981262\n",
      "Step 1801 | Loss: 0.7583751678466797\n",
      "Step 1901 | Loss: 0.6841797828674316\n",
      "Step 2001 | Loss: 0.7296981811523438\n",
      "Step 2101 | Loss: 0.7179952263832092\n",
      "Step 2201 | Loss: 0.586215615272522\n",
      "Step 2301 | Loss: 0.7723062634468079\n",
      "Step 2401 | Loss: 0.6097657084465027\n",
      "0.6806450813502116 0.8125\n",
      "Step 1 | Loss: 1.527757167816162\n",
      "Step 101 | Loss: 0.6800490021705627\n",
      "Step 201 | Loss: 0.5144284963607788\n",
      "Step 301 | Loss: 0.3993756175041199\n",
      "Step 401 | Loss: 0.5162962675094604\n",
      "Step 501 | Loss: 0.28940337896347046\n",
      "Step 601 | Loss: 0.34731194376945496\n",
      "Step 701 | Loss: 0.26699385046958923\n",
      "Step 801 | Loss: 0.3693065941333771\n",
      "Step 901 | Loss: 0.22578753530979156\n",
      "Step 1001 | Loss: 0.3127121031284332\n",
      "Step 1101 | Loss: 0.21446265280246735\n",
      "Step 1201 | Loss: 0.5326616168022156\n",
      "Step 1301 | Loss: 0.20622724294662476\n",
      "Step 1401 | Loss: 0.3745773732662201\n",
      "Step 1501 | Loss: 0.32734519243240356\n",
      "Step 1601 | Loss: 0.2518279552459717\n",
      "Step 1701 | Loss: 0.3639841079711914\n",
      "Step 1801 | Loss: 0.4037841260433197\n",
      "Step 1901 | Loss: 0.22077898681163788\n",
      "Step 2001 | Loss: 0.293342262506485\n",
      "Step 2101 | Loss: 0.4308987259864807\n",
      "Step 2201 | Loss: 0.3200772702693939\n",
      "Step 2301 | Loss: 0.24548429250717163\n",
      "Step 2401 | Loss: 0.3092151880264282\n",
      "0.2556811203670689 0.97\n",
      "Step 1 | Loss: 0.38648274540901184\n",
      "Step 101 | Loss: 0.3773632347583771\n",
      "Step 201 | Loss: 0.37362661957740784\n",
      "Step 301 | Loss: 0.3949648141860962\n",
      "Step 401 | Loss: 0.5482402443885803\n",
      "Step 501 | Loss: 0.27830174565315247\n",
      "Step 601 | Loss: 0.3355143964290619\n",
      "Step 701 | Loss: 0.30419641733169556\n",
      "Step 801 | Loss: 0.3219861388206482\n",
      "Step 901 | Loss: 0.4319532513618469\n",
      "Step 1001 | Loss: 0.4331223964691162\n",
      "Step 1101 | Loss: 0.5393304228782654\n",
      "Step 1201 | Loss: 0.25645139813423157\n",
      "Step 1301 | Loss: 0.462952584028244\n",
      "Step 1401 | Loss: 0.3323115110397339\n",
      "Step 1501 | Loss: 0.33209705352783203\n",
      "Step 1601 | Loss: 0.2848256528377533\n",
      "Step 1701 | Loss: 0.46097809076309204\n",
      "Step 1801 | Loss: 0.2649901509284973\n",
      "Step 1901 | Loss: 0.38446474075317383\n",
      "Step 2001 | Loss: 0.5040860772132874\n",
      "Step 2101 | Loss: 0.3066710829734802\n",
      "Step 2201 | Loss: 0.29110583662986755\n",
      "Step 2301 | Loss: 0.4098657965660095\n",
      "Step 2401 | Loss: 0.22337542474269867\n",
      "0.28615217554689765 0.945\n",
      "Step 1 | Loss: 0.9596393704414368\n",
      "Step 101 | Loss: 0.6531662344932556\n",
      "Step 201 | Loss: 0.44434887170791626\n",
      "Step 301 | Loss: 0.34806400537490845\n",
      "Step 401 | Loss: 0.2695065140724182\n",
      "Step 501 | Loss: 0.4371371269226074\n",
      "Step 601 | Loss: 0.27513089776039124\n",
      "Step 701 | Loss: 0.26684048771858215\n",
      "Step 801 | Loss: 0.44239258766174316\n",
      "Step 901 | Loss: 0.2551269233226776\n",
      "Step 1001 | Loss: 0.20566216111183167\n",
      "Step 1101 | Loss: 0.20845367014408112\n",
      "Step 1201 | Loss: 0.3585508465766907\n",
      "Step 1301 | Loss: 0.2162267416715622\n",
      "Step 1401 | Loss: 0.22717352211475372\n",
      "Step 1501 | Loss: 0.3336353003978729\n",
      "Step 1601 | Loss: 0.2851655185222626\n",
      "Step 1701 | Loss: 0.28792718052864075\n",
      "Step 1801 | Loss: 0.31225085258483887\n",
      "Step 1901 | Loss: 0.22407734394073486\n",
      "Step 2001 | Loss: 0.23992082476615906\n",
      "Step 2101 | Loss: 0.2460901439189911\n",
      "Step 2201 | Loss: 0.22731521725654602\n",
      "Step 2301 | Loss: 0.2702833414077759\n",
      "Step 2401 | Loss: 0.22366873919963837\n",
      "0.2421278415492742 0.975\n",
      "Step 1 | Loss: 1.8536661863327026\n",
      "Step 101 | Loss: 0.588491678237915\n",
      "Step 201 | Loss: 0.37703657150268555\n",
      "Step 301 | Loss: 0.37814855575561523\n",
      "Step 401 | Loss: 0.46739786863327026\n",
      "Step 501 | Loss: 0.2788640558719635\n",
      "Step 601 | Loss: 0.35186833143234253\n",
      "Step 701 | Loss: 0.3688434958457947\n",
      "Step 801 | Loss: 0.29444563388824463\n",
      "Step 901 | Loss: 0.2490268051624298\n",
      "Step 1001 | Loss: 0.3344154357910156\n",
      "Step 1101 | Loss: 0.33261415362358093\n",
      "Step 1201 | Loss: 0.3808625042438507\n",
      "Step 1301 | Loss: 0.3419174253940582\n",
      "Step 1401 | Loss: 0.24667955935001373\n",
      "Step 1501 | Loss: 0.31763726472854614\n",
      "Step 1601 | Loss: 0.42023950815200806\n",
      "Step 1701 | Loss: 0.1919620931148529\n",
      "Step 1801 | Loss: 0.2694534659385681\n",
      "Step 1901 | Loss: 0.34795942902565\n",
      "Step 2001 | Loss: 0.33549898862838745\n",
      "Step 2101 | Loss: 0.29978495836257935\n",
      "Step 2201 | Loss: 0.19523051381111145\n"
     ]
    }
   ],
   "source": [
    "from datasets import TorchDataset\n",
    "from create_gate_circs_np import get_circ_params, TQCirc, generate_true_random_gate_circ\n",
    "from train_circ_np import train_tq_model, TQMseLoss\n",
    "\n",
    "import pickle as pkl\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "dataset = 'fmnist_2'\n",
    "curr_dir = f'./experiment_data/{dataset}/trained_circuits/'\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "\n",
    "num_train_steps = 2500\n",
    "num_test_data = 400\n",
    "\n",
    "num_meas_qubits = 1\n",
    "\n",
    "loss = TQMseLoss(num_meas_qubits, 4)\n",
    "\n",
    "dev = qml.device('lightning.qubit', wires=4)\n",
    "device = 'cpu'\n",
    "\n",
    "train_data = TorchDataset(dataset, 'angle', 1, reshape_labels=True)\n",
    "test_data = TorchDataset(dataset, 'angle', 1, False, reshape_labels=True)\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=32, sampler=torch.utils.data.RandomSampler(train_data))\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=32, sampler=torch.utils.data.SequentialSampler(test_data))\n",
    "\n",
    "for i in range(800, 1000):    \n",
    "    circ_dir = curr_dir + f'circ_{i + 1}'\n",
    "    \n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(circ_dir)\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    for j in range(5):\n",
    "        curr_train_dir = circ_dir + '/run_{}'.format(j + 1)\n",
    "        \n",
    "        if os.path.exists(curr_train_dir):\n",
    "            pass\n",
    "        else:\n",
    "            os.mkdir(curr_train_dir)\n",
    "    \n",
    "        model = TQCirc(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, False).to(device)\n",
    "        opt = torch.optim.SGD(model.parameters(), lr=0.05)\n",
    "    \n",
    "        curr_loss, curr_acc = train_tq_model(model, num_meas_qubits, opt, loss, train_data_loader, test_data_loader, num_test_data, num_train_steps, 100, 10)\n",
    "        \n",
    "        print(curr_loss, curr_acc)\n",
    "        \n",
    "        torch.save(model.state_dict(), curr_train_dir + '/model.pt')\n",
    "\n",
    "        losses_list.append(curr_loss)\n",
    "        accs_list.append(curr_acc)\n",
    "        \n",
    "    np.savetxt(circ_dir + '/val_losses.txt', losses_list)\n",
    "    np.savetxt(circ_dir + '/accs.txt', accs_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## noise metris for correlation circuits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'create_gate_circs' from '/root/create_gate_circs.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import create_gate_circs\n",
    "\n",
    "reload(create_gate_circs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-11-08 09:36:00,149: Credentials are already in use. The existing account in the session will be replaced.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8274917602539062 0.8373260498046875 0\n",
      "0.9281463623046875 0.923980712890625 1\n",
      "0.8760147094726562 0.8889083862304688 2\n",
      "0.49957275390625 0.6451644897460938 3\n",
      "0.57659912109375 0.6991958618164062 4\n",
      "0.7619247436523438 0.7932968139648438 5\n",
      "0.703277587890625 0.70416259765625 6\n",
      "0.8485794067382812 0.8330307006835938 7\n",
      "0.5754165649414062 0.6112213134765625 8\n",
      "0.430877685546875 0.21771240234375 9\n",
      "0.9415969848632812 0.9387130737304688 10\n",
      "0.6963424682617188 0.7333602905273438 11\n",
      "0.7458953857421875 0.7218856811523438 12\n",
      "0.6496124267578125 0.6650619506835938 13\n",
      "0.5886001586914062 0.6567916870117188 14\n",
      "0.7606353759765625 0.786041259765625 15\n",
      "0.8507156372070312 0.8256378173828125 16\n",
      "0.9256973266601562 0.7718124389648438 17\n",
      "0.5761260986328125 0.6028823852539062 18\n",
      "0.6667938232421875 0.7109451293945312 19\n",
      "0.7703323364257812 0.7227630615234375 20\n",
      "0.7348251342773438 0.7983856201171875 21\n",
      "0.7387771606445312 0.7050247192382812 22\n",
      "0.7100830078125 0.717620849609375 23\n",
      "0.41106414794921875 0.596649169921875 24\n",
      "0.8197250366210938 0.8366775512695312 25\n",
      "0.549468994140625 0.504486083984375 26\n",
      "0.8002700805664062 0.7692489624023438 27\n",
      "0.7346725463867188 0.7144699096679688 28\n",
      "0.9257278442382812 0.931060791015625 29\n",
      "0.941802978515625 0.9403457641601562 30\n",
      "0.745361328125 0.7448883056640625 31\n",
      "0.7383499145507812 0.6298599243164062 32\n",
      "0.869171142578125 0.856201171875 33\n",
      "0.65673828125 0.6871414184570312 34\n",
      "0.5410232543945312 0.5117263793945312 35\n",
      "0.8272781372070312 0.7990341186523438 36\n",
      "0.9242706298828125 0.9219894409179688 37\n",
      "0.40195465087890625 0.5260391235351562 38\n",
      "0.7233657836914062 0.808502197265625 39\n",
      "0.811187744140625 0.8122024536132812 40\n",
      "0.6018524169921875 0.686767578125 41\n",
      "0.6811294555664062 0.5745010375976562 42\n",
      "0.9344940185546875 0.9310836791992188 43\n"
     ]
    }
   ],
   "source": [
    "from create_noise_models import get_real_backend_dev, noisy_dev_from_backend\n",
    "from create_gate_circs import create_batched_gate_circ, get_circ_params\n",
    "from datasets_nt import load_dataset\n",
    "from metrics import compute_noise_metric\n",
    "\n",
    "dataset = 'fmnist_2'\n",
    "\n",
    "x_train, y_train, _, __ = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "curr_dir = './ours/{}'.format(dataset)\n",
    "\n",
    "num_qubits = 4\n",
    "num_trial_params = 128\n",
    "meas_qubits = [0, 1, 2, 3]\n",
    "\n",
    "param_nums = [16, 20, 24]\n",
    "\n",
    "num_cdcs = 128\n",
    "num_shots = 1024\n",
    "\n",
    "device_name = 'ibm_oslo'\n",
    "\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "noiseless_dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "\n",
    "score_tvds = []\n",
    "actual_tvds = []\n",
    "\n",
    "for p in param_nums:\n",
    "    param_dir = curr_dir + '/{}_params'.format(p)\n",
    "    \n",
    "    for i in range(2500):\n",
    "        circ_dir = param_dir + '/circ_{}'.format(i + 1)\n",
    "\n",
    "        circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(circ_dir)\n",
    "\n",
    "        noisy_circ = create_batched_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                    weights_bounds, meas_qubits, 'probs') \n",
    "\n",
    "        noiseless_circ = create_batched_gate_circ(noiseless_dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                    weights_bounds, meas_qubits, 'probs') \n",
    "\n",
    "        noise_metric_dir = circ_dir + '/noise_metric'\n",
    "\n",
    "        if not os.path.exists(noise_metric_dir):\n",
    "            os.mkdir(noise_metric_dir)\n",
    "\n",
    "        device_noise_metric_dir = noise_metric_dir + '/{}'.format(device_name)\n",
    "\n",
    "        if not os.path.exists(device_noise_metric_dir):\n",
    "            os.mkdir(device_noise_metric_dir)\n",
    "\n",
    "        params = np.random.sample((num_trial_params, weights_bounds[-1])) * 2 * np.pi\n",
    "        batch_data = x_train[np.random.choice(len(x_train), num_trial_params, False)]\n",
    "\n",
    "        noiseless_res_raw = np.array(noiseless_circ(batch_data, params, shots=num_shots))\n",
    "        noisy_res_raw = np.array(noisy_circ(batch_data, params, shots=num_shots))\n",
    "\n",
    "        actual_tvd = 1 - np.mean(np.sum(0.5 * np.abs(noiseless_res_raw - noisy_res_raw), 1))\n",
    "\n",
    "        tvd = compute_noise_metric(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, noiseless_dev, dev, num_cdcs=num_cdcs, num_shots=num_shots)\n",
    "\n",
    "        np.savetxt(device_noise_metric_dir + '/metric_tvd_score.txt', [1 - tvd])\n",
    "        np.savetxt(device_noise_metric_dir + '/actual_tvd_score.txt', [actual_tvd])\n",
    "\n",
    "        score_tvds.append(1 - tvd)\n",
    "        actual_tvds.append(actual_tvd)\n",
    "\n",
    "        print(1 - tvd, actual_tvd, i)\n",
    "        \n",
    "    np.savetxt(param_dir + '/act_tvds.txt', actual_tvds)\n",
    "    np.savetxt(param_dir + '/metric_tvds.txt', score_tvds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-09-14 04:19:31,785: Credentials are already in use. The existing account in the session will be replaced.\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (92038.51932346408 > 2 * 2551.431037973455). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n",
      "/usr/local/lib/python3.7/site-packages/qiskit/providers/aer/noise/device/models.py:266: UserWarning: Device model returned an invalid T_2 relaxation time greater than the theoretical maximum value 2 * T_1 (9.203851932346408e-05 > 2 * 2.5514310379734552e-06). Truncating to maximum value.\n",
      "  \" Truncating to maximum value.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.199634404683853 1.21474975512387 301\n",
      "1.0374430730151272 1.239637414514575 302\n",
      "1.066567561734048 1.1227739556105196 303\n",
      "1.3398419045390846 1.3422871892537744 304\n",
      "1.7071067811865477 1.6404855063268897 305\n",
      "0.890458522069639 0.9055996595060093 306\n",
      "1.2506872330476575 1.2626274584599355 307\n",
      "1.1278161449880393 1.1617028852908189 308\n",
      "1.0739668298766045 1.0855949210758071 309\n"
     ]
    }
   ],
   "source": [
    "from create_noise_models import noisy_dev_from_backend\n",
    "from datasets import load_dataset\n",
    "from create_gate_circs import create_gate_circ, get_circ_params\n",
    "from train_circ import train_qnn, mse_vec_loss\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "dataset = 'fmnist_4'\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "num_qubits = 4\n",
    "num_embeds = 16\n",
    "num_params = 18\n",
    "\n",
    "device_name = 'ibmq_belem'\n",
    "\n",
    "# dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "\n",
    "for i in range(300, 400):\n",
    "    curr_dir = './experiment_data/fmnist_4/trained_circuits/circ_{}'.format(i + 1)\n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(curr_dir) \n",
    "\n",
    "    circ = create_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                    weights_bounds, [0, 1], 'exp')\n",
    "    \n",
    "    \n",
    "    noiseless_losses = np.genfromtxt(curr_dir + '/val_losses.txt')\n",
    "\n",
    "    losses_list = []\n",
    "    accs_list = []\n",
    "    \n",
    "    curr_dev_dir = curr_dir + '/' + device_name\n",
    "\n",
    "    if not os.path.exists(curr_dev_dir + '/accs_inference_only.txt'):   \n",
    "        if not os.path.exists(curr_dev_dir):\n",
    "            os.mkdir(curr_dev_dir)\n",
    "\n",
    "        for j in range(5):\n",
    "            curr_train_dir = curr_dir + '/run_{}'.format(j + 1)\n",
    "            curr_params = np.genfromtxt(curr_train_dir + '/params_{}.txt'.format(j + 1))[-1]\n",
    "\n",
    "            val_exps = [circ(x_test[i], curr_params) for i in range(len(x_test))]\n",
    "            val_loss = np.array([mse_vec_loss(y_test[k], val_exps[k]) for k in range(len(x_test))]).flatten()\n",
    "\n",
    "            acc = np.mean(np.sum(np.multiply(val_exps, y_test) > 0, 1) == 2)\n",
    "    #         acc = np.mean(val_loss < 1)\n",
    "\n",
    "            losses_list.append(val_loss)\n",
    "            accs_list.append(acc)\n",
    "\n",
    "        print(np.mean(noiseless_losses), np.mean(losses_list), i + 1)\n",
    "\n",
    "        np.save(curr_dev_dir + '/val_losses_inference_only.npy', losses_list)\n",
    "        np.savetxt(curr_dev_dir + '/accs_inference_only.txt', accs_list)\n",
    "    else:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ibmqfactory.load_account:WARNING:2022-11-05 17:24:05,338: Credentials are already in use. The existing account in the session will be replaced.\n",
      "WARNING:qiskit_aer.noise.device.models:Device reported a gate error parameter greater than maximum allowed value (1.000000 > 0.800000). Truncating to maximum value.\n",
      "WARNING:qiskit_aer.noise.device.models:Device model returned a depolarizing error parameter greater than maximum allowed value (1.067366 > 1.066667). Truncating to maximum value.\n",
      "WARNING:qiskit_aer.noise.device.models:Device reported a gate error parameter greater than maximum allowed value (1.000000 > 0.800000). Truncating to maximum value.\n",
      "WARNING:qiskit_aer.noise.device.models:Device model returned a depolarizing error parameter greater than maximum allowed value (1.067557 > 1.066667). Truncating to maximum value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39470672607421875 0.5943145751953125 0\n",
      "0.945465087890625 0.943634033203125 1\n",
      "0.5880661010742188 0.6153717041015625 2\n",
      "0.6478347778320312 0.6675186157226562 3\n",
      "0.6592636108398438 0.6649246215820312 4\n",
      "0.4537353515625 0.64727783203125 5\n",
      "0.39359283447265625 0.5631256103515625 6\n",
      "0.6658706665039062 0.67681884765625 7\n",
      "0.5809097290039062 0.6390304565429688 8\n",
      "0.40633392333984375 0.6287765502929688 9\n",
      "0.6321182250976562 0.6239395141601562 10\n"
     ]
    }
   ],
   "source": [
    "from create_noise_models import get_real_backend_dev, noisy_dev_from_backend\n",
    "from create_gate_circs import create_batched_gate_circ, get_circ_params\n",
    "from datasets_nt import load_dataset\n",
    "from metrics import compute_noise_metric\n",
    "\n",
    "dataset = 'vowel_4'\n",
    "\n",
    "x_train, y_train, _, __ = load_dataset(dataset, 'angle', 1)\n",
    "\n",
    "curr_dir = './experiment_data/{}/trained_circuits'.format(dataset)\n",
    "\n",
    "num_qubits = 4\n",
    "num_trial_params = 128\n",
    "meas_qubits = [0, 1, 2, 3]\n",
    "\n",
    "num_cdcs = 128\n",
    "num_shots = 1024\n",
    "\n",
    "device_name = 'ibm_oslo'\n",
    "\n",
    "dev = noisy_dev_from_backend(device_name, num_qubits)\n",
    "noiseless_dev = qml.device('lightning.qubit', wires=num_qubits)\n",
    "\n",
    "score_tvds = []\n",
    "actual_tvds = []\n",
    "\n",
    "for i in range(1000):\n",
    "    circ_dir = curr_dir + '/circ_{}'.format(i + 1)\n",
    "    \n",
    "    circ_gates, gate_params, inputs_bounds, weights_bounds = get_circ_params(circ_dir)\n",
    "\n",
    "    noisy_circ = create_batched_gate_circ(dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'probs') \n",
    "\n",
    "    noiseless_circ = create_batched_gate_circ(noiseless_dev, circ_gates, gate_params, inputs_bounds,\n",
    "                                                                weights_bounds, meas_qubits, 'probs') \n",
    "    \n",
    "    noise_metric_dir = circ_dir + '/noise_metric'\n",
    "    \n",
    "    if not os.path.exists(noise_metric_dir):\n",
    "        os.mkdir(noise_metric_dir)\n",
    "    \n",
    "    device_noise_metric_dir = noise_metric_dir + '/{}'.format(device_name)\n",
    "\n",
    "    if not os.path.exists(device_noise_metric_dir):\n",
    "        os.mkdir(device_noise_metric_dir)\n",
    "    \n",
    "    params = np.random.sample((num_trial_params, weights_bounds[-1])) * 2 * np.pi\n",
    "    batch_data = x_train[np.random.choice(len(x_train), num_trial_params, False)]\n",
    "    \n",
    "    noiseless_res_raw = np.array(noiseless_circ(batch_data, params, shots=num_shots))\n",
    "    noisy_res_raw = np.array(noisy_circ(batch_data, params, shots=num_shots))\n",
    "\n",
    "    actual_tvd = 1 - np.mean(np.sum(0.5 * np.abs(noiseless_res_raw - noisy_res_raw), 1))\n",
    "    \n",
    "    tvd = compute_noise_metric(circ_gates, gate_params, inputs_bounds, weights_bounds, num_qubits, noiseless_dev, dev, num_cdcs=num_cdcs, num_shots=num_shots)\n",
    "    \n",
    "    np.savetxt(device_noise_metric_dir + '/metric_tvd_score.txt', [1 - tvd])\n",
    "    np.savetxt(device_noise_metric_dir + '/actual_tvd_score.txt', [actual_tvd])\n",
    "    \n",
    "    score_tvds.append(1 - tvd)\n",
    "    actual_tvds.append(actual_tvd)\n",
    "    \n",
    "    print(1 - tvd, actual_tvd, i)\n",
    "\n",
    "# np.savetxt(curr_dir + '/noise_scores.txt', score_tvds)\n",
    "# np.savetxt(curr_dir + '/noisy_fids.txt', actual_tvds)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.3 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.3-cpu-py37-ubuntu18.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
